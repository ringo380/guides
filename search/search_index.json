{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"The Runbook","text":"<p>Practical, open-access technical guides covering Linux, DNS, Git, databases, and development. Created for practitioners - be aware some content may be outdated and should be verified.</p>"},{"location":"#topics","title":"Topics","text":"<ul> <li>Linux Essentials - 12 guides from shell basics to scripting best practices</li> <li>DNS Administration - 8 guides from DNS fundamentals to architecture and operations</li> <li>Git - 17 guides from version control basics to internals and scaling</li> <li>Databases - 17 guides from relational fundamentals through NoSQL and operations</li> <li>Perl - Developer introduction and learning roadmap</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is dual-licensed:</p> <ul> <li>Written content (guides, documentation) - CC BY-NC-ND 4.0 - Copyright (c) 2025-2026 Ryan Thomas Robson / Robworks Software LLC</li> <li>Code (JavaScript, Python, CSS, Shell, HTML templates) - MIT - Copyright (c) 2025-2026 Robworks Software LLC</li> </ul> <p>See LICENSE for details.</p>"},{"location":"DNS%20Administration/","title":"DNS Administration","text":"<p>A comprehensive guide to DNS - from how name resolution actually works to running authoritative servers, signing zones with DNSSEC, and designing resilient architectures. These guides take you from \"I can query DNS\" to understanding the system deeply enough to build and operate it.</p> <p>Each topic is covered in its own guide. Start anywhere - they're self-contained, but the order below follows a natural learning path.</p>"},{"location":"DNS%20Administration/#guides","title":"Guides","text":""},{"location":"DNS%20Administration/#dns-fundamentals","title":"DNS Fundamentals","text":"<p>What DNS actually is, how the hierarchy works from root servers to your browser, and how resolution happens step by step. Covers the history behind DNS, the difference between authoritative and recursive servers, the root zone, caching, TTLs, and glue records.</p>"},{"location":"DNS%20Administration/#zone-files-and-records","title":"Zone Files and Records","text":"<p>The anatomy of a zone file and every record type you'll encounter in practice. Covers SOA fields, A/AAAA, CNAME (and why you can't use it at the apex), MX, NS, TXT (SPF/DKIM/DMARC), PTR, SRV, and CAA - with complete annotated zone file examples for both forward and reverse zones.</p>"},{"location":"DNS%20Administration/#dns-tools-and-troubleshooting","title":"DNS Tools and Troubleshooting","text":"<p>The essential toolkit for querying, diagnosing, and debugging DNS. Covers <code>dig</code> in depth (including reading full output, <code>+trace</code>, <code>+dnssec</code>), <code>drill</code>, <code>delv</code>, <code>host</code>, <code>nslookup</code>, and <code>whois</code> - plus systematic troubleshooting playbooks for common failures.</p>"},{"location":"DNS%20Administration/#bind","title":"BIND","text":"<p>The reference DNS implementation. Covers installation, <code>named.conf</code> structure, configuring caching resolvers and authoritative servers, primary/secondary with TSIG authentication, split-horizon views, security hardening, and <code>rndc</code> operations.</p>"},{"location":"DNS%20Administration/#nsd-and-unbound","title":"NSD and Unbound","text":"<p>NLnet Labs' approach to DNS - separate authoritative (NSD) and recursive (Unbound) into purpose-built software. Covers both servers from installation through primary/secondary configuration, running them together, and when to choose them over BIND.</p>"},{"location":"DNS%20Administration/#powerdns","title":"PowerDNS","text":"<p>Database-backed DNS with a built-in HTTP API. Covers the authoritative server with MySQL and SQLite backends, zone management via <code>pdnsutil</code> and the REST API, the recursor, primary/secondary replication, and DNSSEC signing.</p>"},{"location":"DNS%20Administration/#dnssec","title":"DNSSEC","text":"<p>How DNS authentication works, from the Kaminsky attack that made it urgent to the ICANN root key ceremonies that anchor it. Covers the trust chain, DNSKEY/RRSIG/DS/NSEC records, key management, signing with BIND/NSD/PowerDNS, validation, and debugging failures.</p>"},{"location":"DNS%20Administration/#dns-architecture-and-operations","title":"DNS Architecture and Operations","text":"<p>Designing DNS infrastructure for the real world. Covers primary/secondary topologies, zone transfers (AXFR/IXFR), hidden primaries, split-horizon, anycast, DNS for email (SPF/DKIM/DMARC deep-dive), monitoring, and migration patterns.</p>"},{"location":"DNS%20Administration/bind/","title":"BIND","text":"<p>This guide covers BIND - the reference implementation of DNS and the most widely deployed DNS server software in the world. You'll learn to configure it as a caching recursive resolver, an authoritative server, and in primary/secondary configurations with TSIG authentication.</p>"},{"location":"DNS%20Administration/bind/#what-bind-is","title":"What BIND Is","text":"<p>BIND (Berkeley Internet Name Domain) was written in 1984 by four graduate students at UC Berkeley as part of a DARPA grant. It's maintained by the Internet Systems Consortium (ISC) and remains the reference implementation against which other DNS software is measured.</p> <p>BIND 9 (the current major version, in development since 2000) is a complete rewrite of earlier versions. It handles both authoritative and recursive DNS, supports DNSSEC, TSIG, views (split-horizon), Response Policy Zones (RPZ), and virtually every DNS feature that exists.</p> <p>When to choose BIND:</p> <ul> <li>You need a single server that does both authoritative and recursive DNS</li> <li>You need advanced features like views, RPZ, or DNSSEC with automated key rollover</li> <li>You want the widest community support and documentation</li> <li>You need to match the behavior of the DNS reference implementation</li> </ul> <p>When to consider alternatives:</p> <ul> <li>You need maximum authoritative performance (NSD is faster for pure authoritative serving)</li> <li>You want a database backend (PowerDNS supports MySQL, PostgreSQL, SQLite)</li> <li>You want separate authority and recursion with minimal attack surface (NSD + Unbound)</li> </ul>"},{"location":"DNS%20Administration/bind/#architecture","title":"Architecture","text":"<p>BIND 9 underwent a significant internal change in version 9.20: the data structure for the in-memory zone database was replaced from a Red-Black Tree to a QP-trie (Quadbit Patricia trie). The QP-trie provides faster lookups and better memory efficiency for large zones. This is transparent to configuration - no settings changed - but it explains why BIND 9.20+ handles large zones measurably better than earlier versions.</p>"},{"location":"DNS%20Administration/bind/#installation-and-file-layout","title":"Installation and File Layout","text":""},{"location":"DNS%20Administration/bind/#installation","title":"Installation","text":"<p>RHEL / AlmaLinux / Rocky:</p> <pre><code>sudo dnf install bind bind-utils\nsudo systemctl enable --now named\n</code></pre> <p>Debian / Ubuntu:</p> <pre><code>sudo apt install bind9 bind9-utils\nsudo systemctl enable --now named\n</code></pre>"},{"location":"DNS%20Administration/bind/#key-files","title":"Key Files","text":"Path Purpose <code>/etc/named.conf</code> (RHEL) or <code>/etc/bind/named.conf</code> (Debian) Main configuration <code>/var/named/</code> (RHEL) or <code>/var/cache/bind/</code> (Debian) Zone files directory <code>/etc/named/</code> or <code>/etc/bind/</code> Additional config files <code>/var/log/named/</code> Logs (if configured) <code>/etc/rndc.key</code> or <code>/etc/bind/rndc.key</code> <code>rndc</code> authentication key"},{"location":"DNS%20Administration/bind/#namedconf-structure","title":"named.conf Structure","text":"<p><code>named.conf</code> is organized into blocks:</p> <pre><code>options { ... };       // Global server settings\nlogging { ... };       // Log channels and categories\nacl name { ... };      // Named access control lists\nzone \"name\" { ... };   // Zone definitions\nview \"name\" { ... };   // Split-horizon views (contain zone blocks)\nkey \"name\" { ... };    // TSIG keys for authentication\n</code></pre> <p>You can split the configuration across multiple files using <code>include</code>:</p> <pre><code>include \"/etc/named/zones.conf\";\ninclude \"/etc/named/acls.conf\";\n</code></pre> <p>In BIND's named.conf, what is the difference between an 'options' block and a 'zone' block? (requires JavaScript)</p> <p>Setting Up named.conf from Scratch (requires JavaScript)</p>"},{"location":"DNS%20Administration/bind/#configuring-a-caching-recursive-resolver","title":"Configuring a Caching Recursive Resolver","text":"<p>A caching resolver accepts queries from your network, resolves them iteratively, and caches the results. This is the simplest useful BIND configuration.</p> <pre><code>// /etc/named.conf - Caching recursive resolver\n\nacl \"trusted\" {\n    192.168.1.0/24;       // office network\n    10.0.0.0/8;           // internal network\n    localhost;\n    localnets;\n};\n\noptions {\n    directory \"/var/named\";\n    pid-file \"/run/named/named.pid\";\n\n    // Listen on localhost and the internal interface\n    listen-on    { 127.0.0.1; 192.168.1.1; };\n    listen-on-v6 { ::1; };\n\n    // Only allow recursion for trusted networks\n    allow-recursion { trusted; };\n    allow-query     { trusted; };\n\n    // Forward unresolved queries to upstream resolvers (optional)\n    // forwarders { 8.8.8.8; 1.1.1.1; };\n    // forward only;  // \"only\" means don't try iterative if forwarders fail\n\n    // DNSSEC validation\n    dnssec-validation auto;\n\n    // Hide the BIND version from queries\n    version \"not disclosed\";\n\n    // Rate limiting to mitigate amplification attacks\n    rate-limit {\n        responses-per-second 10;\n    };\n};\n\nlogging {\n    channel default_log {\n        file \"/var/log/named/default.log\" versions 3 size 5m;\n        severity info;\n        print-time yes;\n        print-severity yes;\n    };\n    channel query_log {\n        file \"/var/log/named/queries.log\" versions 5 size 10m;\n        severity info;\n        print-time yes;\n    };\n\n    category default  { default_log; };\n    category queries  { query_log; };\n};\n</code></pre> <p>Key decisions in this configuration:</p> <p><code>allow-recursion</code> restricts who can use this server as a resolver. Without this, you're running an open resolver - a vector for DNS amplification attacks. Only allow your own networks.</p> <p><code>dnssec-validation auto</code> enables DNSSEC validation using the built-in root trust anchor. BIND ships with the root zone's KSK and updates it automatically via RFC 5011 trust anchor management.</p> <p><code>forwarders</code> (commented out) - uncomment if you want BIND to ask upstream resolvers instead of doing full iterative resolution. <code>forward only</code> means if the forwarders don't respond, BIND gives up. Without <code>forward only</code>, BIND falls back to iterative resolution.</p> <p>After editing, always validate and reload:</p> <pre><code>named-checkconf                      # validate configuration syntax\nsudo systemctl reload named          # apply changes without restart\nsudo rndc status                     # verify the server is running\n</code></pre> <p>Caching Resolver named.conf (requires JavaScript)</p>"},{"location":"DNS%20Administration/bind/#configuring-an-authoritative-server","title":"Configuring an Authoritative Server","text":"<p>An authoritative server hosts zone data and answers queries from that data. It does not perform recursion.</p>"},{"location":"DNS%20Administration/bind/#zone-definition","title":"Zone Definition","text":"<p>Add a zone to <code>named.conf</code>:</p> <pre><code>zone \"example.com\" {\n    type primary;\n    file \"zones/example.com.zone\";\n    allow-transfer { none; };           // restrict zone transfers\n    notify yes;                         // notify secondaries on changes\n};\n</code></pre>"},{"location":"DNS%20Administration/bind/#zone-file","title":"Zone File","text":"<p>Create the zone file at <code>/var/named/zones/example.com.zone</code> (see the Zone Files and Records guide for full syntax):</p> <pre><code>$TTL 3600\n$ORIGIN example.com.\n\n@   IN  SOA ns1.example.com. admin.example.com. (\n            2025011501  ; serial\n            3600        ; refresh\n            900         ; retry\n            1209600     ; expire\n            300         ; minimum\n)\n\n@           IN  NS      ns1.example.com.\n@           IN  NS      ns2.example.com.\nns1         IN  A       198.51.100.1\nns2         IN  A       198.51.100.2\n\n@           IN  A       198.51.100.10\nwww         IN  CNAME   example.com.\nmail        IN  A       198.51.100.20\n@           IN  MX      10 mail.example.com.\n@           IN  TXT     \"v=spf1 ip4:198.51.100.0/24 -all\"\n</code></pre>"},{"location":"DNS%20Administration/bind/#validation-and-reload","title":"Validation and Reload","text":"<p>Always validate before reloading:</p> <pre><code># Check configuration syntax\nnamed-checkconf\n\n# Check zone file syntax and data consistency\nnamed-checkzone example.com /var/named/zones/example.com.zone\n</code></pre> <p><code>named-checkzone</code> catches trailing-dot errors, duplicate records, CNAME conflicts, and other common mistakes:</p> <pre><code>zone example.com/IN: loaded serial 2025011501\nOK\n</code></pre> <p>If there's an error:</p> <pre><code>zone example.com/IN: mail.example.com/MX 'mail.example.com.example.com' is a CNAME (illegal)\nzone example.com/IN: not loaded due to errors.\n</code></pre> <p>Reload the zone without restarting BIND:</p> <pre><code>sudo rndc reload example.com          # reload a specific zone\nsudo rndc reload                      # reload all zones\n</code></pre> <p>Configure a Forward Zone in BIND (requires JavaScript)</p>"},{"location":"DNS%20Administration/bind/#primarysecondary-configuration","title":"Primary/Secondary Configuration","text":"<p>A primary server holds the master copy of zone data. A secondary server receives zone data from the primary via zone transfer and serves it identically. Secondaries provide redundancy and distribute query load.</p>"},{"location":"DNS%20Administration/bind/#primary-configuration","title":"Primary Configuration","text":"<p>On the primary server, allow zone transfers to the secondary's IP:</p> <pre><code>zone \"example.com\" {\n    type primary;\n    file \"zones/example.com.zone\";\n    allow-transfer { 203.0.113.2; };    // secondary server IP\n    also-notify    { 203.0.113.2; };    // actively notify on changes\n};\n</code></pre>"},{"location":"DNS%20Administration/bind/#secondary-configuration","title":"Secondary Configuration","text":"<p>On the secondary server:</p> <pre><code>zone \"example.com\" {\n    type secondary;\n    file \"zones/example.com.zone\";\n    primaries { 198.51.100.1; };        // primary server IP\n    allow-transfer { none; };\n};\n</code></pre> <p>When the secondary starts (or when it receives a NOTIFY from the primary), it checks the primary's SOA serial. If the serial is higher, it initiates a zone transfer.</p>"},{"location":"DNS%20Administration/bind/#testing-zone-transfers","title":"Testing Zone Transfers","text":"<p>You can test zone transfers manually with <code>dig</code>:</p> <pre><code>dig @198.51.100.1 example.com AXFR\n</code></pre> <pre><code>example.com.        3600    IN  SOA ns1.example.com. admin.example.com. 2025011501 ...\nexample.com.        3600    IN  NS  ns1.example.com.\nexample.com.        3600    IN  NS  ns2.example.com.\nexample.com.        3600    IN  A   198.51.100.10\n;; [... all records in the zone ...]\nexample.com.        3600    IN  SOA ns1.example.com. admin.example.com. 2025011501 ...\n</code></pre> <p>AXFR transfers the entire zone. The SOA record appears at both the beginning and end - that's the protocol's way of marking the transfer boundaries.</p>"},{"location":"DNS%20Administration/bind/#tsig-authentication","title":"TSIG Authentication","text":"<p>Plain IP-based access control is vulnerable to spoofing. TSIG (Transaction Signature) adds cryptographic authentication to zone transfers and <code>rndc</code> commands.</p> <p>Generate a shared key:</p> <pre><code>tsig-keygen example-transfer-key\n</code></pre> <pre><code>key \"example-transfer-key\" {\n    algorithm hmac-sha256;\n    secret \"jF3K8vQ2+xN7wP5dR9mB0kT4yH1cA6uZ...\";\n};\n</code></pre> <p>Add this key block to <code>named.conf</code> on both servers.</p> <p>Primary - require TSIG for transfers:</p> <pre><code>zone \"example.com\" {\n    type primary;\n    file \"zones/example.com.zone\";\n    allow-transfer { key \"example-transfer-key\"; };\n    also-notify    { 203.0.113.2; };\n};\n</code></pre> <p>Secondary - authenticate with TSIG:</p> <pre><code>server 198.51.100.1 {\n    keys { \"example-transfer-key\"; };\n};\n\nzone \"example.com\" {\n    type secondary;\n    file \"zones/example.com.zone\";\n    primaries { 198.51.100.1; };\n};\n</code></pre> <p>Test the authenticated transfer:</p> <pre><code>dig @198.51.100.1 example.com AXFR -y hmac-sha256:example-transfer-key:jF3K8vQ2+xN7wP...\n</code></pre> <p>What is the purpose of TSIG in DNS? (requires JavaScript)</p>"},{"location":"DNS%20Administration/bind/#views-split-horizon-dns","title":"Views (Split-Horizon DNS)","text":"<p>Views let a single BIND server return different answers depending on who's asking. The most common use case is returning internal IP addresses for internal clients and public IP addresses for everyone else.</p> <pre><code>flowchart TD\n    Q[\"DNS Query arrives&lt;br/&gt;www.example.com?\"]\n    BIND[\"BIND Server\"]\n    ACL{\"match-clients ACL&lt;br/&gt;check source IP\"}\n\n    Q --&gt; BIND --&gt; ACL\n\n    ACL --&gt;|\"192.168.x / 10.x / 172.16.x\"| INT\n    ACL --&gt;|\"any other IP\"| EXT\n\n    subgraph INT [\"Internal View\"]\n        IR[\"www.example.com&lt;br/&gt;\u2192 192.168.1.100\"]\n        IF[\"Recursion: yes\"]\n    end\n\n    subgraph EXT [\"External View\"]\n        ER[\"www.example.com&lt;br/&gt;\u2192 203.0.113.50\"]\n        EF[\"Recursion: no\"]\n    end</code></pre> <pre><code>acl \"internal\" {\n    192.168.0.0/16;\n    10.0.0.0/8;\n    172.16.0.0/12;\n};\n\nview \"internal\" {\n    match-clients { internal; };\n    recursion yes;\n\n    zone \"example.com\" {\n        type primary;\n        file \"zones/example.com.internal.zone\";\n    };\n};\n\nview \"external\" {\n    match-clients { any; };\n    recursion no;\n\n    zone \"example.com\" {\n        type primary;\n        file \"zones/example.com.external.zone\";\n    };\n};\n</code></pre> <p>The internal zone file might have:</p> <pre><code>www     IN  A   192.168.1.100       ; internal web server\n</code></pre> <p>While the external zone file has:</p> <pre><code>www     IN  A   203.0.113.50        ; public IP\n</code></pre> <p>Important: when using views, every zone must be inside a view - including the root hints and localhost zones. You can't mix view and non-view zone statements. The views are evaluated in order, and a client matches the first view whose <code>match-clients</code> ACL includes them.</p>"},{"location":"DNS%20Administration/bind/#security-and-hardening","title":"Security and Hardening","text":""},{"location":"DNS%20Administration/bind/#restrict-recursion","title":"Restrict Recursion","text":"<p>If your server is authoritative-only, disable recursion entirely:</p> <pre><code>options {\n    recursion no;\n    allow-query { any; };       // authoritative servers should answer everyone\n};\n</code></pre> <p>If it serves both roles, restrict recursion to trusted networks (shown in the caching resolver config above).</p>"},{"location":"DNS%20Administration/bind/#rate-limiting","title":"Rate Limiting","text":"<p>Mitigate DNS amplification attacks with response rate limiting:</p> <pre><code>options {\n    rate-limit {\n        responses-per-second 10;\n        window 5;\n    };\n};\n</code></pre> <p>This drops responses when a single source receives more than 10 identical responses per second. Legitimate clients rarely trigger this, but it significantly limits amplification attacks.</p>"},{"location":"DNS%20Administration/bind/#version-hiding","title":"Version Hiding","text":"<p>By default, BIND responds to <code>dig version.bind chaos txt</code> with its version number. Attackers use this to identify vulnerable versions. Hide it:</p> <pre><code>options {\n    version \"not disclosed\";\n};\n</code></pre>"},{"location":"DNS%20Administration/bind/#response-policy-zones-rpz","title":"Response Policy Zones (RPZ)","text":"<p>RPZ lets you override DNS responses based on policy - essentially a DNS-level firewall. You can block known malicious domains, redirect them, or return NXDOMAIN:</p> <pre><code>options {\n    response-policy {\n        zone \"rpz.example.com\";\n    };\n};\n\nzone \"rpz.example.com\" {\n    type primary;\n    file \"zones/rpz.example.com.zone\";\n};\n</code></pre> <p>RPZ zone file:</p> <pre><code>$TTL 300\n@   IN  SOA localhost. admin.example.com. ( 1 3600 900 604800 300 )\n@   IN  NS  localhost.\n\n; Block a known malicious domain\nmalware.bad-domain.com   IN  CNAME  .        ; return NXDOMAIN\n; Redirect another domain\nphishing-site.com        IN  A      127.0.0.1 ; redirect to localhost\n</code></pre> <p>Several threat intelligence providers publish RPZ feeds that you can use as secondary zones for automatic protection.</p> <p>What is a Response Policy Zone (RPZ) used for? (requires JavaScript)</p>"},{"location":"DNS%20Administration/bind/#operational-commands-reference","title":"Operational Commands Reference","text":"<p><code>rndc</code> (Remote Name Daemon Control) manages a running BIND server:</p> Command Action <code>rndc status</code> Show server status <code>rndc reload</code> Reload all zones <code>rndc reload example.com</code> Reload a specific zone <code>rndc reconfig</code> Re-read <code>named.conf</code> for new/removed zones (without reloading existing zones) <code>rndc flush</code> Clear the entire cache <code>rndc flush example.com</code> Clear cache for one domain <code>rndc dumpdb -cache</code> Dump the cache to a file for inspection <code>rndc querylog on</code> Enable query logging <code>rndc querylog off</code> Disable query logging <code>rndc retransfer example.com</code> Force a secondary to re-transfer a zone <code>rndc notify example.com</code> Send NOTIFY to secondaries for a zone <code>rndc signing -list example.com</code> List DNSSEC signing operations in progress <code>rndc zonestatus example.com</code> Show zone status details <p><code>rndc</code> authenticates to <code>named</code> using a shared key. If <code>rndc</code> commands fail with \"connection refused,\" check that <code>rndc.key</code> exists and matches between <code>rndc.conf</code> and <code>named.conf</code>.</p> <p>Managing BIND with rndc (requires JavaScript)</p>"},{"location":"DNS%20Administration/bind/#further-reading","title":"Further Reading","text":"<ul> <li>BIND 9 Administrator Reference Manual - ISC's official comprehensive documentation</li> <li>ISC BIND - downloads and release notes</li> <li>RFC 5936 - DNS Zone Transfer Protocol (AXFR)</li> <li>RFC 1996 - DNS NOTIFY mechanism</li> <li>RFC 8945 - Secret Key Transaction Authentication (TSIG)</li> </ul> <p>Previous: DNS Tools and Troubleshooting | Next: NSD and Unbound | Back to Index</p>"},{"location":"DNS%20Administration/dns-architecture/","title":"DNS Architecture and Operations","text":"<p>This guide covers the operational side of DNS - how to design resilient architectures, transfer zones between servers, get email DNS right, monitor your infrastructure, and migrate records without downtime.</p>"},{"location":"DNS%20Administration/dns-architecture/#primarysecondary-topologies","title":"Primary/Secondary Topologies","text":"<p>Every production DNS deployment uses at least two authoritative servers. The question is how to organize them.</p>"},{"location":"DNS%20Administration/dns-architecture/#standard-primarysecondary","title":"Standard Primary/Secondary","text":"<p>The simplest topology: one primary server where you edit zone data, and one or more secondaries that receive copies via zone transfer.</p> <pre><code>         Zone edits\n             |\n             v\n     +-------+-------+\n     |    Primary     |   (ns1.example.com)\n     |  198.51.100.1  |\n     +-------+-------+\n             |\n        NOTIFY + AXFR/IXFR\n             |\n     +-------+-------+\n     |   Secondary    |   (ns2.example.com)\n     |  203.0.113.2   |\n     +-------+-------+\n</code></pre> <p>Both servers are listed in the NS records and serve queries equally. The primary is authoritative for edits; the secondaries are authoritative for serving.</p>"},{"location":"DNS%20Administration/dns-architecture/#hidden-primary","title":"Hidden Primary","text":"<p>In a hidden primary topology, the primary server is not listed in NS records and doesn't serve public queries. Only the secondaries are public-facing.</p> <pre><code>     +-------+-------+\n     |  Hidden Primary |   (NOT in NS records)\n     |  10.0.0.1       |   (internal IP only)\n     +-------+-------+\n         |           |\n    AXFR/IXFR   AXFR/IXFR\n         |           |\n   +-----+-----+ +--+--------+\n   | Secondary  | | Secondary  |   (ns1.example.com, ns2.example.com)\n   | 198.51.100.1| | 203.0.113.2|   (public-facing)\n   +-----------+ +------------+\n</code></pre> <p>Why use a hidden primary:</p> <ul> <li>Security - the server where you edit zones is not exposed to the internet. Attackers can't target it with DDoS or exploit attempts.</li> <li>Flexibility - you can use any software or configuration on the primary (including database-backed systems like PowerDNS) and serve via lightweight secondaries (NSD).</li> <li>Maintenance - you can take the primary offline for maintenance without affecting DNS resolution.</li> </ul> <p>The primary still needs to be reachable by the secondaries for zone transfers. Put it on a private network or restrict access by IP.</p> <p>What is a hidden primary DNS server? (requires JavaScript)</p>"},{"location":"DNS%20Administration/dns-architecture/#zone-transfers-in-depth","title":"Zone Transfers In Depth","text":""},{"location":"DNS%20Administration/dns-architecture/#axfr-full-transfer","title":"AXFR (Full Transfer)","text":"<p>AXFR transfers the entire zone from primary to secondary over TCP. The secondary requests a transfer, and the primary sends every record in the zone.</p> <pre><code># Test an AXFR\ndig @198.51.100.1 example.com AXFR\n</code></pre> <pre><code>example.com.     3600 IN SOA  ns1.example.com. admin.example.com. 2025011501 ...\nexample.com.     3600 IN NS   ns1.example.com.\nexample.com.     3600 IN NS   ns2.example.com.\nexample.com.     3600 IN A    198.51.100.10\nwww.example.com. 3600 IN A    198.51.100.10\nmail.example.com. 3600 IN A   198.51.100.20\nexample.com.     3600 IN SOA  ns1.example.com. admin.example.com. 2025011501 ...\n</code></pre> <p>The SOA record appears at both the beginning and end - this marks the boundaries of the transfer.</p>"},{"location":"DNS%20Administration/dns-architecture/#ixfr-incremental-transfer","title":"IXFR (Incremental Transfer)","text":"<p>IXFR (RFC 1995) transfers only the changes since the secondary's last known serial. This is dramatically more efficient for large zones with small changes.</p> <pre><code># Request an IXFR from serial 2025011500\ndig @198.51.100.1 example.com IXFR=2025011500\n</code></pre> <p>An IXFR response contains: 1. The new SOA (with the current serial) 2. The old SOA (with the serial the secondary had) 3. Records to remove (from the old version) 4. The new SOA again 5. Records to add (in the new version)</p> <p>Not all servers support IXFR. When IXFR isn't available (or the change is too large), the server falls back to AXFR.</p>"},{"location":"DNS%20Administration/dns-architecture/#notify","title":"NOTIFY","text":"<p>When you change a zone on the primary, you don't want secondaries to wait until their next SOA refresh poll. The NOTIFY mechanism (RFC 1996) lets the primary immediately tell secondaries that the zone has changed.</p> <p>The primary sends a NOTIFY message to all configured secondaries. Each secondary responds by checking the primary's SOA serial and initiating a transfer if the serial is higher.</p> <pre><code>sequenceDiagram\n    participant Admin as Admin\n    participant Primary as Primary (ns1)\n    participant Secondary as Secondary (ns2)\n\n    Admin-&gt;&gt;Primary: Edit zone, increment serial\n    Primary-&gt;&gt;Secondary: NOTIFY (UDP)\n    Secondary-&gt;&gt;Primary: SOA query (check serial)\n    Primary--&gt;&gt;Secondary: SOA reply (serial: 2025011502)\n    Note over Secondary: New serial &gt; my serial (2025011501)\n    Secondary-&gt;&gt;Primary: AXFR or IXFR request (TCP)\n    Primary--&gt;&gt;Secondary: Zone data transfer\n    Note over Secondary: Zone loaded, serving new data</code></pre>"},{"location":"DNS%20Administration/dns-architecture/#tsig-for-zone-transfer-security","title":"TSIG for Zone Transfer Security","text":"<p>Zone transfers should always be authenticated with TSIG (see the BIND and NSD and Unbound guides for configuration details). Without TSIG, anyone who can reach your primary can request a full dump of your zone data.</p>"},{"location":"DNS%20Administration/dns-architecture/#troubleshooting-zone-transfers","title":"Troubleshooting Zone Transfers","text":"<pre><code># Check if secondaries are in sync (compare serials)\ndig @ns1.example.com example.com SOA +short\ndig @ns2.example.com example.com SOA +short\n\n# Test if the primary allows transfers\ndig @198.51.100.1 example.com AXFR\n\n# On the secondary, force a transfer\n# BIND:\nrndc retransfer example.com\n# NSD:\nnsd-control force_transfer example.com\n\n# Check zone transfer logs\njournalctl -u named | grep transfer\njournalctl -u nsd | grep xfr\n</code></pre> <p>Common zone transfer problems:</p> Symptom Cause \"Transfer refused\" Primary's <code>allow-transfer</code> doesn't include the secondary's IP Serial numbers match but data differs Someone edited the zone without incrementing the serial NOTIFY not received Firewall blocking UDP/53 from primary to secondary TSIG verification failure Key mismatch or clock skew between servers"},{"location":"DNS%20Administration/dns-architecture/#split-horizon-dns","title":"Split-Horizon DNS","text":"<p>Split-horizon (or split-brain) DNS returns different answers depending on who's asking. Internal clients get internal IP addresses; external clients get public IP addresses.</p>"},{"location":"DNS%20Administration/dns-architecture/#bind-views","title":"BIND Views","text":"<p>BIND's <code>view</code> blocks are the most common implementation (see the BIND guide for full configuration). Each view has its own zone data and matching criteria.</p>"},{"location":"DNS%20Administration/dns-architecture/#separate-servers","title":"Separate Servers","text":"<p>An alternative to views is running separate DNS servers for internal and external resolution:</p> <pre><code>Internal clients  \u2500\u2500&gt;  Internal resolver (10.0.0.1)\n                       \u251c\u2500\u2500 serves internal zones directly\n                       \u2514\u2500\u2500 forwards external queries to public DNS\n\nExternal clients  \u2500\u2500&gt;  Public authoritative server (198.51.100.1)\n                       \u2514\u2500\u2500 serves only public zone data\n</code></pre> <p>This is simpler to reason about and avoids the complexity of view configuration. The tradeoff is managing two separate sets of zone data.</p>"},{"location":"DNS%20Administration/dns-architecture/#when-to-use-split-horizon","title":"When to Use Split-Horizon","text":"<ul> <li>Internal services with private IPs that shouldn't be exposed externally</li> <li>VPN environments where the same hostname should resolve differently inside and outside the VPN</li> <li>Development environments where <code>api.example.com</code> should point to a staging server internally</li> </ul> <p>What problem does split-horizon DNS solve? (requires JavaScript)</p> <p>Configure Split-Horizon DNS with BIND Views (requires JavaScript)</p>"},{"location":"DNS%20Administration/dns-architecture/#anycast-dns","title":"Anycast DNS","text":"<p>Anycast is a routing technique where the same IP address is announced from multiple physical locations via BGP. When a client queries that IP, the network routes the query to the nearest (in network terms) instance.</p> <p></p> <p>All nodes serve identical zone data (typically synced from a hidden primary) but operate independently. If one node goes down, BGP withdraws its route advertisement and traffic automatically shifts to the next nearest node.</p> <p>Anycast is how the root servers operate - 13 identities served by roughly 1,954 instances worldwide. It's also how major DNS providers like Cloudflare (<code>1.1.1.1</code>) and Google (<code>8.8.8.8</code>) deliver low-latency DNS.</p> <p>Cloudflare's <code>1.1.1.1</code> is an interesting case. Before Cloudflare obtained the address for their public DNS resolver, <code>1.1.1.1</code> had been used as a dummy or placeholder address for years (like <code>127.0.0.1</code> but on the public internet). When Cloudflare started announcing it via anycast, the address was already receiving enormous volumes of junk traffic from misconfigured networks, testing scripts, and devices hardcoded with the address. They had to build significant filtering capacity just to handle the noise.</p>"},{"location":"DNS%20Administration/dns-architecture/#notable-dns-outages","title":"Notable DNS Outages","text":"<p>Understanding why large-scale DNS architectures fail teaches you what to design for:</p> <p>Facebook, October 4, 2021. Facebook's DNS servers withdrew their BGP route advertisements, making Facebook's authoritative DNS unreachable from the entire internet. The BGP withdrawal was triggered by a maintenance command that was supposed to assess backbone capacity - but a bug in the audit tool caused it to withdraw all routes instead. Facebook's internal DNS servers were designed to withdraw BGP routes when they detected they couldn't reach the backbone, interpreting it as a connectivity failure. This was a safety mechanism working exactly as designed - the servers concluded they were disconnected and stopped advertising themselves. The result was a 7-hour global outage affecting Facebook, Instagram, WhatsApp, and Messenger.</p> <p>Dyn, October 21, 2016. The Mirai botnet (composed of compromised IoT devices like cameras and DVRs) launched a massive DDoS attack against Dyn, a major managed DNS provider. Since Dyn provided authoritative DNS for Twitter, Reddit, Netflix, Amazon, Spotify, and many others, the attack took all of these services offline for hours - even though the services themselves were running fine. The outage demonstrated the risk of concentrating authoritative DNS with a single provider.</p> <p>These incidents share a theme: DNS is so fundamental that when it breaks, everything built on top of it breaks too, often in ways that prevent you from fixing the DNS problem itself (Facebook engineers couldn't access internal tools because DNS was down).</p> <p>What is anycast DNS? (requires JavaScript)</p> <p>Design a DNS Architecture (requires JavaScript)</p> <p>Verifying DNS Architecture with dig (requires JavaScript)</p>"},{"location":"DNS%20Administration/dns-architecture/#dns-and-email-getting-it-right","title":"DNS and Email: Getting It Right","text":"<p>Email deliverability depends on getting five DNS record types correct. Missing or misconfigured email DNS is the most common reason legitimate email ends up in spam folders.</p> <pre><code>sequenceDiagram\n    participant Sender as Sending MTA (mail.sender.com)\n    participant DNS as DNS\n    participant Receiver as Receiving MTA (mail.example.com)\n\n    Sender-&gt;&gt;DNS: MX lookup for example.com\n    DNS--&gt;&gt;Sender: 10 mail.example.com\n    Sender-&gt;&gt;DNS: A lookup for mail.example.com\n    DNS--&gt;&gt;Sender: 198.51.100.20\n    Sender-&gt;&gt;Receiver: SMTP connect to 198.51.100.20:25\n    Sender-&gt;&gt;Receiver: MAIL FROM: user@sender.com\n    Note over Receiver: Check SPF\n    Receiver-&gt;&gt;DNS: TXT lookup for sender.com\n    DNS--&gt;&gt;Receiver: v=spf1 ... -all\n    Note over Receiver: SPF: PASS \u2713\n    Note over Receiver: Check DKIM signature\n    Receiver-&gt;&gt;DNS: TXT lookup for selector._domainkey.sender.com\n    DNS--&gt;&gt;Receiver: v=DKIM1; k=rsa; p=...\n    Note over Receiver: DKIM: PASS \u2713\n    Note over Receiver: Check DMARC policy\n    Receiver-&gt;&gt;DNS: TXT lookup for _dmarc.sender.com\n    DNS--&gt;&gt;Receiver: v=DMARC1; p=reject\n    Note over Receiver: DMARC: PASS \u2713\n    Note over Receiver: Check reverse DNS\n    Receiver-&gt;&gt;DNS: PTR lookup for 198.51.100.10\n    DNS--&gt;&gt;Receiver: mail.sender.com\n    Note over Receiver: FCrDNS: PASS \u2713\n    Receiver--&gt;&gt;Sender: 250 OK - Message accepted</code></pre>"},{"location":"DNS%20Administration/dns-architecture/#mx-records","title":"MX Records","text":"<p>MX records tell sending mail servers where to deliver email for your domain:</p> <pre><code>example.com.    IN  MX  10  mail1.example.com.\nexample.com.    IN  MX  20  mail2.example.com.\n</code></pre> <p>Lower priority numbers are tried first. Having at least two MX records with different priorities ensures email delivery if one server is down.</p>"},{"location":"DNS%20Administration/dns-architecture/#spf-sender-policy-framework","title":"SPF (Sender Policy Framework)","text":"<p>SPF tells receiving servers which IP addresses are authorized to send email for your domain:</p> <pre><code>example.com.    IN  TXT  \"v=spf1 ip4:198.51.100.0/24 include:_spf.google.com -all\"\n</code></pre> <p>Breaking down the syntax:</p> Mechanism Meaning <code>v=spf1</code> SPF version 1 (required) <code>ip4:198.51.100.0/24</code> This IP range is authorized <code>include:_spf.google.com</code> Also allow whatever Google's SPF record allows <code>-all</code> Reject everything not explicitly listed <p>The trailing qualifier matters:</p> Qualifier Meaning <code>-all</code> Hard fail - reject unauthorized senders <code>~all</code> Soft fail - accept but mark as suspicious <code>?all</code> Neutral - no opinion <code>+all</code> Pass everything (defeats the purpose of SPF) <p>Start with <code>~all</code> while testing, switch to <code>-all</code> when confident.</p> <p>SPF has a 10 DNS lookup limit. Every <code>include:</code>, <code>a:</code>, <code>mx:</code>, and <code>redirect=</code> mechanism triggers a lookup. Exceeding the limit causes SPF to return <code>permerror</code>, and receiving servers may reject your email. Flatten nested includes if you hit this limit.</p>"},{"location":"DNS%20Administration/dns-architecture/#dkim-domainkeys-identified-mail","title":"DKIM (DomainKeys Identified Mail)","text":"<p>DKIM adds a cryptographic signature to outgoing email. The receiving server looks up the public key in DNS and verifies the signature:</p> <pre><code>selector1._domainkey.example.com. IN TXT \"v=DKIM1; k=rsa; p=MIGfMA0GCSq...\"\n</code></pre> <p>The selector (<code>selector1</code>) lets you rotate keys without disrupting verification. You can have multiple selectors active simultaneously - old emails still verify against the old selector while new emails use the new one.</p> <p>When rotating DKIM keys: 1. Generate a new key pair with a new selector name 2. Publish the new selector's public key in DNS 3. Configure your mail server to sign with the new selector 4. Keep the old selector's DNS record for at least 30 days (so emails in transit can still be verified) 5. Remove the old selector</p>"},{"location":"DNS%20Administration/dns-architecture/#dmarc-domain-based-message-authentication-reporting-and-conformance","title":"DMARC (Domain-based Message Authentication, Reporting and Conformance)","text":"<p>DMARC ties SPF and DKIM together and tells receiving servers what to do when authentication fails:</p> <pre><code>_dmarc.example.com. IN TXT \"v=DMARC1; p=reject; sp=reject; rua=mailto:dmarc-reports@example.com; pct=100\"\n</code></pre> Tag Meaning <code>p=reject</code> Policy: reject email that fails both SPF and DKIM <code>sp=reject</code> Subdomain policy: same as main domain <code>rua=mailto:...</code> Send aggregate reports to this address <code>pct=100</code> Apply the policy to 100% of failing messages <p>Start with <code>p=none</code> to collect reports without affecting delivery, then move to <code>p=quarantine</code>, then <code>p=reject</code>.</p>"},{"location":"DNS%20Administration/dns-architecture/#ptr-reverse-dns","title":"PTR (Reverse DNS)","text":"<p>Many mail servers verify that the sending IP has a PTR record that resolves back to a hostname, and that hostname resolves forward to the same IP (forward-confirmed reverse DNS, or FCrDNS):</p> <pre><code>10.100.51.198.in-addr.arpa. IN PTR mail.example.com.\nmail.example.com.            IN A   198.51.100.10\n</code></pre>"},{"location":"DNS%20Administration/dns-architecture/#complete-email-dns-audit","title":"Complete Email DNS Audit","text":"<p>Run this sequence to verify all email DNS for a domain:</p> <pre><code>#!/bin/bash\nDOMAIN=\"${1:?Usage: $0 domain.com}\"\n\necho \"=== MX Records ===\"\ndig +short MX \"$DOMAIN\"\n\necho -e \"\\n=== SPF ===\"\ndig +short TXT \"$DOMAIN\" | grep spf\n\necho -e \"\\n=== DKIM (common selectors) ===\"\nfor sel in default selector1 selector2 google dkim; do\n    result=$(dig +short TXT \"${sel}._domainkey.${DOMAIN}\" 2&gt;/dev/null)\n    [ -n \"$result\" ] &amp;&amp; echo \"$sel: $result\"\ndone\n\necho -e \"\\n=== DMARC ===\"\ndig +short TXT \"_dmarc.${DOMAIN}\"\n\necho -e \"\\n=== MX host A records ===\"\nfor mx in $(dig +short MX \"$DOMAIN\" | awk '{print $2}'); do\n    echo \"$mx -&gt; $(dig +short A \"$mx\")\"\ndone\n\necho -e \"\\n=== PTR for MX IPs ===\"\nfor mx in $(dig +short MX \"$DOMAIN\" | awk '{print $2}'); do\n    ip=$(dig +short A \"$mx\")\n    [ -n \"$ip\" ] &amp;&amp; echo \"$ip -&gt; $(dig +short -x \"$ip\")\"\ndone\n</code></pre> <p>DNS Email Record Audit Script (requires JavaScript)</p>"},{"location":"DNS%20Administration/dns-architecture/#monitoring-and-alerting","title":"Monitoring and Alerting","text":""},{"location":"DNS%20Administration/dns-architecture/#what-to-monitor","title":"What to Monitor","text":"Check How Alert When Authoritative server responding <code>dig @ns1 example.com SOA</code> Query fails or times out Zone serial consistency Compare SOA serial across all NS Secondaries behind primary DNSSEC signature expiry <code>dig +dnssec</code> and check RRSIG dates Signatures expire within 7 days Resolution time <code>dig</code> query time Consistently above 500ms Zone transfer working Compare SOA serial on secondary Secondary serial behind for &gt;2x refresh interval Certificate authority (CAA) <code>dig CAA example.com</code> Missing or wrong CAA records"},{"location":"DNS%20Administration/dns-architecture/#dig-based-health-checks","title":"dig-Based Health Checks","text":"<p>Simple monitoring script using <code>dig</code>:</p> <pre><code>#!/bin/bash\n# dns-healthcheck.sh - basic DNS monitoring\n\nDOMAIN=\"example.com\"\nSERVERS=\"ns1.example.com ns2.example.com\"\nEXPECTED_SERIAL=\"\"  # auto-detect from first server\n\nfor server in $SERVERS; do\n    result=$(dig @\"$server\" \"$DOMAIN\" SOA +short +time=5 +tries=2 2&gt;/dev/null)\n    if [ -z \"$result\" ]; then\n        echo \"CRITICAL: $server not responding for $DOMAIN\"\n        continue\n    fi\n\n    serial=$(echo \"$result\" | awk '{print $3}')\n\n    if [ -z \"$EXPECTED_SERIAL\" ]; then\n        EXPECTED_SERIAL=\"$serial\"\n    elif [ \"$serial\" != \"$EXPECTED_SERIAL\" ]; then\n        echo \"WARNING: $server serial $serial != expected $EXPECTED_SERIAL\"\n    else\n        echo \"OK: $server serial $serial\"\n    fi\ndone\n</code></pre>"},{"location":"DNS%20Administration/dns-architecture/#dnstap","title":"dnstap","text":"<p>dnstap is a structured logging format for DNS traffic. BIND, Unbound, and NSD all support it. Instead of parsing text logs, dnstap captures DNS queries and responses as Protocol Buffer messages, making them easy to process programmatically.</p> <p>Enable in Unbound:</p> <pre><code>server:\n    dnstap:\n        dnstap-enable: yes\n        dnstap-socket-path: \"/var/run/unbound/dnstap.sock\"\n        dnstap-send-identity: yes\n        dnstap-send-version: yes\n        dnstap-log-resolver-query-messages: yes\n        dnstap-log-resolver-response-messages: yes\n</code></pre>"},{"location":"DNS%20Administration/dns-architecture/#migration-patterns","title":"Migration Patterns","text":""},{"location":"DNS%20Administration/dns-architecture/#ttl-lowering-strategy","title":"TTL Lowering Strategy","text":"<p>When migrating DNS records (changing providers, moving servers, changing IPs), lower the TTL well in advance:</p> <p>Timeline:</p> <pre><code>gantt\n    title DNS Record Migration Timeline\n    dateFormat YYYY-MM-DD\n    axisFormat %a %b %d\n    section TTL\n        Original TTL: 86400s (24h)        :done, ttl1, 2025-01-13, 1d\n        Lower TTL to 300s (5min)          :crit, ttl2, 2025-01-14, 0d\n        Low TTL active                    :active, ttl3, 2025-01-14, 2d\n        Raise TTL back to 86400s          :ttl4, 2025-01-17, 0d\n    section Cache Drain\n        Wait for old 24h caches to expire :crit, drain, 2025-01-14, 1d\n    section Record Change\n        All resolvers on 5min TTL         :milestone, 2025-01-15, 0d\n        Update DNS record                 :crit, change, 2025-01-16, 0d\n        Monitor (5min full propagation)   :monitor, 2025-01-16, 1d</code></pre> <p>The critical detail: you must wait for the old TTL to expire before making the change. If your TTL was 86400 and you lower it to 300, you need to wait 24 hours before the change so that all cached copies of the old record (with the old TTL) have expired. Only then will all resolvers be honoring the new 5-minute TTL.</p>"},{"location":"DNS%20Administration/dns-architecture/#cutover-checklist","title":"Cutover Checklist","text":"<p>Before cutting over DNS to new infrastructure:</p> <pre><code>[ ] TTL lowered at least (old TTL) seconds before cutover\n[ ] New servers tested with direct queries (dig @new-server domain)\n[ ] Zone data verified on new servers (named-checkzone / nsd-checkzone)\n[ ] DNSSEC chain valid on new servers (if applicable)\n[ ] Reverse DNS (PTR) configured on new IP ranges\n[ ] Email records (MX, SPF, DKIM, DMARC) updated for new IPs\n[ ] Monitoring configured for new servers\n[ ] Rollback plan documented (old server kept running)\n[ ] NS records at registrar updated\n[ ] Old servers kept running for at least 48 hours after cutover\n</code></pre>"},{"location":"DNS%20Administration/dns-architecture/#the-io-tld-situation","title":"The .io TLD Situation","text":"<p>A cautionary note on infrastructure dependency: the <code>.io</code> country-code TLD, popular with tech companies, may face an uncertain future. In 2024, the UK announced it would hand sovereignty of the Chagos Archipelago (British Indian Ocean Territory) to Mauritius. The <code>.io</code> TLD is assigned based on the ISO 3166-1 country code for the British Indian Ocean Territory. If the territory ceases to exist as a distinct entity, the country code could be retired - and with it, the TLD.</p> <p>Historical precedent exists: Yugoslavia's <code>.yu</code> was retired after the country dissolved, and the Soviet Union's <code>.su</code> technically should have been retired but persists 34 years later due to its large user base. The <code>.io</code> situation is being watched closely by the thousands of companies that built their identity on the domain.</p> <p>The lesson: when choosing a TLD for critical infrastructure, gTLDs like <code>.com</code> or <code>.net</code> carry less geopolitical risk than ccTLDs.</p>"},{"location":"DNS%20Administration/dns-architecture/#further-reading","title":"Further Reading","text":"<ul> <li>RFC 5936 - DNS Zone Transfer Protocol (AXFR)</li> <li>RFC 1995 - Incremental Zone Transfer (IXFR)</li> <li>RFC 1996 - DNS NOTIFY Mechanism</li> <li>RFC 7208 - Sender Policy Framework (SPF)</li> <li>RFC 6376 - DomainKeys Identified Mail (DKIM)</li> <li>RFC 7489 - Domain-based Message Authentication (DMARC)</li> <li>dnstap - structured DNS logging format</li> <li>Cloudflare Learning Center: DNS - accessible DNS architecture explanations</li> </ul> <p>Previous: DNSSEC | Back to Index</p>"},{"location":"DNS%20Administration/dns-fundamentals/","title":"DNS Fundamentals","text":"<p>This guide covers what DNS actually is, how it evolved from a single text file into the largest distributed database on Earth, and how every name resolution works from root servers down to the answer your browser needs.</p>"},{"location":"DNS%20Administration/dns-fundamentals/#before-dns-the-hoststxt-era","title":"Before DNS: The HOSTS.TXT Era","text":"<p>Before DNS existed, the entire internet ran on a single text file.</p> <p>Every machine on the ARPANET had a file called <code>HOSTS.TXT</code> that mapped hostnames to IP addresses - the same concept as <code>/etc/hosts</code> on your machine today. The Stanford Research Institute's Network Information Center (SRI-NIC) maintained the master copy. If you added a new host to the network, you called SRI-NIC on the phone (during California business hours), asked them to add your entry, and then every other site on the network would periodically FTP the updated file from SRI-NIC.</p> <p>By the early 1980s this system was falling apart. The ARPANET was growing exponentially, and HOSTS.TXT had fundamental problems:</p> <ul> <li>No scalability - a single file can't track thousands of hosts</li> <li>No delegation - SRI-NIC had to approve every name</li> <li>No consistency - sites fetched updates at different times, so different machines had different views of the network</li> <li>Name collisions - nothing prevented two sites from claiming the same hostname</li> </ul>  Paul Mockapetris, inventor of DNS, at a conference in Barcelona. Photo: Jordiipa, CC BY-SA 3.0 <p>In 1983, Paul Mockapetris at USC's Information Sciences Institute published RFC 882 and RFC 883, proposing a distributed, hierarchical naming system. His first implementation was called \"Jeeves.\" Two years later, he refined the design into RFC 1034 and RFC 1035 - the specifications that still define DNS today.</p>"},{"location":"DNS%20Administration/dns-fundamentals/#what-dns-actually-does","title":"What DNS Actually Does","text":"<p>DNS is a distributed hierarchical database that maps names to data. Most people think of it as \"the phone book of the internet\" that converts domain names to IP addresses, but that undersells it significantly.</p> <p>DNS handles:</p> <ul> <li>Name-to-address mapping - <code>example.com</code> to <code>93.184.216.34</code> (A records)</li> <li>Mail routing - which servers accept email for a domain (MX records)</li> <li>Service discovery - where to find SIP, LDAP, or XMPP servers (SRV records)</li> <li>TLS certificate validation - which certificate authorities can issue certs for a domain (CAA records)</li> <li>Email authentication - SPF, DKIM, and DMARC policies (TXT records)</li> <li>Delegation - which servers are authoritative for a zone (NS records)</li> <li>Reverse lookups - IP address to hostname (PTR records)</li> </ul> <p>Every HTTPS connection, every email delivery, every API call starts with a DNS query. DNS processes an estimated 2 trillion queries per day worldwide. It's the most queried database in existence.</p>"},{"location":"DNS%20Administration/dns-fundamentals/#the-dns-hierarchy","title":"The DNS Hierarchy","text":"<p>DNS organizes names into a tree structure, read from right to left. The name <code>www.example.com.</code> (note the trailing dot) breaks down like this:</p> <pre><code>graph TD\n    root(\". (root)\")\n    root --&gt; com(\".com\")\n    root --&gt; org(\".org\")\n    root --&gt; uk(\".uk\")\n    root --&gt; arpa(\".arpa\")\n    com --&gt; example(\"example\")\n    org --&gt; wikipedia(\"wikipedia\")\n    uk --&gt; couk(\".co.uk\")\n    example --&gt; www(\"www\")\n    example --&gt; mail(\"mail\")</code></pre>"},{"location":"DNS%20Administration/dns-fundamentals/#the-root-zone","title":"The Root Zone","text":"<p>At the top of the tree is the root zone, represented by a single dot (<code>.</code>). This is the starting point for every DNS resolution.</p> <p>You'll often hear that there are \"13 root servers,\" named <code>a.root-servers.net</code> through <code>m.root-servers.net</code>. That's technically true but deeply misleading. The number 13 is a legacy constraint - early DNS responses had to fit in a single 512-byte UDP packet, and 13 sets of A and AAAA records was the maximum that would fit.</p> <p>In reality, those 13 identities are served by roughly 1,954 instances deployed worldwide using anycast routing. Anycast means the same IP address is announced from multiple physical locations, and your query reaches whichever instance is closest in network terms. Verisign alone operates over 290 instances for <code>a.root-servers.net</code> and <code>j.root-servers.net</code>.</p> <p>The root servers are operated by 12 independent organizations:</p> Letter Operator A Verisign B USC-ISI C Cogent Communications D University of Maryland E NASA Ames Research Center F Internet Systems Consortium (ISC) G U.S. Department of Defense (DISA) H U.S. Army Research Lab I Netnod (Sweden) J Verisign K RIPE NCC (Netherlands) L ICANN M WIDE Project (Japan) <p>NASA and the U.S. Army Research Lab each operate a root server. The geographic and organizational diversity is deliberate - no single failure, attack, or political action can take down DNS.</p>"},{"location":"DNS%20Administration/dns-fundamentals/#top-level-domains","title":"Top-Level Domains","text":"<p>Below the root are top-level domains (TLDs). These fall into several categories:</p> <p>Generic TLDs (gTLDs) - <code>.com</code>, <code>.net</code>, <code>.org</code>, <code>.info</code>, and since 2012, thousands of new gTLDs like <code>.app</code>, <code>.dev</code>, and <code>.io</code>.</p> <p>Country-code TLDs (ccTLDs) - two-letter codes assigned by ISO 3166: <code>.uk</code>, <code>.de</code>, <code>.jp</code>, <code>.au</code>. Some ccTLDs have taken on broader use - <code>.io</code> (British Indian Ocean Territory) became popular with tech companies, and <code>.tv</code> (Tuvalu) earns that country millions in licensing revenue.</p> <p>Country-code TLDs occasionally outlive their countries. The <code>.su</code> TLD was assigned to the Soviet Union and is still active over 34 years after the USSR dissolved. Yugoslavia's <code>.yu</code> survived through multiple wars and name changes before finally being retired in 2010.</p> <p>Infrastructure TLD - <code>.arpa</code> is used for reverse DNS lookups and other infrastructure purposes.</p>"},{"location":"DNS%20Administration/dns-fundamentals/#zones-vs-domains","title":"Zones vs Domains","text":"<p>These terms are often confused, but the distinction matters when you're running DNS servers.</p> <p>A domain is a name in the hierarchy. <code>example.com</code> is a domain. <code>mail.example.com</code> is a subdomain of <code>example.com</code>.</p> <p>A zone is the portion of the DNS tree that a particular server is authoritative for. If <code>example.com</code> delegates <code>mail.example.com</code> to a different set of nameservers, then <code>example.com</code> and <code>mail.example.com</code> are two separate zones even though they're part of the same domain.</p> <p>A zone always starts at a delegation point (where NS records hand authority to specific nameservers) and extends down the tree until it hits another delegation point.</p>"},{"location":"DNS%20Administration/dns-fundamentals/#how-dns-resolution-works","title":"How DNS Resolution Works","text":"<p>When you type <code>www.example.com</code> into a browser, a chain of queries and responses happens in milliseconds. Understanding this process is essential for troubleshooting DNS issues.</p>"},{"location":"DNS%20Administration/dns-fundamentals/#the-players","title":"The Players","text":"<p>Stub resolver - the DNS client built into your operating system. When an application calls <code>getaddrinfo()</code> or similar functions, the stub resolver handles it. The stub resolver is simple - it sends a query to a configured recursive resolver and waits for the final answer. Check your configured resolver with:</p> <pre><code># Linux\ncat /etc/resolv.conf\n\n# macOS\nscutil --dns | head -20\n</code></pre> <p>Recursive resolver - the server that does the actual work of chasing down answers. Your ISP runs one, and public options include Google (<code>8.8.8.8</code>), Cloudflare (<code>1.1.1.1</code>), and Quad9 (<code>9.9.9.9</code>). When you point your machine at <code>8.8.8.8</code>, you're telling your stub resolver to use Google's recursive resolver.</p> <p>Authoritative nameserver - a server that holds the actual DNS data for a zone and can answer queries about it without asking anyone else.</p>"},{"location":"DNS%20Administration/dns-fundamentals/#the-resolution-process","title":"The Resolution Process","text":"<p>Here's what happens when your recursive resolver needs to look up <code>www.example.com</code> and has nothing cached:</p> <pre><code>sequenceDiagram\n    participant App as Application\n    participant Stub as Stub Resolver\n    participant Rec as Recursive Resolver\n    participant Root as Root Server (.)\n    participant TLD as .com TLD Server\n    participant Auth as example.com Auth Server\n\n    App-&gt;&gt;Stub: www.example.com?\n    Stub-&gt;&gt;Rec: www.example.com? (rd flag set)\n    Rec-&gt;&gt;Root: www.example.com?\n    Root--&gt;&gt;Rec: Referral: .com NS a.gtld-servers.net\n    Rec-&gt;&gt;TLD: www.example.com?\n    TLD--&gt;&gt;Rec: Referral: example.com NS a.iana-servers.net\n    Rec-&gt;&gt;Auth: www.example.com?\n    Auth--&gt;&gt;Rec: Answer: 93.184.216.34 (aa flag set)\n    Note over Rec: Cache answer (TTL: 86400s)\n    Rec--&gt;&gt;Stub: 93.184.216.34\n    Stub--&gt;&gt;App: 93.184.216.34</code></pre> <p>Step 1 - Query the root. The resolver picks a root server and asks: \"What are the nameservers for <code>www.example.com</code>?\" The root server doesn't know the final answer, but it knows who handles <code>.com</code>. It responds with a referral - the NS records and IP addresses for the <code>.com</code> TLD servers.</p> <p>Step 2 - Query the TLD. The resolver asks a <code>.com</code> TLD server the same question. The TLD server doesn't know the final answer either, but it knows who handles <code>example.com</code>. It responds with another referral - the NS records for <code>example.com</code>'s authoritative nameservers.</p> <p>Step 3 - Query the authoritative server. The resolver asks <code>example.com</code>'s nameserver for the A record for <code>www.example.com</code>. This server owns the data, so it responds with the answer: <code>93.184.216.34</code>.</p> <p>Step 4 - Return and cache. The resolver sends the answer back to your stub resolver and caches it according to the response's TTL.</p> <p>You can watch this entire process yourself with:</p> <pre><code>dig +trace www.example.com\n</code></pre> <pre><code>; &lt;&lt;&gt;&gt; DiG 9.18.28 &lt;&lt;&gt;&gt; +trace www.example.com\n;; global options: +cmd\n.                       515195  IN      NS      a.root-servers.net.\n.                       515195  IN      NS      b.root-servers.net.\n;; [... other root servers ...]\n;; Received 525 bytes from 127.0.0.1#53(127.0.0.1) in 0 ms\n\ncom.                    172800  IN      NS      a.gtld-servers.net.\ncom.                    172800  IN      NS      b.gtld-servers.net.\n;; [... other .com servers ...]\n;; Received 1170 bytes from 198.41.0.4#53(a.root-servers.net) in 24 ms\n\nexample.com.            172800  IN      NS      a.iana-servers.net.\nexample.com.            172800  IN      NS      b.iana-servers.net.\n;; Received 326 bytes from 192.5.6.30#53(a.gtld-servers.net) in 12 ms\n\nwww.example.com.        86400   IN      A       93.184.216.34\n;; Received 56 bytes from 199.43.135.53#53(a.iana-servers.net) in 88 ms\n</code></pre> <p>Each section shows a hop in the resolution chain. The IP addresses in the <code>from</code> field show you which server answered each step.</p> <p>In what order does a recursive resolver look up a domain name like www.example.com? (requires JavaScript)</p> <p>Tracing DNS Resolution with dig +trace (requires JavaScript)</p> <p>Trace DNS Resolution Manually (requires JavaScript)</p>"},{"location":"DNS%20Administration/dns-fundamentals/#caching-and-ttls","title":"Caching and TTLs","text":"<pre><code>graph LR\n    Auth[\"Authoritative Server&lt;br/&gt;example.com A 93.184.216.34&lt;br/&gt;TTL: 86400\"]\n\n    subgraph Resolver A\n        direction TB\n        RA_Q[\"Query at 09:00\"] --&gt; RA_C[\"Cached until 09:00 +24h\"]\n        RA_C --&gt; RA_E[\"Cache expires 09:00 next day\"]\n    end\n\n    subgraph Resolver B\n        direction TB\n        RB_Q[\"Query at 14:00\"] --&gt; RB_C[\"Cached until 14:00 +24h\"]\n        RB_C --&gt; RB_E[\"Cache expires 14:00 next day\"]\n    end\n\n    subgraph Resolver C\n        direction TB\n        RC_Q[\"Query at 22:00\"] --&gt; RC_C[\"Cached until 22:00 +24h\"]\n        RC_C --&gt; RC_E[\"Cache expires 22:00 next day\"]\n    end\n\n    Auth -.-&gt; RA_Q\n    Auth -.-&gt; RB_Q\n    Auth -.-&gt; RC_Q</code></pre> <p>Each resolver's TTL countdown starts independently from when it first queried. There is no synchronization between them - this is why \"DNS propagation\" is a myth. It's really independent cache expiration.</p> <p>If your resolver had to chase every query from the root, DNS would be unbearably slow. Caching makes the system practical.</p> <p>Every DNS response includes a TTL (Time To Live) - a number in seconds that tells the resolver how long to cache the answer. When you query for <code>example.com</code> and get a TTL of 86400, your resolver will serve that cached answer for the next 24 hours without asking anyone.</p> <p>You can watch TTLs count down by querying the same name repeatedly:</p> <pre><code>dig example.com | grep -A1 \"ANSWER SECTION\"\n</code></pre> <pre><code>;; ANSWER SECTION:\nexample.com.            86400   IN      A       93.184.216.34\n</code></pre> <p>Wait a few seconds and query again - the TTL will be lower. Your resolver is counting down until it expires and needs to ask again.</p> <p>There is no \"DNS propagation.\" This is one of the most misunderstood concepts in DNS. When you change a DNS record, there is no mechanism that pushes the change to resolvers around the world. What actually happens is cache expiration - resolvers everywhere are holding the old answer with a countdown timer. As each resolver's cache expires, it asks your authoritative server and gets the new answer.</p> <p>This is why lowering your TTL before making a change is standard practice. If your TTL is 86400 (24 hours) and you change an A record, some resolvers may serve the old answer for up to 24 hours. If you lower the TTL to 300 (5 minutes) a day before the change, all caches will expire within 5 minutes after you make the switch.</p>"},{"location":"DNS%20Administration/dns-fundamentals/#negative-caching","title":"Negative Caching","text":"<p>Resolvers also cache negative answers. When a name doesn't exist, the authoritative server returns an NXDOMAIN response. The resolver caches this too, based on the SOA record's minimum TTL field (see the Zone Files and Records guide). This prevents repeated queries for names that don't exist.</p> <p>RFC 2308 defines negative caching behavior. It's worth knowing this exists because it explains a common gotcha: if you query a name before you've created the record, the NXDOMAIN gets cached, and you may have to wait for that negative cache to expire before the new record is visible.</p> <p>If a DNS record has a TTL of 3600, what does that mean? (requires JavaScript)</p>"},{"location":"DNS%20Administration/dns-fundamentals/#authoritative-vs-recursive","title":"Authoritative vs Recursive","text":"<p>These are two fundamentally different roles, and understanding the distinction prevents a whole category of configuration mistakes.</p> <p>An authoritative server owns the data. It has the zone file (or database) for a domain and answers queries from that data. When it answers, it sets the <code>aa</code> (authoritative answer) flag in the response. If it's asked about a domain it doesn't own, it doesn't try to find the answer - it simply refuses or returns a referral.</p> <p>A recursive resolver owns nothing. It receives queries from clients, chases referrals from root to TLD to authoritative, caches the results, and returns the final answer. It never sets the <code>aa</code> flag (unless it happens to also be authoritative for the queried zone).</p> <p>You can see the difference with <code>dig</code>:</p> <pre><code># Query an authoritative server directly - note the \"aa\" flag\ndig @a.iana-servers.net example.com\n\n;; flags: qr aa rd; QUERY: 1, ANSWER: 1\n\n# Query a recursive resolver - no \"aa\" flag\ndig @8.8.8.8 example.com\n\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1\n</code></pre> <p>The <code>aa</code> flag is present in the first response and absent in the second. <code>rd</code> means \"recursion desired\" (the client asked for recursion), and <code>ra</code> means \"recursion available\" (the server supports it).</p>"},{"location":"DNS%20Administration/dns-fundamentals/#why-not-both-on-the-same-ip","title":"Why Not Both on the Same IP?","text":"<p>Running authoritative and recursive services on the same server at the same IP address creates problems:</p> <ul> <li>Security - a recursive resolver must accept queries from clients, but an authoritative server should accept queries from anyone. Combining them makes access control difficult.</li> <li>Attack surface - recursive resolvers are targets for DNS amplification attacks. If your authoritative server is also an open resolver, attackers can use it to amplify DDoS attacks against third parties.</li> <li>Performance - recursive resolution is CPU-intensive (chasing referrals, validating DNSSEC). Authoritative serving is I/O-bound (reading zone data). Mixing them causes unpredictable resource contention.</li> </ul> <p>Many ISPs run resolvers that also happen to be authoritative for customer reverse DNS zones. That works because access is already restricted to their customers. But if you're building your own DNS infrastructure, keep them separate.</p>"},{"location":"DNS%20Administration/dns-fundamentals/#open-resolvers","title":"Open Resolvers","text":"<p>An open resolver is a recursive server that accepts queries from anyone on the internet. This is dangerous because DNS responses are larger than queries - an attacker can send a small query with a spoofed source IP, and the resolver sends a large response to the victim. A typical DNS amplification attack achieves 50x amplification: a 1 Mbps query stream becomes 50 Mbps hitting the target.</p> <p>ISPs routinely operate resolvers, but they restrict access to their own customers' IP ranges. If you run your own recursive resolver, restrict it to your network with ACLs.</p> <p>Some ISPs also hijack NXDOMAIN responses. Instead of telling your browser that a domain doesn't exist, they redirect you to a search/advertising page. This violates DNS standards (RFC 4924 Section 2.5.2.3) and breaks applications that rely on NXDOMAIN behavior. Public resolvers like <code>1.1.1.1</code> and <code>9.9.9.9</code> don't do this.</p> <p>What is the difference between a recursive and an iterative DNS query? (requires JavaScript)</p>"},{"location":"DNS%20Administration/dns-fundamentals/#registrars-registries-and-glue-records","title":"Registrars, Registries, and Glue Records","text":"<p>When you \"buy\" a domain name, you're interacting with a specific chain of organizations:</p> <p>A registry operates a TLD. Verisign operates the <code>.com</code> registry. The registry maintains the authoritative database of all domain names in that TLD. You never interact with the registry directly.</p> <p>A registrar is an organization accredited to sell domain names. GoDaddy, Namecheap, Cloudflare Registrar, and Google Domains (now Squarespace) are registrars. When you register a domain through a registrar, they submit the registration to the registry on your behalf using the Extensible Provisioning Protocol (EPP).</p> <p>The WHOIS system (and its modern replacement, RDAP) lets you look up registration information for a domain:</p> <pre><code>whois example.com\n</code></pre> <pre><code>   Domain Name: EXAMPLE.COM\n   Registry Domain ID: 2336799_DOMAIN_COM-VRSN\n   Registrar WHOIS Server: whois.iana.org\n   Updated Date: 2024-08-14T07:01:34Z\n   Creation Date: 1995-08-14T04:00:00Z\n   Registry Expiry Date: 2025-08-13T04:00:00Z\n   Registrar: RESERVED-Internet Assigned Numbers Authority\n   Name Server: A.IANA-SERVERS.NET\n   Name Server: B.IANA-SERVERS.NET\n   DNSSEC: signedDelegation\n</code></pre>"},{"location":"DNS%20Administration/dns-fundamentals/#the-trailing-dot","title":"The Trailing Dot","text":"<p>Every fully qualified domain name (FQDN) ends with a dot: <code>www.example.com.</code> - that final dot represents the root zone. Most software adds it implicitly, so you never type it. But inside DNS zone files, the trailing dot is critical. Without it, the name is treated as relative and the zone's origin is appended.</p> <p>This creates one of the most common DNS bugs. In a zone file for <code>example.com</code>:</p> <pre><code>www     IN  CNAME   other.example.com     ; WRONG - becomes other.example.com.example.com.\nwww     IN  CNAME   other.example.com.    ; RIGHT - the dot makes it absolute\n</code></pre> <p>This distinction also matters in Kubernetes. A pod looking up <code>example.com</code> (without a trailing dot) may first try <code>example.com.mycluster.local</code>, <code>example.com.svc.cluster.local</code>, and other search domain suffixes before trying the actual name. Adding the trailing dot (<code>example.com.</code>) skips the search list and goes straight to the intended name.</p>"},{"location":"DNS%20Administration/dns-fundamentals/#glue-records","title":"Glue Records","text":"<p>There's a chicken-and-egg problem in DNS. Suppose <code>example.com</code> uses <code>ns1.example.com</code> as its nameserver. To resolve anything under <code>example.com</code>, you need to reach <code>ns1.example.com</code>. But to find the IP address of <code>ns1.example.com</code>, you need to query... the nameserver for <code>example.com</code>. That's circular.</p> <p>Glue records break this cycle. When <code>example.com</code> is registered, the parent zone (<code>.com</code>) stores not just the NS record (<code>example.com NS ns1.example.com</code>) but also an A record for the nameserver itself (<code>ns1.example.com A 198.51.100.1</code>). These A records in the parent zone are the \"glue\" that bootstraps the delegation.</p> <pre><code>graph TD\n    subgraph problem [\"The Problem: Circular Dependency\"]\n        P1[\"Resolve anything under example.com\"]\n        P2[\"Need ns1.example.com IP\"]\n        P3[\"Query example.com nameserver\"]\n        P1 --&gt;|\"requires\"| P2\n        P2 --&gt;|\"requires\"| P3\n        P3 --&gt;|\"requires\"| P1\n    end\n\n    subgraph solution [\"The Solution: Glue Records\"]\n        S1[\".com TLD Zone&lt;br/&gt;&lt;br/&gt;example.com NS ns1.example.com&lt;br/&gt;ns1.example.com A 198.51.100.1 \u2190 glue\"]\n        S2[\"Resolver gets NS + IP&lt;br/&gt;in the same referral\"]\n        S3[\"Connects directly to&lt;br/&gt;198.51.100.1\"]\n        S1 --&gt; S2 --&gt; S3\n    end</code></pre> <p>Glue is only needed when a nameserver's name is within the zone it serves. If <code>example.com</code> uses <code>ns1.dnsprovider.net</code> as its nameserver, no glue is needed - the resolver can find <code>ns1.dnsprovider.net</code> by following the normal delegation chain through <code>.net</code>.</p> <p>You can see glue records in the additional section of a DNS referral:</p> <pre><code>dig +norec @a.gtld-servers.net example.com NS\n</code></pre> <pre><code>;; AUTHORITY SECTION:\nexample.com.        172800  IN  NS  a.iana-servers.net.\nexample.com.        172800  IN  NS  b.iana-servers.net.\n\n;; ADDITIONAL SECTION:\na.iana-servers.net. 172800  IN  A   199.43.135.53\nb.iana-servers.net. 172800  IN  A   199.43.133.53\n</code></pre> <p>The AUTHORITY section says \"these are the nameservers.\" The ADDITIONAL section says \"and here are their IP addresses so you don't have to look them up separately.\" That's glue.</p>"},{"location":"DNS%20Administration/dns-fundamentals/#dns-transport","title":"DNS Transport","text":"<p>DNS traditionally uses UDP on port 53 for queries under 512 bytes and TCP on port 53 for larger responses and zone transfers. The 512-byte UDP limit was a practical constraint from the 1980s when network reliability was poor and UDP was faster.</p> <p>The Extension Mechanisms for DNS (EDNS0) specification raised the effective UDP message size. Modern resolvers advertise a buffer size of 1232-4096 bytes, allowing larger responses (like DNSSEC-signed answers) to travel over UDP.</p> <p>Newer transport protocols are emerging for privacy:</p> <ul> <li>DNS over TLS (DoT) - DNS queries encrypted with TLS on port 853</li> <li>DNS over HTTPS (DoH) - DNS queries sent as HTTPS requests on port 443, making them indistinguishable from normal web traffic</li> <li>DNS over QUIC (DoQ) - DNS queries over the QUIC protocol, offering TLS encryption with lower latency than DoT</li> </ul> <p>These encrypted transports prevent ISPs and network operators from seeing (or tampering with) your DNS queries. They don't change how DNS resolution works - they just encrypt the transport between your stub resolver and the recursive resolver.</p> <p>What problem does DNSSEC solve? (requires JavaScript)</p>"},{"location":"DNS%20Administration/dns-fundamentals/#further-reading","title":"Further Reading","text":"<ul> <li>RFC 1034 - Domain Names: Concepts and Facilities</li> <li>RFC 1035 - Domain Names: Implementation and Specification</li> <li>RFC 9499 - DNS Terminology (current, supersedes RFC 8499)</li> <li>RFC 2308 - Negative Caching of DNS Queries</li> <li>IANA Root Servers - official root server list</li> <li>root-servers.org - root server instance map and statistics</li> <li>Cloudflare Learning Center: What is DNS? - accessible overview with diagrams</li> </ul> <p>Next: Zone Files and Records | Back to Index</p>"},{"location":"DNS%20Administration/dns-tools/","title":"DNS Tools and Troubleshooting","text":"<p>This guide covers the essential tools for querying, diagnosing, and debugging DNS. You'll learn to read <code>dig</code> output fluently, trace the full resolution path, and work through systematic troubleshooting playbooks for the most common DNS failures.</p>"},{"location":"DNS%20Administration/dns-tools/#dig-the-swiss-army-knife","title":"dig: The Swiss Army Knife","text":"<p><code>dig</code> (Domain Information Groper) is the most powerful DNS query tool. It's part of the BIND package and available on virtually every system. If you learn one DNS tool well, make it <code>dig</code>.</p>"},{"location":"DNS%20Administration/dns-tools/#basic-queries","title":"Basic Queries","text":"<pre><code>dig example.com                  # query the default record type (A)\ndig example.com AAAA             # query a specific record type\ndig example.com MX               # mail exchange records\ndig example.com ANY              # request all available record types\ndig example.com +short           # just the answer, nothing else\n</code></pre>"},{"location":"DNS%20Administration/dns-tools/#reading-full-output","title":"Reading Full Output","text":"<p>Most people use <code>+short</code> and miss the information that actually matters. The annotated diagram below shows each section of a full <code>dig</code> response:</p> <p></p> <p>Here's what a full <code>dig</code> response looks like:</p> <pre><code>dig example.com\n</code></pre> <pre><code>; &lt;&lt;&gt;&gt; DiG 9.18.28 &lt;&lt;&gt;&gt; example.com\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 41923\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 1232\n;; QUESTION SECTION:\n;example.com.                   IN      A\n\n;; ANSWER SECTION:\nexample.com.            86400   IN      A       93.184.216.34\n\n;; Query time: 12 msec\n;; SERVER: 127.0.0.53#53(127.0.0.53) (UDP)\n;; WHEN: Wed Jan 15 10:30:00 UTC 2025\n;; MSG SIZE  rcvd: 56\n</code></pre> <p>Breaking this down section by section:</p> <p>HEADER - <code>status: NOERROR</code> means the query succeeded. Other values you'll see: <code>NXDOMAIN</code> (name doesn't exist), <code>SERVFAIL</code> (the server failed to process the query), <code>REFUSED</code> (the server refused to answer).</p> <p>Flags - these single letters tell you what happened: - <code>qr</code> - this is a response (query response) - <code>rd</code> - recursion was requested (recursion desired) - <code>ra</code> - the server supports recursion (recursion available) - <code>aa</code> - this is an authoritative answer (the server owns the data)</p> <p>The <code>aa</code> flag is one of the most important things to check. If you're querying your own authoritative server and <code>aa</code> is missing, something is wrong - the server doesn't think it's authoritative for that zone.</p> <p>QUESTION SECTION - confirms what you asked.</p> <p>ANSWER SECTION - the actual response. The number <code>86400</code> is the TTL in seconds (24 hours). <code>IN</code> means the Internet class. <code>A</code> is the record type. <code>93.184.216.34</code> is the data.</p> <p>Query time - how long the query took. Under 100ms is normal. Over 1000ms suggests a problem.</p> <p>SERVER - which resolver answered your query. Critical when troubleshooting - you need to know who you're asking.</p> <p>In dig output, what information does the AUTHORITY section contain? (requires JavaScript)</p>"},{"location":"DNS%20Administration/dns-tools/#querying-specific-servers","title":"Querying Specific Servers","text":"<p>By default, <code>dig</code> queries whatever resolver your system is configured to use. You can query any server directly with <code>@</code>:</p> <pre><code>dig @8.8.8.8 example.com              # query Google's resolver\ndig @1.1.1.1 example.com              # query Cloudflare's resolver\ndig @ns1.example.com example.com      # query an authoritative server directly\n</code></pre> <p>Comparing results from different servers is a fundamental debugging technique. If your authoritative server has the right answer but <code>8.8.8.8</code> has the wrong one, the old answer is cached at Google and you need to wait for TTL expiration.</p> <p>Reading dig Output (requires JavaScript)</p>"},{"location":"DNS%20Administration/dns-tools/#tracing-resolution","title":"Tracing Resolution","text":"<p><code>+trace</code> replays the entire resolution process from root to answer:</p> <pre><code>dig +trace www.example.com\n</code></pre> <pre><code>.                       507399  IN      NS      a.root-servers.net.\n.                       507399  IN      NS      b.root-servers.net.\n.                       507399  IN      NS      c.root-servers.net.\n;; Received 525 bytes from 127.0.0.53#53(127.0.0.53) in 0 ms\n\ncom.                    172800  IN      NS      a.gtld-servers.net.\ncom.                    172800  IN      NS      b.gtld-servers.net.\n;; Received 1170 bytes from 199.7.83.42#53(l.root-servers.net) in 20 ms\n\nexample.com.            172800  IN      NS      a.iana-servers.net.\nexample.com.            172800  IN      NS      b.iana-servers.net.\n;; Received 326 bytes from 192.5.6.30#53(a.gtld-servers.net) in 16 ms\n\nwww.example.com.        86400   IN      A       93.184.216.34\n;; Received 56 bytes from 199.43.135.53#53(a.iana-servers.net) in 88 ms\n</code></pre> <p>Each block is one hop in the resolution chain. The <code>from</code> line tells you which server responded and how long it took. <code>+trace</code> bypasses your local cache entirely, so you always see the current state of DNS - not what was cached hours ago.</p> <p>Use <code>+trace</code> when you want to know whether the authoritative answer is correct, regardless of what your local resolver has cached.</p>"},{"location":"DNS%20Administration/dns-tools/#other-useful-dig-options","title":"Other Useful dig Options","text":"<pre><code>dig example.com +norecurse          # ask without requesting recursion (iterative query)\ndig example.com +dnssec             # request DNSSEC records along with the answer\ndig example.com +tcp                # force TCP instead of UDP\ndig example.com +nsid               # request the server's identity (if supported)\ndig -x 93.184.216.34                # reverse DNS lookup\ndig example.com SOA +multiline      # SOA in readable format\ndig @ns1.example.com example.com AXFR  # attempt a full zone transfer\n</code></pre> <p>The <code>-x</code> flag for reverse lookups is a shortcut. <code>dig -x 93.184.216.34</code> is equivalent to <code>dig 34.216.184.93.in-addr.arpa. PTR</code> - it reverses the octets and appends <code>.in-addr.arpa.</code> for you.</p>"},{"location":"DNS%20Administration/dns-tools/#batch-mode","title":"Batch Mode","text":"<p>You can put multiple queries in a file and run them all at once:</p> <pre><code># queries.txt\nexample.com A\nexample.com MX\nexample.com TXT\n\ndig -f queries.txt +short\n</code></pre> <p>Command Builder (requires JavaScript)</p>"},{"location":"DNS%20Administration/dns-tools/#drill-and-delv","title":"drill and delv","text":""},{"location":"DNS%20Administration/dns-tools/#drill","title":"drill","text":"<p><code>drill</code> is a DNS query tool from NLnet Labs, built on the ldns library. It's functionally similar to <code>dig</code> with a slightly different output format.</p> <pre><code>drill example.com                   # basic query\ndrill -T example.com                # trace (like dig +trace)\ndrill -S example.com                # chase DNSSEC signatures\n</code></pre> <p><code>drill</code> shines for DNSSEC debugging. <code>drill -S</code> (chase) follows the entire DNSSEC trust chain from the root, validating each link:</p> <pre><code>drill -S example.com\n</code></pre> <pre><code>;; Number of trusted keys: 1\n;; Chasing: example.com. A\n;; DNSSEC Trust tree:\nexample.com. (A)\n|---example.com. (DNSKEY keytag: 31406 alg: 13 flags: 256)\n    |---example.com. (DS keytag: 31406 digest type: 2)\n        |---com. (DNSKEY keytag: 19718 alg: 13 flags: 256)\n            |---com. (DS keytag: 19718 digest type: 2)\n                |---. (DNSKEY keytag: 20326 alg: 8 flags: 257)\n;; Chase successful\n</code></pre>"},{"location":"DNS%20Administration/dns-tools/#delv","title":"delv","text":"<p><code>delv</code> (Domain Entity Lookup and Validation) is ISC's DNSSEC-aware debugging tool, included with BIND. Unlike <code>drill</code>, <code>delv</code> does full DNSSEC validation internally rather than relying on the resolver:</p> <pre><code>delv example.com\n</code></pre> <pre><code>; fully validated\nexample.com.        86400   IN  A   93.184.216.34\nexample.com.        86400   IN  RRSIG   A 13 2 86400 ...\n</code></pre> <p>The key output is the first line: \"fully validated\" means the DNSSEC chain is intact. If validation fails, <code>delv</code> tells you exactly where:</p> <pre><code>delv @8.8.8.8 dnssec-failed.org\n</code></pre> <pre><code>;; resolution failed: SERVFAIL\n;; validating dnssec-failed.org/A: no valid signature found\n</code></pre> <p>Use <code>delv</code> when <code>dig</code> gives you a SERVFAIL and you suspect DNSSEC is the cause.</p> <p>When should you use delv instead of dig? (requires JavaScript)</p>"},{"location":"DNS%20Administration/dns-tools/#host-and-nslookup","title":"host and nslookup","text":""},{"location":"DNS%20Administration/dns-tools/#host","title":"host","text":"<p><code>host</code> is the simplest DNS lookup tool. Good for quick checks:</p> <pre><code>host example.com                    # A record lookup\nhost -t MX example.com              # specific record type\nhost -t TXT example.com             # TXT records\nhost 93.184.216.34                  # reverse lookup\nhost -a example.com                 # all records (verbose)\n</code></pre> <p><code>host</code> output is human-readable sentences rather than the structured format <code>dig</code> uses:</p> <pre><code>example.com has address 93.184.216.34\nexample.com has IPv6 address 2606:2800:21f:cb07:6820:80da:af6b:8b2c\nexample.com mail is handled by 0 .\n</code></pre>"},{"location":"DNS%20Administration/dns-tools/#nslookup","title":"nslookup","text":"<p><code>nslookup</code> is the oldest of these tools and has an interactive mode:</p> <pre><code>nslookup example.com                  # basic lookup\nnslookup -type=MX example.com         # specific record type\nnslookup example.com 8.8.8.8          # use a specific server\n</code></pre> <p>Interactive mode:</p> <pre><code>nslookup\n&gt; server 8.8.8.8\n&gt; set type=MX\n&gt; example.com\n&gt; exit\n</code></pre> <p><code>nslookup</code> uses its own resolver logic rather than the system resolver, which can give different results than <code>dig</code>. For troubleshooting, prefer <code>dig</code> - it gives you more control and more information.</p> <p>See also</p> <p>Many DNS tools overlap with general networking utilities. For broader coverage of <code>ip</code>, <code>ss</code>, <code>netstat</code>, and network troubleshooting, see Networking.</p>"},{"location":"DNS%20Administration/dns-tools/#whois","title":"whois","text":"<p><code>whois</code> queries domain registration databases. It tells you who owns a domain, when it was registered, when it expires, and which nameservers it uses.</p> <pre><code>whois example.com\n</code></pre> <pre><code>   Domain Name: EXAMPLE.COM\n   Registrar: RESERVED-Internet Assigned Numbers Authority\n   Creation Date: 1995-08-14T04:00:00Z\n   Registry Expiry Date: 2025-08-13T04:00:00Z\n   Name Server: A.IANA-SERVERS.NET\n   Name Server: B.IANA-SERVERS.NET\n   DNSSEC: signedDelegation\n</code></pre> <p>Key fields:</p> <ul> <li>Registrar - who the domain was registered through</li> <li>Name Server - the authoritative nameservers (this is what the TLD has on file)</li> <li>Registry Expiry Date - when the domain expires (expired domains stop resolving)</li> <li>DNSSEC - whether the domain has a signed delegation</li> </ul> <p>Domain status codes tell you about the domain's state:</p> Status Meaning <code>clientTransferProhibited</code> Registrar has locked the domain against transfers <code>serverTransferProhibited</code> Registry has locked the domain <code>clientDeleteProhibited</code> Domain can't be deleted <code>redemptionPeriod</code> Domain has expired and is in a grace period <code>pendingDelete</code> Domain will be released soon <p>RDAP (Registration Data Access Protocol) is the modern replacement for WHOIS. It returns structured JSON instead of free-form text and supports authenticated access. Most registries now serve RDAP, and tools are catching up.</p>"},{"location":"DNS%20Administration/dns-tools/#troubleshooting-playbook","title":"Troubleshooting Playbook","text":""},{"location":"DNS%20Administration/dns-tools/#domain-not-resolving","title":"Domain Not Resolving","text":"<p>When a domain doesn't resolve, work through this systematically:</p> <p>Step 1 - Check what the error actually is:</p> <pre><code>dig example.com\n</code></pre> <p>Look at <code>status:</code> in the header. <code>NXDOMAIN</code> means the name doesn't exist. <code>SERVFAIL</code> means the server couldn't process the query (often a DNSSEC issue). <code>REFUSED</code> means the server won't answer your query.</p> <p>Step 2 - Check if the domain is registered:</p> <pre><code>whois example.com | grep -i \"name server\\|status\\|expir\"\n</code></pre> <p>If the domain has expired or has no nameservers listed, that's your problem.</p> <p>Step 3 - Query the authoritative server directly:</p> <pre><code># Find the authoritative nameservers\ndig example.com NS +short\n\n# Query one of them directly\ndig @ns1.example.com example.com\n</code></pre> <p>If the authoritative server has the right answer, the problem is caching or connectivity between your resolver and the authoritative server.</p> <p>Step 4 - Trace the full resolution path:</p> <pre><code>dig +trace example.com\n</code></pre> <p>Watch for where the chain breaks. If the root and TLD respond correctly but the authoritative server doesn't, the problem is at the authoritative server. If the TLD returns the wrong nameservers, the NS records at the registrar are wrong.</p> <p>Step 5 - Check from multiple resolvers:</p> <pre><code>dig @8.8.8.8 example.com +short\ndig @1.1.1.1 example.com +short\ndig @9.9.9.9 example.com +short\n</code></pre> <p>If one resolver has the right answer and another doesn't, the wrong resolver has stale cached data.</p> <p>What is the difference between NOERROR with an empty answer and NXDOMAIN? (requires JavaScript)</p>"},{"location":"DNS%20Administration/dns-tools/#dns-changes-not-showing-up","title":"DNS Changes Not Showing Up","text":"<p>Step 1 - Verify the change is on the authoritative server:</p> <pre><code>dig @ns1.example.com example.com A +short\n</code></pre> <p>If the authoritative server doesn't have the new value, the zone file wasn't reloaded or the serial wasn't incremented.</p> <p>Step 2 - Check the TTL on the old cached answer:</p> <pre><code>dig example.com A\n</code></pre> <p>The TTL in the response tells you how many seconds until the cache expires. If it says 3600, you'll wait up to an hour.</p> <p>Step 3 - Check if secondaries are in sync:</p> <pre><code>dig @ns1.example.com example.com SOA +short\ndig @ns2.example.com example.com SOA +short\n</code></pre> <p>If the serial numbers don't match, zone transfer hasn't happened. Check that the primary is notifying the secondary and that zone transfers are allowed.</p> <p>DNS Troubleshooting Scenario (requires JavaScript)</p>"},{"location":"DNS%20Administration/dns-tools/#email-going-to-spam","title":"Email Going to Spam","text":"<p>Email deliverability issues are almost always DNS problems. Run this audit:</p> <pre><code># Step 1: Check MX records\ndig example.com MX +short\n# Expected: priority and hostname for your mail server(s)\n\n# Step 2: Check SPF\ndig example.com TXT +short | grep spf\n# Expected: \"v=spf1 ...\" listing your sending IPs/includes\n\n# Step 3: Check DKIM\ndig selector1._domainkey.example.com TXT +short\n# Expected: \"v=DKIM1; k=rsa; p=...\" with the public key\n# (replace \"selector1\" with your actual DKIM selector)\n\n# Step 4: Check DMARC\ndig _dmarc.example.com TXT +short\n# Expected: \"v=DMARC1; p=...\" with your policy\n\n# Step 5: Check reverse DNS on the sending IP\ndig -x 198.51.100.10 +short\n# Expected: hostname that matches or relates to your domain\n\n# Step 6: Verify the PTR hostname resolves back\ndig $(dig -x 198.51.100.10 +short) A +short\n# Expected: should resolve back to 198.51.100.10 (forward-confirmed reverse DNS)\n</code></pre> <p>If any step returns empty or wrong results, that's where to focus.</p>"},{"location":"DNS%20Administration/dns-tools/#servfail-responses","title":"SERVFAIL Responses","text":"<p>SERVFAIL is the most frustrating DNS error because it tells you almost nothing. Common causes:</p> <p>DNSSEC validation failure - the most common cause. Test by disabling validation:</p> <pre><code>dig example.com                    # SERVFAIL\ndig +cd example.com                # bypass DNSSEC validation\n</code></pre> <p>If <code>+cd</code> (checking disabled) gives you an answer, the problem is DNSSEC. See the DNSSEC guide for debugging.</p> <p>Authoritative server unreachable - the recursive resolver can't reach any authoritative server for the zone:</p> <pre><code>dig +trace example.com\n</code></pre> <p>Watch for timeouts in the trace output.</p> <p>Broken delegation - the TLD points to nameservers that don't actually serve the zone:</p> <pre><code># What the TLD says\ndig @a.gtld-servers.net example.com NS +short\n\n# Test if those servers actually respond\ndig @ns1.example.com example.com SOA\n</code></pre>"},{"location":"DNS%20Administration/dns-tools/#inconsistent-answers","title":"Inconsistent Answers","text":"<p>When different resolvers return different answers:</p> <pre><code>dig @8.8.8.8 example.com +short\ndig @1.1.1.1 example.com +short\ndig @ns1.example.com example.com +short\n</code></pre> <p>If the authoritative answer is correct but resolvers disagree, it's a caching issue - each resolver cached at a different time. Wait for TTL expiration.</p> <p>If authoritative servers disagree with each other, check zone transfer status:</p> <pre><code>dig @ns1.example.com example.com SOA +short\ndig @ns2.example.com example.com SOA +short\n</code></pre> <p>Different serial numbers mean the secondary is out of sync.</p> <p>Debug a Domain That Isn't Resolving (requires JavaScript)</p>"},{"location":"DNS%20Administration/dns-tools/#dns-as-a-covert-channel","title":"DNS as a Covert Channel","text":"<p>A security note: DNS can be used as a data exfiltration channel. Tools like <code>iodine</code> tunnel IP traffic inside DNS queries, encoding data in subdomain labels. An exfiltration query might look like:</p> <pre><code>dGhpcyBpcyBzZWNyZXQ.tunnel.attacker.com\n</code></pre> <p>That base64-encoded subdomain carries data to the attacker's authoritative server for <code>tunnel.attacker.com</code>. This works even on heavily firewalled networks because DNS (port 53) is almost never blocked.</p> <p>If you run a recursive resolver, watch for unusually long query names, high query volumes to a single domain, or queries with high-entropy labels. These are signatures of DNS tunneling.</p>"},{"location":"DNS%20Administration/dns-tools/#further-reading","title":"Further Reading","text":"<ul> <li>ISC BIND / dig - dig is distributed with BIND</li> <li>ldns / drill - DNS library and drill tool from NLnet Labs</li> <li>iodine - IP over DNS tunnel</li> <li>DNSViz - DNSSEC visualization and analysis</li> <li>RDAP - modern replacement for WHOIS</li> <li>Cloudflare Learning Center: DNS Troubleshooting - visual DNS explanations</li> </ul> <p>Previous: Zone Files and Records | Next: BIND | Back to Index</p>"},{"location":"DNS%20Administration/dnssec/","title":"DNSSEC","text":"<p>This guide covers DNSSEC - the system that adds cryptographic authentication to DNS. You'll learn why it was created, how the trust chain works from the root zone down to individual records, how to sign zones with BIND, NSD, and PowerDNS, and how to debug validation failures.</p>"},{"location":"DNS%20Administration/dnssec/#why-dnssec-exists","title":"Why DNSSEC Exists","text":"<p>DNS was designed in 1983 with no authentication whatsoever. When a resolver receives a DNS response, it has no way to verify that the answer actually came from the authoritative server and wasn't modified in transit. This is a problem because DNS is infrastructure - every HTTPS connection, email delivery, and API call starts with a DNS query. If an attacker can forge DNS responses, they can redirect traffic anywhere.</p>"},{"location":"DNS%20Administration/dnssec/#the-kaminsky-attack","title":"The Kaminsky Attack","text":"<p>DNS cache poisoning attacks had been known for years, but in 2008, security researcher Dan Kaminsky discovered a technique that made them devastatingly practical. The attack exploited the way resolvers handle referrals to flood a resolver with forged responses, each trying a different transaction ID. Because the transaction ID was only 16 bits (65,536 possibilities), a high-speed attacker could guess the right ID within seconds and inject a poisoned cache entry.</p> <p>Kaminsky recognized the severity and did something unusual - instead of publishing immediately, he coordinated a massive multi-vendor patching effort through the Department of Homeland Security's US-CERT. DNS software vendors quietly developed and deployed source port randomization patches (adding ~16 bits of entropy) before the vulnerability was publicly disclosed. Kaminsky presented the full details at Black Hat 2008, famously wearing rollerskates on stage.</p> <p>The Kaminsky attack was mitigated but not eliminated by source port randomization. DNSSEC is the real fix - it makes forged responses cryptographically detectable, regardless of how clever the spoofing technique is.</p>"},{"location":"DNS%20Administration/dnssec/#what-dnssec-does-and-doesnt-do","title":"What DNSSEC Does (and Doesn't Do)","text":"<p>DNSSEC provides: - Authentication - proof that the response came from the zone owner - Integrity - proof that the response wasn't modified in transit - Authenticated denial of existence - proof that a name genuinely doesn't exist (not a forged NXDOMAIN)</p> <p>DNSSEC does NOT provide: - Confidentiality - DNS queries and responses are still plaintext (that's what DoH/DoT are for) - Protection against a compromised authoritative server - if the attacker controls the zone data and signing keys, DNSSEC signs the attacker's data - DDoS protection - DNSSEC responses are larger than unsigned ones, potentially worsening amplification</p>"},{"location":"DNS%20Administration/dnssec/#the-dnssec-trust-chain","title":"The DNSSEC Trust Chain","text":"<p>DNSSEC works by creating a chain of cryptographic trust from the root zone down to the record you're querying. Each link in the chain is verified by the one above it.</p>"},{"location":"DNS%20Administration/dnssec/#root-zone-signing","title":"Root Zone Signing","text":"<p>The root zone was fully signed with DNSSEC on July 15, 2010. The signing ceremony was (and continues to be) one of the most carefully controlled processes in internet governance.</p> <p>The ICANN root KSK ceremony requires approximately 12-14 people from around the world to physically assemble at one of two secure facilities (in Culver City, California or El Segundo, Virginia). Trusted Community Representatives unlock safe deposit boxes containing smart cards. Multiple HSMs (Hardware Security Modules) are involved. The ceremony follows a rigid script, is livestreamed, and typically takes 3-8 hours. A complete audit trail is published. The entire point is that no single person or organization can sign the root zone alone.</p>"},{"location":"DNS%20Administration/dnssec/#how-the-chain-works","title":"How the Chain Works","text":"<p>Every signed zone has: 1. A DNSKEY record containing the zone's public key(s) 2. RRSIG records - cryptographic signatures over each record set 3. A DS (Delegation Signer) record in the parent zone that hashes the child's key</p> <p>The chain works like this:</p> <pre><code>graph TD\n    TA[\"Trust Anchor&lt;br/&gt;(built into resolver)\"]\n    TA --&gt;|\"verifies\"| RootKey\n\n    subgraph root [\"Root Zone (.)\"]\n        RootKey[\"DNSKEY (KSK + ZSK)\"]\n        RootKey --&gt;|\"signs\"| RootDS[\"DS for .com\"]\n        RootKey --&gt;|\"signs\"| RootRRSIG[\"RRSIG over all root records\"]\n    end\n\n    RootDS --&gt;|\"hash matches\"| ComKey\n\n    subgraph dotcom [\".com Zone\"]\n        ComKey[\"DNSKEY (KSK + ZSK)\"]\n        ComKey --&gt;|\"signs\"| ComDS[\"DS for example.com\"]\n        ComKey --&gt;|\"signs\"| ComRRSIG[\"RRSIG over all .com records\"]\n    end\n\n    ComDS --&gt;|\"hash matches\"| ExKey\n\n    subgraph example [\"example.com Zone\"]\n        ExKey[\"DNSKEY (KSK + ZSK)\"]\n        ExKey --&gt;|\"signs\"| ExRRSIG[\"RRSIG over A record\"]\n        ExRRSIG --&gt;|\"validates\"| Answer\n    end\n\n    Answer[\"www.example.com A 93.184.216.34\"]</code></pre> <p>A validating resolver starts at the root (whose key it already knows - the \"trust anchor\") and follows the chain down. At each level:</p> <ol> <li>Fetch the DNSKEY for the zone</li> <li>Verify it matches the DS record in the parent zone</li> <li>Use the DNSKEY to verify the RRSIG signatures on the records</li> <li>If any step fails, return SERVFAIL instead of the (potentially forged) answer</li> </ol> <p>How does a DNSSEC validator verify that a response is authentic? (requires JavaScript)</p> <p>The DS record is the critical link between zones. Without it, a resolver has no way to verify that a child zone's DNSKEY is legitimate. When you register DNSSEC with your registrar, you're providing the DS record (a hash of your KSK) to be placed in the parent zone. This creates a verifiable path from the parent's signature down to your zone's keys.</p> <pre><code>flowchart TD\n    Root[\"Root Zone '.'\"] --&gt;|\"DS record for .com\"| TLD[\"TLD '.com'\"]\n    TLD --&gt;|\"DS record for example.com\"| Domain[\"Zone 'example.com'\"]\n    Domain --&gt;|\"DNSKEY signs\"| RR[\"RRSIG on A record\"]\n\n    Root --&gt;|\"DNSKEY self-signed&lt;br/&gt;by trust anchor\"| RootKey[\"Root KSK\"]\n    TLD --&gt;|\"DNSKEY signed by&lt;br/&gt;parent DS\"| TLDKey[\".com KSK\"]\n    Domain --&gt;|\"DNSKEY signed by&lt;br/&gt;parent DS\"| DomKey[\"example.com KSK\"]</code></pre> <p>What role does the DS (Delegation Signer) record play in the DNSSEC chain of trust? (requires JavaScript)</p>"},{"location":"DNS%20Administration/dnssec/#dnssec-record-types","title":"DNSSEC Record Types","text":""},{"location":"DNS%20Administration/dnssec/#dnskey","title":"DNSKEY","text":"<p>The DNSKEY record publishes a zone's public key. A zone typically has two keys:</p> <p>KSK (Key Signing Key) - flag value 257. Signs only the DNSKEY record set. The KSK's hash is published as a DS record in the parent zone. KSKs are changed infrequently (every 1-2 years).</p> <p>ZSK (Zone Signing Key) - flag value 256. Signs all other record sets in the zone. ZSKs are changed more frequently (every 1-3 months) because they're used heavily and a compromise would be more impactful.</p> <p>Some implementations use a single CSK (Combined Signing Key) - flag value 257 - that does both jobs. PowerDNS defaults to this approach.</p>"},{"location":"DNS%20Administration/dnssec/#rrsig","title":"RRSIG","text":"<p>An RRSIG record is a cryptographic signature over a specific record set (all records of the same name and type). For every A record, MX record, etc., there's a corresponding RRSIG.</p> <pre><code>dig +dnssec example.com A\n</code></pre> <pre><code>;; ANSWER SECTION:\nexample.com.    86400   IN  A       93.184.216.34\nexample.com.    86400   IN  RRSIG   A 13 2 86400 20250215000000 20250201000000 31406 example.com. abc123...\n</code></pre> <p>The RRSIG fields include the algorithm (13 = ECDSAP256SHA256), the number of labels (2), the original TTL, signature expiration and inception timestamps, the key tag (identifying which DNSKEY made the signature), and the signature itself.</p> <p>RRSIG signatures have expiration dates. If signatures expire and aren't refreshed, DNSSEC validation fails and the domain stops resolving for validating resolvers. This is the most common DNSSEC operational failure. In October 2023, Cloudflare experienced a 3-hour outage for some domains due to expired DNSSEC signatures that went undetected.</p>"},{"location":"DNS%20Administration/dnssec/#ds","title":"DS","text":"<p>A DS (Delegation Signer) record in the parent zone links to the child zone's DNSKEY. It's a hash of the child's KSK. You submit the DS record to your registrar, who publishes it in the parent zone (e.g., <code>.com</code>).</p> <pre><code>dig DS example.com +short\n</code></pre> <pre><code>31406 13 2 abc123def456789...\n</code></pre> <p>The fields are: key tag, algorithm, digest type (2 = SHA-256), and the digest.</p>"},{"location":"DNS%20Administration/dnssec/#nsec-and-nsec3","title":"NSEC and NSEC3","text":"<p>How do you prove that a name doesn't exist? You can't sign a record that doesn't exist. DNSSEC solves this with NSEC (Next Secure) records.</p> <p>An NSEC record says \"the next name that exists after this one is X.\" By chaining all existing names together, a resolver can verify that a queried name falls in a gap and genuinely doesn't exist.</p> <pre><code>graph LR\n    A[\"alpha.example.com&lt;br/&gt;NSEC \u2192 beta\"]\n    B[\"beta.example.com&lt;br/&gt;NSEC \u2192 delta\"]\n    D[\"delta.example.com&lt;br/&gt;NSEC \u2192 mail\"]\n    M[\"mail.example.com&lt;br/&gt;NSEC \u2192 www\"]\n    W[\"www.example.com&lt;br/&gt;NSEC \u2192 alpha\"]\n\n    A --&gt; B --&gt; D --&gt; M --&gt; W --&gt; A\n\n    Q{{\"Query: charlie.example.com\"}}\n    Q -.-&gt;|\"falls between&lt;br/&gt;beta and delta\"| GAP[\"NSEC proves: no name&lt;br/&gt;exists between beta and delta&lt;br/&gt;\u2192 NXDOMAIN is authentic\"]</code></pre> <p>The problem with NSEC is zone walking. An attacker can follow the NSEC chain to enumerate every name in a zone:</p> <pre><code># Start with a query for a name that doesn't exist\ndig @ns1.example.com nonexistent.example.com\n\n# The NSEC record reveals the next existing name\n# Query for a name just after that, get another NSEC, repeat...\n</code></pre> <p>NSEC3 was created to prevent zone walking. Instead of listing names directly, NSEC3 uses hashed names. The chain still proves non-existence, but an attacker can't read the actual names from the hashes (though offline dictionary attacks against the hashes are possible).</p> <p>Most modern zones use NSEC3 for zone walking prevention.</p>"},{"location":"DNS%20Administration/dnssec/#key-management-ksk-and-zsk","title":"Key Management: KSK and ZSK","text":""},{"location":"DNS%20Administration/dnssec/#why-two-keys","title":"Why Two Keys?","text":"<p>Using separate KSK and ZSK keys serves a practical purpose. The DS record in the parent zone is a hash of the KSK. Changing the KSK requires updating the DS record at the registrar - a manual, error-prone process. The ZSK, on the other hand, can be changed without touching the parent zone.</p> <p>This separation means you can rotate ZSKs frequently (for security) without the operational burden of updating DS records each time. KSKs rotate much less often.</p> <p>Why does DNSSEC use two types of keys (ZSK and KSK) instead of just one? (requires JavaScript)</p>"},{"location":"DNS%20Administration/dnssec/#algorithm-recommendations","title":"Algorithm Recommendations","text":"<p>The current recommended algorithm is ECDSAP256SHA256 (algorithm 13). It produces much smaller signatures than RSA, reducing DNS response sizes and improving performance. Older zones may use RSA (algorithms 5, 7, 8), but new deployments should use algorithm 13.</p>"},{"location":"DNS%20Administration/dnssec/#rollover-procedures","title":"Rollover Procedures","text":"<pre><code>gantt\n    title ZSK Pre-Publish Rollover\n    dateFormat YYYY-MM-DD\n    axisFormat %b %d\n    section Old ZSK\n        Active (signing)          :done, ozsk1, 2025-01-01, 30d\n        Still published           :done, ozsk2, after ozsk1, 14d\n        Removed                   :milestone, after ozsk2, 0d\n    section New ZSK\n        Published (not signing)   :active, nzsk1, 2025-01-17, 14d\n        Active (signing)          :nzsk2, after nzsk1, 90d\n    section DNSKEY TTL\n        Wait for cache expiry     :crit, ttl1, 2025-01-17, 14d</code></pre> <pre><code>gantt\n    title KSK Double-DS Rollover\n    dateFormat YYYY-MM-DD\n    axisFormat %b %d\n    section Old KSK\n        Active                    :done, oksk1, 2025-01-01, 60d\n        Removed from DNSKEY       :milestone, 2025-03-02, 0d\n    section New KSK\n        Published in DNSKEY       :active, nksk1, 2025-01-15, 46d\n        Active                    :nksk2, after nksk1, 365d\n    section DS Records\n        Old DS in parent          :done, ods, 2025-01-01, 60d\n        New DS submitted          :crit, nds1, 2025-01-15, 14d\n        Both DS in parent         :nds2, 2025-01-29, 18d\n        Old DS removed            :milestone, 2025-02-16, 0d</code></pre> <p>ZSK rollover (pre-publish method): 1. Generate the new ZSK and publish it in DNSKEY (but don't sign with it yet) 2. Wait for the old DNSKEY TTL to expire (so all resolvers have the new key) 3. Start signing with the new ZSK 4. Remove the old ZSK signatures 5. Remove the old ZSK from DNSKEY</p> <p>KSK rollover (double-DS method): 1. Generate the new KSK and publish it in DNSKEY 2. Submit the new DS record to the registrar (keep the old one too) 3. Wait for the parent zone to publish the new DS 4. Wait for the old DS TTL to expire 5. Remove the old KSK from DNSKEY 6. Remove the old DS from the parent</p> <p>Key rollovers are the most operationally risky part of DNSSEC. Automate them whenever possible.</p> <p>What makes a KSK rollover more complex than a ZSK rollover? (requires JavaScript)</p> <p>ZSK Rollover Timeline (Pre-Publication Method) (requires JavaScript)</p>"},{"location":"DNS%20Administration/dnssec/#signing-with-bind","title":"Signing with BIND","text":""},{"location":"DNS%20Administration/dnssec/#automated-signing-with-dnssec-policy-bind-916","title":"Automated Signing with dnssec-policy (BIND 9.16+)","text":"<p>BIND 9.16 introduced <code>dnssec-policy</code>, which fully automates DNSSEC signing, key generation, and rollover. This is the recommended approach.</p> <pre><code>// named.conf\n\ndnssec-policy \"standard\" {\n    keys {\n        ksk key-directory lifetime P2Y algorithm ecdsap256sha256;\n        zsk key-directory lifetime P3M algorithm ecdsap256sha256;\n    };\n    nsec3param iterations 0 optout no salt-length 0;\n    // Timing parameters (automatic rollover)\n    dnskey-ttl PT1H;\n    publish-safety PT1H;\n    retire-safety PT1H;\n    signatures-refresh P5D;\n    signatures-validity P14D;\n    signatures-validity-dnskey P14D;\n    max-zone-ttl P1D;\n    zone-propagation-delay PT5M;\n    parent-ds-ttl P1D;\n    parent-propagation-delay PT1H;\n};\n\nzone \"example.com\" {\n    type primary;\n    file \"zones/example.com.zone\";\n    dnssec-policy \"standard\";\n    inline-signing yes;\n};\n</code></pre> <p>With this configuration, BIND: - Generates KSK and ZSK keys automatically - Signs all records in the zone - Re-signs before signatures expire - Rolls ZSKs every 3 months and KSKs every 2 years - Publishes new keys in advance of rollover</p> <p>After enabling, check the signing status:</p> <pre><code>rndc signing -list example.com\nrndc zonestatus example.com\n</code></pre> <p>Get the DS record to submit to your registrar:</p> <pre><code>dig @localhost example.com DNSKEY | dnssec-dsfromkey -2 -f - example.com\n</code></pre>"},{"location":"DNS%20Administration/dnssec/#manual-signing-legacy","title":"Manual Signing (Legacy)","text":"<p>For older BIND versions or when you need explicit control:</p> <pre><code># Generate keys\ndnssec-keygen -a ECDSAP256SHA256 -f KSK example.com\ndnssec-keygen -a ECDSAP256SHA256 example.com\n\n# Include keys in zone file\n$INCLUDE Kexample.com.+013+31406.key\n$INCLUDE Kexample.com.+013+52918.key\n\n# Sign the zone\ndnssec-signzone -A -3 $(head -c 8 /dev/urandom | od -A n -t x | tr -d ' ') \\\n    -N INCREMENT -o example.com -t zones/example.com.zone\n</code></pre> <p>This generates a signed zone file (<code>example.com.zone.signed</code>) that must be referenced in <code>named.conf</code>. You need to re-sign before the signatures expire - typically via a cron job. <code>dnssec-policy</code> eliminates all of this manual work.</p> <p>Generating DNSSEC Keys, Signing a Zone, and Verifying (requires JavaScript)</p>"},{"location":"DNS%20Administration/dnssec/#signing-with-nsd","title":"Signing with NSD","text":"<p>NSD itself doesn't sign zones. You use external tools from the ldns library:</p> <pre><code># Generate keys\nldns-keygen -a ECDSAP256SHA256 -k example.com    # KSK\nldns-keygen -a ECDSAP256SHA256 example.com        # ZSK\n\n# Sign the zone\nldns-signzone -n -p -s $(head -c 8 /dev/urandom | od -A n -t x | tr -d ' ') \\\n    example.com.zone Kexample.com.+013+*.private\n</code></pre> <p>Configure NSD to serve the signed zone:</p> <pre><code>zone:\n    name: \"example.com\"\n    zonefile: \"example.com.zone.signed\"\n</code></pre> <p>You'll need a cron job or script to re-sign periodically:</p> <pre><code># Re-sign weekly (signatures typically valid 30 days)\n0 3 * * 0 ldns-signzone -n -p example.com.zone Kexample.com.*.private &amp;&amp; nsd-control reload example.com\n</code></pre>"},{"location":"DNS%20Administration/dnssec/#signing-with-powerdns","title":"Signing with PowerDNS","text":"<p>PowerDNS has the simplest DNSSEC signing workflow:</p> <pre><code># Sign a zone (generates keys, signs everything)\npdnsutil secure-zone example.com\n\n# View DNSSEC status\npdnsutil show-zone example.com\n\n# Get the DS record for your registrar\npdnsutil export-zone-ds example.com\n</code></pre> <p>PowerDNS signs records on-the-fly as they're served, using keys stored in the database. There's no separate signed zone file and no need for re-signing cron jobs. When you add or modify records, they're automatically signed.</p> <p>Rectify the zone after direct database modifications:</p> <pre><code>pdnsutil rectify-zone example.com\n</code></pre>"},{"location":"DNS%20Administration/dnssec/#validating-dnssec","title":"Validating DNSSEC","text":""},{"location":"DNS%20Administration/dnssec/#dig-dnssec","title":"dig +dnssec","text":"<pre><code>dig +dnssec example.com A\n</code></pre> <p>Look for the <code>ad</code> flag in the response header:</p> <pre><code>;; flags: qr rd ra ad; QUERY: 1, ANSWER: 2\n</code></pre> <p><code>ad</code> (Authenticated Data) means your resolver validated the DNSSEC signatures. If <code>ad</code> is absent, either the zone isn't signed or your resolver isn't validating.</p> <p>The response includes RRSIG records alongside the answer:</p> <pre><code>;; ANSWER SECTION:\nexample.com.    86400   IN  A       93.184.216.34\nexample.com.    86400   IN  RRSIG   A 13 2 86400 ...\n</code></pre>"},{"location":"DNS%20Administration/dnssec/#delv","title":"delv","text":"<p><code>delv</code> does its own validation, independent of your resolver:</p> <pre><code>delv example.com A\n</code></pre> <pre><code>; fully validated\nexample.com.    86400   IN  A   93.184.216.34\nexample.com.    86400   IN  RRSIG   A 13 2 86400 ...\n</code></pre> <p>\"fully validated\" means the entire trust chain checked out.</p>"},{"location":"DNS%20Administration/dnssec/#dig-cd-checking-disabled","title":"dig +cd (Checking Disabled)","text":"<p>When you suspect DNSSEC is causing a SERVFAIL, use <code>+cd</code> to bypass validation:</p> <pre><code>dig example.com A              # SERVFAIL (DNSSEC issue)\ndig +cd example.com A          # returns the answer, skipping validation\n</code></pre> <p>If <code>+cd</code> gives you an answer when the normal query returns SERVFAIL, the problem is definitely DNSSEC.</p>"},{"location":"DNS%20Administration/dnssec/#online-tools","title":"Online Tools","text":"<ul> <li>DNSViz - visualizes the entire DNSSEC trust chain as a graph, showing exactly where validation succeeds or fails</li> <li>Verisign DNSSEC Debugger - tests delegation and signing from root to your zone</li> </ul>"},{"location":"DNS%20Administration/dnssec/#common-dnssec-failures","title":"Common DNSSEC Failures","text":""},{"location":"DNS%20Administration/dnssec/#expired-signatures","title":"Expired Signatures","text":"<p>RRSIG records have inception and expiration timestamps. If the current time is outside that window, validation fails.</p> <p>Diagnosis:</p> <pre><code>dig +dnssec example.com A | grep RRSIG\n</code></pre> <p>Check the expiration timestamp. If it's in the past, signatures need to be refreshed.</p> <p>Fix: Re-sign the zone (or fix the automated signing process). If using <code>dnssec-policy</code> in BIND, check that the signing process is running. If using PowerDNS, this shouldn't happen unless the server was down for an extended period.</p>"},{"location":"DNS%20Administration/dnssec/#dsdnskey-mismatch","title":"DS/DNSKEY Mismatch","text":"<p>The DS record in the parent zone doesn't match any DNSKEY in the child zone. This happens during botched key rollovers.</p> <p>Diagnosis:</p> <pre><code># Get the DS from the parent\ndig DS example.com +short\n\n# Get the DNSKEYs from the authoritative server\ndig @ns1.example.com DNSKEY example.com\n\n# Compare - the DS key tag should match a DNSKEY key tag\n</code></pre> <p>Fix: Either update the DS record at the registrar to match the current DNSKEY, or add the DNSKEY that matches the existing DS record back to the zone.</p>"},{"location":"DNS%20Administration/dnssec/#clock-skew","title":"Clock Skew","text":"<p>DNSSEC signatures are time-bounded. If your server's clock is significantly wrong, valid signatures appear expired (or not yet valid).</p> <p>Diagnosis:</p> <pre><code>date              # check current time\ndig +dnssec example.com | grep RRSIG   # check inception/expiration\n</code></pre> <p>Fix: Synchronize your clock with NTP:</p> <pre><code>timedatectl set-ntp true\n</code></pre>"},{"location":"DNS%20Administration/dnssec/#algorithm-downgrade","title":"Algorithm Downgrade","text":"<p>If the DS record specifies algorithm 13 but the DNSKEY uses algorithm 8 (or vice versa), validation fails.</p> <p>Diagnosis:</p> <pre><code>dig DS example.com +short      # shows algorithm number\ndig DNSKEY example.com +short  # shows algorithm number\n</code></pre> <p>The algorithm numbers must match between the DS and the DNSKEY it references.</p>"},{"location":"DNS%20Administration/dnssec/#debugging-workflow","title":"Debugging Workflow","text":"<p>When a domain returns SERVFAIL and you suspect DNSSEC:</p> <pre><code># Step 1: Confirm it's a DNSSEC issue\ndig example.com                    # SERVFAIL\ndig +cd example.com                # works? Then it's DNSSEC\n\n# Step 2: Check the trust chain\ndelv example.com                   # shows where validation fails\n# OR\ndrill -S example.com               # traces the entire chain\n\n# Step 3: Check specific components\ndig +dnssec example.com DNSKEY     # are DNSKEYs present?\ndig DS example.com                 # is the DS record correct?\ndig +dnssec example.com A          # are RRSIGs present and current?\n\n# Step 4: Use DNSViz for visual analysis\n# Visit https://dnsviz.net/ and enter the domain\n</code></pre> <p>Command Builder (requires JavaScript)</p>"},{"location":"DNS%20Administration/dnssec/#further-reading","title":"Further Reading","text":"<ul> <li>RFC 4033 - DNS Security Introduction and Requirements</li> <li>RFC 4034 - Resource Records for DNS Security Extensions</li> <li>RFC 4035 - Protocol Modifications for DNS Security Extensions</li> <li>RFC 5155 - DNS Security (DNSSEC) Hashed Authenticated Denial of Existence (NSEC3)</li> <li>DNSViz - DNSSEC visualization and analysis</li> <li>Verisign DNSSEC Debugger - online DNSSEC validation testing</li> <li>ICANN Root KSK Ceremonies - ceremony scripts and documentation</li> </ul> <p>Previous: PowerDNS | Next: DNS Architecture and Operations | Back to Index</p>"},{"location":"DNS%20Administration/nsd-and-unbound/","title":"NSD and Unbound","text":"<p>This guide covers NLnet Labs' approach to DNS - purpose-built software for each role. NSD handles authoritative serving, Unbound handles recursive resolution, and splitting them eliminates an entire class of configuration mistakes and security vulnerabilities.</p>"},{"location":"DNS%20Administration/nsd-and-unbound/#the-nlnet-labs-approach","title":"The NLnet Labs Approach","text":"<p>NLnet Labs is a Dutch non-profit that develops open-source DNS software. Their core philosophy is that authoritative and recursive DNS are fundamentally different tasks that should be handled by different software.</p> <p>BIND does both. NLnet Labs argues this is like having one program that's both a web server and a web browser - technically possible, but mixing the roles increases complexity, attack surface, and the chance of misconfiguration. An authoritative server that accidentally enables recursion becomes an open resolver. A recursive server with a zone file bug can stop resolving entirely.</p> <p>NLnet Labs' solution:</p> <ul> <li>NSD (Name Server Daemon) - authoritative only, reads zone files, serves answers</li> <li>Unbound - recursive only, chases referrals, caches, validates DNSSEC</li> </ul> <p>You run them together: NSD serves your zones, Unbound handles recursion for your clients. You can run them on the same machine (different ports or IPs) or on different machines.</p> <p>When to choose NSD + Unbound over BIND:</p> <ul> <li>You want clear separation of concerns</li> <li>You need maximum authoritative performance (NSD achieves roughly 10x the query throughput of BIND for authoritative serving, because it never has to handle recursion logic)</li> <li>You want DNSSEC validation enabled by default with minimal configuration</li> <li>You want to minimize attack surface</li> </ul> <p>When BIND is better:</p> <ul> <li>You need views (split-horizon) on the authoritative side (NSD doesn't support views)</li> <li>You need RPZ</li> <li>You want one software to manage</li> </ul> <p>Why might you choose NSD over BIND for an authoritative DNS server? (requires JavaScript)</p>"},{"location":"DNS%20Administration/nsd-and-unbound/#nsd-authoritative-dns","title":"NSD: Authoritative DNS","text":""},{"location":"DNS%20Administration/nsd-and-unbound/#installation","title":"Installation","text":"<p>RHEL / AlmaLinux / Rocky:</p> <pre><code>sudo dnf install nsd\nsudo systemctl enable --now nsd\n</code></pre> <p>Debian / Ubuntu:</p> <pre><code>sudo apt install nsd\nsudo systemctl enable --now nsd\n</code></pre>"},{"location":"DNS%20Administration/nsd-and-unbound/#nsdconf-structure","title":"nsd.conf Structure","text":"<p>NSD's configuration lives in <code>/etc/nsd/nsd.conf</code> and follows a clean block syntax:</p> <pre><code># /etc/nsd/nsd.conf\n\nserver:\n    # Network\n    ip-address: 198.51.100.1\n    ip-address: 198.51.100.1@5353    # alternate port\n    port: 53\n\n    # Server identity\n    server-count: 2                   # worker processes (match CPU cores)\n    hide-version: yes\n    identity: \"\"\n\n    # Zone storage\n    zonesdir: \"/etc/nsd/zones\"\n    zonelistfile: \"/var/lib/nsd/zone.list\"\n\n    # Database\n    database: \"\"                      # empty = use zone files directly (NSD 4.x+)\n\n    # Logging\n    logfile: \"/var/log/nsd/nsd.log\"\n    verbosity: 1\n\n# Zones defined below or in included files\ninclude: \"/etc/nsd/zones.conf\"\n</code></pre>"},{"location":"DNS%20Administration/nsd-and-unbound/#key-differences-from-bind","title":"Key Differences from BIND","text":"Feature BIND NSD Recursion Yes No Zone file format Standard Standard (identical) Configuration syntax C-like blocks YAML-like key-value Reload behavior Re-parses zone files Compiles zones to optimized format Views / split-horizon Yes No RPZ Yes No Memory usage Higher Lower Authoritative throughput Baseline ~10x higher <p>NSD uses the exact same zone file format as BIND - you can copy zone files directly between them.</p>"},{"location":"DNS%20Administration/nsd-and-unbound/#zone-definitions","title":"Zone Definitions","text":"<pre><code># /etc/nsd/zones.conf\n\nzone:\n    name: \"example.com\"\n    zonefile: \"example.com.zone\"\n</code></pre> <p>The zone file goes in the <code>zonesdir</code> directory (e.g., <code>/etc/nsd/zones/example.com.zone</code>). The format is identical to BIND - see the Zone Files and Records guide.</p>"},{"location":"DNS%20Administration/nsd-and-unbound/#validation-and-reload","title":"Validation and Reload","text":"<pre><code># Check configuration syntax\nnsd-checkconf /etc/nsd/nsd.conf\n\n# Check a zone file\nnsd-checkzone example.com /etc/nsd/zones/example.com.zone\n\n# Reload a zone after editing\nnsd-control reload example.com\n\n# Reload all zones\nnsd-control reload\n\n# Force re-read of nsd.conf (picks up new zones)\nnsd-control reconfig\n</code></pre>"},{"location":"DNS%20Administration/nsd-and-unbound/#nsd-control-commands","title":"nsd-control Commands","text":"Command Action <code>nsd-control status</code> Show server status <code>nsd-control reload</code> Reload all zones <code>nsd-control reload example.com</code> Reload a specific zone <code>nsd-control reconfig</code> Re-read config, add/remove zones <code>nsd-control write</code> Write zone data to zone files <code>nsd-control stats</code> Show query statistics <code>nsd-control stats_noreset</code> Show stats without resetting counters <code>nsd-control zonestatus example.com</code> Show zone details <code>nsd-control transfer example.com</code> Force a zone transfer (for secondaries) <code>nsd-control force_transfer example.com</code> Transfer even if serial hasn't changed <code>nsd-control log_reopen</code> Reopen log files (for log rotation)"},{"location":"DNS%20Administration/nsd-and-unbound/#nsd-primarysecondary","title":"NSD Primary/Secondary","text":""},{"location":"DNS%20Administration/nsd-and-unbound/#primary-configuration","title":"Primary Configuration","text":"<p>On the primary, allow zone transfers and send notifications:</p> <pre><code>zone:\n    name: \"example.com\"\n    zonefile: \"example.com.zone\"\n    provide-xfr: 203.0.113.2 NOKEY          # allow transfers to this IP\n    notify: 203.0.113.2 NOKEY               # notify this IP on changes\n</code></pre>"},{"location":"DNS%20Administration/nsd-and-unbound/#secondary-configuration","title":"Secondary Configuration","text":"<p>On the secondary:</p> <pre><code>zone:\n    name: \"example.com\"\n    zonefile: \"example.com.zone\"\n    request-xfr: 198.51.100.1 NOKEY         # pull zone from this primary\n    allow-notify: 198.51.100.1 NOKEY        # accept NOTIFY from primary\n</code></pre>"},{"location":"DNS%20Administration/nsd-and-unbound/#tsig-authentication","title":"TSIG Authentication","text":"<p>Generate a TSIG key:</p> <pre><code># Using ldns-keygen (from the ldns package)\nldns-keygen -a hmac-sha256 -b 256 example-transfer-key\n</code></pre> <p>Or generate manually:</p> <pre><code>dd if=/dev/urandom bs=32 count=1 2&gt;/dev/null | base64\n</code></pre> <p>Add the key to both servers' <code>nsd.conf</code>:</p> <pre><code>key:\n    name: \"example-transfer-key\"\n    algorithm: hmac-sha256\n    secret: \"jF3K8vQ2+xN7wP5dR9mB0kT4yH1cA6uZ...\"\n</code></pre> <p>Reference the key in zone definitions:</p> <p>Primary:</p> <pre><code>zone:\n    name: \"example.com\"\n    zonefile: \"example.com.zone\"\n    provide-xfr: 203.0.113.2 example-transfer-key\n    notify: 203.0.113.2 example-transfer-key\n</code></pre> <p>Secondary:</p> <pre><code>zone:\n    name: \"example.com\"\n    zonefile: \"example.com.zone\"\n    request-xfr: 198.51.100.1 example-transfer-key\n    allow-notify: 198.51.100.1 example-transfer-key\n</code></pre> <p>Test the transfer:</p> <pre><code># On the secondary\nnsd-control force_transfer example.com\nnsd-control zonestatus example.com\n</code></pre> <p>In an NSD primary/secondary setup, where must the TSIG key be configured? (requires JavaScript)</p> <p>Configure NSD Primary/Secondary with TSIG (requires JavaScript)</p>"},{"location":"DNS%20Administration/nsd-and-unbound/#unbound-recursive-dns","title":"Unbound: Recursive DNS","text":"<p>Unbound is a validating, recursive, caching DNS resolver. It validates DNSSEC by default, supports DNS over TLS (DoT), DNS over HTTPS (DoH), and experimentally DNS over QUIC (DoQ).</p>"},{"location":"DNS%20Administration/nsd-and-unbound/#installation_1","title":"Installation","text":"<p>RHEL / AlmaLinux / Rocky:</p> <pre><code>sudo dnf install unbound\nsudo systemctl enable --now unbound\n</code></pre> <p>Debian / Ubuntu:</p> <pre><code>sudo apt install unbound\nsudo systemctl enable --now unbound\n</code></pre>"},{"location":"DNS%20Administration/nsd-and-unbound/#basic-caching-resolver","title":"Basic Caching Resolver","text":"<pre><code># /etc/unbound/unbound.conf\n\nserver:\n    # Network\n    interface: 0.0.0.0\n    interface: ::0\n    port: 53\n\n    # Access control\n    access-control: 127.0.0.0/8 allow\n    access-control: 192.168.0.0/16 allow\n    access-control: 10.0.0.0/8 allow\n    access-control: 0.0.0.0/0 refuse           # deny everyone else\n\n    # Performance\n    num-threads: 2                               # match CPU cores\n    msg-cache-size: 64m\n    rrset-cache-size: 128m                       # should be 2x msg-cache-size\n    cache-max-ttl: 86400\n    cache-min-ttl: 0\n\n    # Privacy and security\n    hide-identity: yes\n    hide-version: yes\n    harden-glue: yes\n    harden-dnssec-stripped: yes\n    use-caps-for-id: yes                         # 0x20-encoded randomization\n\n    # DNSSEC - enabled by default with auto-trust-anchor-file\n    auto-trust-anchor-file: \"/var/lib/unbound/root.key\"\n\n    # Logging\n    verbosity: 1\n    logfile: \"/var/log/unbound/unbound.log\"\n    log-queries: no                              # enable for debugging only\n</code></pre> <p><code>auto-trust-anchor-file</code> - Unbound ships with the root zone's DNSSEC trust anchor and keeps it updated automatically using RFC 5011. DNSSEC validation is on by default - you get it for free.</p> <p><code>use-caps-for-id</code> - implements 0x20 encoding, which randomizes the case of letters in DNS queries. This makes cache poisoning attacks harder because the attacker has to guess the exact case pattern. <code>ExAmPlE.CoM</code> and <code>example.com</code> are the same name in DNS, but the response must preserve the query's case, adding entropy to the transaction.</p>"},{"location":"DNS%20Administration/nsd-and-unbound/#forwarding-vs-full-recursion","title":"Forwarding vs Full Recursion","text":"<p>By default, Unbound does full recursion - it starts at the root servers and chases referrals down to the authoritative server. You can configure it to forward queries to another resolver instead:</p> <pre><code># Forward all queries to upstream resolvers\nforward-zone:\n    name: \".\"                                    # all domains\n    forward-addr: 1.1.1.1                        # Cloudflare\n    forward-addr: 8.8.8.8                        # Google\n    forward-tls-upstream: yes                     # use DNS over TLS\n</code></pre> <p>Full recursion gives you the most control and privacy (your queries go directly to authoritative servers, not through a third party). Forwarding is simpler and may be faster if the upstream resolver has a large cache.</p>"},{"location":"DNS%20Administration/nsd-and-unbound/#local-zone-overrides","title":"Local Zone Overrides","text":"<p>Unbound can override responses for specific domains - useful for internal DNS, split-horizon, or blocking:</p> <pre><code>server:\n    # Internal hostname\n    local-zone: \"internal.example.com.\" static\n    local-data: \"server1.internal.example.com. A 10.0.0.10\"\n    local-data: \"server2.internal.example.com. A 10.0.0.11\"\n\n    # Block a domain (return NXDOMAIN)\n    local-zone: \"ads.badsite.com.\" always_nxdomain\n\n    # Redirect a domain\n    local-zone: \"oldsite.example.com.\" redirect\n    local-data: \"oldsite.example.com. A 198.51.100.50\"\n</code></pre> <p>Local zone types:</p> Type Behavior <code>static</code> Only answer with local-data, NXDOMAIN for anything else <code>redirect</code> Answer queries for the exact name with local-data <code>always_nxdomain</code> Always return NXDOMAIN <code>always_refuse</code> Always return REFUSED <code>transparent</code> Use local-data if available, otherwise resolve normally <code>deny</code> Drop the query silently"},{"location":"DNS%20Administration/nsd-and-unbound/#unbound-control-commands","title":"unbound-control Commands","text":"<pre><code># Setup (first time only - generates TLS keys for control)\nunbound-control-setup\n\n# Status and statistics\nunbound-control status\nunbound-control stats_noreset\n</code></pre> <p>Annotated stats output:</p> <pre><code>unbound-control stats_noreset\n</code></pre> <pre><code>total.num.queries=152847          # total queries received\ntotal.num.cachehits=139201        # answered from cache (91%)\ntotal.num.cachemiss=13646         # required upstream resolution\ntotal.num.recursivereplies=13640  # recursive queries completed\ntotal.requestlist.avg=0.3         # average pending queries\ntotal.requestlist.max=12          # peak concurrent queries\nnum.query.type.A=98241            # A record queries\nnum.query.type.AAAA=42103         # AAAA record queries\n</code></pre> <p>A cache hit rate above 80-90% is normal for a resolver serving active users.</p> <p>Other useful commands:</p> Command Action <code>unbound-control reload</code> Reload configuration <code>unbound-control flush example.com</code> Flush cache for a name <code>unbound-control flush_zone example.com</code> Flush entire zone from cache <code>unbound-control dump_cache</code> Dump cache contents to stdout <code>unbound-control load_cache</code> Load cache from stdin <code>unbound-control list_local_zones</code> Show configured local zones <code>unbound-control list_forwards</code> Show configured forwarders"},{"location":"DNS%20Administration/nsd-and-unbound/#running-nsd-unbound-together","title":"Running NSD + Unbound Together","text":"<p>The most common NLnet Labs deployment runs NSD and Unbound on the same machine: NSD serves your authoritative zones, and Unbound handles recursion for your clients.</p>"},{"location":"DNS%20Administration/nsd-and-unbound/#architecture","title":"Architecture","text":""},{"location":"DNS%20Administration/nsd-and-unbound/#configuration","title":"Configuration","text":"<p>NSD listens on a non-standard port or localhost-only:</p> <pre><code># /etc/nsd/nsd.conf\nserver:\n    ip-address: 127.0.0.1@5353\n    ip-address: 198.51.100.1         # still serve external auth queries on port 53\n    port: 53\n\nzone:\n    name: \"example.com\"\n    zonefile: \"example.com.zone\"\n</code></pre> <p>Unbound handles all client queries and forwards your own zones to NSD:</p> <pre><code># /etc/unbound/unbound.conf\nserver:\n    interface: 0.0.0.0\n    port: 53\n    access-control: 192.168.0.0/16 allow\n    access-control: 0.0.0.0/0 refuse\n\n# Forward queries for your zones to local NSD\nstub-zone:\n    name: \"example.com\"\n    stub-addr: 127.0.0.1@5353\n    stub-prime: no                    # don't use NS records, always use stub-addr\n\nstub-zone:\n    name: \"100.51.198.in-addr.arpa\"\n    stub-addr: 127.0.0.1@5353\n    stub-prime: no\n</code></pre> <p><code>stub-zone</code> (not <code>forward-zone</code>) is the right directive here. A stub zone tells Unbound to query the specified server directly for that zone, as if it were an authoritative server. A forward zone tells Unbound to use the specified server as a recursive resolver. Since NSD is authoritative (not recursive), use <code>stub-zone</code>.</p> <p>What is a stub-zone in Unbound's configuration? (requires JavaScript)</p> <p>NSD + Unbound Split Architecture (requires JavaScript)</p>"},{"location":"DNS%20Administration/nsd-and-unbound/#nsd-vs-bind-when-to-choose-what","title":"NSD vs BIND: When to Choose What","text":"Consideration NSD BIND Role Authoritative only Authoritative + recursive Performance (authoritative) Higher throughput Baseline Memory usage Lower Higher Configuration Simple YAML-like Feature-rich C-like Views / split-horizon Not supported Full support RPZ Not supported Full support DNSSEC signing External tools (ldns) Built-in (<code>dnssec-policy</code>) Zone file format Standard (identical) Standard Community/docs Smaller but dedicated Largest Pair with Unbound (for recursion) Standalone <p>Choose NSD when you want a fast, focused authoritative server with minimal attack surface. Pair it with Unbound for recursion.</p> <p>Choose BIND when you need views, RPZ, built-in DNSSEC key management, or a single server that handles everything.</p> <p>Both are production-quality, well-maintained, and used by major organizations. The choice is more about operational philosophy than capability.</p>"},{"location":"DNS%20Administration/nsd-and-unbound/#further-reading","title":"Further Reading","text":"<ul> <li>NSD Documentation - NLnet Labs official NSD docs</li> <li>Unbound Documentation - NLnet Labs official Unbound docs</li> <li>NLnet Labs - organization overview and projects</li> <li>RFC 5011 - Automated Updates of DNSSEC Trust Anchors</li> <li>DNS Flag Day - DNS standards compliance initiative</li> </ul> <p>Previous: BIND | Next: PowerDNS | Back to Index</p>"},{"location":"DNS%20Administration/powerdns/","title":"PowerDNS","text":"<p>This guide covers PowerDNS - a DNS server that stores zone data in databases instead of flat files, and exposes a full HTTP API for automation. You'll learn to set up the authoritative server with MySQL and SQLite backends, manage zones through the API, and configure the recursor.</p>"},{"location":"DNS%20Administration/powerdns/#what-makes-powerdns-different","title":"What Makes PowerDNS Different","text":"<p>PowerDNS takes a fundamentally different approach to DNS than BIND or NSD. Instead of reading zone files from disk, PowerDNS uses backends - pluggable modules that can read zone data from MySQL, PostgreSQL, SQLite, LDAP, plain files, or even custom scripts.</p> <p></p> <p>This makes PowerDNS the natural choice when DNS data needs to be managed programmatically. Hosting providers, cloud platforms, and any system where DNS records change frequently through automation benefit from the database-backed model. You can <code>INSERT</code> a DNS record with SQL and it's live immediately - no zone file editing, no serial incrementing, no reload.</p> <p>PowerDNS effectively replaces MyDNS, a database-backed DNS server that was popular in the hosting industry but hasn't been maintained since 2008.</p> <p>Like NLnet Labs' approach, PowerDNS separates authority from recursion into two distinct programs:</p> <ul> <li>PowerDNS Authoritative Server (<code>pdns</code>) - serves zone data from backends</li> <li>PowerDNS Recursor (<code>pdns-recursor</code>) - recursive resolution and caching</li> </ul> <p>Key features:</p> <ul> <li>Database backends - MySQL, PostgreSQL, SQLite, LMDB, and more</li> <li>HTTP API - full RESTful API with OpenAPI/Swagger documentation for managing zones and records</li> <li>Built-in DNSSEC - sign zones with a single command</li> <li>Lua scripting - customize query processing in the recursor</li> <li>Split-horizon - PowerDNS 5.0 (2025) added views, one of its most requested features for years</li> </ul> <p>What is the main advantage of PowerDNS's backend architecture? (requires JavaScript)</p>"},{"location":"DNS%20Administration/powerdns/#powerdns-authoritative-server","title":"PowerDNS Authoritative Server","text":""},{"location":"DNS%20Administration/powerdns/#installation","title":"Installation","text":"<p>RHEL / AlmaLinux / Rocky:</p> <pre><code>sudo dnf install pdns pdns-backend-mysql     # or pdns-backend-sqlite\nsudo systemctl enable --now pdns\n</code></pre> <p>Debian / Ubuntu:</p> <pre><code>sudo apt install pdns-server pdns-backend-mysql    # or pdns-backend-sqlite3\nsudo systemctl enable --now pdns\n</code></pre>"},{"location":"DNS%20Administration/powerdns/#mysql-backend-setup","title":"MySQL Backend Setup","text":"<p>The MySQL (or MariaDB) backend is the most common choice for production deployments.</p> <p>Create the database and user:</p> <pre><code>CREATE DATABASE powerdns CHARACTER SET utf8mb4;\nCREATE USER 'pdns'@'localhost' IDENTIFIED BY 'your-secure-password';\nGRANT ALL ON powerdns.* TO 'pdns'@'localhost';\n</code></pre> <p>Import the schema:</p> <pre><code>mysql -u root -p powerdns &lt; /usr/share/doc/pdns-backend-mysql/schema.mysql.sql\n</code></pre> <p>The schema creates tables for domains, records, comments, cryptokeys (for DNSSEC), and domain metadata.</p> <p>Configure PowerDNS:</p> <pre><code># /etc/pdns/pdns.conf\n\n# Backend\nlaunch=gmysql\ngmysql-host=127.0.0.1\ngmysql-port=3306\ngmysql-dbname=powerdns\ngmysql-user=pdns\ngmysql-password=your-secure-password\ngmysql-dnssec=yes\n\n# Network\nlocal-address=0.0.0.0, ::\nlocal-port=53\n\n# General\ndaemon=yes\nguardian=yes\nsetuid=pdns\nsetgid=pdns\n</code></pre>"},{"location":"DNS%20Administration/powerdns/#sqlite-backend","title":"SQLite Backend","text":"<p>For smaller deployments or development, SQLite requires no separate database server:</p> <pre><code># /etc/pdns/pdns.conf\nlaunch=gsqlite3\ngsqlite3-database=/var/lib/pdns/powerdns.sqlite3\ngsqlite3-dnssec=yes\n</code></pre> <p>Initialize the database:</p> <pre><code>sqlite3 /var/lib/pdns/powerdns.sqlite3 &lt; /usr/share/doc/pdns-backend-sqlite3/schema.sqlite3.sql\nsudo chown pdns:pdns /var/lib/pdns/powerdns.sqlite3\n</code></pre>"},{"location":"DNS%20Administration/powerdns/#managing-zones-with-pdnsutil","title":"Managing Zones with pdnsutil","text":"<p><code>pdnsutil</code> is the command-line tool for managing PowerDNS zones and records.</p> <p>Create a zone:</p> <pre><code>pdnsutil create-zone example.com ns1.example.com\n</code></pre> <p>This creates the zone with a default SOA record.</p> <p>Add records:</p> <pre><code>pdnsutil add-record example.com . A 3600 198.51.100.10\npdnsutil add-record example.com www A 3600 198.51.100.10\npdnsutil add-record example.com . MX 3600 \"10 mail.example.com.\"\npdnsutil add-record example.com mail A 3600 198.51.100.20\npdnsutil add-record example.com . TXT 3600 '\"v=spf1 ip4:198.51.100.0/24 -all\"'\npdnsutil add-record example.com . NS 3600 ns1.example.com.\npdnsutil add-record example.com . NS 3600 ns2.example.com.\npdnsutil add-record example.com ns1 A 3600 198.51.100.1\npdnsutil add-record example.com ns2 A 3600 198.51.100.2\n</code></pre> <p>Note the <code>.</code> for the zone apex and the trailing dots on FQDNs in record data.</p> <p>List and check zones:</p> <pre><code>pdnsutil list-all-zones\npdnsutil list-zone example.com\npdnsutil check-zone example.com\n</code></pre> <p><code>pdnsutil list-zone</code> output:</p> <pre><code>example.com.    3600    IN      SOA     ns1.example.com. hostmaster.example.com. 2025011501 10800 3600 604800 3600\nexample.com.    3600    IN      NS      ns1.example.com.\nexample.com.    3600    IN      NS      ns2.example.com.\nexample.com.    3600    IN      A       198.51.100.10\nexample.com.    3600    IN      MX      10 mail.example.com.\nexample.com.    3600    IN      TXT     \"v=spf1 ip4:198.51.100.0/24 -all\"\nwww.example.com.        3600    IN      A       198.51.100.10\nmail.example.com.       3600    IN      A       198.51.100.20\nns1.example.com.        3600    IN      A       198.51.100.1\nns2.example.com.        3600    IN      A       198.51.100.2\n</code></pre> <p>Other useful commands:</p> <pre><code>pdnsutil edit-zone example.com          # open zone in $EDITOR\npdnsutil delete-rrset example.com www A # delete specific records\npdnsutil replace-rrset example.com www A 3600 198.51.100.11  # replace records\npdnsutil increase-serial example.com    # manually bump the serial\npdnsutil check-all-zones                # validate all zones\n</code></pre> <p>What is pdnsutil primarily used for? (requires JavaScript)</p> <p>PowerDNS Zone Management with pdnsutil (requires JavaScript)</p>"},{"location":"DNS%20Administration/powerdns/#the-http-api","title":"The HTTP API","text":"<p>PowerDNS includes a built-in REST API that can do everything <code>pdnsutil</code> does and more. The API serves OpenAPI/Swagger documentation, making it easy to integrate with automation tools.</p>"},{"location":"DNS%20Administration/powerdns/#enabling-the-api","title":"Enabling the API","text":"<pre><code># /etc/pdns/pdns.conf\napi=yes\napi-key=your-api-key-here\nwebserver=yes\nwebserver-address=127.0.0.1\nwebserver-port=8081\nwebserver-allow-from=127.0.0.1,::1\n</code></pre> <p>Restart PowerDNS after enabling the API:</p> <pre><code>sudo systemctl restart pdns\n</code></pre>"},{"location":"DNS%20Administration/powerdns/#api-operations","title":"API Operations","text":"<p>List all zones:</p> <pre><code>curl -s -H \"X-API-Key: your-api-key-here\" \\\n    http://127.0.0.1:8081/api/v1/servers/localhost/zones | python3 -m json.tool\n</code></pre> <p>Create a zone:</p> <pre><code>curl -s -X POST -H \"X-API-Key: your-api-key-here\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"name\": \"newdomain.com.\",\n        \"kind\": \"Native\",\n        \"nameservers\": [\"ns1.newdomain.com.\", \"ns2.newdomain.com.\"],\n        \"rrsets\": [\n            {\n                \"name\": \"newdomain.com.\",\n                \"type\": \"A\",\n                \"ttl\": 3600,\n                \"records\": [{\"content\": \"198.51.100.50\", \"disabled\": false}]\n            }\n        ]\n    }' \\\n    http://127.0.0.1:8081/api/v1/servers/localhost/zones\n</code></pre> <p>Add or update records (PATCH):</p> <pre><code>curl -s -X PATCH -H \"X-API-Key: your-api-key-here\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"rrsets\": [\n            {\n                \"name\": \"www.newdomain.com.\",\n                \"type\": \"A\",\n                \"ttl\": 3600,\n                \"changetype\": \"REPLACE\",\n                \"records\": [\n                    {\"content\": \"198.51.100.50\", \"disabled\": false},\n                    {\"content\": \"198.51.100.51\", \"disabled\": false}\n                ]\n            }\n        ]\n    }' \\\n    http://127.0.0.1:8081/api/v1/servers/localhost/zones/newdomain.com.\n</code></pre> <p><code>changetype</code> can be <code>REPLACE</code> (replace all records of that name and type) or <code>DELETE</code> (remove them).</p> <p>Query zone data:</p> <pre><code>curl -s -H \"X-API-Key: your-api-key-here\" \\\n    http://127.0.0.1:8081/api/v1/servers/localhost/zones/newdomain.com. | python3 -m json.tool\n</code></pre> <p>Delete a zone:</p> <pre><code>curl -s -X DELETE -H \"X-API-Key: your-api-key-here\" \\\n    http://127.0.0.1:8081/api/v1/servers/localhost/zones/newdomain.com.\n</code></pre> <p>The API returns JSON for all operations, making it straightforward to integrate with configuration management tools, CI/CD pipelines, or custom provisioning systems.</p> <p>What can you do with the PowerDNS HTTP API? (requires JavaScript)</p> <p>Command Builder (requires JavaScript)</p>"},{"location":"DNS%20Administration/powerdns/#powerdns-recursor","title":"PowerDNS Recursor","text":"<p>The PowerDNS Recursor is a separate binary (<code>pdns_recursor</code>) that handles recursive DNS resolution. It's independent from the authoritative server and can run alongside it or on a different machine.</p>"},{"location":"DNS%20Administration/powerdns/#installation_1","title":"Installation","text":"<pre><code># RHEL\nsudo dnf install pdns-recursor\n\n# Debian/Ubuntu\nsudo apt install pdns-recursor\n</code></pre>"},{"location":"DNS%20Administration/powerdns/#basic-configuration","title":"Basic Configuration","text":"<pre><code># /etc/pdns-recursor/recursor.conf\n\n# Network\nlocal-address=127.0.0.1, 192.168.1.1\nlocal-port=53\n\n# Access control\nallow-from=127.0.0.0/8, 192.168.0.0/16, 10.0.0.0/8\n\n# DNSSEC\ndnssec=validate\n\n# Performance\nthreads=2\nmax-cache-entries=500000\n\n# Logging\nquiet=yes\ntrace=no\n</code></pre>"},{"location":"DNS%20Administration/powerdns/#forwarding-to-local-authoritative-server","title":"Forwarding to Local Authoritative Server","text":"<p>If you run the PowerDNS authoritative server on the same machine, forward your own zones to it:</p> <pre><code># Forward your zones to the local authoritative server\nforward-zones=example.com=127.0.0.1:5300, 100.51.198.in-addr.arpa=127.0.0.1:5300\n</code></pre>"},{"location":"DNS%20Administration/powerdns/#lua-scripting","title":"Lua Scripting","text":"<p>The recursor supports Lua scripts for custom query processing. This is a powerful feature for implementing custom filtering, logging, or response modification:</p> <pre><code>-- /etc/pdns-recursor/filter.lua\n\nfunction preresolve(dq)\n    -- Block a specific domain\n    if dq.qname:equal(\"blocked.example.com.\") then\n        dq.rcode = pdns.REFUSED\n        return true\n    end\n    return false\nend\n</code></pre> <p>Enable the script in <code>recursor.conf</code>:</p> <pre><code>lua-dns-script=/etc/pdns-recursor/filter.lua\n</code></pre>"},{"location":"DNS%20Administration/powerdns/#primarysecondary-with-powerdns","title":"Primary/Secondary with PowerDNS","text":"<p>PowerDNS supports two replication modes:</p>"},{"location":"DNS%20Administration/powerdns/#native-replication","title":"Native Replication","text":"<p>With native replication, both servers read from the same database (or a replicated database). Both are authoritative, and zone data is synchronized at the database level (MySQL replication, PostgreSQL streaming replication, etc.). No AXFR zone transfers are involved.</p> <pre><code># Both servers\nlaunch=gmysql\ngmysql-host=db-primary.internal\n</code></pre> <p>Set the zone type to <code>Native</code>:</p> <pre><code>pdnsutil set-kind example.com native\n</code></pre> <p>This is the simplest approach when you already have database replication infrastructure.</p>"},{"location":"DNS%20Administration/powerdns/#axfr-based-replication","title":"AXFR-Based Replication","text":"<p>For traditional zone transfer replication:</p> <p>Primary:</p> <pre><code># /etc/pdns/pdns.conf\nallow-axfr-ips=203.0.113.2/32\nalso-notify=203.0.113.2\n</code></pre> <pre><code>pdnsutil set-kind example.com primary\n</code></pre> <p>Secondary:</p> <pre><code># /etc/pdns/pdns.conf\nautosecondary=yes\n</code></pre> <p>Or configure each zone explicitly:</p> <pre><code>pdnsutil create-secondary-zone example.com 198.51.100.1\n</code></pre> <p>Check transfer status:</p> <pre><code>pdnsutil list-zone example.com\npdnsutil check-zone example.com\n</code></pre>"},{"location":"DNS%20Administration/powerdns/#supermastersuperslave","title":"Supermaster/Superslave","text":"<p>PowerDNS has a supermaster feature where a secondary automatically creates zone entries for any zone the primary notifies it about. This is useful when managing many zones - add a zone to the primary and the secondary picks it up automatically.</p> <p>On the secondary, configure the primary as a supermaster:</p> <pre><code>INSERT INTO supermasters (ip, nameserver, account)\nVALUES ('198.51.100.1', 'ns1.example.com', 'admin');\n</code></pre> <pre><code># Secondary pdns.conf\nautosecondary=yes\n</code></pre>"},{"location":"DNS%20Administration/powerdns/#dnssec-with-powerdns","title":"DNSSEC with PowerDNS","text":"<p>PowerDNS makes DNSSEC signing remarkably simple compared to managing it manually.</p> <p>Sign a zone:</p> <pre><code>pdnsutil secure-zone example.com\n</code></pre> <p>That's it. PowerDNS generates the KSK and ZSK, signs all records, and serves DNSSEC responses. No key generation commands, no signing commands, no cron jobs.</p> <p>View DNSSEC status:</p> <pre><code>pdnsutil show-zone example.com\n</code></pre> <pre><code>Zone is actively secured\nZone has NSEC semantics\nkeys:\nID = 1 (CSK), flags = 257, tag = 31406, algo = 13 (ECDSAP256SHA256), bits = 256\nActive   Published   ( CSK )\n</code></pre> <p>PowerDNS defaults to a single Combined Signing Key (CSK) using ECDSAP256SHA256 (algorithm 13), which is the current best practice for most zones.</p> <p>Get the DS record for your registrar:</p> <pre><code>pdnsutil export-zone-ds example.com\n</code></pre> <pre><code>example.com. IN DS 31406 13 2 abc123def456...\n</code></pre> <p>Submit this DS record to your registrar to complete the DNSSEC chain.</p> <p>Rectify zone data:</p> <pre><code>pdnsutil rectify-zone example.com\n</code></pre> <p>Rectification ensures NSEC/NSEC3 chains and other DNSSEC metadata are consistent. Run this if you modify zone data directly in the database.</p> <p>For a deeper dive into DNSSEC concepts and how the trust chain works, see the DNSSEC guide.</p>"},{"location":"DNS%20Administration/powerdns/#further-reading","title":"Further Reading","text":"<ul> <li>PowerDNS Authoritative Documentation - official auth server docs</li> <li>PowerDNS Recursor Documentation - official recursor docs</li> <li>PowerDNS HTTP API Reference - API endpoints and examples</li> <li>PowerDNS GitHub - source code and issue tracker</li> <li>PowerDNS DNSSEC Guide - DNSSEC-specific documentation</li> </ul> <p>Previous: NSD and Unbound | Next: DNSSEC | Back to Index</p>"},{"location":"DNS%20Administration/zone-files-and-records/","title":"Zone Files and Records","text":"<p>This guide covers the anatomy of DNS zone files - the files that define what your domain actually does. You'll learn every record type you'll encounter in practice, the syntax that trips up even experienced administrators, and how to write complete zone files for both forward and reverse zones.</p>"},{"location":"DNS%20Administration/zone-files-and-records/#zone-file-anatomy","title":"Zone File Anatomy","text":"<p>A zone file is a plain text file that describes a DNS zone. The format was defined in RFC 1035 in 1987, and the syntax hasn't changed since. Every authoritative DNS server reads zone data in this format (or loads equivalent data from a database).</p>"},{"location":"DNS%20Administration/zone-files-and-records/#directives","title":"Directives","text":"<p>Zone files support several directives that control how the file is parsed:</p> <p><code>$ORIGIN</code> sets the default domain name appended to any name that doesn't end with a dot. If you don't set <code>$ORIGIN</code>, it defaults to the zone name from your server configuration.</p> <pre><code>$ORIGIN example.com.\n</code></pre> <p>With this directive, a record like <code>www IN A 198.51.100.10</code> is interpreted as <code>www.example.com. IN A 198.51.100.10</code>.</p> <p><code>$TTL</code> sets the default TTL for records that don't specify one explicitly.</p> <pre><code>$TTL 3600\n</code></pre> <p>This means \"any record without its own TTL value lives in caches for 3600 seconds (1 hour).\"</p> <p><code>$INCLUDE</code> inserts another file's contents at this point in the zone. Useful for splitting large zones into manageable pieces.</p>"},{"location":"DNS%20Administration/zone-files-and-records/#names-and-the-trailing-dot","title":"Names and the Trailing Dot","text":"<p>This is the single most common source of zone file bugs. Names in a zone file are either absolute (ending with a dot) or relative (without a dot, so <code>$ORIGIN</code> is appended).</p> <pre><code>; With $ORIGIN example.com.\n\nwww                 IN  A     198.51.100.10     ; becomes www.example.com.\nmail.example.com.   IN  A     198.51.100.20     ; stays mail.example.com. (absolute)\nmail.example.com    IN  A     198.51.100.20     ; becomes mail.example.com.example.com. (BUG!)\n</code></pre> <p>That third line is the classic trailing-dot mistake. Without the dot, the parser appends the origin, producing a nonsensical name. You will make this mistake at least once. Zone checking tools like <code>named-checkzone</code> catch it.</p> <p>In a zone file, what does the trailing dot in 'example.com.' signify? (requires JavaScript)</p>"},{"location":"DNS%20Administration/zone-files-and-records/#the-shorthand","title":"The <code>@</code> Shorthand","text":"<p>The <code>@</code> symbol means \"the current origin\" (the zone apex). So in a zone file for <code>example.com.</code>:</p> <pre><code>@   IN  A     198.51.100.10     ; means example.com. IN A 198.51.100.10\n@   IN  MX    10 mail           ; means example.com. IN MX 10 mail.example.com.\n</code></pre>"},{"location":"DNS%20Administration/zone-files-and-records/#comments","title":"Comments","text":"<p>Anything after a semicolon (<code>;</code>) is a comment:</p> <pre><code>www     IN  A   198.51.100.10   ; main web server\n</code></pre>"},{"location":"DNS%20Administration/zone-files-and-records/#the-soa-record-in-depth","title":"The SOA Record In Depth","text":"<p>Every zone file must start with a SOA (Start of Authority) record. It declares which server is the primary nameserver for the zone and provides parameters that control zone transfers and caching.</p> <pre><code>$TTL 3600\n$ORIGIN example.com.\n@   IN  SOA ns1.example.com. admin.example.com. (\n            2025011501  ; serial\n            3600        ; refresh (1 hour)\n            900         ; retry (15 minutes)\n            1209600     ; expire (2 weeks)\n            300         ; minimum / negative cache TTL (5 minutes)\n)\n</code></pre> <p>Every field matters:</p> <p>MNAME (<code>ns1.example.com.</code>) - the primary nameserver for the zone. This should be the server where you edit zone data.</p> <p>RNAME (<code>admin.example.com.</code>) - the responsible person's email address, encoded in a DNS-specific way. The first dot replaces the <code>@</code> sign. So <code>admin.example.com.</code> means <code>admin@example.com</code>. If the local part of the email contains a literal dot (like <code>first.last@example.com</code>), you escape it with a backslash: <code>first\\.last.example.com.</code></p> <p>Serial (<code>2025011501</code>) - a version number that secondaries use to determine if the zone has changed. If the serial hasn't increased, the secondary won't transfer the zone. The most common convention is <code>YYYYMMDDNN</code> (date plus a two-digit revision number). January 15, 2025, first change of the day would be <code>2025011501</code>. Second change: <code>2025011502</code>.</p> <p>Forgetting to increment the serial after editing a zone is one of the most common DNS mistakes. Your changes will take effect on the primary but never reach the secondaries.</p> <p>Refresh (<code>3600</code>) - how often (in seconds) secondaries should check the primary for changes. One hour is typical.</p> <p>Retry (<code>900</code>) - if a refresh check fails, how long before the secondary tries again. Should be shorter than refresh.</p> <p>Expire (<code>1209600</code>) - if the secondary can't reach the primary for this long, it stops serving the zone entirely. Two weeks is standard. This prevents stale data from being served indefinitely.</p> <p>Minimum (<code>300</code>) - this field was originally the minimum TTL for records in the zone. RFC 2308 redefined it as the negative cache TTL - how long resolvers should cache NXDOMAIN (name not found) responses. Five minutes is a reasonable value.</p> <p>You can query the SOA record in a readable format with:</p> <pre><code>dig SOA example.com +multiline\n</code></pre> <pre><code>example.com.        3600 IN SOA ns1.example.com. admin.example.com. (\n                            2025011501 ; serial\n                            3600       ; refresh (1 hour)\n                            900        ; retry (15 minutes)\n                            1209600    ; expire (2 weeks)\n                            300        ; minimum (5 minutes)\n                        )\n</code></pre> <p>The <code>+multiline</code> flag is essential for reading SOA records - without it, all the fields run together on one line.</p> <p>Why is the SOA serial number typically formatted as YYYYMMDDNN? (requires JavaScript)</p>"},{"location":"DNS%20Administration/zone-files-and-records/#core-record-types","title":"Core Record Types","text":""},{"location":"DNS%20Administration/zone-files-and-records/#a-and-aaaa","title":"A and AAAA","text":"<p>A records map a name to an IPv4 address. AAAA records map a name to an IPv6 address.</p> <pre><code>www         IN  A       198.51.100.10\nwww         IN  AAAA    2001:db8::1\n</code></pre> <p>A domain can have multiple A records. DNS responds with all of them, and clients typically try them in order (or randomize). This is the simplest form of load distribution - called DNS round-robin.</p> <pre><code>www         IN  A       198.51.100.10\nwww         IN  A       198.51.100.11\nwww         IN  A       198.51.100.12\n</code></pre>"},{"location":"DNS%20Administration/zone-files-and-records/#cname","title":"CNAME","text":"<p>A CNAME (Canonical Name) record creates an alias from one name to another.</p> <pre><code>blog        IN  CNAME   www.example.com.\n</code></pre> <p>When a resolver looks up <code>blog.example.com</code>, it sees the CNAME and follows it to <code>www.example.com</code>, then resolves that name to its A/AAAA records.</p> <p>CNAME has a strict rule that catches people off guard: a CNAME cannot coexist with any other record at the same name. This means you cannot put a CNAME at the zone apex (<code>example.com.</code>) because the apex already has SOA and NS records (and typically MX and TXT records too).</p> <p>This is why \"CNAME at the apex\" is forbidden by RFC 1034. Everyone wants to do <code>example.com CNAME myapp.herokuapp.com</code> to point their bare domain at a cloud provider, but the RFC says no.</p> <p>The workaround is provider-specific:</p> <ul> <li>ALIAS (DNSimple, NS1) / ANAME (PowerDNS, RFC draft) - the authoritative server resolves the target internally and returns A/AAAA records</li> <li>CNAME flattening (Cloudflare) - similar to ALIAS, done at the DNS provider level</li> <li>A record pointing directly at the provider's IP (if they offer a static IP)</li> </ul> <p>Why can't you put a CNAME record at the zone apex (e.g., example.com)? (requires JavaScript)</p>"},{"location":"DNS%20Administration/zone-files-and-records/#mx","title":"MX","text":"<p>MX (Mail Exchange) records specify which servers accept email for the domain. Each MX record has a priority (lower number = higher priority).</p> <pre><code>@           IN  MX  10  mail1.example.com.\n@           IN  MX  20  mail2.example.com.\n@           IN  MX  30  backupmx.example.com.\n</code></pre> <p>Mail servers try the lowest-priority MX first. If <code>mail1</code> is unreachable, they try <code>mail2</code>, then <code>backupmx</code>. Equal priorities mean the sender picks randomly among them.</p> <p>An MX record must point to a hostname, never an IP address, and the target must not be a CNAME. Both constraints come from the RFCs and are enforced by many mail servers. Violating them causes subtle delivery failures.</p> <p>If a domain has MX records with priorities 10, 20, and 30, which server receives mail first? (requires JavaScript)</p>"},{"location":"DNS%20Administration/zone-files-and-records/#ns","title":"NS","text":"<p>NS (Name Server) records declare which servers are authoritative for a zone.</p> <pre><code>@           IN  NS  ns1.example.com.\n@           IN  NS  ns2.example.com.\n</code></pre> <p>NS records at the zone apex tell resolvers where to find authoritative data. NS records at a subdomain create a delegation - they carve off a portion of the namespace to be handled by different servers.</p> <pre><code>; Delegate dev.example.com to different nameservers\ndev         IN  NS  ns1.dev.example.com.\ndev         IN  NS  ns2.dev.example.com.\n\n; Glue records (needed because the NS names are within the delegated zone)\nns1.dev     IN  A   203.0.113.10\nns2.dev     IN  A   203.0.113.11\n</code></pre>"},{"location":"DNS%20Administration/zone-files-and-records/#txt","title":"TXT","text":"<p>TXT records hold arbitrary text. They've become one of the most heavily used record types because they're the mechanism for email authentication and domain verification.</p> <pre><code>; SPF - which servers may send email for this domain\n@           IN  TXT \"v=spf1 ip4:198.51.100.0/24 include:_spf.google.com -all\"\n\n; DKIM - public key for email signature verification\nselector1._domainkey IN TXT \"v=DKIM1; k=rsa; p=MIGfMA0GCSqGSIb3...\"\n\n; DMARC - policy for handling authentication failures\n_dmarc      IN  TXT \"v=DMARC1; p=reject; rua=mailto:dmarc@example.com\"\n\n; Domain verification\n@           IN  TXT \"google-site-verification=abc123...\"\n</code></pre> <p>TXT records have a structural quirk: each character-string within a TXT record is limited to 255 bytes. For data longer than 255 bytes (common with DKIM keys), you split it into multiple quoted strings that are concatenated:</p> <pre><code>selector1._domainkey IN TXT (\"v=DKIM1; k=rsa; p=MIGfMA0GCSqGSIb3DQEB\"\n                              \"AQUAA4GNADCBiQKBgQC7ZYwx...\")\n</code></pre> <p>The DNS protocol concatenates the strings automatically. But some providers' web interfaces handle this badly, silently truncating long values. If your DKIM verification is failing, check whether the full key made it into the record.</p> <p>Write Email Authentication Records (requires JavaScript)</p>"},{"location":"DNS%20Administration/zone-files-and-records/#ptr","title":"PTR","text":"<p>PTR (Pointer) records do reverse DNS - mapping an IP address back to a hostname. They live in a special zone under <code>.in-addr.arpa</code> for IPv4 and <code>.ip6.arpa</code> for IPv6.</p> <p>The trick with PTR records is that IP addresses are reversed. The PTR for <code>198.51.100.25</code> lives at <code>25.100.51.198.in-addr.arpa.</code>:</p> <pre><code>$ORIGIN 100.51.198.in-addr.arpa.\n25      IN  PTR     mail.example.com.\n</code></pre> <p>Reverse DNS is controlled by whoever owns the IP address (typically your hosting provider or ISP), not whoever owns the domain. You usually set PTR records through your provider's control panel, not your own DNS server.</p> <p>PTR records are critical for email. Many mail servers check that the sending IP's PTR record matches the SMTP HELO hostname, and reject or penalize email from IPs without valid reverse DNS.</p>"},{"location":"DNS%20Administration/zone-files-and-records/#service-and-infrastructure-records","title":"Service and Infrastructure Records","text":""},{"location":"DNS%20Administration/zone-files-and-records/#srv","title":"SRV","text":"<p>SRV (Service) records specify the location of a service at a particular host and port.</p> <pre><code>_sip._tcp.example.com. 3600 IN SRV 10 60 5060 sipserver.example.com.\n;                                   |  |  |     |\n;                          priority-+  |  |     target host\n;                              weight--+  |\n;                                   port--+\n</code></pre> <p>The name format is <code>_service._protocol.domain</code>. Priority works like MX (lower first). Weight is for load balancing among records with the same priority - a server with weight 60 gets three times the traffic of one with weight 20.</p> <p>SRV records are used by SIP, XMPP, LDAP, Kerberos, and many internal service discovery systems.</p>"},{"location":"DNS%20Administration/zone-files-and-records/#caa","title":"CAA","text":"<p>CAA (Certification Authority Authorization) records specify which certificate authorities may issue TLS certificates for a domain.</p> <pre><code>@           IN  CAA 0 issue \"letsencrypt.org\"\n@           IN  CAA 0 issuewild \"letsencrypt.org\"\n@           IN  CAA 0 iodef \"mailto:security@example.com\"\n</code></pre> <p><code>issue</code> controls regular certificates, <code>issuewild</code> controls wildcard certificates, and <code>iodef</code> specifies where to report violations. If you don't have CAA records, any CA can issue certificates for your domain.</p>"},{"location":"DNS%20Administration/zone-files-and-records/#sshfp","title":"SSHFP","text":"<p>SSHFP records publish SSH host key fingerprints in DNS. When a client connects to an SSH server for the first time, it can verify the host key against the SSHFP record instead of showing \"The authenticity of host... can't be established\":</p> <pre><code>@           IN  SSHFP   4 2 abc123def456...\n;                       |  |\n;          algorithm ---+  hash type\n</code></pre> <p>This only works with DNSSEC validation enabled (otherwise an attacker could spoof the SSHFP record too).</p>"},{"location":"DNS%20Administration/zone-files-and-records/#complete-zone-file-examples","title":"Complete Zone File Examples","text":""},{"location":"DNS%20Administration/zone-files-and-records/#forward-zone","title":"Forward Zone","text":"<p>A complete zone file for <code>example.com</code>:</p> <pre><code>$TTL 3600\n$ORIGIN example.com.\n\n; --- SOA ---\n@   IN  SOA ns1.example.com. admin.example.com. (\n            2025011501  ; serial\n            3600        ; refresh (1 hour)\n            900         ; retry (15 minutes)\n            1209600     ; expire (2 weeks)\n            300         ; minimum / negative cache TTL\n)\n\n; --- Nameservers ---\n@           IN  NS      ns1.example.com.\n@           IN  NS      ns2.example.com.\n\n; --- Nameserver A records (glue) ---\nns1         IN  A       198.51.100.1\nns2         IN  A       198.51.100.2\n\n; --- Mail ---\n@           IN  MX      10 mail1.example.com.\n@           IN  MX      20 mail2.example.com.\nmail1       IN  A       198.51.100.10\nmail2       IN  A       198.51.100.11\n\n; --- Email authentication ---\n@           IN  TXT     \"v=spf1 ip4:198.51.100.0/24 -all\"\nselector1._domainkey IN TXT (\"v=DKIM1; k=rsa; p=MIGfMA0GCSqGSIb3DQEB\"\n                              \"AQUAA4GNADCBiQKBgQC7ZYwx...\")\n_dmarc      IN  TXT     \"v=DMARC1; p=reject; rua=mailto:dmarc@example.com\"\n\n; --- Web ---\n@           IN  A       198.51.100.20\n@           IN  AAAA    2001:db8::20\nwww         IN  CNAME   example.com.\n\n; --- Other services ---\nftp         IN  A       198.51.100.30\ndev         IN  A       198.51.100.40\n\n; --- Certificate authority ---\n@           IN  CAA     0 issue \"letsencrypt.org\"\n@           IN  CAA     0 issuewild \"letsencrypt.org\"\n</code></pre> <p>Complete Zone File Anatomy (requires JavaScript)</p>"},{"location":"DNS%20Administration/zone-files-and-records/#reverse-zone","title":"Reverse Zone","text":"<p>The corresponding reverse zone for <code>198.51.100.0/24</code>:</p> <pre><code>$TTL 3600\n$ORIGIN 100.51.198.in-addr.arpa.\n\n@   IN  SOA ns1.example.com. admin.example.com. (\n            2025011501  ; serial\n            3600        ; refresh\n            900         ; retry\n            1209600     ; expire\n            300         ; minimum\n)\n\n@           IN  NS      ns1.example.com.\n@           IN  NS      ns2.example.com.\n\n; IP -&gt; hostname mappings (last octet only, $ORIGIN handles the rest)\n1           IN  PTR     ns1.example.com.\n2           IN  PTR     ns2.example.com.\n10          IN  PTR     mail1.example.com.\n11          IN  PTR     mail2.example.com.\n20          IN  PTR     www.example.com.\n30          IN  PTR     ftp.example.com.\n40          IN  PTR     dev.example.com.\n</code></pre> <p>Notice the PTR records only have the last octet because <code>$ORIGIN</code> is set to <code>100.51.198.in-addr.arpa.</code>. The entry <code>1 IN PTR ns1.example.com.</code> expands to <code>1.100.51.198.in-addr.arpa. IN PTR ns1.example.com.</code>.</p>"},{"location":"DNS%20Administration/zone-files-and-records/#common-mistakes","title":"Common Mistakes","text":"<p>Missing trailing dot on FQDNs. Already covered above, but it bears repeating. <code>mail.example.com</code> in a zone file becomes <code>mail.example.com.example.com.</code> because the origin is appended. Always use the trailing dot for fully qualified names.</p> <p>CNAME coexistence. A CNAME cannot share a name with any other record type. This will fail:</p> <pre><code>; BROKEN - CNAME can't coexist with MX\n@   IN  CNAME   other.example.com.\n@   IN  MX      10 mail.example.com.\n</code></pre> <p>Forgetting to increment the serial. If you edit the zone file but don't change the serial, secondaries will never pick up the changes. They compare serials and only transfer when the primary's serial is higher.</p> <p>TTL too low. A TTL of 60 seconds means every query hits your authoritative servers. During normal operations, 3600 (1 hour) to 86400 (24 hours) is reasonable. Lower to 300 (5 minutes) only when you're planning a change.</p> <p>TTL too high. A TTL of 604800 (1 week) means if you make a mistake, resolvers worldwide will serve the wrong answer for up to a week. For records that change, keep TTLs reasonable.</p> <p>MX pointing to a CNAME. The RFCs say MX targets must be A/AAAA records, not CNAMEs. Some servers handle this, many don't. It causes hard-to-diagnose email delivery failures.</p> <p>Fix a Broken Zone File (requires JavaScript)</p>"},{"location":"DNS%20Administration/zone-files-and-records/#further-reading","title":"Further Reading","text":"<ul> <li>RFC 1035 - Domain Names: Implementation and Specification (zone file format)</li> <li>RFC 2308 - Negative Caching of DNS Queries (SOA minimum field)</li> <li>RFC 7208 - Sender Policy Framework (SPF)</li> <li>RFC 6376 - DomainKeys Identified Mail (DKIM)</li> <li>RFC 7489 - Domain-based Message Authentication (DMARC)</li> <li>RFC 8659 - DNS Certification Authority Authorization (CAA)</li> <li>IANA DNS Parameters - authoritative registry of record types and codes</li> </ul> <p>Previous: DNS Fundamentals | Next: DNS Tools and Troubleshooting | Back to Index</p>"},{"location":"Databases/","title":"Databases","text":"<p>A comprehensive course on databases - from relational fundamentals and SQL through MySQL administration, PostgreSQL internals, NoSQL systems, and production operations. These guides take you from \"I know I need a database\" to understanding storage engines, query optimization, replication topologies, and recovery procedures well enough to architect, tune, and operate database systems in production.</p> <p>Each guide is self-contained, but the order below follows a natural learning path. Basic Linux CLI familiarity is assumed throughout.</p>"},{"location":"Databases/#foundations","title":"Foundations","text":""},{"location":"Databases/#database-fundamentals","title":"Database Fundamentals","text":"<p>The history of data management from flat files through hierarchical and network models to the relational revolution and the NoSQL movement. Covers the relational model, ACID properties, CAP theorem, storage engine basics, and a decision framework for choosing between RDBMS and NoSQL systems.</p>"},{"location":"Databases/#sql-essentials","title":"SQL Essentials","text":"<p>The language every relational database speaks. Covers DDL for defining schema, DML for manipulating data, all JOIN types with visual explanations, subqueries, aggregations with GROUP BY and HAVING, transaction control, and the functions you will use daily.</p>"},{"location":"Databases/#database-design-modeling","title":"Database Design &amp; Modeling","text":"<p>Turning requirements into schema. Covers entity-relationship diagrams, normalization from first normal form through Boyce-Codd, denormalization trade-offs for read performance, index theory covering B-tree and hash structures, data type selection, and constraint enforcement with primary keys, foreign keys, unique, and check constraints.</p>"},{"location":"Databases/#mysql-mariadb","title":"MySQL &amp; MariaDB","text":""},{"location":"Databases/#mysql-installation-configuration","title":"MySQL Installation &amp; Configuration","text":"<p>Getting MySQL running and configured correctly. Covers installation via package managers and Docker, the <code>my.cnf</code> file structure with section precedence, InnoDB versus MyISAM storage engines, buffer pool sizing, character sets and collations, and where MariaDB diverges from MySQL.</p>"},{"location":"Databases/#mysql-administration","title":"MySQL Administration","text":"<p>Day-to-day MySQL management. Covers the <code>mysql</code> CLI, the user and privilege system including roles in MySQL 8.0+, log types from error through binary logs, table maintenance with <code>mysqlcheck</code> and <code>OPTIMIZE TABLE</code>, the <code>information_schema</code>, and routine operational tasks.</p>"},{"location":"Databases/#mysql-performance-optimization","title":"MySQL Performance &amp; Optimization","text":"<p>Finding and fixing slow queries. Covers <code>EXPLAIN</code> and <code>EXPLAIN ANALYZE</code> output interpretation, index strategies including covering and composite indexes, buffer pool tuning, query profiling, slow query log analysis, <code>pt-query-digest</code>, optimizer hints, and common anti-patterns that kill performance.</p>"},{"location":"Databases/#mysql-replication-high-availability","title":"MySQL Replication &amp; High Availability","text":"<p>Scaling reads and surviving failures. Covers binary log replication, GTID mode, semi-synchronous replication, Group Replication, InnoDB Cluster, ProxySQL for connection routing, failover patterns, and monitoring replication lag.</p>"},{"location":"Databases/#postgresql","title":"PostgreSQL","text":""},{"location":"Databases/#postgresql-fundamentals","title":"PostgreSQL Fundamentals","text":"<p>Getting started with PostgreSQL. Covers installation and <code>initdb</code>, the <code>psql</code> CLI with meta-commands and <code>.psqlrc</code> customization, <code>postgresql.conf</code> tuning, <code>pg_hba.conf</code> authentication methods, the schema/database hierarchy, system catalogs, and <code>pg_catalog</code> internals.</p>"},{"location":"Databases/#postgresql-administration","title":"PostgreSQL Administration","text":"<p>Running PostgreSQL in production. Covers roles and privileges including row-level security, tablespace management, VACUUM and ANALYZE with autovacuum tuning, <code>pg_stat_*</code> monitoring views, essential extensions like <code>pg_stat_statements</code> and <code>pgcrypto</code>, and WAL management for crash recovery.</p>"},{"location":"Databases/#postgresql-advanced-features","title":"PostgreSQL Advanced Features","text":"<p>PostgreSQL capabilities beyond standard SQL. Covers recursive CTEs, window functions, JSONB operators with GIN indexing, table partitioning strategies, full-text search with <code>tsvector</code> and <code>tsquery</code>, connection pooling with PgBouncer, and foreign data wrappers for querying external sources.</p>"},{"location":"Databases/#nosql","title":"NoSQL","text":""},{"location":"Databases/#nosql-concepts-architecture","title":"NoSQL Concepts &amp; Architecture","text":"<p>Understanding when and why to go beyond relational. Covers document stores, key-value stores, wide-column stores, and graph databases, with a deep dive into CAP theorem trade-offs, consistency models from eventual through strong, polyglot persistence patterns, and a practical decision framework.</p>"},{"location":"Databases/#mongodb","title":"MongoDB","text":"<p>The dominant document database. Covers the document model and BSON types, CRUD operations, the aggregation pipeline with stages and operators, index types including compound, text, and geospatial, replica sets for availability, sharding fundamentals, and the <code>mongosh</code> CLI.</p>"},{"location":"Databases/#redis","title":"Redis","text":"<p>In-memory data structures for speed. Covers strings, hashes, lists, sets, sorted sets, and streams, caching patterns including cache-aside and write-through, TTL and eviction policies, pub/sub messaging, Lua scripting for atomic operations, persistence with RDB and AOF, and Sentinel and Cluster for high availability.</p>"},{"location":"Databases/#operations","title":"Operations","text":""},{"location":"Databases/#backup-recovery-strategies","title":"Backup &amp; Recovery Strategies","text":"<p>Protecting data against loss. Covers logical versus physical backups, <code>mysqldump</code> and <code>mydumper</code> for MySQL, <code>pg_dump</code> and <code>pg_basebackup</code> for PostgreSQL, Percona XtraBackup for hot InnoDB backups, point-in-time recovery with WAL and binary logs, backup verification, scheduling, and retention policies.</p>"},{"location":"Databases/#database-security","title":"Database Security","text":"<p>Hardening databases against threats. Covers authentication methods from passwords through Kerberos, TLS/SSL configuration, encryption at rest with transparent data encryption, SQL injection prevention with parameterized queries, audit logging, privilege hardening principles, and the OWASP database security guidelines.</p>"},{"location":"Databases/#scaling-architecture-patterns","title":"Scaling &amp; Architecture Patterns","text":"<p>Growing beyond a single server. Covers read replicas, connection pooling with ProxySQL and PgBouncer, horizontal versus vertical scaling trade-offs, sharding strategies, microservices data patterns including database-per-service, CQRS, and an overview of event sourcing.</p>"},{"location":"Databases/#innodb-recovery-with-pdrt","title":"InnoDB Recovery with PDRT","text":"<p>The last-resort recovery guide. Covers the Percona Data Recovery Tool workflow including <code>constraints_parser</code> and <code>page_parser</code>, InnoDB file architecture with <code>ibdata1</code> and per-table <code>.ibd</code> files, first-response triage procedures, manual recovery with custom table definitions, and the cPanel/WHM operational context.</p>"},{"location":"Databases/backup-and-recovery/","title":"Backup &amp; Recovery Strategies","text":"<p>Your database is only as reliable as your ability to restore it. A backup you have never tested is not a backup - it is a hope. This guide covers the tools, techniques, and strategies for protecting MySQL and PostgreSQL data against every failure mode from accidental <code>DELETE</code> to total disk loss.</p>"},{"location":"Databases/backup-and-recovery/#logical-vs-physical-backups","title":"Logical vs Physical Backups","text":"<p>Every backup method falls into one of two categories, and understanding the trade-offs determines which tools you reach for.</p> <p>A logical backup exports data as SQL statements or delimited text. You get a portable, human-readable file that can be loaded into any compatible database version. The trade-off is speed - logical backups read every row through the SQL layer, and restoring means re-executing every <code>INSERT</code>. For a 500 GB database, that can take hours or days.</p> <p>A physical backup copies the raw data files (tablespace files, WAL segments, redo logs) at the filesystem or storage level. These are fast to create and fast to restore because they skip the SQL layer entirely. The trade-off is portability - a physical backup is tied to the exact database version, architecture, and often the same configuration.</p> Factor Logical Physical Speed (backup) Slow - reads through SQL Fast - copies raw files Speed (restore) Slow - re-executes SQL Fast - copies files back Size Smaller (text, compresses well) Larger (raw pages, internal fragmentation) Portability High - works across versions and platforms Low - same version and architecture Granularity Per-table, per-database, or full Usually full instance (some tools support incremental) Consistency Snapshot via transactions or locking Requires coordination (flush, freeze, or hot backup tool) PITR support No (without binary log/WAL) Yes (with log replay) <p>When to use each:</p> <ul> <li>Logical: migrating between versions, exporting specific tables, small-to-medium databases, cross-platform transfers</li> <li>Physical: large databases where restore speed matters, disaster recovery, setting up replicas, when you need point-in-time recovery</li> </ul> <p>Most production environments use both - nightly physical backups for fast disaster recovery, plus periodic logical backups for portability and schema versioning.</p> <p>A 200 GB production MySQL database needs disaster recovery with a maximum 30-minute restore window. Which backup type is most appropriate? (requires JavaScript)</p>"},{"location":"Databases/backup-and-recovery/#mysql-backup-tools","title":"MySQL Backup Tools","text":""},{"location":"Databases/backup-and-recovery/#mysqldump","title":"mysqldump","text":"<p><code>mysqldump</code> is the standard logical backup tool shipped with MySQL. It produces SQL output that recreates the schema and data.</p> <p>A basic full backup:</p> <pre><code>mysqldump --single-transaction --routines --triggers --events \\\n  --all-databases &gt; full_backup.sql\n</code></pre> <p>The flags that matter:</p> Flag Purpose <code>--single-transaction</code> Takes a consistent snapshot using a transaction (InnoDB only, no table locks) <code>--routines</code> Includes stored procedures and functions <code>--triggers</code> Includes triggers (on by default in MySQL 5.7+, but be explicit) <code>--events</code> Includes scheduled events <code>--all-databases</code> Dumps every database <code>--flush-logs</code> Rotates binary logs after the dump, useful for PITR <code>--source-data=2</code> Records the binary log position as a comment (MySQL 8.0+, replaces <code>--master-data</code>) <code>--set-gtid-purged=OFF</code> Prevents GTID-related errors when restoring to a non-GTID server <p>MyISAM and --single-transaction</p> <p><code>--single-transaction</code> only provides consistency for InnoDB tables. If your database contains MyISAM tables, you need <code>--lock-all-tables</code> instead, which blocks all writes during the dump. On modern MySQL, you should be using InnoDB for everything.</p> <p>Restoring a <code>mysqldump</code> backup:</p> <pre><code>mysql &lt; full_backup.sql\n</code></pre> <p>For large restores, disable checks to speed up the import:</p> <pre><code>mysql -e \"SET GLOBAL innodb_flush_log_at_trx_commit=2; SET GLOBAL sync_binlog=0;\"\nmysql &lt; full_backup.sql\nmysql -e \"SET GLOBAL innodb_flush_log_at_trx_commit=1; SET GLOBAL sync_binlog=1;\"\n</code></pre> <p>Command Builder (requires JavaScript)</p>"},{"location":"Databases/backup-and-recovery/#mydumper-and-myloader","title":"mydumper and myloader","text":"<p><code>mydumper</code> is a third-party tool that parallelizes logical backups. Where <code>mysqldump</code> uses a single thread, <code>mydumper</code> can dump multiple tables simultaneously and splits large tables into chunks. The companion tool <code>myloader</code> restores in parallel.</p> <pre><code># Parallel backup with 4 threads\nmydumper --threads 4 --compress --outputdir /backup/mysql/daily\n\n# Parallel restore\nmyloader --threads 4 --directory /backup/mysql/daily\n</code></pre> <p>For a 100 GB database, <code>mydumper</code>/<code>myloader</code> can be 4-8x faster than <code>mysqldump</code>/<code>mysql</code> depending on hardware and table count.</p>"},{"location":"Databases/backup-and-recovery/#mysqlpump","title":"mysqlpump","text":"<p><code>mysqlpump</code> is MySQL's built-in parallel dump tool (introduced in 5.7). It supports parallel processing and progress reporting but has seen less adoption than <code>mydumper</code> and was deprecated in MySQL 8.0.3 in favor of MySQL Shell's dump utilities.</p> <pre><code>mysqlpump --default-parallelism=4 --all-databases &gt; backup.sql\n</code></pre>"},{"location":"Databases/backup-and-recovery/#postgresql-backup-tools","title":"PostgreSQL Backup Tools","text":""},{"location":"Databases/backup-and-recovery/#pg_dump","title":"pg_dump","text":"<p><code>pg_dump</code> is the standard logical backup tool for PostgreSQL. Unlike <code>mysqldump</code>, it supports multiple output formats.</p> Format Flag Description Plain SQL <code>-Fp</code> SQL text output (default), restored with <code>psql</code> Custom <code>-Fc</code> Compressed, supports selective restore, restored with <code>pg_restore</code> Directory <code>-Fd</code> One file per table, supports parallel dump and restore Tar <code>-Ft</code> Tar archive, restored with <code>pg_restore</code> <p>The custom format (<code>-Fc</code>) is the best default choice - it compresses automatically, supports selective table restore, and allows parallel restore with <code>pg_restore -j</code>.</p> <pre><code># Custom format (recommended)\npg_dump -Fc mydb &gt; mydb.dump\n\n# Directory format with parallel dump (4 jobs)\npg_dump -Fd -j 4 mydb -f /backup/mydb_dir\n\n# Plain SQL (human-readable)\npg_dump -Fp mydb &gt; mydb.sql\n</code></pre> <p>Restoring:</p> <pre><code># From custom or tar format\npg_restore -d mydb mydb.dump\n\n# Parallel restore (4 jobs, custom or directory format only)\npg_restore -d mydb -j 4 mydb.dump\n\n# From plain SQL\npsql -d mydb -f mydb.sql\n\n# Restore specific tables only\npg_restore -d mydb -t orders -t customers mydb.dump\n</code></pre>"},{"location":"Databases/backup-and-recovery/#pg_dumpall","title":"pg_dumpall","text":"<p><code>pg_dumpall</code> dumps the entire cluster including roles, tablespaces, and other global objects that <code>pg_dump</code> does not capture.</p> <pre><code># Full cluster dump (roles + all databases)\npg_dumpall &gt; cluster_backup.sql\n\n# Globals only (roles and tablespaces, no data)\npg_dumpall --globals-only &gt; globals.sql\n</code></pre> <p>A common pattern is to use <code>pg_dumpall --globals-only</code> for roles and <code>pg_dump -Fc</code> per database for data. This gives you the flexibility of custom format for data while still capturing global objects.</p>"},{"location":"Databases/backup-and-recovery/#pg_basebackup","title":"pg_basebackup","text":"<p><code>pg_basebackup</code> creates a physical backup by streaming a copy of the entire PostgreSQL data directory. This is the foundation for setting up streaming replicas and for physical backup strategies.</p> <pre><code># Stream a physical backup to a directory\npg_basebackup -D /backup/pg_base -Fp -Xs -P -R\n\n# Create a compressed tar backup\npg_basebackup -D /backup/pg_base -Ft -Xf -z\n</code></pre> Flag Purpose <code>-D</code> Destination directory <code>-Fp</code> Plain format (data directory copy) <code>-Ft</code> Tar format <code>-Xs</code> Stream WAL during backup (recommended) <code>-Xf</code> Fetch WAL after backup completes <code>-z</code> Compress output (tar format only) <code>-P</code> Show progress <code>-R</code> Write <code>standby.signal</code> and connection settings for replica setup <p>What is the advantage of pg_dump -Fc (custom format) over pg_dump -Fp (plain SQL)? (requires JavaScript)</p> <p>MySQL and PostgreSQL Logical Backup Workflow (requires JavaScript)</p>"},{"location":"Databases/backup-and-recovery/#percona-xtrabackup","title":"Percona XtraBackup","text":"<p>Percona XtraBackup creates hot physical backups of InnoDB without locking tables or interrupting transactions. It copies the InnoDB data files while tracking the redo log to capture changes made during the copy. This makes it the standard tool for MySQL physical backups in production.</p>"},{"location":"Databases/backup-and-recovery/#full-backup","title":"Full Backup","text":"<pre><code># Create a full backup\nxtrabackup --backup --target-dir=/backup/base\n\n# Prepare the backup (apply redo log for consistency)\nxtrabackup --prepare --target-dir=/backup/base\n\n# Restore (MySQL must be stopped)\nsystemctl stop mysql\nrm -rf /var/lib/mysql/*\nxtrabackup --copy-back --target-dir=/backup/base\nchown -R mysql:mysql /var/lib/mysql\nsystemctl start mysql\n</code></pre> <p>The prepare step is critical. A raw XtraBackup is not consistent - it contains data files from different points in time plus redo log entries. The prepare step replays the redo log to bring all data files to a consistent state, just like InnoDB crash recovery.</p>"},{"location":"Databases/backup-and-recovery/#incremental-backups","title":"Incremental Backups","text":"<p>XtraBackup supports incremental backups that only copy pages changed since the last backup. This dramatically reduces backup time and storage for large databases.</p> <pre><code># Full base backup (Sunday)\nxtrabackup --backup --target-dir=/backup/base\n\n# Incremental backup (Monday) - only changed pages\nxtrabackup --backup --target-dir=/backup/inc1 \\\n  --incremental-basedir=/backup/base\n\n# Incremental backup (Tuesday)\nxtrabackup --backup --target-dir=/backup/inc2 \\\n  --incremental-basedir=/backup/inc1\n</code></pre> <p>Restoring an incremental chain requires preparing each increment in order:</p> <pre><code># Prepare base (with --apply-log-only to preserve uncommitted transactions)\nxtrabackup --prepare --apply-log-only --target-dir=/backup/base\n\n# Apply incremental backups in order\nxtrabackup --prepare --apply-log-only --target-dir=/backup/base \\\n  --incremental-dir=/backup/inc1\n\nxtrabackup --prepare --apply-log-only --target-dir=/backup/base \\\n  --incremental-dir=/backup/inc2\n\n# Final prepare (without --apply-log-only)\nxtrabackup --prepare --target-dir=/backup/base\n\n# Restore\nsystemctl stop mysql\nrm -rf /var/lib/mysql/*\nxtrabackup --copy-back --target-dir=/backup/base\nchown -R mysql:mysql /var/lib/mysql\nsystemctl start mysql\n</code></pre> <p>The --apply-log-only flag</p> <p>Use <code>--apply-log-only</code> during intermediate prepare steps. Without it, XtraBackup rolls back uncommitted transactions, which makes subsequent incremental applies fail. Only omit <code>--apply-log-only</code> on the final prepare step.</p>"},{"location":"Databases/backup-and-recovery/#point-in-time-recovery-pitr","title":"Point-in-Time Recovery (PITR)","text":"<p>Backups give you a snapshot. Point-in-time recovery fills the gap between the snapshot and the moment of failure (or the moment before an accidental change). PITR replays transaction logs from the backup's position forward to a specific timestamp or log position.</p> <pre><code>flowchart LR\n    FB[Full Backup&lt;br/&gt;Sunday 02:00] --&gt; WAL1[WAL Segment 1]\n    WAL1 --&gt; WAL2[WAL Segment 2]\n    WAL2 --&gt; WAL3[WAL Segment 3]\n    WAL3 --&gt; F[Failure&lt;br/&gt;Wednesday 14:30]\n\n    FB --&gt; R[Restore Base Backup]\n    R --&gt; RP[Replay WAL/Binlog&lt;br/&gt;to Target Time]\n    RP --&gt; REC[Recovered Database&lt;br/&gt;Wednesday 14:29]</code></pre>"},{"location":"Databases/backup-and-recovery/#mysql-binary-log-replay","title":"MySQL: Binary Log Replay","text":"<p>MySQL's binary log records every data-modifying statement or row change. PITR replays these logs with <code>mysqlbinlog</code>.</p> <p>Step 1: Restore from the most recent full backup.</p> <pre><code>mysql &lt; /backup/mysql_full_20260218.sql\n</code></pre> <p>Step 2: Identify the binary logs created after the backup.</p> <pre><code>ls -la /var/lib/mysql/binlog.*\n</code></pre> <p>Step 3: Replay binary logs up to the target time (just before the accidental operation).</p> <pre><code>mysqlbinlog --stop-datetime=\"2026-02-19 14:30:00\" \\\n  /var/lib/mysql/binlog.000042 \\\n  /var/lib/mysql/binlog.000043 | mysql\n</code></pre> <p>To skip a specific bad transaction, use <code>--start-position</code> and <code>--stop-position</code> to replay around it:</p> <pre><code># Replay everything up to the bad statement\nmysqlbinlog --stop-position=12345 /var/lib/mysql/binlog.000042 | mysql\n\n# Skip the bad statement and continue from after it\nmysqlbinlog --start-position=12400 /var/lib/mysql/binlog.000042 | mysql\n</code></pre> <p>Binary logging must be enabled BEFORE disaster</p> <p>PITR only works if <code>log_bin</code> is enabled and binary logs exist from the backup point forward. Verify with <code>SHOW VARIABLES LIKE 'log_bin'</code>. If binary logging was not enabled, you can only restore to the backup snapshot - everything after it is lost.</p>"},{"location":"Databases/backup-and-recovery/#postgresql-wal-archiving-and-recovery","title":"PostgreSQL: WAL Archiving and Recovery","text":"<p>PostgreSQL's equivalent is Write-Ahead Log (WAL) archiving. Every change is written to WAL segments before it reaches the data files. By archiving these segments, you can replay them to recover to any point in time.</p> <p>Step 1: Configure WAL archiving in <code>postgresql.conf</code>:</p> <pre><code>wal_level = replica\narchive_mode = on\narchive_command = 'cp %p /backup/wal_archive/%f'\n</code></pre> <p>Step 2: Take a base backup:</p> <pre><code>pg_basebackup -D /backup/pg_base -Fp -Xs -P\n</code></pre> <p>Step 3: When recovery is needed, copy the base backup into position and configure recovery:</p> <pre><code># Stop PostgreSQL\nsystemctl stop postgresql\n\n# Replace the data directory with the backup\nrm -rf /var/lib/postgresql/16/main\ncp -a /backup/pg_base /var/lib/postgresql/16/main\n\n# Configure recovery target\ncat &gt;&gt; /var/lib/postgresql/16/main/postgresql.auto.conf &lt;&lt;EOF\nrestore_command = 'cp /backup/wal_archive/%f %p'\nrecovery_target_time = '2026-02-19 14:30:00'\nEOF\n\n# Create the recovery signal file\ntouch /var/lib/postgresql/16/main/recovery.signal\n\n# Start PostgreSQL - it will replay WAL to the target time\nchown -R postgres:postgres /var/lib/postgresql/16/main\nsystemctl start postgresql\n</code></pre> <p>PostgreSQL replays archived WAL segments using <code>restore_command</code> until it reaches <code>recovery_target_time</code>, then pauses. You verify the data and promote to normal operation:</p> <pre><code>SELECT pg_wal_replay_resume();\n</code></pre> <p>Point-in-Time Recovery with MySQL Binary Logs (requires JavaScript)</p> <p>You restored a MySQL backup taken at midnight and need to recover to 14:30. Binary logs binlog.000042 through binlog.000045 exist. What is the correct mysqlbinlog command? (requires JavaScript)</p>"},{"location":"Databases/backup-and-recovery/#backup-verification","title":"Backup Verification","text":"<p>A backup that has never been restored is an untested assumption. Schedule regular verification to ensure your backups actually work.</p>"},{"location":"Databases/backup-and-recovery/#restore-to-a-test-instance","title":"Restore to a Test Instance","text":"<p>The most reliable verification is a full restore to a separate server or container:</p> <pre><code># MySQL: restore to a test instance\nmysql -h test-db-host &lt; /backup/mysql_full_20260219.sql\nmysql -h test-db-host -e \"SELECT COUNT(*) FROM mydb.orders\"\n\n# PostgreSQL: restore to a test database\ncreatedb -h test-db-host mydb_test\npg_restore -d mydb_test -h test-db-host /backup/pg_myapp_20260219.dump\npsql -h test-db-host -d mydb_test -c \"SELECT COUNT(*) FROM orders\"\n</code></pre>"},{"location":"Databases/backup-and-recovery/#checksum-validation","title":"Checksum Validation","text":"<p>Verify backup file integrity before and after transfer:</p> <pre><code># Generate checksums at backup time\nsha256sum /backup/mysql_full_20260219.sql.gz &gt; /backup/mysql_full_20260219.sha256\n\n# Verify after transfer to offsite storage\nsha256sum -c /backup/mysql_full_20260219.sha256\n</code></pre> <p>For XtraBackup, the <code>--prepare</code> step itself validates consistency. If prepare succeeds, the backup is restorable.</p>"},{"location":"Databases/backup-and-recovery/#automated-verification-script","title":"Automated Verification Script","text":"<pre><code>#!/bin/bash\n# verify_backup.sh - restore and validate the latest backup\n\nBACKUP_FILE=$(ls -t /backup/mysql_full_*.sql.gz | head -1)\nTEST_DB=\"backup_verify_$(date +%Y%m%d)\"\n\n# Create test database and restore\nmysql -e \"CREATE DATABASE ${TEST_DB}\"\nzcat \"${BACKUP_FILE}\" | mysql \"${TEST_DB}\"\n\n# Run validation queries\nTABLES=$(mysql -NBe \"SELECT COUNT(*) FROM information_schema.tables WHERE table_schema='${TEST_DB}'\")\nROWS=$(mysql -NBe \"SELECT SUM(table_rows) FROM information_schema.tables WHERE table_schema='${TEST_DB}'\")\n\necho \"Backup: ${BACKUP_FILE}\"\necho \"Tables: ${TABLES}\"\necho \"Total rows: ${ROWS}\"\n\n# Compare against expected minimums\nif [ \"${TABLES}\" -lt 10 ]; then\n  echo \"ALERT: Table count below threshold\" &gt;&amp;2\n  exit 1\nfi\n\n# Cleanup\nmysql -e \"DROP DATABASE ${TEST_DB}\"\necho \"Verification passed\"\n</code></pre> <p>Automate verification weekly</p> <p>Run backup verification on a schedule, not just when you suspect a problem. A cron job that restores to a test instance and checks row counts catches silent backup failures before they become disasters.</p>"},{"location":"Databases/backup-and-recovery/#scheduling-and-rotation","title":"Scheduling and Rotation","text":""},{"location":"Databases/backup-and-recovery/#cron-based-backup-scripts","title":"Cron-Based Backup Scripts","text":"<p>A production backup script handles compression, logging, error reporting, and cleanup:</p> <pre><code>#!/bin/bash\n# mysql_backup.sh - daily MySQL backup with rotation\n\nset -euo pipefail\n\nBACKUP_DIR=\"/backup/mysql\"\nDATE=$(date +%Y%m%d_%H%M%S)\nLOG_FILE=\"${BACKUP_DIR}/backup_${DATE}.log\"\nRETENTION_DAYS=30\n\nexec &gt; &gt;(tee -a \"${LOG_FILE}\") 2&gt;&amp;1\necho \"Backup started: $(date)\"\n\n# Full logical backup\nmysqldump --single-transaction --routines --triggers --events \\\n  --all-databases | gzip &gt; \"${BACKUP_DIR}/full_${DATE}.sql.gz\"\n\n# Generate checksum\nsha256sum \"${BACKUP_DIR}/full_${DATE}.sql.gz\" &gt; \"${BACKUP_DIR}/full_${DATE}.sha256\"\n\n# Remove backups older than retention period\nfind \"${BACKUP_DIR}\" -name \"full_*.sql.gz\" -mtime +${RETENTION_DAYS} -delete\nfind \"${BACKUP_DIR}\" -name \"full_*.sha256\" -mtime +${RETENTION_DAYS} -delete\nfind \"${BACKUP_DIR}\" -name \"backup_*.log\" -mtime +${RETENTION_DAYS} -delete\n\necho \"Backup completed: $(date)\"\necho \"Size: $(du -h \"${BACKUP_DIR}/full_${DATE}.sql.gz\" | cut -f1)\"\n</code></pre> <p>Schedule with cron:</p> <pre><code># Daily at 2:00 AM\n0 2 * * * /usr/local/bin/mysql_backup.sh\n\n# Weekly XtraBackup full + daily incrementals\n0 3 * * 0 /usr/local/bin/xtrabackup_full.sh\n0 3 * * 1-6 /usr/local/bin/xtrabackup_incremental.sh\n</code></pre>"},{"location":"Databases/backup-and-recovery/#grandfather-father-son-rotation","title":"Grandfather-Father-Son Rotation","text":"<p>The GFS rotation strategy balances retention depth with storage costs:</p> Level Frequency Retention Purpose Son (daily) Every day 7-14 days Recent recovery, accidental changes Father (weekly) Every Sunday 4-5 weeks Weekly recovery points Grandfather (monthly) First of month 12 months Monthly recovery, compliance Yearly January 1 3-7 years Regulatory, legal holds <p>A 500 GB database with GFS rotation:</p> <ul> <li>14 daily backups: 14 x 500 GB = 7 TB (or ~700 GB compressed)</li> <li>4 weekly: 4 x 500 GB = 2 TB (or ~200 GB compressed)</li> <li>12 monthly: 12 x 500 GB = 6 TB (or ~600 GB compressed)</li> <li>Total: ~1.5 TB compressed storage for a full year of recovery points</li> </ul>"},{"location":"Databases/backup-and-recovery/#cloud-and-snapshot-backups","title":"Cloud and Snapshot Backups","text":""},{"location":"Databases/backup-and-recovery/#filesystem-snapshots","title":"Filesystem Snapshots","text":"<p>Filesystem snapshots (LVM, ZFS, EBS) create near-instant point-in-time copies of the underlying storage. They are the fastest backup method because they operate below the filesystem level.</p> <p>See also</p> <p>Filesystem-level backups often use LVM snapshots. For LVM concepts, snapshot creation, and filesystem management, see Disk and Filesystem.</p> <p>LVM snapshots:</p> <pre><code># Flush tables and acquire read lock\nmysql -e \"FLUSH TABLES WITH READ LOCK; SYSTEM lvcreate -L 10G -s -n mysql_snap /dev/vg0/mysql_data\"\n\n# Release the lock\nmysql -e \"UNLOCK TABLES\"\n\n# Mount and copy the snapshot\nmkdir /mnt/snap\nmount /dev/vg0/mysql_snap /mnt/snap\nrsync -a /mnt/snap/ /backup/mysql_snapshot/\numount /mnt/snap\nlvremove -f /dev/vg0/mysql_snap\n</code></pre> <p>ZFS snapshots:</p> <pre><code># Flush and snapshot (ZFS snapshots are atomic and instant)\nmysql -e \"FLUSH TABLES WITH READ LOCK\"\nzfs snapshot tank/mysql@backup_$(date +%Y%m%d)\nmysql -e \"UNLOCK TABLES\"\n\n# Send snapshot to remote for offsite storage\nzfs send tank/mysql@backup_20260219 | ssh backup-host zfs recv backup/mysql\n</code></pre> <p>AWS EBS snapshots:</p> <pre><code># Flush tables\nmysql -e \"FLUSH TABLES WITH READ LOCK\"\n\n# Create EBS snapshot\naws ec2 create-snapshot --volume-id vol-0123456789abcdef0 \\\n  --description \"MySQL backup $(date +%Y%m%d)\"\n\nmysql -e \"UNLOCK TABLES\"\n</code></pre> <p>Consistency during snapshots</p> <p>Without flushing tables and acquiring a lock (or using <code>xtrabackup</code>), a filesystem snapshot captures data files in an inconsistent state - some pages may be half-written. InnoDB's crash recovery can fix this on startup (similar to recovering from a power failure), but it is safer to flush first. For PostgreSQL, use <code>pg_backup_start()</code> and <code>pg_backup_stop()</code> (or <code>pg_start_backup()</code>/<code>pg_stop_backup()</code> on versions before 15) to ensure WAL consistency.</p>"},{"location":"Databases/backup-and-recovery/#consistency-considerations","title":"Consistency Considerations","text":"Database Snapshot Method Consistency Approach MySQL/InnoDB LVM/ZFS/EBS <code>FLUSH TABLES WITH READ LOCK</code> then snapshot, or use XtraBackup PostgreSQL LVM/ZFS/EBS <code>pg_backup_start()</code> / <code>pg_backup_stop()</code> + archive WAL Any Application-level Quiesce writes at application layer before snapshot"},{"location":"Databases/backup-and-recovery/#retention-policies","title":"Retention Policies","text":"<p>How long you keep backups depends on three factors: recovery requirements, regulatory obligations, and storage budget.</p>"},{"location":"Databases/backup-and-recovery/#recovery-requirements","title":"Recovery Requirements","text":"<p>Define your Recovery Point Objective (RPO) and Recovery Time Objective (RTO):</p> <ul> <li>RPO: How much data loss is acceptable? An RPO of 1 hour means you need backups or log archiving at least every hour.</li> <li>RTO: How quickly must you restore service? An RTO of 30 minutes rules out logical backups for large databases.</li> </ul>"},{"location":"Databases/backup-and-recovery/#regulatory-requirements","title":"Regulatory Requirements","text":"<p>Some industries mandate specific retention periods:</p> Regulation Retention Requirement PCI DSS 1 year minimum for audit logs HIPAA 6 years for health records SOX 7 years for financial records GDPR Only as long as necessary (minimum retention) <p>GDPR is the outlier - it requires you to delete data you no longer need, which can conflict with long backup retention. Consult your compliance team.</p>"},{"location":"Databases/backup-and-recovery/#storage-cost-optimization","title":"Storage Cost Optimization","text":"<p>Strategies for managing backup storage costs:</p> <ul> <li>Tiered storage: keep recent backups on fast storage (SSD/NVMe), move older backups to cheaper storage (HDD, object storage like S3 Glacier)</li> <li>Incremental chains: store one full backup per week and incrementals daily (reduces storage by 60-80%)</li> <li>Compression: <code>gzip</code> reduces logical backups by 80-90%; <code>zstd</code> offers better compression ratios at similar speed</li> <li>Deduplication: tools like <code>borgbackup</code> or ZFS deduplication eliminate redundant blocks across backup versions</li> </ul> <pre><code># Example: tiered storage with S3\n# Recent backups on local disk\n/backup/mysql/daily/     # last 7 days\n\n# Weekly backups pushed to S3 Standard\naws s3 cp /backup/mysql/weekly/ s3://mycompany-db-backups/weekly/ --recursive\n\n# Monthly backups moved to S3 Glacier\naws s3 cp /backup/mysql/monthly/ s3://mycompany-db-backups/monthly/ \\\n  --storage-class GLACIER --recursive\n</code></pre>"},{"location":"Databases/backup-and-recovery/#backup-strategy-design","title":"Backup Strategy Design","text":"<p>Design a Backup Strategy (requires JavaScript)</p>"},{"location":"Databases/backup-and-recovery/#further-reading","title":"Further Reading","text":"<ul> <li>MySQL Backup and Recovery Documentation - official MySQL backup reference covering all built-in tools and strategies</li> <li>PostgreSQL Continuous Archiving and Point-in-Time Recovery - official PostgreSQL PITR documentation with WAL archiving setup</li> <li>Percona XtraBackup Documentation - full reference for hot InnoDB backups, incremental strategies, and restore procedures</li> <li>mydumper Project - parallel MySQL logical backup tool documentation and usage</li> <li>pgBackRest - production-grade PostgreSQL backup tool with parallel backup/restore, incremental backups, and S3 support</li> <li>Barman by EnterpriseDB - backup and recovery manager for PostgreSQL with remote backup, WAL archiving, and retention policies</li> </ul> <p>Previous: Redis | Next: Database Security | Back to Index</p>"},{"location":"Databases/database-design/","title":"Database Design &amp; Modeling","text":"<p>Good schema design is the foundation that everything else rests on - query performance, data integrity, application complexity, and operational burden. A poorly designed schema creates problems that no amount of indexing or hardware can fix. A well-designed one makes queries obvious, constraints enforceable, and future changes manageable.</p> <p>This guide covers the full lifecycle of turning requirements into schema: modeling entities and relationships, normalizing to eliminate redundancy, choosing when to denormalize for performance, selecting appropriate data types, enforcing integrity through constraints, and building indexes that make your queries fast.</p>"},{"location":"Databases/database-design/#entity-relationship-modeling","title":"Entity-Relationship Modeling","text":"<p>Before writing any <code>CREATE TABLE</code> statements, you need a clear picture of what your data looks like and how the pieces connect. Entity-Relationship (ER) diagrams are the standard tool for this. They capture the nouns in your system (entities), the attributes those nouns have, and the connections (relationships) between them.</p>"},{"location":"Databases/database-design/#entities-and-attributes","title":"Entities and Attributes","text":"<p>An entity is a thing your system tracks - a customer, an order, a product, an employee. Each entity has attributes: a customer has a name and email, an order has a date and total, a product has a price and description.</p> <p>In a relational database, each entity becomes a table. Each attribute becomes a column.</p>"},{"location":"Databases/database-design/#relationships-and-cardinality","title":"Relationships and Cardinality","text":"<p>Entities connect to each other through relationships, and those relationships have cardinality - rules about how many of one entity can relate to another:</p> Cardinality Meaning Example One-to-one (1:1) Each row in A matches exactly one row in B User and user profile One-to-many (1:N) Each row in A matches zero or more rows in B Customer and orders Many-to-many (M:N) Rows in A can match multiple rows in B and vice versa Students and courses <p>Many-to-many relationships require a junction table (also called a join table or associative table) to implement in a relational database. The junction table holds foreign keys pointing to both sides.</p>"},{"location":"Databases/database-design/#reading-an-er-diagram","title":"Reading an ER Diagram","text":"<p>Here is an ER diagram for a simple e-commerce system. Each line represents a relationship, and the symbols at each end indicate cardinality:</p> <pre><code>erDiagram\n    CUSTOMER ||--o{ ORDER : places\n    ORDER ||--|{ ORDER_LINE : contains\n    PRODUCT ||--o{ ORDER_LINE : \"appears in\"\n    PRODUCT }o--|| CATEGORY : \"belongs to\"\n    CUSTOMER {\n        int customer_id PK\n        varchar name\n        varchar email\n        timestamp created_at\n    }\n    ORDER {\n        int order_id PK\n        int customer_id FK\n        date order_date\n        decimal total\n        varchar status\n    }\n    ORDER_LINE {\n        int order_id FK\n        int product_id FK\n        int quantity\n        decimal unit_price\n    }\n    PRODUCT {\n        int product_id PK\n        int category_id FK\n        varchar name\n        decimal price\n        text description\n    }\n    CATEGORY {\n        int category_id PK\n        varchar name\n    }</code></pre> <p>The notation works like this:</p> <ul> <li><code>||</code> means \"exactly one\"</li> <li><code>o{</code> means \"zero or more\"</li> <li><code>|{</code> means \"one or more\"</li> </ul> <p>So <code>CUSTOMER ||--o{ ORDER</code> reads as: each customer places zero or more orders, and each order belongs to exactly one customer.</p> <p>Start with the diagram</p> <p>Sketch your ER diagram before writing SQL. It forces you to think about relationships and cardinality up front, and catches design mistakes that are painful to fix after tables are populated with data.</p> <p>In the e-commerce ER diagram above, why does ORDER_LINE exist as a separate table instead of putting product details directly in ORDER? (requires JavaScript)</p>"},{"location":"Databases/database-design/#normalization","title":"Normalization","text":"<p>Normalization is the process of organizing columns and tables to reduce data redundancy and prevent update anomalies. Each normal form builds on the previous one, imposing stricter rules about how data is structured.</p> <p>The goal is not normalization for its own sake. The goal is a schema where each fact is stored exactly once, so updates cannot create inconsistencies.</p>"},{"location":"Databases/database-design/#first-normal-form-1nf","title":"First Normal Form (1NF)","text":"<p>A table is in first normal form if:</p> <ol> <li>Every column contains atomic (indivisible) values - no lists, sets, or nested structures</li> <li>Each row is unique (the table has a primary key)</li> </ol> <p>Problem - a table violating 1NF:</p> order_id customer products 1 Alice Widget, Gadget, Sprocket 2 Bob Gadget <p>The <code>products</code> column contains a comma-separated list. You cannot query for all orders containing \"Widget\" without string parsing. You cannot count products per order without splitting strings. You cannot enforce that each product name is valid.</p> <p>Fix - split into atomic values with a proper relationship:</p> order_id customer 1 Alice 2 Bob order_id product 1 Widget 1 Gadget 1 Sprocket 2 Gadget <p>Now each cell holds one value, each row is unique, and you can query, index, and constrain the data properly.</p>"},{"location":"Databases/database-design/#second-normal-form-2nf","title":"Second Normal Form (2NF)","text":"<p>A table is in second normal form if:</p> <ol> <li>It is in 1NF</li> <li>Every non-key column depends on the entire primary key, not just part of it</li> </ol> <p>2NF only matters for tables with composite primary keys. If your primary key is a single column, 1NF compliance automatically gives you 2NF.</p> <p>Problem - a table violating 2NF:</p> order_id product_id product_name quantity 1 10 Widget 3 1 20 Gadget 1 2 10 Widget 5 <p>The primary key is (<code>order_id</code>, <code>product_id</code>). But <code>product_name</code> depends only on <code>product_id</code>, not on the full composite key. If you rename \"Widget\" to \"Super Widget\", you must update every row where <code>product_id = 10</code>. Miss one and your data is inconsistent.</p> <p>Fix - move partial dependencies to their own table:</p> <p>order_lines:</p> order_id product_id quantity 1 10 3 1 20 1 2 10 5 <p>products:</p> product_id product_name 10 Widget 20 Gadget <p>Now <code>product_name</code> is stored once and referenced by <code>product_id</code>.</p>"},{"location":"Databases/database-design/#third-normal-form-3nf","title":"Third Normal Form (3NF)","text":"<p>A table is in third normal form if:</p> <ol> <li>It is in 2NF</li> <li>No non-key column depends on another non-key column (no transitive dependencies)</li> </ol> <p>Problem - a table violating 3NF:</p> employee_id name department_id department_name department_location 1 Alice 10 Engineering Building A 2 Bob 10 Engineering Building A 3 Carol 20 Marketing Building B <p>Here, <code>department_name</code> and <code>department_location</code> depend on <code>department_id</code>, not on <code>employee_id</code>. If Engineering moves to Building C, you update multiple rows. Miss one and you have Engineering in two buildings.</p> <p>Fix - extract the transitive dependency:</p> <p>employees:</p> employee_id name department_id 1 Alice 10 2 Bob 10 3 Carol 20 <p>departments:</p> department_id department_name department_location 10 Engineering Building A 20 Marketing Building B <p>The department fact is stored once. Updates happen in one place.</p>"},{"location":"Databases/database-design/#boyce-codd-normal-form-bcnf","title":"Boyce-Codd Normal Form (BCNF)","text":"<p>Boyce-Codd Normal Form is a stricter version of 3NF. A table is in BCNF if every determinant (a column or set of columns that other columns depend on) is a candidate key.</p> <p>The difference from 3NF is subtle and only surfaces in tables with multiple overlapping candidate keys. Here is a classic example:</p> <p>Problem - a table in 3NF but not BCNF:</p> <p>Consider a university where each subject is taught by one professor, but a professor can teach only one subject. Students can enroll in multiple subjects.</p> student_id subject professor 1 Math Dr. Smith 1 Physics Dr. Jones 2 Math Dr. Smith <p>The candidate keys are (<code>student_id</code>, <code>subject</code>) and (<code>student_id</code>, <code>professor</code>). But <code>subject</code> determines <code>professor</code> and <code>professor</code> determines <code>subject</code> - and neither is a candidate key on its own. If Dr. Smith switches from Math to Chemistry, you update multiple rows.</p> <p>Fix - decompose so every determinant is a candidate key:</p> <p>professor_subjects:</p> professor subject Dr. Smith Math Dr. Jones Physics <p>student_enrollments:</p> student_id professor 1 Dr. Smith 1 Dr. Jones 2 Dr. Smith <p>Now the <code>professor -&gt; subject</code> dependency lives in a table where <code>professor</code> is the key.</p> <p>Normalization is a tool, not a religion</p> <p>In practice, most production schemas aim for 3NF. BCNF violations are rare and usually harmless. Going beyond 3NF (there are 4NF, 5NF, and even 6NF) brings diminishing returns and increasing complexity. Know the rules so you can make informed decisions about when to follow them and when to break them.</p> <p>A table has columns (order_id, product_id, product_name, quantity) with primary key (order_id, product_id). product_name depends only on product_id. Which normal form does this table violate? (requires JavaScript)</p>"},{"location":"Databases/database-design/#denormalization","title":"Denormalization","text":"<p>Fully normalized schemas store each fact once, which is excellent for writes and data integrity. But reads can suffer because answering a single question may require joining five or six tables. Denormalization is the deliberate introduction of redundancy to speed up reads.</p>"},{"location":"Databases/database-design/#when-to-denormalize","title":"When to Denormalize","text":"<p>Denormalization makes sense when:</p> <ul> <li>A query is run frequently and the joins are measurably slow</li> <li>The data changes rarely but is read constantly (high read-to-write ratio)</li> <li>Reporting queries need pre-aggregated data to avoid scanning millions of rows</li> <li>You have measured the problem with <code>EXPLAIN</code> and confirmed that joins are the bottleneck</li> </ul> <p>Denormalization does not make sense when:</p> <ul> <li>You are guessing about performance before measuring</li> <li>Write consistency is critical and you cannot tolerate stale duplicated data</li> <li>The application is write-heavy</li> </ul>"},{"location":"Databases/database-design/#common-denormalization-strategies","title":"Common Denormalization Strategies","text":"<p>Pre-computed columns. Store a calculated value directly on the row instead of computing it at query time:</p> <pre><code>-- Instead of SUM(order_lines.quantity * order_lines.unit_price) every time:\nALTER TABLE orders ADD COLUMN total_amount DECIMAL(10,2);\n</code></pre> <p>You update <code>total_amount</code> whenever an order line changes. The trade-off is that your application code (or a trigger) must keep it in sync.</p> <p>Duplicated columns. Copy a frequently accessed column from a related table:</p> <pre><code>-- Instead of joining to customers for every order display:\nALTER TABLE orders ADD COLUMN customer_name VARCHAR(255);\n</code></pre> <p>If the customer's name changes, every order must be updated. This is acceptable if name changes are rare and order listing queries are constant.</p> <p>Summary tables. Maintain a separate table with pre-aggregated data:</p> <pre><code>CREATE TABLE daily_sales_summary (\n    summary_date DATE PRIMARY KEY,\n    total_orders INT NOT NULL,\n    total_revenue DECIMAL(12,2) NOT NULL,\n    avg_order_value DECIMAL(10,2) NOT NULL\n);\n</code></pre> <p>Populated by a nightly batch job or maintained by triggers. Dashboard queries hit this table instead of scanning the entire orders table.</p> <p>Denormalization creates maintenance burden</p> <p>Every piece of duplicated data is a consistency risk. When you denormalize, document which columns are derived, what the source of truth is, and how synchronization happens (trigger, application code, batch job). If you cannot answer those questions, do not denormalize.</p>"},{"location":"Databases/database-design/#index-theory","title":"Index Theory","text":"<p>Indexes are separate data structures that help the database find rows without scanning every page in the table. Without an index, a query against a million-row table reads every row. With the right index, it reads a handful of pages.</p>"},{"location":"Databases/database-design/#b-tree-indexes","title":"B-tree Indexes","text":"<p>The B-tree (balanced tree) is the default index structure in virtually every relational database. It keeps data sorted in a tree structure where:</p> <ul> <li>The root node points to intermediate nodes</li> <li>Intermediate nodes point to leaf nodes</li> <li>Leaf nodes contain the indexed values and pointers to the actual table rows</li> <li>All leaf nodes are at the same depth, so lookup time is consistent</li> </ul> <p>B-tree indexes support:</p> <ul> <li>Exact match lookups: <code>WHERE email = 'alice@example.com'</code></li> <li>Range scans: <code>WHERE price BETWEEN 10 AND 50</code></li> <li>Prefix matching: <code>WHERE name LIKE 'Ali%'</code></li> <li>Sorting: <code>ORDER BY created_at DESC</code></li> <li>Min/max: <code>SELECT MIN(price) FROM products</code></li> </ul> <p>They do not efficiently support:</p> <ul> <li>Suffix or infix matching: <code>WHERE name LIKE '%ice'</code></li> <li>Functions on the indexed column: <code>WHERE YEAR(created_at) = 2025</code> (unless you create a functional index)</li> </ul> <p>Composite indexes (multi-column) follow the leftmost prefix rule. An index on <code>(country, city, zipcode)</code> can satisfy queries on <code>country</code>, <code>country + city</code>, or <code>country + city + zipcode</code>, but not <code>city</code> alone or <code>zipcode</code> alone.</p>"},{"location":"Databases/database-design/#hash-indexes","title":"Hash Indexes","text":"<p>Hash indexes use a hash function to map values to buckets. They are fast for exact equality lookups (<code>=</code> and <code>IN</code>) but cannot help with range queries, sorting, or prefix matching.</p> <p>In MySQL, the MEMORY storage engine supports explicit hash indexes. InnoDB uses an internal adaptive hash index that it builds automatically on top of B-tree indexes for frequently accessed pages - you do not create it manually.</p> <p>In PostgreSQL, you can create hash indexes explicitly:</p> <pre><code>CREATE INDEX idx_sessions_token ON sessions USING HASH (token);\n</code></pre> <p>Hash indexes in PostgreSQL were not crash-safe until version 10. If you are on PostgreSQL 10+, they are a reasonable choice for columns that are only ever queried with <code>=</code>.</p>"},{"location":"Databases/database-design/#gin-and-gist-indexes-postgresql","title":"GIN and GiST Indexes (PostgreSQL)","text":"<p>PostgreSQL offers specialized index types for non-scalar data:</p> <p>GIN (Generalized Inverted Index) is designed for values that contain multiple elements - arrays, JSONB documents, and full-text search vectors. A GIN index on a JSONB column lets you query for documents containing specific keys or values efficiently:</p> <pre><code>CREATE INDEX idx_metadata ON events USING GIN (metadata);\n-- Now this query uses the index:\nSELECT * FROM events WHERE metadata @&gt; '{\"type\": \"click\"}';\n</code></pre> <p>GiST (Generalized Search Tree) supports geometric data, ranges, and full-text search. It is the index type for PostGIS spatial queries and range type operations:</p> <pre><code>CREATE INDEX idx_location ON stores USING GIST (location);\n-- Spatial query using the index:\nSELECT * FROM stores WHERE location &lt;-&gt; point '(40.7, -74.0)' &lt; 0.1;\n</code></pre> Index Type Best For Range Queries Equality Multi-Value B-tree Most columns Yes Yes No Hash Equality-only lookups No Yes No GIN JSONB, arrays, full-text No Yes Yes GiST Spatial, ranges, full-text Yes Yes Yes <p>Creating Indexes and Reading EXPLAIN Output (requires JavaScript)</p> <p>Index only what you query</p> <p>Every index speeds up reads but slows down writes because the database must update the index on every <code>INSERT</code>, <code>UPDATE</code>, and <code>DELETE</code>. Do not create indexes speculatively. Look at your actual query patterns, identify slow queries with <code>EXPLAIN</code>, and add indexes to address specific bottlenecks.</p> <p>You have a composite B-tree index on (country, city, zipcode). Which of these queries can use the index? (requires JavaScript)</p>"},{"location":"Databases/database-design/#data-type-selection","title":"Data Type Selection","text":"<p>Choosing the right data type affects storage size, query performance, and the correctness of your application. The wrong type can waste disk space, prevent useful optimizations, or silently lose precision.</p>"},{"location":"Databases/database-design/#integers","title":"Integers","text":"Type Storage Range Use When <code>TINYINT</code> / <code>SMALLINT</code> 1-2 bytes Up to 32,767 (signed) Status codes, small enumerations <code>INT</code> 4 bytes Up to ~2.1 billion Most primary keys, counters, foreign keys <code>BIGINT</code> 8 bytes Up to ~9.2 quintillion Tables expected to exceed 2 billion rows, timestamps as epoch milliseconds <p>Use <code>INT</code> for primary keys unless you have a concrete reason for <code>BIGINT</code>. A table with a <code>BIGINT</code> primary key and three <code>BIGINT</code> foreign keys uses 16 bytes more per row than the <code>INT</code> equivalent. Over 100 million rows, that is 1.6 GB of extra storage just for those four columns - and the indexes are larger too, which means more memory pressure and slower scans.</p> <p>Use <code>BIGINT</code> when you know the table will grow past 2 billion rows, or when you are storing values like Unix timestamps in milliseconds or external IDs from systems that use 64-bit identifiers.</p>"},{"location":"Databases/database-design/#strings","title":"Strings","text":"Type Behavior Use When <code>CHAR(N)</code> Fixed-length, padded with spaces Codes with known length (country codes, currency codes) <code>VARCHAR(N)</code> Variable-length, stores only what you need Most string data (names, emails, addresses) <code>TEXT</code> Variable-length, no practical limit Large content (descriptions, comments, articles) <p>In PostgreSQL, there is no performance difference between <code>VARCHAR(N)</code> and <code>TEXT</code>. The length limit on <code>VARCHAR(N)</code> is purely a constraint - if you want to enforce a maximum length, use it. If you do not need a maximum, <code>TEXT</code> is simpler.</p> <p>In MySQL, <code>VARCHAR</code> columns longer than a threshold (roughly 768 bytes for InnoDB) cannot be fully included in B-tree indexes. Keep indexed <code>VARCHAR</code> columns reasonably sized.</p>"},{"location":"Databases/database-design/#numeric-precision","title":"Numeric Precision","text":"Type Behavior Use When <code>DECIMAL(P,S)</code> / <code>NUMERIC(P,S)</code> Exact arithmetic Money, financial calculations, anything where rounding errors are unacceptable <code>FLOAT</code> / <code>DOUBLE</code> Approximate (IEEE 754 floating point) Scientific measurements, coordinates, values where small rounding errors are acceptable <p>Never store money as <code>FLOAT</code> or <code>DOUBLE</code>. The value <code>0.1</code> cannot be represented exactly in binary floating point:</p> <pre><code>-- This might not return your row:\nSELECT * FROM accounts WHERE balance = 0.10;\n\n-- Use DECIMAL instead:\nCREATE TABLE accounts (\n    account_id INT PRIMARY KEY,\n    balance DECIMAL(12,2) NOT NULL DEFAULT 0.00\n);\n</code></pre>"},{"location":"Databases/database-design/#timestamps","title":"Timestamps","text":"Type Stores Use When <code>TIMESTAMP</code> Date and time, usually UTC-aware Event times, <code>created_at</code>, <code>updated_at</code> <code>DATETIME</code> (MySQL) Date and time, no timezone conversion Scheduling (you want the literal date/time stored) <code>TIMESTAMPTZ</code> (PostgreSQL) Timestamp with timezone awareness Almost always - PostgreSQL converts to UTC on storage and back to the session timezone on retrieval <code>DATE</code> Date only Birthdays, due dates, report dates <p>Always store timestamps in UTC</p> <p>Store all timestamps in UTC and convert to local time in the application layer. Mixing timezones in the database leads to subtle bugs that are extraordinarily painful to debug. In PostgreSQL, use <code>TIMESTAMPTZ</code>. In MySQL, set your server to <code>UTC</code> and use <code>TIMESTAMP</code>.</p>"},{"location":"Databases/database-design/#boolean-and-enumeration-types","title":"Boolean and Enumeration Types","text":"<p>For columns with a small fixed set of values, you have options:</p> <ul> <li><code>BOOLEAN</code> (or <code>TINYINT(1)</code> in MySQL) for true/false flags</li> <li><code>ENUM</code> types for small fixed sets - convenient but creates schema coupling (adding a new value requires <code>ALTER TABLE</code>)</li> <li>A <code>VARCHAR</code> column with a <code>CHECK</code> constraint - more flexible than <code>ENUM</code> and easier to extend</li> <li>A separate lookup table with a foreign key - the most normalized approach, best when the set of values may grow or has its own attributes</li> </ul>"},{"location":"Databases/database-design/#constraints","title":"Constraints","text":"<p>Constraints are rules enforced by the database engine that prevent invalid data from entering your tables. They are your last line of defense against application bugs, bad imports, and manual mistakes.</p>"},{"location":"Databases/database-design/#primary-key","title":"PRIMARY KEY","text":"<p>Every table should have a primary key - a column or combination of columns that uniquely identifies each row. The database enforces uniqueness and creates an index automatically.</p> <pre><code>CREATE TABLE products (\n    product_id INT PRIMARY KEY,\n    name VARCHAR(255) NOT NULL\n);\n\n-- Composite primary key:\nCREATE TABLE order_lines (\n    order_id INT,\n    product_id INT,\n    quantity INT NOT NULL,\n    PRIMARY KEY (order_id, product_id)\n);\n</code></pre>"},{"location":"Databases/database-design/#foreign-key","title":"FOREIGN KEY","text":"<p>A foreign key constraint ensures that a value in one table references a valid row in another table. It prevents orphaned records and enforces referential integrity.</p> <p>The <code>ON DELETE</code> and <code>ON UPDATE</code> clauses control what happens when the referenced row changes:</p> Action Behavior <code>CASCADE</code> Delete/update the child rows automatically <code>SET NULL</code> Set the foreign key column to <code>NULL</code> <code>SET DEFAULT</code> Set the foreign key column to its default value <code>RESTRICT</code> Block the delete/update if child rows exist (checked immediately) <code>NO ACTION</code> Same as <code>RESTRICT</code> in most databases (checked at end of statement in PostgreSQL) <pre><code>CREATE TABLE orders (\n    order_id INT PRIMARY KEY,\n    customer_id INT NOT NULL,\n    FOREIGN KEY (customer_id) REFERENCES customers(customer_id)\n        ON DELETE RESTRICT\n        ON UPDATE CASCADE\n);\n</code></pre> <p>This means: if you try to delete a customer who has orders, the database blocks it (<code>RESTRICT</code>). If you change a customer's ID, the change propagates to all their orders (<code>CASCADE</code>).</p>"},{"location":"Databases/database-design/#unique-check-not-null-and-default","title":"UNIQUE, CHECK, NOT NULL, and DEFAULT","text":"<pre><code>CREATE TABLE employees (\n    employee_id INT PRIMARY KEY,\n    email VARCHAR(255) NOT NULL UNIQUE,\n    name VARCHAR(255) NOT NULL,\n    department_id INT REFERENCES departments(department_id),\n    salary DECIMAL(10,2) CHECK (salary &gt; 0),\n    hire_date DATE NOT NULL DEFAULT CURRENT_DATE,\n    status VARCHAR(20) NOT NULL DEFAULT 'active'\n        CHECK (status IN ('active', 'inactive', 'terminated'))\n);\n</code></pre> <ul> <li><code>NOT NULL</code> prevents missing values - use it on every column that should always have a value</li> <li><code>UNIQUE</code> prevents duplicates - use it for natural identifiers like email, username, or SKU</li> <li><code>CHECK</code> enforces arbitrary conditions - salaries must be positive, status must be one of a known set</li> <li><code>DEFAULT</code> provides a value when one is not specified during insert</li> </ul>"},{"location":"Databases/database-design/#putting-it-all-together","title":"Putting It All Together","text":"<p>Here is a complete schema demonstrating constraints, data types, and relationships. The following ER diagram shows the structure:</p> <pre><code>erDiagram\n    DEPARTMENT ||--o{ EMPLOYEE : employs\n    EMPLOYEE ||--o{ PROJECT_ASSIGNMENT : \"assigned to\"\n    PROJECT ||--o{ PROJECT_ASSIGNMENT : \"staffed by\"\n    DEPARTMENT {\n        int department_id PK\n        varchar name UK\n        varchar location\n    }\n    EMPLOYEE {\n        int employee_id PK\n        varchar email UK\n        varchar name\n        int department_id FK\n        decimal salary\n        date hire_date\n    }\n    PROJECT {\n        int project_id PK\n        varchar name\n        date start_date\n        date end_date\n    }\n    PROJECT_ASSIGNMENT {\n        int employee_id FK\n        int project_id FK\n        varchar role\n        date assigned_date\n    }</code></pre> <p>Complete Schema with Constraints (requires JavaScript)</p>"},{"location":"Databases/database-design/#design-decisions-in-practice","title":"Design Decisions in Practice","text":"<p>Real schema design involves trade-offs. Here is a summary of the key decisions and when each option wins:</p> Decision Option A Option B Choose A When Choose B When Normalization level 3NF (fully normalized) Denormalized with redundancy Write-heavy, data integrity critical Read-heavy dashboards, rare updates Primary key type Natural key (email, SSN) Surrogate key (auto-increment INT) Key is truly immutable and unique Natural keys can change or are large <code>INT</code> vs <code>BIGINT</code> <code>INT</code> (4 bytes) <code>BIGINT</code> (8 bytes) Table stays under 2B rows Table will exceed 2B rows <code>VARCHAR(N)</code> vs <code>TEXT</code> <code>VARCHAR(N)</code> <code>TEXT</code> You want a length constraint No meaningful max length <code>DECIMAL</code> vs <code>FLOAT</code> <code>DECIMAL</code> <code>FLOAT</code>/<code>DOUBLE</code> Money, exact arithmetic Scientific data, acceptable rounding <code>ENUM</code> vs <code>CHECK</code> <code>ENUM</code> type <code>VARCHAR</code> + <code>CHECK</code> MySQL with stable small sets PostgreSQL, or sets that may grow <code>CASCADE</code> vs <code>RESTRICT</code> <code>ON DELETE CASCADE</code> <code>ON DELETE RESTRICT</code> Child rows meaningless without parent Child rows should block parent deletion <p>Normalize a Flat Table (requires JavaScript)</p>"},{"location":"Databases/database-design/#further-reading","title":"Further Reading","text":"<ul> <li>Database Normalization Basics - Open textbook chapter with detailed worked examples through 3NF</li> <li>Use The Index, Luke - The definitive free resource on SQL indexing and query performance, covering B-tree internals, composite indexes, and execution plans</li> <li>PostgreSQL Index Types - Official documentation on B-tree, Hash, GIN, GiST, SP-GiST, and BRIN index types</li> <li>MySQL Data Types - Official MySQL reference for storage requirements and behavior of each data type</li> <li>SQL Constraints on W3Schools - Quick reference for PRIMARY KEY, FOREIGN KEY, UNIQUE, CHECK, NOT NULL, and DEFAULT</li> </ul> <p>Previous: SQL Essentials | Next: MySQL Installation &amp; Configuration | Back to Index</p>"},{"location":"Databases/database-fundamentals/","title":"Database Fundamentals","text":"<p>Every application you use - from the banking app on your phone to the search engine returning this page - depends on a database. But databases did not spring into existence fully formed. They evolved over decades, shaped by real engineering failures, shifting hardware constraints, and a few landmark academic papers. Understanding that evolution gives you a vocabulary and mental model that makes every subsequent guide in this course easier to absorb.</p>"},{"location":"Databases/database-fundamentals/#the-evolution-of-data-management","title":"The Evolution of Data Management","text":"<p>Before databases existed, programs stored data in flat files - plain text or binary files on disk, each structured however the programmer saw fit. A payroll system might write employee records as fixed-width lines in a <code>.dat</code> file. A different department's inventory system used a different format. There was no standard way to query, update, or relate data across systems. If the file format changed, every program that read it broke.</p> <p>This worked when computers served one department and ran one program at a time. It fell apart the moment organizations needed to share data across applications. The problems were predictable: data duplication across files led to inconsistencies, concurrent access caused corruption, and there was no mechanism for enforcing data integrity. If two programs tried to update the same file simultaneously, the result depended on timing - a problem known as a race condition.</p>"},{"location":"Databases/database-fundamentals/#hierarchical-model-ims","title":"Hierarchical Model: IMS","text":"<p>In 1966, IBM and North American Aviation (now part of Boeing) built IMS (Information Management System) to manage the bill of materials for the Saturn V rocket. IMS organized data as a tree - parent records owned child records, which owned grandchild records, and so on. Navigating from a rocket stage to its components to their suppliers meant walking down the tree.</p>  Saturn V first stage components at NASA's Michoud Assembly Facility, 1967. Tracking millions of parts like these drove the creation of IMS. Photo: NASA, public domain <p>The hierarchical model was fast for the access patterns it was designed for. If your queries always started at the root and walked downward, IMS performed well. But if you needed to query across branches - \"show me all suppliers who provide parts to more than one stage\" - you were stuck writing complex application code to traverse multiple trees and correlate the results.</p>"},{"location":"Databases/database-fundamentals/#network-model-codasyl","title":"Network Model: CODASYL","text":"<p>The CODASYL (Conference on Data Systems Languages) committee proposed the network model in 1969 to address the hierarchical model's rigidity. Instead of trees, data was organized as a graph: records could have multiple parent-child relationships through sets (essentially pointers linking record types). A supplier record could be linked to multiple part records across different assemblies.</p> <p>This was more flexible than IMS, but the programmer had to navigate the graph manually - following pointers from record to record. Queries were imperative: \"start at this record, follow this set, move to the next member.\" If the data relationships changed, the navigation code changed too. The database and the application were tightly coupled.</p>"},{"location":"Databases/database-fundamentals/#the-relational-revolution-codd","title":"The Relational Revolution: Codd","text":"The first page of Codd's landmark 1970 paper in Communications of the ACM.     Read the full paper (PDF) <p>In June 1970, Edgar F. Codd, a researcher at IBM's San Jose lab, published \"A Relational Model of Data for Large Shared Data Banks\" in Communications of the ACM. The paper proposed something radical: separate the logical organization of data from its physical storage. Data would be organized into relations (tables), and users would declare what data they wanted, not how to navigate to it.</p> <p>This was the birth of the relational model, and it changed everything. Instead of writing imperative navigation code, you would write declarative queries. The database system would figure out the optimal access path. This separation meant that the physical storage could change - indexes added or removed, data reorganized on disk - without breaking application code.</p>  Michael Stonebraker at UC Berkeley, co-creator of Ingres and the 2014 Turing Award recipient. Photo: Dcoetzee, CC0 <p>IBM was slow to commercialize Codd's ideas (IMS was a cash cow), but two Berkeley researchers, Michael Stonebraker and Eugene Wong, built Ingres in the mid-1970s.</p>  Larry Ellison, co-founder of Oracle, on stage at Oracle OpenWorld 2009. Photo: Oracle Corporate Communications, CC BY 2.0 <p>Larry Ellison read Codd's papers and built what became Oracle. IBM eventually shipped System R, which evolved into DB2. The SQL language emerged from System R and was standardized in 1986.</p>"},{"location":"Databases/database-fundamentals/#object-oriented-and-object-relational-databases","title":"Object-Oriented and Object-Relational Databases","text":"<p>In the late 1980s and 1990s, as object-oriented programming gained traction, some argued that the \"impedance mismatch\" between objects in code and tables in databases required a new approach. Object-oriented databases (OODBs) like GemStone, ObjectStore, and Versant stored objects directly, preserving inheritance and encapsulation.</p> <p>OODBs never displaced relational systems in the mainstream. The relational model's mathematical foundation, mature tooling, and the practical effectiveness of object-relational mapping (ORM) layers proved \"good enough.\" The ORM approach - mapping objects to rows through libraries like Hibernate (Java), SQLAlchemy (Python), and ActiveRecord (Ruby) - became the standard way to bridge the gap.</p> <p>However, the object-oriented movement influenced relational databases in lasting ways. PostgreSQL added support for user-defined types, table inheritance, and eventually JSON/JSONB columns - blurring the line between relational and object storage. MySQL 5.7 added a native JSON type. SQL Server added XML and JSON support. The relational model absorbed the useful ideas and kept its position.</p>"},{"location":"Databases/database-fundamentals/#the-nosql-movement","title":"The NoSQL Movement","text":"<p>By the mid-2000s, web-scale companies hit walls that traditional RDBMS systems were not built for. Google published the Bigtable paper (2006). Amazon published the Dynamo paper (2007). These systems sacrificed some relational guarantees - joins, rigid schemas, multi-row transactions - in exchange for horizontal scalability and availability across distributed clusters.</p> <p>The term NoSQL (sometimes backronymed as \"Not Only SQL\") became an umbrella for a diverse set of systems: document stores like MongoDB, key-value stores like Redis, wide-column stores like Apache Cassandra, and graph databases like Neo4j. Each traded away different relational features to optimize for specific access patterns or scale requirements.</p> <p>Today, the industry has largely moved past the \"NoSQL vs. SQL\" debate. Most organizations practice polyglot persistence - using whichever database best fits each workload. A single application might use PostgreSQL for transactional data, Redis for caching and session state, and Elasticsearch for full-text search.</p> <pre><code>graph LR\n    A[Flat Files&lt;br&gt;1950s-60s] --&gt; B[Hierarchical&lt;br&gt;IMS 1966]\n    B --&gt; C[Network&lt;br&gt;CODASYL 1969]\n    C --&gt; D[Relational&lt;br&gt;Codd 1970]\n    D --&gt; E[Object-Oriented&lt;br&gt;1980s-90s]\n    D --&gt; F[Object-Relational&lt;br&gt;1990s-2000s]\n    D --&gt; G[NoSQL&lt;br&gt;2000s-2010s]\n    G --&gt; H[Polyglot&lt;br&gt;Persistence]\n    F --&gt; H</code></pre> <p>Why did Edgar Codd's relational model represent such a fundamental shift from hierarchical and network databases? (requires JavaScript)</p>"},{"location":"Databases/database-fundamentals/#the-relational-model","title":"The Relational Model","text":"<p>The relational model is the foundation for MySQL, PostgreSQL, Oracle, SQL Server, SQLite, and every other RDBMS. Even if you work primarily with NoSQL systems, understanding relational concepts gives you a baseline for evaluating trade-offs.</p>"},{"location":"Databases/database-fundamentals/#tables-rows-and-columns","title":"Tables, Rows, and Columns","text":"<p>A table (or relation in formal terminology) is a structured collection of data about a single entity type. An <code>employees</code> table stores data about employees. An <code>orders</code> table stores data about orders.</p> <p>Each row (or tuple) represents one instance of that entity - one employee, one order. Each column (or attribute) represents one property of that entity - name, hire date, salary, order total.</p> <pre><code>CREATE TABLE employees (\n    employee_id   INT PRIMARY KEY,\n    first_name    VARCHAR(50) NOT NULL,\n    last_name     VARCHAR(50) NOT NULL,\n    department_id INT,\n    hire_date     DATE NOT NULL,\n    salary        DECIMAL(10,2)\n);\n</code></pre> <p>Every column has a data type that constrains what values it can hold. <code>INT</code> stores integers. <code>VARCHAR(50)</code> stores variable-length strings up to 50 characters. <code>DATE</code> stores calendar dates. <code>DECIMAL(10,2)</code> stores numbers with up to 10 digits and 2 decimal places. Choosing the right data type matters for storage efficiency, query performance, and data integrity.</p>"},{"location":"Databases/database-fundamentals/#keys","title":"Keys","text":"<p>A primary key uniquely identifies each row in a table. No two rows can share the same primary key value, and the value cannot be <code>NULL</code>. In the example above, <code>employee_id</code> is the primary key.</p> <p>A foreign key is a column (or set of columns) in one table that references the primary key of another table. It establishes a relationship between the two tables and enforces referential integrity - the database refuses to insert a row with a foreign key value that does not exist in the referenced table.</p> <pre><code>CREATE TABLE departments (\n    department_id  INT PRIMARY KEY,\n    department_name VARCHAR(100) NOT NULL\n);\n\n-- The foreign key links employees to departments\nALTER TABLE employees\nADD CONSTRAINT fk_department\nFOREIGN KEY (department_id) REFERENCES departments(department_id);\n</code></pre> <p>A composite key uses multiple columns together as the primary key. An <code>enrollments</code> table might use <code>(student_id, course_id)</code> as its composite primary key - neither column alone identifies a unique enrollment, but together they do.</p> <p>A candidate key is any column or combination of columns that could serve as the primary key. An <code>employees</code> table might have both <code>employee_id</code> (a surrogate key) and <code>email</code> (a natural key) as candidate keys. You pick one as the primary key; the others can be enforced with <code>UNIQUE</code> constraints.</p>"},{"location":"Databases/database-fundamentals/#relationships","title":"Relationships","text":"<p>Tables relate to each other through three patterns:</p> <p>One-to-many is the most common. One department has many employees. The \"many\" side holds the foreign key: <code>employees.department_id</code> references <code>departments.department_id</code>.</p> <p>Many-to-many uses a junction table (also called a join table or associative table). Students enroll in many courses; courses have many students. The <code>enrollments</code> table holds foreign keys to both <code>students</code> and <code>courses</code>.</p> <pre><code>CREATE TABLE enrollments (\n    student_id  INT REFERENCES students(student_id),\n    course_id   INT REFERENCES courses(course_id),\n    enrolled_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    PRIMARY KEY (student_id, course_id)\n);\n</code></pre> <p>One-to-one is less common. A <code>users</code> table and a <code>user_profiles</code> table might have a one-to-one relationship where each user has exactly one profile. This is often done to separate frequently accessed columns from rarely accessed ones, or to isolate sensitive data (like billing information) into a table with different access controls.</p> <p>An `order_items` table has columns `order_id` and `product_id` that together form a composite primary key. What does this enforce? (requires JavaScript)</p>"},{"location":"Databases/database-fundamentals/#acid-properties","title":"ACID Properties","text":"<p>When you transfer money between bank accounts, you need guarantees. The transfer should either complete fully or not happen at all. The account balances should remain consistent. Concurrent transfers should not corrupt each other. And once confirmed, the transfer should survive a power failure.</p> <p>These guarantees have a name: ACID. Every transaction in a relational database is expected to satisfy all four properties.</p>"},{"location":"Databases/database-fundamentals/#atomicity","title":"Atomicity","text":"<p>Atomicity means a transaction is all-or-nothing. If a transaction contains five SQL statements and the third one fails, the first two are rolled back. The database never reflects a partial transaction.</p> <p>Consider a bank transfer:</p> <pre><code>BEGIN TRANSACTION;\nUPDATE accounts SET balance = balance - 500 WHERE account_id = 1001;\nUPDATE accounts SET balance = balance + 500 WHERE account_id = 1002;\nCOMMIT;\n</code></pre> <p>If the system crashes after the first <code>UPDATE</code> but before the <code>COMMIT</code>, atomicity guarantees the first debit is rolled back. The $500 does not vanish.</p>"},{"location":"Databases/database-fundamentals/#consistency","title":"Consistency","text":"<p>Consistency means a transaction brings the database from one valid state to another. It cannot violate constraints - primary keys, foreign keys, check constraints, unique constraints, or any rules defined in the schema. If a transaction would leave the database in an invalid state, the database rejects it.</p> <p>For example, if a <code>CHECK</code> constraint requires <code>balance &gt;= 0</code>, a withdrawal that would push the balance below zero is rejected - even if the SQL is syntactically correct.</p> <pre><code>ALTER TABLE accounts ADD CONSTRAINT positive_balance CHECK (balance &gt;= 0);\n\n-- This transaction is rejected because it violates the CHECK constraint\nBEGIN TRANSACTION;\nUPDATE accounts SET balance = balance - 10000 WHERE account_id = 1001;\n-- ERROR: new row for relation \"accounts\" violates check constraint \"positive_balance\"\nROLLBACK;\n</code></pre>"},{"location":"Databases/database-fundamentals/#isolation","title":"Isolation","text":"<p>Isolation determines how concurrent transactions interact. Without isolation, two simultaneous transactions could read and write the same rows, producing results that neither transaction intended.</p> <p>SQL defines four isolation levels, each trading correctness for performance:</p> Isolation Level Dirty Reads Non-Repeatable Reads Phantom Reads Read Uncommitted Possible Possible Possible Read Committed Prevented Possible Possible Repeatable Read Prevented Prevented Possible Serializable Prevented Prevented Prevented <p>A dirty read sees data from another transaction that has not yet committed (and might be rolled back). A non-repeatable read means re-reading a row within the same transaction returns different values because another transaction modified and committed it in between. A phantom read means a query returns different rows on re-execution because another transaction inserted or deleted matching rows.</p> <p>Most production databases default to Read Committed (PostgreSQL, Oracle) or Repeatable Read (MySQL/InnoDB). Serializable provides the strongest guarantees but reduces concurrency.</p> <p>Isolation is not free</p> <p>Higher isolation levels reduce concurrency. Serializable isolation can cause transactions to wait or abort due to conflicts. Choose the lowest isolation level that provides the correctness guarantees your application requires. Most OLTP workloads run fine at Read Committed.</p>"},{"location":"Databases/database-fundamentals/#durability","title":"Durability","text":"<p>Durability means that once a transaction is committed, the data persists even if the system crashes, loses power, or the operating system panics. Databases achieve this through write-ahead logging (WAL) - changes are written to a sequential log on durable storage before being applied to the actual data files. On recovery, the database replays the log to reconstruct any committed transactions that had not yet been flushed to the data files.</p> <p>WAL is everywhere</p> <p>Write-ahead logging is not unique to databases. Journaling filesystems like ext4 and XFS use the same principle. The idea is ancient in computing terms: write your intentions to a sequential log before modifying the actual data structure. If you crash, replay the log.</p> <p>A banking application executes a transfer: debit account A, then credit account B. The system crashes after the debit but before the credit is committed. What ACID property ensures the debit is rolled back? (requires JavaScript)</p>"},{"location":"Databases/database-fundamentals/#cap-theorem","title":"CAP Theorem","text":"<p>In 2000, Eric Brewer proposed the CAP theorem (formally proved by Gilbert and Lynch in 2002), which states that a distributed data system can provide at most two of three guarantees simultaneously:</p> <ul> <li>Consistency - every read receives the most recent write (all nodes see the same data at the same time)</li> <li>Availability - every request receives a response (no timeouts, even if some nodes are down)</li> <li>Partition tolerance - the system continues to operate despite network partitions between nodes</li> </ul> <p>Since network partitions are inevitable in distributed systems (cables get cut, switches fail, cloud availability zones lose connectivity), the practical choice is between CP (consistency + partition tolerance) and AP (availability + partition tolerance).</p> <p>CAP Consistency is not ACID Consistency</p> <p>These are different definitions of \"consistency.\" ACID consistency means a transaction does not violate database constraints. CAP consistency (also called linearizability) means all nodes in a distributed system agree on the current value of every piece of data at any given moment. A single-node PostgreSQL instance provides ACID consistency but the CAP theorem does not apply to it because it is not distributed.</p>"},{"location":"Databases/database-fundamentals/#real-world-cap-trade-offs","title":"Real-World CAP Trade-offs","text":"<p>CP systems prioritize consistency over availability. If a network partition occurs, the system refuses to serve requests on the side that cannot confirm it has the latest data, rather than risk returning stale results.</p> <ul> <li>etcd and ZooKeeper - coordination services that use consensus protocols (Raft, ZAB) to ensure all reads return the latest write. During a partition, the minority side becomes unavailable.</li> <li>HBase - a wide-column store built on HDFS that provides strong consistency for row-level operations.</li> <li>Traditional RDBMS clusters with synchronous replication behave as CP systems.</li> </ul> <p>AP systems prioritize availability over consistency. During a partition, all nodes continue serving requests, but some may return stale data.</p> <ul> <li>Cassandra - a wide-column store designed for write-heavy workloads across multiple data centers. With tunable consistency levels, Cassandra defaults to eventual consistency but can be configured for stronger guarantees per query.</li> <li>DynamoDB - Amazon's managed key-value and document store, designed from the Dynamo paper's principles. Eventually consistent reads are the default; strongly consistent reads are available at higher cost.</li> <li>CouchDB - a document store designed around eventual consistency and multi-master replication.</li> </ul> <p>In practice, CAP is a spectrum, not a strict three-way partition. Systems like Cassandra offer tunable consistency - you can request <code>QUORUM</code> reads (a majority of replicas must agree) to get stronger guarantees at the cost of higher latency and reduced availability during partitions.</p> System CAP Category Consistency Model Use Case PostgreSQL (single node) N/A (not distributed) Strong (ACID) Transactional workloads etcd / ZooKeeper CP Linearizable Cluster coordination, config Cassandra AP (tunable) Eventual to strong Write-heavy, multi-DC DynamoDB AP (tunable) Eventual or strong Managed key-value at scale MongoDB (replica set) CP Linearizable (with majority reads) Document storage CockroachDB CP Serializable Distributed SQL <p>A globally distributed e-commerce platform needs a database for shopping cart data. Carts are accessed by session ID, must remain available even during network partitions between data centers, and brief staleness is acceptable. Which CAP category best fits this requirement? (requires JavaScript)</p>"},{"location":"Databases/database-fundamentals/#storage-engine-landscape","title":"Storage Engine Landscape","text":"<p>The storage engine is the component that actually writes data to disk, reads it back, manages indexes, and handles concurrency. The same database server can sometimes use different storage engines for different tables.</p>"},{"location":"Databases/database-fundamentals/#b-trees-and-b-trees","title":"B-Trees and B+ Trees","text":"<p>The B-tree (and its variant the B+ tree) is the dominant index structure in relational databases. It keeps data sorted and allows searches, insertions, and deletions in O(log n) time. B+ trees store all data in leaf nodes and link them together, making range scans efficient - you find the starting leaf and follow pointers to traverse sequential values.</p> <p>Almost every RDBMS uses B+ trees for its primary index structure: InnoDB, PostgreSQL, Oracle, SQL Server, and SQLite. The Database Design guide covers index internals in depth.</p>"},{"location":"Databases/database-fundamentals/#innodb-mysql","title":"InnoDB (MySQL)","text":"<p>InnoDB is the default storage engine for MySQL and MariaDB. It is a clustered index engine: the table data is physically stored in primary key order within a B+ tree. Secondary indexes store a copy of the primary key value and require a lookup back to the clustered index to retrieve the full row.</p> <p>Key characteristics:</p> <ul> <li>Full ACID compliance with row-level locking</li> <li>Multi-version concurrency control (MVCC) for non-locking reads</li> <li>Write-ahead logging (the redo log) for crash recovery</li> <li>The buffer pool caches data and index pages in memory</li> <li>Doublewrite buffer prevents torn pages on crash</li> <li>Foreign key support</li> </ul>"},{"location":"Databases/database-fundamentals/#myisam-mysql-legacy","title":"MyISAM (MySQL - Legacy)","text":"<p>MyISAM was the default MySQL engine before version 5.5. It uses table-level locking, has no transaction support, and does not enforce foreign keys. MyISAM stores data in a heap file (rows in insertion order) with separate B-tree index files.</p> <p>MyISAM still exists but is rarely appropriate for new applications. It appears in legacy systems and in MySQL's <code>system</code> tablespace for internal tables (though MySQL 8.0 moved those to InnoDB as well).</p> <p>MyISAM is not crash-safe</p> <p>If MySQL crashes while MyISAM is writing data, the table can become corrupted and require repair with <code>myisamchk</code> or <code>REPAIR TABLE</code>. InnoDB's write-ahead log and doublewrite buffer prevent this class of failure. If you encounter MyISAM tables in a legacy system, migrating them to InnoDB is almost always worthwhile.</p>"},{"location":"Databases/database-fundamentals/#postgresqls-heap-storage","title":"PostgreSQL's Heap Storage","text":"<p>PostgreSQL uses a heap-based storage engine. Table rows are stored in an unordered heap, and all indexes (including the primary key index) are secondary indexes that point to the row's physical location via a tuple identifier (TID - a page number and offset).</p> <p>Key characteristics:</p> <ul> <li>MVCC implemented through tuple versioning - old row versions coexist with new ones in the heap</li> <li>VACUUM reclaims space from dead tuples left by updates and deletes</li> <li>TOAST (The Oversized-Attribute Storage Technique) transparently compresses and out-of-line stores large values</li> <li>All indexes are equal - there is no clustered index by default (though <code>CLUSTER</code> can reorder the heap once)</li> </ul>"},{"location":"Databases/database-fundamentals/#lsm-trees","title":"LSM-Trees","text":"<p>Log-Structured Merge-trees (LSM-trees) optimize for write-heavy workloads. Instead of updating data in place (as B-trees do), LSM-trees write everything sequentially to an in-memory buffer (the memtable), then flush it as sorted immutable files (called SSTables or sorted string tables) to disk. Background processes periodically merge and compact these files.</p> <p>LSM-trees power:</p> <ul> <li>RocksDB (developed by Facebook, used as an embedded engine in many systems)</li> <li>Cassandra (each node's local storage)</li> <li>LevelDB (Google's original implementation)</li> <li>MyRocks (an LSM-tree storage engine for MySQL, based on RocksDB)</li> </ul> <p>The trade-off: LSM-trees have faster writes and better space efficiency for write-heavy workloads, but reads can be slower because they may need to check multiple SSTables. Bloom filters are used to skip SSTables that definitely do not contain the requested key.</p> Property B+ Tree (InnoDB, PostgreSQL) LSM-Tree (RocksDB, Cassandra) Write pattern Random I/O (update in place) Sequential I/O (append-only) Read pattern Single lookup (O(log n)) May check multiple files Write throughput Moderate High Read latency Low and predictable Higher, varies with compaction Space amplification Lower (one copy of data) Higher (multiple copies during compaction) Best for Read-heavy, OLTP Write-heavy, time-series, logs <p>Exploring a MySQL Database (requires JavaScript)</p>"},{"location":"Databases/database-fundamentals/#rdbms-vs-nosql-a-decision-framework","title":"RDBMS vs. NoSQL: A Decision Framework","text":"<p>The choice between a relational database and a NoSQL system is not about which technology is \"better.\" It is about which set of trade-offs fits your workload, consistency requirements, and operational constraints.</p>"},{"location":"Databases/database-fundamentals/#when-to-choose-an-rdbms","title":"When to Choose an RDBMS","text":"<p>Relational databases are the right default when:</p> <ul> <li>Your data has clear relationships. Orders have line items. Customers have addresses. Products belong to categories. The relational model expresses these naturally with foreign keys and joins.</li> <li>You need ACID transactions across multiple entities. Transferring funds, placing orders with inventory checks, updating a user and their permissions in one atomic operation - these require multi-statement transactions with rollback.</li> <li>Your schema is relatively stable. If you know your entities and their attributes up front and they change slowly, a defined schema protects data quality and enables the optimizer to generate efficient query plans.</li> <li>You need ad-hoc queries. SQL is expressive enough to answer questions you did not anticipate when designing the schema. Business intelligence, reporting, and analytics workflows depend on this flexibility.</li> <li>Your data fits on a single server (or a small cluster). Most applications never outgrow what a single well-provisioned PostgreSQL or MySQL instance can handle. Vertical scaling is simpler than distributed systems.</li> </ul>"},{"location":"Databases/database-fundamentals/#when-to-choose-nosql","title":"When to Choose NoSQL","text":"<p>NoSQL systems earn their keep when:</p> <ul> <li>Your data is semi-structured or schema-less. Product catalogs where each item has different attributes. User-generated content with varying fields. Configuration data that evolves rapidly. Document stores handle this naturally.</li> <li>You need horizontal scalability for write-heavy workloads. Time-series data, event logs, IoT sensor data, clickstream analytics - workloads that generate millions of writes per second across distributed nodes.</li> <li>Your access patterns are known and narrow. Key-value stores excel when you always look up data by a known key. If 95% of your queries are \"get user profile by user_id,\" a key-value or document store may serve that pattern with lower latency than a relational join.</li> <li>Availability trumps consistency. If your system must remain responsive even during network partitions (a global CDN, a shopping cart that should never return an error), an AP system like Cassandra or DynamoDB may be appropriate.</li> <li>You need specialized data models. Graph databases for social networks and recommendation engines. Time-series databases for metrics and monitoring. Search engines for full-text queries. These are purpose-built tools, not general-purpose replacements for an RDBMS.</li> </ul>"},{"location":"Databases/database-fundamentals/#the-hybrid-approach","title":"The Hybrid Approach","text":"<p>Most production architectures use both. A common pattern:</p> <ul> <li>PostgreSQL or MySQL as the system of record for transactional data</li> <li>Redis as a caching layer and for session storage</li> <li>Elasticsearch or OpenSearch for search and log analytics</li> <li>A message queue (Kafka, RabbitMQ) for decoupling writes between systems</li> </ul> <p>Start relational, add NoSQL when you have evidence</p> <p>If you are starting a new project and are unsure which database to use, start with PostgreSQL or MySQL. They are well-documented, widely supported, and handle most workloads. Add specialized NoSQL systems later when you have concrete evidence that a specific workload needs different trade-offs - not because you anticipate scaling problems that may never materialize.</p>"},{"location":"Databases/database-fundamentals/#putting-it-all-together","title":"Putting It All Together","text":"<p>Choosing the Right Database (requires JavaScript)</p>"},{"location":"Databases/database-fundamentals/#further-reading","title":"Further Reading","text":"<ul> <li>A Relational Model of Data for Large Shared Data Banks - E.F. Codd (1970) - the paper that started the relational revolution</li> <li>Designing Data-Intensive Applications - Martin Kleppmann - the best modern overview of database internals, distributed systems, and data engineering trade-offs</li> <li>Use The Index, Luke - Markus Winand - a practical guide to SQL indexing and B-tree internals</li> <li>CAP Twelve Years Later: How the \"Rules\" Have Changed - Eric Brewer - Brewer's own retrospective on the CAP theorem</li> <li>MySQL InnoDB Storage Engine Documentation - official reference for InnoDB internals</li> <li>PostgreSQL Documentation - Chapter 70: Database Physical Storage - how PostgreSQL organizes data on disk</li> </ul> <p>Next: SQL Essentials | Back to Index</p>"},{"location":"Databases/database-security/","title":"Database Security","text":"<p>A database is only as secure as its weakest access path. Misconfigurations, default credentials, unencrypted connections, and injectable queries have been behind the majority of data breaches for decades. This guide covers the full security surface of production database systems - authentication, encryption, injection prevention, auditing, and privilege hardening - across both MySQL and PostgreSQL.</p>"},{"location":"Databases/database-security/#authentication-methods","title":"Authentication Methods","text":"<p>Authentication is the first gate. Every connection to your database must prove identity before it can do anything. Different methods vary in strength, complexity, and operational overhead.</p>"},{"location":"Databases/database-security/#password-based-authentication","title":"Password-Based Authentication","text":"<p>The most common method. Both MySQL and PostgreSQL support multiple password hashing plugins:</p> Plugin/Method Database Notes <code>mysql_native_password</code> MySQL SHA-1 based, legacy default pre-8.0 <code>caching_sha2_password</code> MySQL SHA-256 based, default in MySQL 8.0+ <code>scram-sha-256</code> PostgreSQL Salted challenge-response, default in PG 14+ <code>md5</code> PostgreSQL Legacy, still common but weaker <p>MySQL password plugin configuration:</p> <pre><code>-- Create a user with a specific auth plugin (MySQL 8.0+)\nCREATE USER 'app_user'@'10.0.0.%'\n  IDENTIFIED WITH caching_sha2_password BY 'strong_random_passphrase';\n\n-- Check which plugin a user is using\nSELECT user, host, plugin FROM mysql.user WHERE user = 'app_user';\n\n-- Change an existing user's plugin\nALTER USER 'legacy_user'@'%'\n  IDENTIFIED WITH caching_sha2_password BY 'new_passphrase';\n</code></pre> <p>PostgreSQL password method configuration in <code>pg_hba.conf</code>:</p> <pre><code># TYPE  DATABASE  USER       ADDRESS         METHOD\nhost    all       app_user   10.0.0.0/24     scram-sha-256\nhost    all       all        127.0.0.1/32    scram-sha-256\n</code></pre> <p>Set the default in <code>postgresql.conf</code>:</p> <pre><code>password_encryption = scram-sha-256\n</code></pre> <p>Avoid md5 and mysql_native_password in new deployments</p> <p>Both use weak hashing algorithms. <code>mysql_native_password</code> stores SHA-1 hashes that are vulnerable to rainbow table attacks. PostgreSQL's <code>md5</code> method is similarly outdated. Use <code>caching_sha2_password</code> for MySQL and <code>scram-sha-256</code> for PostgreSQL.</p>"},{"location":"Databases/database-security/#certificate-based-authentication-x509","title":"Certificate-Based Authentication (x509)","text":"<p>Instead of passwords, clients present a TLS certificate signed by a trusted Certificate Authority. The database verifies the certificate chain.</p> <p>MySQL x509 authentication:</p> <pre><code>-- Require a valid client certificate\nCREATE USER 'secure_user'@'%'\n  IDENTIFIED WITH caching_sha2_password BY 'passphrase'\n  REQUIRE X509;\n\n-- Require a specific certificate subject\nCREATE USER 'strict_user'@'%'\n  REQUIRE SUBJECT '/CN=app-server/O=MyCompany';\n</code></pre> <p>PostgreSQL certificate authentication in <code>pg_hba.conf</code>:</p> <pre><code># Client must present a valid certificate signed by the server's root CA\nhostssl   all   cert_user   10.0.0.0/24   cert   clientcert=verify-full\n</code></pre>"},{"location":"Databases/database-security/#ldap-integration","title":"LDAP Integration","text":"<p>LDAP authentication delegates credential verification to a directory service like Active Directory or OpenLDAP. Users authenticate with their corporate credentials.</p> <p>MySQL LDAP plugin:</p> <pre><code>-- Install the LDAP plugin (MySQL Enterprise)\nINSTALL PLUGIN authentication_ldap_simple\n  SONAME 'authentication_ldap_simple.so';\n\n-- Create a user that authenticates against LDAP\nCREATE USER 'ldap_user'@'%'\n  IDENTIFIED WITH authentication_ldap_simple;\n</code></pre> <p>PostgreSQL LDAP in <code>pg_hba.conf</code>:</p> <pre><code>host  all  all  10.0.0.0/24  ldap\n  ldapserver=ldap.company.com\n  ldapbasedn=\"ou=People,dc=company,dc=com\"\n  ldapsearchattribute=uid\n</code></pre>"},{"location":"Databases/database-security/#kerberosgssapi-and-pam","title":"Kerberos/GSSAPI and PAM","text":"<p>For enterprise environments with existing Kerberos infrastructure:</p> <ul> <li>Kerberos/GSSAPI: Both MySQL (with the <code>authentication_kerberos</code> plugin) and PostgreSQL (via the <code>gss</code> method in <code>pg_hba.conf</code>) support Kerberos tickets for single sign-on</li> <li>PAM (Pluggable Authentication Modules): Delegates authentication to the operating system's PAM stack, which can chain multiple authentication backends - LDAP, Kerberos, local files, MFA tokens</li> </ul> <p>PostgreSQL PAM configuration:</p> <pre><code>host  all  all  10.0.0.0/24  pam  pamservice=postgresql\n</code></pre> <p>The corresponding PAM service file (<code>/etc/pam.d/postgresql</code>) defines the actual authentication chain.</p> <p>Your company uses Active Directory for all employee credentials. Which authentication method avoids creating separate database passwords? (requires JavaScript)</p>"},{"location":"Databases/database-security/#tlsssl-configuration","title":"TLS/SSL Configuration","text":"<p>Authentication proves identity, but without encryption the credentials (and all subsequent data) travel in plaintext. TLS (Transport Layer Security, the successor to SSL) encrypts the connection between client and server.</p>"},{"location":"Databases/database-security/#generating-certificates","title":"Generating Certificates","text":"<p>For production, use certificates from your organization's internal CA or a public CA. For testing, self-signed certificates work:</p> <pre><code># Generate a CA key and certificate\nopenssl genrsa 4096 &gt; ca-key.pem\nopenssl req -new -x509 -nodes -days 3650 \\\n  -key ca-key.pem -out ca-cert.pem \\\n  -subj \"/CN=MySQL-CA/O=MyCompany\"\n\n# Generate the server key and certificate signing request\nopenssl genrsa 4096 &gt; server-key.pem\nopenssl req -new -key server-key.pem -out server-req.pem \\\n  -subj \"/CN=db-server.company.com\"\n\n# Sign the server certificate with the CA\nopenssl x509 -req -in server-req.pem -days 3650 \\\n  -CA ca-cert.pem -CAkey ca-key.pem -CAcreateserial \\\n  -out server-cert.pem\n\n# Generate client key and certificate (for x509 auth)\nopenssl genrsa 4096 &gt; client-key.pem\nopenssl req -new -key client-key.pem -out client-req.pem \\\n  -subj \"/CN=app-server/O=MyCompany\"\nopenssl x509 -req -in client-req.pem -days 3650 \\\n  -CA ca-cert.pem -CAkey ca-key.pem -CAcreateserial \\\n  -out client-cert.pem\n</code></pre>"},{"location":"Databases/database-security/#mysql-tls-configuration","title":"MySQL TLS Configuration","text":"<p>In <code>my.cnf</code> (or <code>my.ini</code> on Windows):</p> <pre><code>[mysqld]\n# Require all connections to use TLS\nrequire_secure_transport = ON\n\n# Certificate paths\nssl-ca   = /etc/mysql/ssl/ca-cert.pem\nssl-cert = /etc/mysql/ssl/server-cert.pem\nssl-key  = /etc/mysql/ssl/server-key.pem\n\n# Minimum TLS version\ntls_version = TLSv1.2,TLSv1.3\n</code></pre> <p>Verify the server's TLS status:</p> <pre><code>SHOW VARIABLES LIKE '%ssl%';\n-- ssl_ca, ssl_cert, ssl_key should show your paths\n\nSHOW STATUS LIKE 'Ssl_cipher';\n-- Should show a cipher name like TLS_AES_256_GCM_SHA384\n</code></pre>"},{"location":"Databases/database-security/#postgresql-tls-configuration","title":"PostgreSQL TLS Configuration","text":"<p>In <code>postgresql.conf</code>:</p> <pre><code>ssl = on\nssl_cert_file = '/etc/postgresql/ssl/server-cert.pem'\nssl_key_file  = '/etc/postgresql/ssl/server-key.pem'\nssl_ca_file   = '/etc/postgresql/ssl/ca-cert.pem'\nssl_min_protocol_version = 'TLSv1.2'\n</code></pre> <p>In <code>pg_hba.conf</code>, use <code>hostssl</code> instead of <code>host</code> to require TLS:</p> <pre><code># Reject non-TLS connections from the network\nhostssl   all   all   10.0.0.0/24   scram-sha-256\nhostnossl all   all   10.0.0.0/24   reject\n</code></pre>"},{"location":"Databases/database-security/#verifying-encrypted-connections","title":"Verifying Encrypted Connections","text":"<p>MySQL client verification:</p> <pre><code>-- Check your current connection\nSHOW STATUS LIKE 'Ssl_cipher';\n-- Non-empty = encrypted\n\nSELECT * FROM performance_schema.session_status\nWHERE variable_name = 'Ssl_version';\n</code></pre> <p>PostgreSQL client verification:</p> <pre><code>-- In psql\n\\conninfo\n-- Should show \"SSL connection (protocol: TLSv1.3, cipher: ...)\"\n\nSELECT ssl, version, cipher FROM pg_stat_ssl\nWHERE pid = pg_backend_pid();\n</code></pre> <p>Configuring and Verifying MySQL TLS (requires JavaScript)</p> <p>In PostgreSQL, what is the difference between 'host' and 'hostssl' entries in pg_hba.conf? (requires JavaScript)</p>"},{"location":"Databases/database-security/#encryption-at-rest","title":"Encryption at Rest","text":"<p>TLS protects data in transit. Encryption at rest protects data stored on disk - against stolen drives, unauthorized filesystem access, or compromised backups.</p>"},{"location":"Databases/database-security/#transparent-data-encryption-tde","title":"Transparent Data Encryption (TDE)","text":"<p>TDE encrypts data files at the storage engine level. The database handles encryption and decryption transparently - applications need no changes.</p> <p>MySQL Enterprise TDE (InnoDB tablespace encryption):</p> <pre><code>-- Install the keyring plugin (required for TDE)\n-- In my.cnf:\n-- early-plugin-load = keyring_file.so\n-- keyring_file_data = /var/lib/mysql-keyring/keyring\n\n-- Encrypt a tablespace\nALTER TABLE customers ENCRYPTION = 'Y';\n\n-- Encrypt the system tablespace\nALTER TABLESPACE mysql.innodb_system ENCRYPTION = 'Y';\n\n-- Verify encryption status\nSELECT name, encryption\nFROM information_schema.innodb_tablespaces\nWHERE encryption = 'Y';\n</code></pre> <p>Key management is the hard part</p> <p>TDE is only as secure as the encryption keys. Storing the keyring file on the same disk as the database defeats the purpose - a stolen disk includes both the encrypted data and the key. Use a dedicated Key Management Service (KMS) like HashiCorp Vault, AWS KMS, or Azure Key Vault in production.</p>"},{"location":"Databases/database-security/#column-level-encryption-with-pgcrypto","title":"Column-Level Encryption with pgcrypto","text":"<p>PostgreSQL does not have built-in TDE in the community edition, but the pgcrypto extension provides column-level encryption:</p> <pre><code>-- Enable pgcrypto\nCREATE EXTENSION IF NOT EXISTS pgcrypto;\n\n-- Encrypt sensitive columns using symmetric encryption (AES-256)\nINSERT INTO customers (name, ssn_encrypted)\nVALUES (\n  'Jane Smith',\n  pgp_sym_encrypt('123-45-6789', 'encryption-key-from-app')\n);\n\n-- Decrypt when reading\nSELECT name, pgp_sym_decrypt(ssn_encrypted, 'encryption-key-from-app') AS ssn\nFROM customers\nWHERE id = 42;\n</code></pre> <p>The encryption key should come from the application or a KMS - never hardcode it in SQL or store it in the database.</p>"},{"location":"Databases/database-security/#filesystem-level-encryption","title":"Filesystem-Level Encryption","text":"<p>An alternative to database-level encryption is encrypting the entire filesystem or block device:</p> <ul> <li>LUKS/dm-crypt (Linux): Encrypts block devices. The database writes normally; the kernel encrypts and decrypts transparently</li> <li>BitLocker (Windows): Full-disk encryption for Windows Server</li> <li>AWS EBS Encryption / Azure Disk Encryption: Cloud provider managed encryption for virtual disks</li> </ul> <pre><code># Example: LUKS encryption for a database volume\nsudo cryptsetup luksFormat /dev/sdb1\nsudo cryptsetup luksOpen /dev/sdb1 db_encrypted\nsudo mkfs.ext4 /dev/mapper/db_encrypted\nsudo mount /dev/mapper/db_encrypted /var/lib/mysql\n</code></pre> <p>Filesystem encryption protects against physical theft but not against a compromised OS or database process - if the system is running, the filesystem is mounted and decrypted.</p>"},{"location":"Databases/database-security/#sql-injection-prevention","title":"SQL Injection Prevention","text":"<p>SQL injection remains the most exploited database vulnerability year after year. It occurs when user input is concatenated directly into SQL strings, allowing an attacker to modify the query's logic.</p>"},{"location":"Databases/database-security/#the-problem-string-concatenation","title":"The Problem: String Concatenation","text":"<pre><code># VULNERABLE - never do this\nusername = request.form['username']\nquery = \"SELECT * FROM users WHERE username = '\" + username + \"'\"\ncursor.execute(query)\n# If username = \"admin' OR '1'='1\" the query becomes:\n# SELECT * FROM users WHERE username = 'admin' OR '1'='1'\n# This returns ALL users\n</code></pre>"},{"location":"Databases/database-security/#the-solution-parameterized-queries","title":"The Solution: Parameterized Queries","text":"<p>Parameterized queries (also called prepared statements) separate the SQL structure from the data. The database engine treats parameters as literal values, never as SQL syntax.</p> <p>Parameterized Queries: Safe vs Unsafe Patterns (requires JavaScript)</p>"},{"location":"Databases/database-security/#stored-procedures-as-a-defense-layer","title":"Stored Procedures as a Defense Layer","text":"<p>Stored procedures provide an additional layer of protection by encapsulating SQL logic on the server:</p> <pre><code>-- MySQL stored procedure\nDELIMITER //\nCREATE PROCEDURE get_user_by_email(IN p_email VARCHAR(255))\nBEGIN\n  SELECT id, username, email, created_at\n  FROM users\n  WHERE email = p_email;\nEND //\nDELIMITER ;\n\n-- Call from application\nCALL get_user_by_email('user@example.com');\n</code></pre> <p>The application only calls the procedure with parameters - it never constructs raw SQL. The procedure controls exactly which columns are returned and which tables are accessed.</p>"},{"location":"Databases/database-security/#input-validation","title":"Input Validation","text":"<p>Parameterized queries are the primary defense. Input validation is a secondary layer:</p> <ul> <li>Allowlist validation: If a field should be an integer, cast it to an integer before use</li> <li>Length limits: Reject inputs exceeding expected lengths</li> <li>Character restrictions: Email fields should match email patterns</li> <li>Reject known attack patterns: While not sufficient alone, blocking inputs containing <code>'; --</code> or <code>UNION SELECT</code> adds depth</li> </ul>"},{"location":"Databases/database-security/#orm-safety","title":"ORM Safety","text":"<p>Object-Relational Mappers (ORMs) like SQLAlchemy, Django ORM, Sequelize, and Hibernate use parameterized queries internally. They are generally safe, but watch for escape hatches:</p> <pre><code># Django ORM - safe by default\nUser.objects.filter(email=user_email)\n\n# Django raw query - safe if parameterized\nUser.objects.raw(\"SELECT * FROM users WHERE email = %s\", [user_email])\n\n# Django raw query - UNSAFE if concatenated\nUser.objects.raw(\"SELECT * FROM users WHERE email = '\" + user_email + \"'\")\n</code></pre> <p>ORMs don't make you immune</p> <p>Every ORM provides a way to execute raw SQL. When you use it, you are responsible for parameterization. Code review should flag any raw SQL that concatenates user input.</p>"},{"location":"Databases/database-security/#audit-logging","title":"Audit Logging","text":"<p>You cannot protect what you cannot observe. Audit logging records who did what, when, and from where - essential for compliance (PCI DSS, HIPAA, SOX), incident investigation, and detecting unauthorized access.</p>"},{"location":"Databases/database-security/#mysql-enterprise-audit","title":"MySQL Enterprise Audit","text":"<p>The MySQL Enterprise Audit plugin provides comprehensive logging:</p> <pre><code>-- Install the audit plugin\nINSTALL PLUGIN audit_log SONAME 'audit_log.so';\n\n-- Configure in my.cnf\n-- [mysqld]\n-- audit-log-format = JSON\n-- audit-log-policy = ALL\n-- audit-log-file = /var/log/mysql/audit.log\n</code></pre> <p>For the community edition, the general query log provides basic auditing:</p> <pre><code>[mysqld]\ngeneral_log = ON\ngeneral_log_file = /var/log/mysql/general.log\n</code></pre> <p>Performance impact of general_log</p> <p>The general query log records every single query and causes significant I/O overhead. Use it for short-term debugging or auditing, not as a permanent audit solution on high-traffic systems. The Enterprise Audit plugin has filtering capabilities that reduce overhead.</p>"},{"location":"Databases/database-security/#postgresql-pgaudit","title":"PostgreSQL pgAudit","text":"<p>pgAudit is the standard auditing extension for PostgreSQL:</p> <pre><code>-- Enable in postgresql.conf:\n-- shared_preload_libraries = 'pgaudit'\n-- pgaudit.log = 'ddl, role, write'\n\n-- Per-database override\nALTER DATABASE production SET pgaudit.log = 'all';\n\n-- Per-role override (audit everything the admin role does)\nALTER ROLE dba SET pgaudit.log = 'all';\n</code></pre> <p>pgAudit log classes:</p> Class What it logs <code>read</code> SELECT, COPY FROM <code>write</code> INSERT, UPDATE, DELETE, TRUNCATE <code>function</code> Function calls <code>role</code> GRANT, REVOKE, CREATE/ALTER/DROP ROLE <code>ddl</code> CREATE, ALTER, DROP (tables, indexes, etc.) <code>misc</code> DISCARD, FETCH, CHECKPOINT <code>all</code> Everything above"},{"location":"Databases/database-security/#what-to-log","title":"What to Log","text":"<p>At minimum, audit these events:</p> <ul> <li>Authentication events: Successful and failed login attempts (track brute-force patterns)</li> <li>DDL changes: Schema modifications (CREATE, ALTER, DROP) - who changed the table structure?</li> <li>DML on sensitive tables: INSERT, UPDATE, DELETE on tables containing PII, financial data, or credentials</li> <li>Privilege changes: GRANT, REVOKE, CREATE ROLE, ALTER USER</li> <li>Administrative actions: Configuration changes, backup operations, replication setup</li> </ul>"},{"location":"Databases/database-security/#log-management","title":"Log Management","text":"<p>Raw audit logs are useless if nobody reviews them:</p> <ul> <li>Centralize: Ship logs to a SIEM (Splunk, Elasticsearch, Datadog) for search and alerting</li> <li>Protect: Audit logs should be write-only for the database process, stored on separate storage, and backed up independently</li> <li>Retain: Define retention policies based on compliance requirements (PCI DSS requires 1 year, HIPAA requires 6 years)</li> <li>Alert: Set up automated alerts for suspicious patterns - failed login spikes, after-hours DDL, mass data exports</li> </ul> <p>You need to audit all schema changes and privilege modifications in PostgreSQL, but you want to minimize performance impact. Which pgaudit.log setting is most appropriate? (requires JavaScript)</p>"},{"location":"Databases/database-security/#privilege-hardening","title":"Privilege Hardening","text":"<p>Even authenticated users should only have access to what they need. Privilege hardening applies the principle of least privilege: every account gets the minimum permissions required for its function.</p>"},{"location":"Databases/database-security/#separate-accounts-by-function","title":"Separate Accounts by Function","text":"<p>Never use a single account for everything. Create purpose-specific accounts:</p> Account Purpose Typical Privileges <code>app_read</code> Application read queries SELECT on specific tables <code>app_write</code> Application write operations SELECT, INSERT, UPDATE, DELETE on specific tables <code>admin</code> DBA administration Full privileges (used interactively, never by applications) <code>backup_user</code> Backup operations SELECT, LOCK TABLES, SHOW VIEW, RELOAD, REPLICATION CLIENT <code>monitor_user</code> Monitoring/metrics SELECT on <code>performance_schema</code>, <code>information_schema</code> <code>migration_user</code> Schema migrations CREATE, ALTER, DROP, INDEX on application database"},{"location":"Databases/database-security/#mysql-privilege-hardening","title":"MySQL Privilege Hardening","text":"<pre><code>-- Create a read-only application account\nCREATE USER 'app_read'@'10.0.0.%'\n  IDENTIFIED WITH caching_sha2_password BY 'read_passphrase';\nGRANT SELECT ON myapp.* TO 'app_read'@'10.0.0.%';\n\n-- Create a write account with limited DML\nCREATE USER 'app_write'@'10.0.0.%'\n  IDENTIFIED WITH caching_sha2_password BY 'write_passphrase';\nGRANT SELECT, INSERT, UPDATE, DELETE ON myapp.* TO 'app_write'@'10.0.0.%';\n\n-- Monitoring account\nCREATE USER 'monitor'@'10.0.0.%'\n  IDENTIFIED WITH caching_sha2_password BY 'monitor_passphrase';\nGRANT SELECT ON performance_schema.* TO 'monitor'@'10.0.0.%';\nGRANT PROCESS ON *.* TO 'monitor'@'10.0.0.%';\n\n-- Review existing privileges - look for overly broad grants\nSELECT grantee, privilege_type, table_schema\nFROM information_schema.schema_privileges\nORDER BY grantee;\n</code></pre>"},{"location":"Databases/database-security/#postgresql-privilege-hardening","title":"PostgreSQL Privilege Hardening","text":"<pre><code>-- Revoke default public schema access (critical in PostgreSQL)\nREVOKE ALL ON SCHEMA public FROM PUBLIC;\nREVOKE ALL ON ALL TABLES IN SCHEMA public FROM PUBLIC;\n\n-- Grant schema usage explicitly\nGRANT USAGE ON SCHEMA public TO app_read;\nGRANT USAGE ON SCHEMA public TO app_write;\n\n-- Read-only role\nCREATE ROLE app_read LOGIN PASSWORD 'read_passphrase';\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO app_read;\nALTER DEFAULT PRIVILEGES IN SCHEMA public\n  GRANT SELECT ON TABLES TO app_read;\n\n-- Write role\nCREATE ROLE app_write LOGIN PASSWORD 'write_passphrase';\nGRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO app_write;\nGRANT USAGE ON ALL SEQUENCES IN SCHEMA public TO app_write;\nALTER DEFAULT PRIVILEGES IN SCHEMA public\n  GRANT SELECT, INSERT, UPDATE, DELETE ON TABLES TO app_write;\nALTER DEFAULT PRIVILEGES IN SCHEMA public\n  GRANT USAGE ON SEQUENCES TO app_write;\n</code></pre> <p>ALTER DEFAULT PRIVILEGES matters</p> <p><code>GRANT SELECT ON ALL TABLES</code> only applies to tables that exist right now. When new tables are created later, the role will not have access. <code>ALTER DEFAULT PRIVILEGES</code> sets the permissions that future tables inherit automatically.</p>"},{"location":"Databases/database-security/#avoiding-grant-all","title":"Avoiding GRANT ALL","text":"<p><code>GRANT ALL PRIVILEGES</code> gives everything - SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, ALTER, INDEX, and more. For application accounts, this is almost always too broad:</p> <pre><code>-- WRONG - application doesn't need DROP, ALTER, CREATE\nGRANT ALL PRIVILEGES ON myapp.* TO 'app_user'@'%';\n\n-- RIGHT - only what the application actually uses\nGRANT SELECT, INSERT, UPDATE, DELETE ON myapp.* TO 'app_user'@'%';\n</code></pre> <p>If an application account is compromised, <code>GRANT ALL</code> means the attacker can drop tables, alter schema, and read data from every table. Scoped grants limit the blast radius.</p>"},{"location":"Databases/database-security/#network-level-restrictions","title":"Network-Level Restrictions","text":"<p>Database ports should never be exposed to the public internet:</p> <pre><code># MySQL - bind only to internal interface (my.cnf)\n# bind-address = 10.0.0.5\n\n# PostgreSQL - listen only on internal interface (postgresql.conf)\n# listen_addresses = '10.0.0.5'\n\n# Firewall rules (iptables example)\niptables -A INPUT -p tcp --dport 3306 -s 10.0.0.0/24 -j ACCEPT\niptables -A INPUT -p tcp --dport 3306 -j DROP\n\niptables -A INPUT -p tcp --dport 5432 -s 10.0.0.0/24 -j ACCEPT\niptables -A INPUT -p tcp --dport 5432 -j DROP\n</code></pre> <p>Combine network restrictions with MySQL's host-based user system (<code>'user'@'10.0.0.%'</code>) and PostgreSQL's <code>pg_hba.conf</code> address matching for defense in depth.</p>"},{"location":"Databases/database-security/#owasp-database-security","title":"OWASP Database Security","text":"<p>The OWASP (Open Worldwide Application Security Project) identifies recurring patterns in database breaches. These are the risks that consistently appear in real-world incidents:</p>"},{"location":"Databases/database-security/#top-database-security-risks","title":"Top Database Security Risks","text":"<p>1. SQL Injection - The most exploited vulnerability. Parameterized queries eliminate it entirely, yet it persists because legacy code, raw query escape hatches, and dynamic SQL construction remain common.</p> <p>2. Excessive Privileges - Application accounts with DBA-level access. When the application is compromised, the attacker inherits every privilege the account has. The principle of least privilege directly counters this.</p> <p>3. Unpatched Databases - Known vulnerabilities with published CVEs remain exploitable until patched. Establish a patching cadence: critical CVEs within 72 hours, regular patches monthly.</p> <p>4. Default Credentials - MySQL historically created accounts with no password. PostgreSQL's <code>trust</code> authentication in <code>pg_hba.conf</code> allows passwordless connections. Every default credential must be changed or removed before production deployment.</p> <p>5. Exposed Backups - Database dumps stored in world-readable locations, uploaded to unsecured S3 buckets, or left in web-accessible directories. Encrypt backups and restrict access.</p> <p>6. Unnecessary Features Enabled - File loading (<code>LOAD DATA LOCAL INFILE</code> in MySQL), external program execution (<code>COPY ... PROGRAM</code> in PostgreSQL), and unused stored procedures increase the attack surface. Disable what you do not use.</p> <p>7. Insecure Transport - Unencrypted connections between application and database. TLS is non-negotiable in production.</p>"},{"location":"Databases/database-security/#security-checklist","title":"Security Checklist","text":"Check MySQL PostgreSQL Strong auth plugin <code>caching_sha2_password</code> <code>scram-sha-256</code> TLS enforced <code>require_secure_transport = ON</code> <code>hostssl</code> + <code>hostnossl reject</code> No default/empty passwords Check <code>mysql.user</code> Check <code>pg_hba.conf</code> for <code>trust</code> Least privilege accounts Review <code>SHOW GRANTS</code> Review <code>\\du</code> and <code>\\dp</code> Audit logging Enterprise Audit or general_log pgAudit extension Network restricted <code>bind-address</code> + firewall <code>listen_addresses</code> + firewall Backups encrypted Encrypt dump files Encrypt dump files Patches current <code>SELECT VERSION()</code> <code>SELECT version()</code> File loading disabled <code>local_infile = OFF</code> Restrict <code>COPY ... PROGRAM</code>"},{"location":"Databases/database-security/#exercises","title":"Exercises","text":"<p>Database Security Audit (requires JavaScript)</p>"},{"location":"Databases/database-security/#further-reading","title":"Further Reading","text":"<ul> <li>MySQL Security Guide - official MySQL security documentation covering authentication, encryption, and access control</li> <li>PostgreSQL Client Authentication (pg_hba.conf) - comprehensive reference for PostgreSQL authentication configuration</li> <li>OWASP SQL Injection Prevention Cheat Sheet - parameterized query patterns for every major language</li> <li>pgAudit Documentation - PostgreSQL audit logging extension setup and configuration</li> <li>CIS Benchmarks for MySQL and PostgreSQL - industry-standard hardening checklists from the Center for Internet Security</li> </ul> <p>Previous: Backup &amp; Recovery Strategies | Next: Scaling &amp; Architecture Patterns | Back to Index</p>"},{"location":"Databases/innodb-recovery-pdrt/","title":"InnoDB Recovery with PDRT","text":"<p>Active Data Loss?</p> <p>If you have identified data loss on your server, skip directly to the First Response section immediately. The longer MySQL remains online writing to your data files, the less likely recovery becomes. Every second counts.</p> <p>Percona provides a set of tools for recovering lost or corrupted MySQL data from InnoDB data files. The Percona Data Recovery Tool (PDRT) is freely available, and with some setup, can retrieve data that would otherwise be lost permanently or recover from InnoDB corruption that <code>innodb_force_recovery</code> cannot resolve.</p> <p>PDRT is most useful in these scenarios:</p> <ul> <li>A mistaken <code>DROP TABLE</code>, <code>DELETE</code>, <code>TRUNCATE</code>, or <code>UPDATE</code></li> <li>Deletion of the data file at the filesystem level</li> <li>Partial corruption where InnoDB cannot start, even with <code>innodb_force_recovery</code> set to its maximum value</li> </ul> <p>This guide walks through the complete PDRT recovery workflow in a cPanel/WHM environment, though the core techniques apply to any MySQL installation.</p>"},{"location":"Databases/innodb-recovery-pdrt/#innodb-file-architecture","title":"InnoDB File Architecture","text":"<p>Before attempting recovery, you need to understand what you are recovering from. InnoDB stores data across several file types, and knowing which files matter determines your recovery strategy.</p>"},{"location":"Databases/innodb-recovery-pdrt/#the-system-tablespace-ibdata1","title":"The System Tablespace: <code>ibdata1</code>","text":"<p>The system tablespace (<code>ibdata1</code>) is InnoDB's central file. It always exists, regardless of configuration. It contains:</p> <ul> <li>The data dictionary - metadata about every InnoDB table, index, and column</li> <li>The doublewrite buffer - a crash-recovery mechanism that prevents partial page writes</li> <li>The change buffer - cached changes to secondary indexes</li> <li>Undo logs - records needed to roll back uncommitted transactions</li> </ul> <p>When <code>innodb_file_per_table</code> is disabled (the pre-MySQL 5.6 default), <code>ibdata1</code> also stores all table data and indexes. This makes it the single target for recovery - but also means it grows indefinitely and cannot be shrunk without rebuilding.</p>"},{"location":"Databases/innodb-recovery-pdrt/#per-table-files-ibd-and-frm","title":"Per-Table Files: <code>.ibd</code> and <code>.frm</code>","text":"<p>With <code>innodb_file_per_table=ON</code> (the default since MySQL 5.6 and the standard on all cPanel servers), each table gets its own <code>.ibd</code> file containing that table's data and indexes. The <code>.frm</code> file stores the table's structure definition.</p> <p>This per-table layout simplifies some recovery scenarios because each table's data is isolated. However, it complicates <code>DROP TABLE</code> recovery because dropping a table removes both the <code>.ibd</code> and <code>.frm</code> files from the filesystem entirely.</p> <p>MySQL 8.0 changes</p> <p>MySQL 8.0 replaced the <code>.frm</code> files with a transactional data dictionary stored inside InnoDB itself. The recovery techniques in this guide still apply to the data files, but <code>.frm</code>-based schema extraction only works for MySQL 5.x and MariaDB.</p>"},{"location":"Databases/innodb-recovery-pdrt/#innodb-log-files-ib_logfile","title":"InnoDB Log Files: <code>ib_logfile*</code>","text":"<p>The redo logs (<code>ib_logfile0</code>, <code>ib_logfile1</code>) record every change before it reaches the data files. InnoDB replays these logs after a crash to bring the data files up to date. Deleting these files without understanding the server's state can cause permanent data loss if uncommitted transactions needed for recovery are still in the logs.</p> <p>What is the relationship between ibdata1 and .ibd files in InnoDB? (requires JavaScript)</p>"},{"location":"Databases/innodb-recovery-pdrt/#the-pdrt-toolkit","title":"The PDRT Toolkit","text":"<p>PDRT includes three core utilities. If you are in an active data-loss scenario, skip to First Response and come back here after your files are backed up.</p>"},{"location":"Databases/innodb-recovery-pdrt/#constraints_parser","title":"<code>constraints_parser</code>","text":"<p>The primary recovery tool. It scans raw InnoDB data files (either <code>ibdata1</code> or individual <code>.ibd</code> files) and extracts rows that match a defined table structure.</p> <pre><code>constraints_parser -4|-5|-6 [-dDV] -f &lt;InnoDB page or dir&gt; [-T N:M] [-b &lt;external pages dir&gt;]\n</code></pre> Flag Purpose <code>-4</code> REDUNDANT row format (MySQL 4.x) <code>-5</code> COMPACT row format (MySQL 5.0+) <code>-6</code> MySQL 5.6+ format <code>-f</code> Path to the InnoDB data file or directory of pages <code>-d</code> Process only pages that may contain deleted records <code>-D</code> Recover deleted rows only <code>-U</code> Recover undeleted rows only (default) <code>-V</code> Verbose mode with debug information <code>-T N:M</code> Filter to a specific index ID <code>-b &lt;dir&gt;</code> Directory for external BLOB pages <code>-p prefix</code> Prefix for the dump directory in LOAD DATA output <code>-o &lt;file&gt;</code> Save dump to a specific file"},{"location":"Databases/innodb-recovery-pdrt/#page_parser","title":"<code>page_parser</code>","text":"<p>Splits a tablespace file into individual pages grouped by index ID. This is needed when recovering from <code>ibdata1</code> in shared-tablespace configurations where you need to isolate specific table data.</p> <pre><code>page_parser -4|-5 [-dDhcCV] -f &lt;innodb_datafile&gt; [-T N:M] [-s size] [-t size]\n</code></pre> Flag Purpose <code>-c</code> Count pages and group by index ID <code>-C</code> Count pages, ignoring invalid index IDs <code>-s size</code> Disk cache size (e.g., <code>1G</code>, <code>10M</code>, default <code>100M</code>) <code>-t size</code> Override tablespace size detection"},{"location":"Databases/innodb-recovery-pdrt/#create_defspl","title":"<code>create_defs.pl</code>","text":"<p>A Perl script that connects to a running MySQL instance and generates the <code>table_defs.h</code> header file that <code>constraints_parser</code> uses to identify row structures.</p> <pre><code>create_defs.pl --host=localhost --user=root --password=PASS --db=DATABASE --table=TABLE\n</code></pre> <p>What does constraints_parser do? (requires JavaScript)</p>"},{"location":"Databases/innodb-recovery-pdrt/#first-response","title":"First Response","text":"<p>Every data-loss scenario is different, but this section covers the critical first steps from the perspective of someone who has just discovered the loss and needs to act fast. As time passes after data loss, the likelihood of full recovery decreases - deleted data pages get reused, and the window for recovery closes.</p>"},{"location":"Databases/innodb-recovery-pdrt/#step-1-stop-mysql","title":"Step 1: Stop MySQL","text":"<p>If MySQL is still running, stop it immediately:</p> <pre><code>systemctl stop mysql\n# Or on older systems:\n/etc/init.d/mysql stop\n</code></pre> <p>The goal is to prevent new writes from overwriting space in the data files that still contains recoverable data. Deleted data is marked as free space, and InnoDB will reuse it for new writes.</p>"},{"location":"Databases/innodb-recovery-pdrt/#step-2-back-up-innodb-data-files","title":"Step 2: Back Up InnoDB Data Files","text":"<p>Create a working copy of your data files. All recovery work should happen against copies, never the originals:</p> <pre><code>mkdir /root/innodb.bak\ncd /var/lib/mysql\ndd if=ibdata1 of=ibdata1.recovery conv=noerror\ncp ibdata1.recovery /root/innodb.bak/\ncp ib_logfile* /root/innodb.bak/\n</code></pre> <p>The <code>dd</code> command with <code>conv=noerror</code> creates a copy even if there are read errors on disk, which is essential when dealing with filesystem-level corruption.</p>"},{"location":"Databases/innodb-recovery-pdrt/#step-3-back-up-the-database-directory","title":"Step 3: Back Up the Database Directory","text":"<p>Copy the database directory containing the per-table <code>.ibd</code> and <code>.frm</code> files (replace <code>$db_name</code> with your database name):</p> <pre><code>cd /var/lib/mysql\ncp -a ./$db_name /root/innodb.bak/\n</code></pre> <p>On cPanel servers, <code>innodb_file_per_table=1</code> is the default, so each table has its own <code>.ibd</code> file. This makes per-table recovery possible but also means a <code>DROP TABLE</code> removes the <code>.ibd</code> file entirely - recovery from a dropped table requires filesystem-level file recovery tools, which is beyond PDRT's scope.</p>"},{"location":"Databases/innodb-recovery-pdrt/#step-4-restart-mysql-and-dump","title":"Step 4: Restart MySQL and Dump","text":"<p>With copies safely made, you can restart MySQL if it is able to start:</p> <pre><code>systemctl start mysql\n</code></pre> <p>If MySQL starts, take full dumps immediately:</p> <pre><code>mysqldump --single-transaction --all-databases &gt; /root/dump_wtrans.sql\nmysqldump --all-databases &gt; /root/dump.sql\n</code></pre> <p>The <code>--single-transaction</code> flag takes a consistent snapshot without locking tables, which is important for InnoDB. Run both variants - if the InnoDB data is partially corrupted, one may succeed where the other fails.</p> <p>Filesystem corruption</p> <p>If you are dealing with filesystem-level corruption, copy your backup files to a different disk drive or a remote host. Do not keep your only copies on the potentially failing drive.</p> <p>What should be your first action when discovering a corrupted InnoDB database? (requires JavaScript)</p>"},{"location":"Databases/innodb-recovery-pdrt/#assessing-the-situation","title":"Assessing the Situation","text":"<p>With your files backed up, the immediate time pressure is over. Take stock of what you are dealing with before proceeding.</p>"},{"location":"Databases/innodb-recovery-pdrt/#data-loss-severity","title":"Data Loss Severity","text":"Scenario Recovery Outlook Deleted rows Good - recoverable if MySQL was stopped quickly Truncated table Moderate - partial to full recovery depending on time elapsed Widespread corruption Surprisingly recoverable - PDRT can succeed where <code>innodb_force_recovery</code> fails Dropped table (file-per-table on) Difficult - requires filesystem-level recovery of the deleted <code>.ibd</code> file first"},{"location":"Databases/innodb-recovery-pdrt/#existing-backups","title":"Existing Backups","text":"<p>If you have older backups or dumps, they are valuable even if outdated. The table structure from an old backup can serve as the template for PDRT's <code>table_defs.h</code>, which is essential for the recovery process.</p> <p>If the table still exists in a running MySQL instance, capture its structure now:</p> <pre><code>mysql -NBe \"SHOW CREATE TABLE table_name\" database_name\n</code></pre>"},{"location":"Databases/innodb-recovery-pdrt/#mysql-version","title":"MySQL Version","text":"<p>Check your MySQL version - it determines which <code>constraints_parser</code> format flag to use:</p> <pre><code>mysql -V\n</code></pre> Version Format Flag MySQL 4.x (REDUNDANT format) <code>-4</code> MySQL 5.0 - 5.5 (COMPACT format) <code>-5</code> MySQL 5.6+ / MariaDB 10.x+ <code>-6</code>"},{"location":"Databases/innodb-recovery-pdrt/#installing-pdrt","title":"Installing PDRT","text":"<p>The PDRT source is available from Percona's GitHub repository. The original Bazaar/Launchpad repository is no longer maintained.</p>"},{"location":"Databases/innodb-recovery-pdrt/#download-and-compile","title":"Download and Compile","text":"<pre><code>cd /root\ngit clone https://github.com/percona/percona-data-recovery-tool-for-innodb.git pdrt\ncd pdrt/mysql-source\n./configure\ncd ..\nmake\n</code></pre>"},{"location":"Databases/innodb-recovery-pdrt/#install-perl-dependencies","title":"Install Perl Dependencies","text":"<p>The <code>create_defs.pl</code> script requires the <code>DBD::mysql</code> Perl module. Check if it is installed:</p> <pre><code>perl -MDBD::mysql -e 1\n</code></pre> <p>If this prints an error about not finding <code>DBD/mysql.pm</code>, install it. On cPanel servers:</p> <pre><code>/scripts/perlinstaller DBD::mysql\n</code></pre> <p>On other systems:</p> <pre><code>cpanm DBD::mysql\n# Or via package manager:\napt install libdbd-mysql-perl    # Debian/Ubuntu\ndnf install perl-DBD-MySQL       # RHEL/Fedora\n</code></pre>"},{"location":"Databases/innodb-recovery-pdrt/#getting-your-table-structure","title":"Getting Your Table Structure","text":"<p>Recovery requires a table definition that tells <code>constraints_parser</code> what row structure to look for. How you obtain this depends on whether MySQL is running.</p>"},{"location":"Databases/innodb-recovery-pdrt/#if-mysql-is-running","title":"If MySQL Is Running","text":"<p>1. Create a recovery database:</p> <pre><code>mysql -e \"CREATE DATABASE mydb_recovered\"\n</code></pre> <p>2. Extract the table structure:</p> <pre><code>mysql -NBe \"SHOW CREATE TABLE customer\" mydb\n</code></pre> <p>This produces the full <code>CREATE TABLE</code> statement. Save it - you will need it to recreate the table in your recovery database.</p> <p>3. Recreate the table in the recovery database:</p> <pre><code>mysql mydb_recovered\nSET foreign_key_checks=0;\nCREATE TABLE `customer` (\n  `customer_id` smallint(5) unsigned NOT NULL AUTO_INCREMENT,\n  `store_id` tinyint(3) unsigned NOT NULL,\n  `first_name` varchar(45) NOT NULL,\n  `last_name` varchar(45) NOT NULL,\n  `email` varchar(50) DEFAULT NULL,\n  `address_id` smallint(5) unsigned NOT NULL,\n  `active` tinyint(1) NOT NULL DEFAULT '1',\n  `create_date` datetime NOT NULL,\n  `last_update` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n  PRIMARY KEY (`customer_id`),\n  KEY `idx_fk_store_id` (`store_id`),\n  KEY `idx_fk_address_id` (`address_id`),\n  KEY `idx_last_name` (`last_name`),\n  CONSTRAINT `fk_customer_address` FOREIGN KEY (`address_id`)\n    REFERENCES `address` (`address_id`) ON UPDATE CASCADE,\n  CONSTRAINT `fk_customer_store` FOREIGN KEY (`store_id`)\n    REFERENCES `store` (`store_id`) ON UPDATE CASCADE\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n</code></pre> <p>Disabling foreign key checks</p> <p><code>SET foreign_key_checks=0</code> is required when recreating tables that reference other tables. Without it, <code>CREATE TABLE</code> fails if the referenced tables do not exist yet in the recovery database.</p>"},{"location":"Databases/innodb-recovery-pdrt/#if-mysql-will-not-start","title":"If MySQL Will Not Start","text":"<p>1. Try <code>innodb_force_recovery</code>:</p> <p>Add to <code>my.cnf</code> under <code>[mysqld]</code>:</p> <pre><code>innodb_force_recovery = 1\n</code></pre> <p>Start MySQL and try increasing the value from 1 through 6 until it starts. See the MySQL documentation on forcing recovery for what each level does.</p> <p>2. Reset the InnoDB files as a last resort:</p> <p>If no recovery level works, move the InnoDB files aside so MySQL recreates fresh ones:</p> <pre><code>cd /var/lib/mysql\nmv ibdata1 ibdata1.bak\nmv ib_logfile0 ib_logfile0.bak\nmv ib_logfile1 ib_logfile1.bak\nsystemctl start mysql\n</code></pre> <p>3. Extract structure from <code>.frm</code> files:</p> <p>If you have the <code>.frm</code> file but MySQL cannot read the table, copy the <code>.frm</code> to a working MySQL instance (into a database directory where you have created a placeholder table with the same name), restart MySQL, and run <code>SHOW CREATE TABLE</code>.</p> <p>Alternatively, the MySQL Utilities package includes <code>mysqlfrm</code> for extracting structure directly from <code>.frm</code> files.</p>"},{"location":"Databases/innodb-recovery-pdrt/#recovering-data","title":"Recovering Data","text":"<p>At this point you should have:</p> <ul> <li>A copy of <code>ibdata1.recovery</code> and/or the relevant <code>.ibd</code> files</li> <li>MySQL running with a recovery database containing the target table structures</li> <li>PDRT compiled and ready</li> </ul>"},{"location":"Databases/innodb-recovery-pdrt/#automated-recovery","title":"Automated Recovery","text":"<p>Generate the table definitions, compile, and run <code>constraints_parser</code> in one workflow:</p> <pre><code>cd /root/pdrt\n\n# Generate table definitions from the recovery database\nDB=mydb_recovered\nTBL=customer\nperl create_defs.pl \\\n  --host=localhost \\\n  --user=root \\\n  --password=\"$(grep pass /root/.my.cnf | cut -d= -f2 | sed -e 's/^\"//' -e 's/\"$//')\" \\\n  --db=$DB \\\n  --table=$TBL &gt; include/table_defs.h\n\n# Recompile with the new definitions\nmake clean all\n\n# Run constraints_parser against the data file\nmkdir -p dumps/default\n./constraints_parser -5 -f /root/innodb.bak/mydb/customer.ibd &gt; dumps/default/customer\n</code></pre> <p>The output file <code>dumps/default/customer</code> contains tab-separated values for every row that matched the table definition. It will include both valid data and noise - rows where random data happened to match the column structure.</p> <p>Running constraints_parser for Data Recovery (requires JavaScript)</p>"},{"location":"Databases/innodb-recovery-pdrt/#manual-method-with-custom-definitions","title":"Manual Method with Custom Definitions","text":"<p>If the automated approach produces too much noise, you can run the tools manually with more control. Generate the table definitions:</p> <pre><code>cd /root/pdrt\nDB=mydb_recovered; TBL=customer\nperl create_defs.pl --host=localhost --user=root \\\n  --password=\"$(grep pass /root/.my.cnf | cut -d= -f2 | sed 's/^\"//;s/\"$//')\" \\\n  --db=$DB --table=$TBL &gt; include/table_defs.h\n</code></pre> <p>Recompile and run:</p> <pre><code>make clean all\n./constraints_parser -5 -f /var/lib/mysql/ibdata1.recovery &gt; recovery_output.tsv\n</code></pre> <p>To search specifically for deleted records, add the <code>-D</code> flag:</p> <pre><code>./constraints_parser -5 -Df /root/innodb.bak/mydb/customer.ibd\n</code></pre>"},{"location":"Databases/innodb-recovery-pdrt/#tuning-table-definitions","title":"Tuning Table Definitions","text":"<p>The default <code>table_defs.h</code> generated by <code>create_defs.pl</code> uses broad constraints that match many false positives. Tightening the constraints dramatically reduces noise in the recovery output.</p> <p>Here is the auto-generated definition for a <code>first_name</code> column:</p> <pre><code>{ /* varchar(45) */\n    name: \"first_name\",\n    type: FT_CHAR,\n    min_length: 0,\n    max_length: 135,\n\n    has_limits: FALSE,\n    limits: {\n        can_be_null: FALSE,\n        char_min_len: 0,\n        char_max_len: 135,\n        char_ascii_only: TRUE\n    },\n\n    can_be_null: FALSE\n},\n</code></pre> <p>The key field is <code>has_limits: FALSE</code> - with limits disabled, <code>constraints_parser</code> accepts any data that structurally fits the column type. Enabling limits and tightening them filters out garbage data:</p> Field Default Tuned Reasoning <code>has_limits</code> <code>FALSE</code> <code>TRUE</code> Enable constraint checking <code>can_be_null</code> <code>FALSE</code> <code>FALSE</code> Every customer has a first name <code>char_min_len</code> <code>0</code> <code>2</code> No single-character first names in this dataset <code>char_max_len</code> <code>135</code> <code>30</code> No first name exceeds 30 characters <code>char_ascii_only</code> <code>TRUE</code> <code>TRUE</code> Names in this table are ASCII <p>The tuned definition:</p> <pre><code>{ /* varchar(45) */\n    name: \"first_name\",\n    type: FT_CHAR,\n    min_length: 0,\n    max_length: 135,\n\n    has_limits: TRUE,\n    limits: {\n        can_be_null: FALSE,\n        char_min_len: 2,\n        char_max_len: 30,\n        char_ascii_only: TRUE\n    },\n\n    can_be_null: FALSE\n},\n</code></pre> <p>After editing <code>include/table_defs.h</code>, recompile and run again:</p> <pre><code>cd /root/pdrt &amp;&amp; make clean all\n./constraints_parser -5 -Df /root/innodb.bak/mydb/customer.ibd\n</code></pre> <p>With tightened constraints, a search that previously returned thousands of false positives may return exactly the rows you need.</p> <p>What effect does setting has_limits to TRUE in table_defs.h have on constraints_parser output? (requires JavaScript)</p>"},{"location":"Databases/innodb-recovery-pdrt/#importing-recovered-data","title":"Importing Recovered Data","text":"<p>Once you have clean recovery output, you need to import it back into MySQL. The <code>constraints_parser</code> output includes both the data (on stdout) and the <code>LOAD DATA INFILE</code> SQL statement (on stderr).</p>"},{"location":"Databases/innodb-recovery-pdrt/#splitting-output-and-sql","title":"Splitting Output and SQL","text":"<pre><code>mkdir -p /root/pdrt/dumps/mydb_recovered\n\n./constraints_parser -5 -Df /root/innodb.bak/mydb/customer.ibd -p mydb_recovered \\\n  &gt; dumps/mydb_recovered/customer \\\n  2&gt; customer.sql\n\n# Remove progress lines from the SQL file\nsed -i '/done/d' customer.sql\n</code></pre> <p>The <code>-p mydb_recovered</code> flag sets the dump path prefix in the generated <code>LOAD DATA</code> statement. The SQL file will contain something like:</p> <pre><code>SET FOREIGN_KEY_CHECKS=0;\nLOAD DATA INFILE '/root/pdrt/dumps/mydb_recovered/customer'\n  REPLACE INTO TABLE `customer`\n  FIELDS TERMINATED BY '\\t' OPTIONALLY ENCLOSED BY '\"'\n  LINES STARTING BY 'customer\\t'\n  (`customer_id`, `store_id`, `first_name`, `last_name`, `email`,\n   `address_id`, `active`, `create_date`, `last_update`);\n</code></pre> <p>The <code>LINES STARTING BY 'customer\\t'</code> clause is important - it skips all the page metadata comments in the dump file and only imports lines that begin with the table name followed by a tab.</p>"},{"location":"Databases/innodb-recovery-pdrt/#running-the-import","title":"Running the Import","text":"<pre><code>mysql mydb_recovered &lt; /root/pdrt/customer.sql\n</code></pre> <p>If you get an error about being unable to stat the dump file:</p> <pre><code>ERROR 13 (HY000) at line 2: Can't get stat of '/root/pdrt/dumps/mydb_recovered/customer' (Errcode: 13)\n</code></pre> <p>Change <code>LOAD DATA INFILE</code> to <code>LOAD DATA LOCAL INFILE</code>:</p> <pre><code>sed -i 's/LOAD DATA INFILE/LOAD DATA LOCAL INFILE/g' /root/pdrt/customer.sql\nmysql mydb_recovered &lt; /root/pdrt/customer.sql\n</code></pre>"},{"location":"Databases/innodb-recovery-pdrt/#verifying-the-results","title":"Verifying the Results","text":"<pre><code>mysql mydb_recovered -e \"SELECT COUNT(*) FROM customer\"\nmysql mydb_recovered -e \"SELECT * FROM customer LIMIT 5\"\n</code></pre> <p>Compare row counts and spot-check data against any existing backups or dumps you have.</p> <p>Complete PDRT Recovery Workflow (requires JavaScript)</p>"},{"location":"Databases/innodb-recovery-pdrt/#recovery-decision-flowchart","title":"Recovery Decision Flowchart","text":"<p>Use this to determine your recovery path based on the scenario:</p> <pre><code>flowchart TD\n    A[Data Loss Detected] --&gt; B{MySQL Running?}\n    B --&gt;|Yes| C[Stop MySQL Immediately]\n    B --&gt;|No| D[Do NOT Start MySQL]\n    C --&gt; E[Backup ibdata1, .ibd, .frm, ib_logfile*]\n    D --&gt; E\n    E --&gt; F{What happened?}\n    F --&gt;|Deleted rows| G[Use constraints_parser with -D flag on .ibd file]\n    F --&gt;|Truncated table| H[Use constraints_parser on .ibd file]\n    F --&gt;|Dropped table| I{Have .ibd file backup?}\n    F --&gt;|Corruption| J[Try innodb_force_recovery first]\n    I --&gt;|Yes| G\n    I --&gt;|No| K[Filesystem recovery needed]\n    J --&gt;|MySQL starts| L[mysqldump + PDRT on ibdata1]\n    J --&gt;|Still fails| M[Reset ib* files + PDRT on ibdata1.recovery]\n    G --&gt; N[Tune table_defs.h and iterate]\n    H --&gt; N\n    L --&gt; N\n    M --&gt; N\n    N --&gt; O[Import via LOAD DATA INFILE]</code></pre>"},{"location":"Databases/innodb-recovery-pdrt/#practical-exercise","title":"Practical Exercise","text":"<p>PDRT Recovery Planning (requires JavaScript)</p>"},{"location":"Databases/innodb-recovery-pdrt/#further-reading","title":"Further Reading","text":"<ul> <li>MySQL InnoDB Storage Engine Documentation - official InnoDB reference for file formats, recovery, and configuration</li> <li>Percona Data Recovery Tool (GitHub) - PDRT source code and documentation</li> <li>InnoDB Recovery with innodb_force_recovery - MySQL documentation on the force recovery levels</li> <li>Percona XtraBackup - hot backup tool for InnoDB that avoids the need for PDRT in many scenarios</li> <li>How to Recover a Single InnoDB Table from a Full Backup - Percona blog on targeted table recovery</li> </ul> <p>Previous: Scaling &amp; Architecture Patterns | Back to Index</p>"},{"location":"Databases/mongodb/","title":"MongoDB","text":"<p>MongoDB is the most widely adopted document database. Instead of rows and columns, you work with flexible JSON-like documents that map naturally to objects in your application code. No rigid schema, no required joins, no impedance mismatch between your data layer and your programming language.</p> <p>This guide covers the document model, CRUD operations, the aggregation pipeline, indexing, replica sets, sharding, and the <code>mongosh</code> shell.</p>"},{"location":"Databases/mongodb/#the-document-model","title":"The Document Model","text":"<p>MongoDB stores data as BSON (Binary JSON) documents - a binary-encoded superset of JSON that adds data types JSON lacks. A document is a set of key-value pairs, analogous to a row in a relational table but far more flexible.</p>"},{"location":"Databases/mongodb/#a-simple-document","title":"A Simple Document","text":"<pre><code>{\n  \"_id\": ObjectId(\"65a1b2c3d4e5f6a7b8c9d0e1\"),\n  \"name\": \"Jane Chen\",\n  \"email\": \"jane@example.com\",\n  \"age\": 34,\n  \"roles\": [\"admin\", \"developer\"],\n  \"address\": {\n    \"street\": \"742 Evergreen Terrace\",\n    \"city\": \"Portland\",\n    \"state\": \"OR\",\n    \"zip\": \"97201\"\n  },\n  \"created_at\": ISODate(\"2024-01-15T09:30:00Z\"),\n  \"active\": true\n}\n</code></pre> <p>Arrays, nested sub-documents, and typed fields beyond what plain JSON supports - all in a single document.</p>"},{"location":"Databases/mongodb/#bson-types","title":"BSON Types","text":"Type Example Notes String <code>\"hello\"</code> UTF-8 encoded Int32 <code>NumberInt(42)</code> 32-bit signed integer Int64 <code>NumberLong(9007199254740993)</code> 64-bit signed integer Double <code>3.14159</code> 64-bit IEEE 754 floating point Decimal128 <code>NumberDecimal(\"19.99\")</code> 128-bit decimal - use for currency Boolean <code>true</code> / <code>false</code> Date <code>ISODate(\"2024-01-15T09:30:00Z\")</code> UTC datetime, millisecond precision ObjectId <code>ObjectId(\"65a1...\")</code> 12-byte unique identifier Array <code>[\"a\", \"b\", \"c\"]</code> Ordered list of any BSON types Embedded Document <code>{ \"key\": \"value\" }</code> Nested document Binary <code>BinData(0, \"base64...\")</code> Arbitrary binary data Null <code>null</code> Explicit null value <p>Decimal128 for money</p> <p>Never use <code>Double</code> for financial data. Floating-point arithmetic produces rounding errors (<code>0.1 + 0.2 = 0.30000000000000004</code>). <code>Decimal128</code> provides exact decimal representation - use it for prices, balances, and any value where precision matters.</p>"},{"location":"Databases/mongodb/#the-_id-field","title":"The _id Field","text":"<p>Every document must have an <code>_id</code> field that acts as the primary key. If you don't provide one, MongoDB generates an ObjectId automatically - a 12-byte value containing a timestamp (4 bytes), random value (5 bytes), and incrementing counter (3 bytes). ObjectIds are roughly time-ordered and globally unique without coordination.</p> <p>A single BSON document cannot exceed 16 MB. If your data model approaches this limit, restructure it into separate documents or use GridFS for large files.</p>"},{"location":"Databases/mongodb/#the-mongosh-shell","title":"The mongosh Shell","text":"<p>mongosh is MongoDB's modern command-line shell - a JavaScript REPL with syntax highlighting, auto-completion, and full MongoDB API access.</p>"},{"location":"Databases/mongodb/#connecting","title":"Connecting","text":"<pre><code># Connect to localhost on default port 27017\nmongosh\n\n# Connect to a specific host and database\nmongosh \"mongodb://192.168.1.50:27017/myapp\"\n\n# Connect to a replica set\nmongosh \"mongodb://host1:27017,host2:27017,host3:27017/myapp?replicaSet=rs0\"\n\n# Connect with authentication\nmongosh \"mongodb://admin:password@localhost:27017/admin\"\n</code></pre>"},{"location":"Databases/mongodb/#essential-commands","title":"Essential Commands","text":"<pre><code>show dbs              // List databases\nuse myapp             // Switch database (creates on first write)\nshow collections      // List collections in current database\ndb.users.stats()      // Collection statistics\ndb.users.help()       // Method help\n</code></pre>"},{"location":"Databases/mongodb/#customizing-with-mongoshrcjs","title":"Customizing with .mongoshrc.js","text":"<p>Create <code>~/.mongoshrc.js</code> to add shell aliases and custom prompts:</p> <pre><code>// ~/.mongoshrc.js\nconst last = (coll, n = 5) =&gt; db[coll].find().sort({ _id: -1 }).limit(n);\nconst count = (coll, query = {}) =&gt; db[coll].countDocuments(query);\n</code></pre>"},{"location":"Databases/mongodb/#crud-operations","title":"CRUD Operations","text":"<p>All operations target a single collection (the rough equivalent of a relational table).</p>"},{"location":"Databases/mongodb/#create-inserting-documents","title":"Create: Inserting Documents","text":"<pre><code>// Insert a single document\ndb.users.insertOne({ name: \"Alice Rivera\", email: \"alice@example.com\", age: 28, department: \"Engineering\" })\n\n// Insert multiple documents\ndb.users.insertMany([\n  { name: \"Bob Park\", email: \"bob@example.com\", age: 35, department: \"Marketing\" },\n  { name: \"Carol Okafor\", email: \"carol@example.com\", age: 42, department: \"Engineering\" },\n  { name: \"Dave Singh\", email: \"dave@example.com\", age: 31, department: \"Sales\" }\n])\n</code></pre> <p>Both methods generate <code>_id</code> values automatically if omitted.</p>"},{"location":"Databases/mongodb/#read-finding-documents","title":"Read: Finding Documents","text":"<p>Pass a filter document to match records and an optional projection to control which fields come back.</p> <pre><code>db.users.find()                                             // all documents\ndb.users.findOne({ email: \"alice@example.com\" })            // single match\ndb.users.find({ department: \"Engineering\" })                // equality filter\ndb.users.find({ department: \"Engineering\" }, { name: 1, email: 1, _id: 0 })  // with projection\n</code></pre>"},{"location":"Databases/mongodb/#query-operators","title":"Query Operators","text":"<p>Query operators prefixed with <code>$</code> handle comparisons beyond simple equality:</p> <pre><code>// Comparison\ndb.users.find({ age: { $gt: 30 } })                        // greater than\ndb.users.find({ age: { $gte: 28, $lte: 42 } })             // range\n\n// Membership\ndb.users.find({ department: { $in: [\"Engineering\", \"Sales\"] } })\n\n// Logical\ndb.users.find({ $and: [{ age: { $gt: 25 } }, { department: \"Engineering\" }] })\ndb.users.find({ $or: [{ department: \"Engineering\" }, { department: \"Sales\" }] })\n\n// Pattern matching and existence\ndb.users.find({ name: { $regex: /^A/i } })                 // names starting with A\ndb.users.find({ phone: { $exists: true } })                 // has a phone field\n</code></pre> <p>Implicit $and</p> <p>When you specify multiple conditions in the same filter document, MongoDB treats them as an implicit <code>$and</code>. Writing <code>{ age: { $gt: 25 }, department: \"Engineering\" }</code> is equivalent to using <code>$and</code> explicitly. You only need <code>$and</code> when you have multiple conditions on the same field.</p>"},{"location":"Databases/mongodb/#sorting-limiting-and-skipping","title":"Sorting, Limiting, and Skipping","text":"<pre><code>db.users.find().sort({ age: -1, name: 1 })     // sort by age desc, name asc\ndb.users.find().limit(10)                       // limit results\ndb.users.find().skip(20).limit(10)              // pagination (page 3, 10 per page)\n</code></pre> <p>MongoDB CRUD Operations (requires JavaScript)</p>"},{"location":"Databases/mongodb/#update-modifying-documents","title":"Update: Modifying Documents","text":"<p>Update operators modify specific fields without replacing the entire document:</p> <pre><code>// Set or change fields\ndb.users.updateOne({ email: \"alice@example.com\" }, { $set: { age: 29, title: \"Senior Engineer\" } })\n\n// Remove a field entirely\ndb.users.updateOne({ email: \"bob@example.com\" }, { $unset: { phone: \"\" } })\n\n// Increment a numeric field\ndb.orders.updateOne({ _id: orderId }, { $inc: { quantity: 1 } })\n\n// Add to / remove from an array\ndb.users.updateOne({ email: \"alice@example.com\" }, { $push: { roles: \"team-lead\" } })\ndb.users.updateOne({ email: \"alice@example.com\" }, { $pull: { roles: \"junior\" } })\n\n// Update multiple documents\ndb.users.updateMany({ department: \"Engineering\" }, { $set: { building: \"HQ-3\" } })\n</code></pre> <p>Don't forget the operator</p> <p>If you pass a plain document as the second argument to <code>updateOne</code> without <code>$set</code>, MongoDB replaces the entire document (except <code>_id</code>) with that object. This is almost never what you want. Always use update operators like <code>$set</code>, <code>$inc</code>, <code>$push</code>, etc.</p>"},{"location":"Databases/mongodb/#delete-removing-documents","title":"Delete: Removing Documents","text":"<pre><code>db.users.deleteOne({ email: \"dave@example.com\" })    // Delete one match\ndb.sessions.deleteMany({ expired: true })             // Delete all matches\ndb.temp_data.deleteMany({})                           // Delete everything (careful!)\n</code></pre> <p>What happens if you call updateOne with { name: 'New Name' } as the second argument (no $set operator)? (requires JavaScript)</p>"},{"location":"Databases/mongodb/#the-aggregation-pipeline","title":"The Aggregation Pipeline","text":"<p>The aggregation pipeline processes documents through a sequence of stages, each transforming data before passing it to the next - like a Unix pipeline for your database.</p> <pre><code>db.collection.aggregate([\n  { $stage1: { ... } },\n  { $stage2: { ... } },\n  { $stage3: { ... } }\n])\n</code></pre>"},{"location":"Databases/mongodb/#pipeline-stages","title":"Pipeline Stages","text":""},{"location":"Databases/mongodb/#match-filter-documents","title":"$match - Filter Documents","text":"<p>Works like <code>find</code> but as a pipeline stage. Place <code>$match</code> as early as possible to reduce documents flowing through later stages.</p> <pre><code>{ $match: { status: \"active\", age: { $gte: 18 } } }\n</code></pre>"},{"location":"Databases/mongodb/#group-aggregate-values","title":"$group - Aggregate Values","text":"<p>Groups documents by a key and applies accumulator operators (<code>$sum</code>, <code>$avg</code>, <code>$min</code>, <code>$max</code>, <code>$first</code>, <code>$last</code>, <code>$push</code>, <code>$addToSet</code>):</p> <pre><code>{ $group: { _id: \"$department\", avgAge: { $avg: \"$age\" }, total: { $sum: 1 } } }\n</code></pre>"},{"location":"Databases/mongodb/#sort-project-limit-skip","title":"$sort, $project, $limit, $skip","text":"<pre><code>{ $sort: { total: -1 } }          // order results\n{ $project: { name: 1, email: 1, fullName: { $concat: [\"$first\", \" \", \"$last\"] } } }  // reshape\n{ $limit: 10 }                     // cap output\n{ $skip: 20 }                      // offset\n</code></pre>"},{"location":"Databases/mongodb/#lookup-join-collections","title":"$lookup - Join Collections","text":"<p>Performs a left outer join against another collection:</p> <pre><code>{ $lookup: { from: \"orders\", localField: \"_id\", foreignField: \"customer_id\", as: \"customer_orders\" } }\n</code></pre>"},{"location":"Databases/mongodb/#unwind-flatten-arrays","title":"$unwind - Flatten Arrays","text":"<p>Deconstructs an array field, outputting one document per element:</p> <pre><code>{ $unwind: \"$customer_orders\" }\n</code></pre>"},{"location":"Databases/mongodb/#practical-example-revenue-by-category","title":"Practical Example: Revenue by Category","text":"<pre><code>db.orders.aggregate([\n  { $match: { status: \"completed\" } },\n  { $group: { _id: \"$category\", totalRevenue: { $sum: \"$amount\" }, orderCount: { $sum: 1 } } },\n  { $sort: { totalRevenue: -1 } },\n  { $limit: 5 },\n  { $project: { category: \"$_id\", totalRevenue: { $round: [\"$totalRevenue\", 2] }, orderCount: 1, _id: 0 } }\n])\n</code></pre> <p>Aggregation Pipeline in Practice (requires JavaScript)</p> <p>In a MongoDB aggregation pipeline, what does the $unwind stage do? (requires JavaScript)</p>"},{"location":"Databases/mongodb/#indexes","title":"Indexes","text":"<p>Without indexes, MongoDB performs a collection scan - reading every document. Indexes are B-tree structures that let MongoDB locate documents without examining the entire collection.</p>"},{"location":"Databases/mongodb/#single-field-indexes","title":"Single Field Indexes","text":"<pre><code>db.users.createIndex({ email: 1 })          // ascending\ndb.users.createIndex({ created_at: -1 })    // descending (matters for sorted queries)\n</code></pre>"},{"location":"Databases/mongodb/#compound-indexes","title":"Compound Indexes","text":"<p>A compound index covers multiple fields. Field order matters - MongoDB uses the index for prefix queries (left to right) but not for queries that skip leading fields.</p> <pre><code>// Supports queries on: department alone, department+age, department+age+name\n// Does NOT efficiently support queries on age alone or name alone\ndb.users.createIndex({ department: 1, age: -1, name: 1 })\n</code></pre>"},{"location":"Databases/mongodb/#unique-indexes","title":"Unique Indexes","text":"<pre><code>db.users.createIndex({ email: 1 }, { unique: true })  // duplicate values throw an error\n</code></pre>"},{"location":"Databases/mongodb/#text-indexes","title":"Text Indexes","text":"<p>Text indexes support full-text search across string content. A collection can have at most one text index, but it can cover multiple fields:</p> <pre><code>db.articles.createIndex({ title: \"text\", body: \"text\" })\ndb.articles.find(\n  { $text: { $search: \"mongodb aggregation\" } },\n  { score: { $meta: \"textScore\" } }\n).sort({ score: { $meta: \"textScore\" } })\n</code></pre>"},{"location":"Databases/mongodb/#geospatial-indexes","title":"Geospatial Indexes","text":"<p>2dsphere indexes support queries on GeoJSON data - finding documents near a point, within a polygon, or intersecting a geometry:</p> <pre><code>// Create a 2dsphere index\ndb.restaurants.createIndex({ location: \"2dsphere\" })\n\n// Find restaurants within 2km of a point\ndb.restaurants.find({\n  location: {\n    $near: {\n      $geometry: { type: \"Point\", coordinates: [-122.6750, 45.5120] },\n      $maxDistance: 2000  // meters\n    }\n  }\n})\n</code></pre>"},{"location":"Databases/mongodb/#ttl-indexes","title":"TTL Indexes","text":"<p>TTL (Time to Live) indexes automatically delete documents after a specified duration - ideal for sessions, logs, or temporary records:</p> <pre><code>db.sessions.createIndex({ createdAt: 1 }, { expireAfterSeconds: 86400 })  // expire after 24h\n</code></pre>"},{"location":"Databases/mongodb/#query-planning-with-explain","title":"Query Planning with explain()","text":"<p>Use <code>explain()</code> to understand query execution. Key fields in the output:</p> Field Meaning <code>winningPlan.stage</code> <code>IXSCAN</code> (index used) vs <code>COLLSCAN</code> (full scan) <code>totalKeysExamined</code> Number of index entries scanned <code>totalDocsExamined</code> Number of documents loaded <code>nReturned</code> Number of documents returned <code>executionTimeMillis</code> Total execution time <pre><code>db.users.find({ department: \"Engineering\", age: { $gt: 30 } }).explain(\"executionStats\")\n</code></pre> <p>The goal: get <code>totalDocsExamined</code> as close to <code>nReturned</code> as possible.</p> <p>You create a compound index { department: 1, age: -1, name: 1 }. Which query can efficiently use this index? (requires JavaScript)</p>"},{"location":"Databases/mongodb/#replica-sets","title":"Replica Sets","text":"<p>A replica set is a group of MongoDB instances maintaining the same data for redundancy and high availability. Always run MongoDB as a replica set in production - it gives you automatic failover and read scaling.</p>"},{"location":"Databases/mongodb/#architecture","title":"Architecture","text":"<p>A replica set consists of:</p> <ul> <li>Primary: Receives all write operations. There is exactly one primary at any time.</li> <li>Secondary: Replicates data from the primary. Can serve read operations if configured. You typically have two or more secondaries.</li> <li>Arbiter: Participates in elections but holds no data. Used when you need an odd number of voting members but don't want to store a third full copy of the data.</li> </ul> <pre><code>graph TB\n    C[Application] --&gt; P[Primary]\n    P --&gt;|Replication| S1[Secondary 1]\n    P --&gt;|Replication| S2[Secondary 2]\n    P -.-&gt;|Heartbeat| A[Arbiter]\n    S1 -.-&gt;|Heartbeat| P\n    S2 -.-&gt;|Heartbeat| P\n    A -.-&gt;|Heartbeat| P\n    S1 -.-&gt;|Heartbeat| S2\n    S1 -.-&gt;|Heartbeat| A\n    S2 -.-&gt;|Heartbeat| A</code></pre>"},{"location":"Databases/mongodb/#elections","title":"Elections","text":"<p>If the primary becomes unreachable, remaining members hold an election. A majority of voting members must agree - a 3-member set tolerates 1 failure, a 5-member set tolerates 2. A 2-member set cannot elect a new primary (no majority), which is why you always need at least 3 members. Elections typically complete within 10-12 seconds.</p>"},{"location":"Databases/mongodb/#read-preferences","title":"Read Preferences","text":"<p>Read preference controls which members receive read operations:</p> Mode Behavior Use Case <code>primary</code> All reads go to the primary Default. Guaranteed latest data <code>primaryPreferred</code> Reads from primary; falls back to secondary if primary unavailable Availability over consistency <code>secondary</code> Reads go to secondaries only Offload analytics queries from primary <code>secondaryPreferred</code> Reads from secondary; falls back to primary Analytics with fallback <code>nearest</code> Reads from the member with lowest network latency Geographically distributed deployments <pre><code>// In a connection string\n\"mongodb://host1,host2,host3/myapp?readPreference=secondaryPreferred\"\n</code></pre> <p>Stale reads from secondaries</p> <p>Secondaries replicate asynchronously, so they may lag behind the primary. Reading from a secondary can return data that's a few seconds (or, under heavy load, longer) behind. If your application requires reading the data it just wrote, use <code>primary</code> read preference for those queries.</p>"},{"location":"Databases/mongodb/#write-concern","title":"Write Concern","text":"<p>Write concern controls acknowledgment requirements before MongoDB confirms a write:</p> Write Concern Behavior <code>w: 1</code> Acknowledged by the primary only (default) <code>w: \"majority\"</code> Acknowledged by a majority of voting members <code>w: 0</code> No acknowledgment - fire and forget <pre><code>db.orders.insertOne({ item: \"widget\", qty: 5 }, { writeConcern: { w: \"majority\", wtimeout: 5000 } })\n</code></pre> <p>Use <code>w: \"majority\"</code> for critical data. Use <code>w: 1</code> for high-throughput, loss-tolerant workloads.</p>"},{"location":"Databases/mongodb/#sharding","title":"Sharding","text":"<p>Sharding distributes data across multiple machines. When a single replica set can't handle your data volume or throughput, you split data across shards - each shard being its own replica set. Config servers store metadata about data placement, and mongos routers route queries to the correct shard(s). Your application connects to mongos, not directly to shards.</p>"},{"location":"Databases/mongodb/#shard-key-selection","title":"Shard Key Selection","text":"<p>The shard key determines how documents are distributed. A good shard key has high cardinality (many distinct values), even distribution (no hotspots), and supports query targeting (queries including the shard key go to one shard instead of all).</p>"},{"location":"Databases/mongodb/#hashed-vs-ranged-sharding","title":"Hashed vs Ranged Sharding","text":"Strategy How it works Pros Cons Ranged Documents with nearby key values go to the same shard Efficient range queries Hotspots if writes cluster at one end Hashed A hash of the key determines the shard Even write distribution Range queries hit all shards <pre><code>sh.enableSharding(\"myapp\")\nsh.shardCollection(\"myapp.orders\", { customer_id: 1 })   // ranged\nsh.shardCollection(\"myapp.events\", { _id: \"hashed\" })    // hashed\n</code></pre>"},{"location":"Databases/mongodb/#chunks-and-balancing","title":"Chunks and Balancing","text":"<p>MongoDB divides the shard key range into chunks (default 128 MB). The balancer migrates chunks between shards in the background to keep distribution even.</p>"},{"location":"Databases/mongodb/#command-builder","title":"Command Builder","text":"<p>Build a mongosh Query (requires JavaScript)</p>"},{"location":"Databases/mongodb/#putting-it-all-together","title":"Putting It All Together","text":"<p>Product Analytics Pipeline (requires JavaScript)</p>"},{"location":"Databases/mongodb/#further-reading","title":"Further Reading","text":"<ul> <li>MongoDB Manual - official documentation covering all features and versions</li> <li>mongosh Documentation - shell reference, scripting, and configuration</li> <li>Aggregation Pipeline Quick Reference - all stages and operators in one page</li> <li>MongoDB University - free official courses on MongoDB fundamentals, aggregation, and administration</li> <li>Data Modeling Guide - embedding vs referencing, schema design patterns</li> </ul> <p>Previous: NoSQL Concepts &amp; Architecture | Next: Redis | Back to Index</p>"},{"location":"Databases/mysql-administration/","title":"MySQL Administration","text":"<p>Managing a MySQL server goes beyond writing queries. Day-to-day administration means connecting efficiently from the command line, controlling who can access what, understanding where MySQL logs its activity, and keeping tables healthy. This guide covers the core tools and techniques you will use regularly as a MySQL administrator.</p>"},{"location":"Databases/mysql-administration/#the-mysql-cli","title":"The <code>mysql</code> CLI","text":"<p>The <code>mysql</code> command-line client is your primary interface for interacting with a MySQL server. Every administrator needs to be fluent with its options and built-in commands.</p>"},{"location":"Databases/mysql-administration/#connecting","title":"Connecting","text":"<p>The most common connection options:</p> Option Purpose <code>-u</code> Username (default: your OS user) <code>-p</code> Prompt for password (no space before the password if inline) <code>-h</code> Hostname (default: <code>localhost</code>) <code>-P</code> Port (default: <code>3306</code>) <code>-S</code> Socket file path <code>-e</code> Execute a statement and exit <code>-D</code> Select a database on connect <pre><code># Interactive session as root on localhost\nmysql -u root -p\n\n# Connect to a remote server on a non-standard port\nmysql -u admin -p -h db.example.com -P 3307\n\n# Execute a single query and exit\nmysql -u root -p -e \"SHOW DATABASES\"\n\n# Execute against a specific database\nmysql -u root -p -D myapp -e \"SELECT COUNT(*) FROM users\"\n</code></pre> <p>When you connect via <code>localhost</code>, MySQL uses the Unix socket file (usually <code>/var/run/mysqld/mysqld.sock</code> or <code>/tmp/mysql.sock</code>) rather than TCP. If you need a TCP connection to the local machine, use <code>-h 127.0.0.1</code> instead.</p>"},{"location":"Databases/mysql-administration/#the-g-formatter","title":"The <code>\\G</code> Formatter","text":"<p>Wide result sets with many columns are unreadable in a horizontal table. The <code>\\G</code> terminator rotates the output to display each column on its own line:</p> <pre><code>SELECT * FROM information_schema.PROCESSLIST\\G\n</code></pre> <p>This produces output like:</p> <pre><code>*************************** 1. row ***************************\n     ID: 42\n   USER: app_user\n   HOST: 10.0.1.50:54321\n     DB: myapp\nCOMMAND: Query\n   TIME: 0\n  STATE: executing\n   INFO: SELECT * FROM orders WHERE status = 'pending'\n</code></pre> <p>Use <code>\\G</code> any time the output has more than five or six columns - you will use it constantly with <code>SHOW ENGINE INNODB STATUS\\G</code> and process list queries.</p>"},{"location":"Databases/mysql-administration/#the-source-command-and-pager","title":"The <code>source</code> Command and <code>pager</code>","text":"<p>The <code>source</code> command (or <code>\\.</code>) executes SQL from a file within an interactive session:</p> <pre><code>source /path/to/schema.sql\n\\. /path/to/seed-data.sql\n</code></pre> <p>The <code>pager</code> command pipes output through an external program. This is invaluable for large result sets:</p> <pre><code>-- Page through output with less\npager less -S\n\n-- Search output with grep\npager grep -i error\n\n-- Reset to normal output\nnopager\n</code></pre> <p>Combining pager with \\G</p> <p><code>pager less -S</code> combined with <code>\\G</code> output gives you a scrollable, searchable view of wide result sets. This is the fastest way to browse <code>SHOW ENGINE INNODB STATUS</code> output.</p> <p>MySQL CLI Connection Builder (requires JavaScript)</p> <p>When you connect to MySQL using -h localhost, which transport does the client use? (requires JavaScript)</p>"},{"location":"Databases/mysql-administration/#user-and-privilege-system","title":"User and Privilege System","text":"<p>MySQL's privilege system controls who can connect and what they can do. Getting this right is fundamental to both security and operational stability.</p>"},{"location":"Databases/mysql-administration/#creating-users","title":"Creating Users","text":"<pre><code>-- Basic user creation\nCREATE USER 'app_user'@'10.0.1.%' IDENTIFIED BY 'strong_password_here';\n\n-- User with authentication plugin (MySQL 8.0+)\nCREATE USER 'api_svc'@'%' IDENTIFIED WITH caching_sha2_password BY 'another_password';\n\n-- User that expires in 90 days\nCREATE USER 'contractor'@'%' IDENTIFIED BY 'temp_pass' PASSWORD EXPIRE INTERVAL 90 DAY;\n</code></pre> <p>The host portion of a user account matters. <code>'app_user'@'10.0.1.%'</code> and <code>'app_user'@'localhost'</code> are two different accounts with independent passwords and privileges. MySQL matches connections to accounts using the most specific host match.</p>"},{"location":"Databases/mysql-administration/#privilege-levels","title":"Privilege Levels","text":"<p>Privileges can be granted at four levels:</p> Level Syntax Scope Global <code>GRANT ... ON *.*</code> Entire server Database <code>GRANT ... ON mydb.*</code> All tables in a database Table <code>GRANT ... ON mydb.orders</code> One table Column <code>GRANT SELECT(name, email) ON mydb.users</code> Specific columns <pre><code>-- Global: full admin (use sparingly)\nGRANT ALL PRIVILEGES ON *.* TO 'dba'@'localhost' WITH GRANT OPTION;\n\n-- Database: application access\nGRANT SELECT, INSERT, UPDATE, DELETE ON myapp.* TO 'app_user'@'10.0.1.%';\n\n-- Table: read-only on sensitive data\nGRANT SELECT ON myapp.audit_log TO 'auditor'@'10.0.2.%';\n\n-- Column: restrict visible fields\nGRANT SELECT(user_id, username, created_at) ON myapp.users TO 'reporting'@'10.0.3.%';\n</code></pre>"},{"location":"Databases/mysql-administration/#grant-revoke-and-viewing-privileges","title":"GRANT, REVOKE, and Viewing Privileges","text":"<pre><code>-- See what a user can do\nSHOW GRANTS FOR 'app_user'@'10.0.1.%';\n\n-- Revoke specific privileges\nREVOKE DELETE ON myapp.* FROM 'app_user'@'10.0.1.%';\n\n-- Remove a user entirely\nDROP USER 'contractor'@'%';\n</code></pre> <p>After modifying grants in older MySQL versions (5.x), run <code>FLUSH PRIVILEGES</code> to reload the grant tables. MySQL 8.0 applies changes from <code>GRANT</code> and <code>REVOKE</code> immediately without a flush.</p>"},{"location":"Databases/mysql-administration/#roles-mysql-80","title":"Roles (MySQL 8.0+)","text":"<p>Roles group privileges under a named label, making it easier to manage access for multiple users:</p> <pre><code>-- Create roles\nCREATE ROLE 'app_read', 'app_write', 'app_admin';\n\n-- Assign privileges to roles\nGRANT SELECT ON myapp.* TO 'app_read';\nGRANT INSERT, UPDATE, DELETE ON myapp.* TO 'app_write';\nGRANT ALL ON myapp.* TO 'app_admin';\n\n-- Grant roles to users\nGRANT 'app_read', 'app_write' TO 'app_user'@'10.0.1.%';\nGRANT 'app_admin' TO 'dba'@'localhost';\n\n-- Roles must be activated in the session\nSET DEFAULT ROLE ALL TO 'app_user'@'10.0.1.%';\n</code></pre> <p>Roles require activation</p> <p>Granting a role to a user does not activate it automatically. You must either run <code>SET DEFAULT ROLE</code> for the user or have them execute <code>SET ROLE 'role_name'</code> in their session. Without this step, the role's privileges are not in effect.</p>"},{"location":"Databases/mysql-administration/#password-policies","title":"Password Policies","text":"<p>MySQL 8.0 includes the <code>validate_password</code> component for enforcing password strength:</p> <pre><code>-- Check current policy\nSHOW VARIABLES LIKE 'validate_password%';\n\n-- Set policy level (LOW, MEDIUM, STRONG)\nSET GLOBAL validate_password.policy = 'MEDIUM';\n\n-- Minimum length\nSET GLOBAL validate_password.length = 12;\n\n-- Require mixed case, numbers, special characters\nSET GLOBAL validate_password.mixed_case_count = 1;\nSET GLOBAL validate_password.number_count = 1;\nSET GLOBAL validate_password.special_char_count = 1;\n</code></pre> <p>Password policy levels:</p> Policy Checks <code>LOW</code> Length only <code>MEDIUM</code> Length + numbers + mixed case + special characters <code>STRONG</code> MEDIUM + dictionary file check <p>Managing Users and Privileges (requires JavaScript)</p> <p>In MySQL 8.0, what happens after you GRANT a role to a user but don't run SET DEFAULT ROLE? (requires JavaScript)</p>"},{"location":"Databases/mysql-administration/#log-types","title":"Log Types","text":"<p>MySQL writes several log files, each serving a different purpose. Knowing which log to check is half the battle when diagnosing problems.</p>"},{"location":"Databases/mysql-administration/#error-log","title":"Error Log","text":"<p>The error log is MySQL's most important log file. It records startup and shutdown events, crashes, warnings, and critical errors. Check here first when MySQL misbehaves.</p> <pre><code>-- Find the error log location\nSHOW VARIABLES LIKE 'log_error';\n\n-- Typical result: /var/log/mysql/error.log or /var/log/mysqld.log\n</code></pre> <p>MySQL 8.0 introduced a component-based error logging system with configurable output destinations and filtering. The default <code>log_error_services</code> value routes errors to the built-in log filter and sink.</p>"},{"location":"Databases/mysql-administration/#slow-query-log","title":"Slow Query Log","text":"<p>The slow query log captures queries that exceed a configurable time threshold. This is your primary tool for identifying performance problems.</p> <pre><code>-- Enable the slow query log\nSET GLOBAL slow_query_log = 'ON';\n\n-- Set the threshold (in seconds)\nSET GLOBAL long_query_time = 1;\n\n-- Also log queries that don't use indexes\nSET GLOBAL log_queries_not_using_indexes = 'ON';\n\n-- Check the log file location\nSHOW VARIABLES LIKE 'slow_query_log_file';\n</code></pre> <p>The <code>long_query_time</code> variable accepts decimal values. Setting it to <code>0.5</code> captures queries taking more than 500 milliseconds. Start with <code>1</code> second and lower it as you optimize.</p> <p>For persistent configuration, add these to <code>my.cnf</code>:</p> <pre><code>[mysqld]\nslow_query_log = 1\nslow_query_log_file = /var/log/mysql/slow.log\nlong_query_time = 1\nlog_queries_not_using_indexes = 1\n</code></pre>"},{"location":"Databases/mysql-administration/#general-query-log","title":"General Query Log","text":"<p>The general query log records every statement the server receives. It captures connects, disconnects, and all SQL statements regardless of whether they succeed or fail.</p> <pre><code>SET GLOBAL general_log = 'ON';\nSET GLOBAL general_log_file = '/var/log/mysql/general.log';\n</code></pre> <p>Performance impact</p> <p>The general query log writes to disk for every statement. On a busy server, this creates massive I/O overhead and fills disk rapidly. Enable it only for short debugging sessions, never in production long-term. Use the slow query log or Performance Schema for ongoing monitoring instead.</p>"},{"location":"Databases/mysql-administration/#binary-log","title":"Binary Log","text":"<p>The binary log (binlog) records all statements that change data. It serves two critical purposes:</p> <ol> <li>Replication - replicas read the binary log to apply changes from the source</li> <li>Point-in-time recovery (PITR) - replay binary logs to restore data to a specific moment after restoring a backup</li> </ol> <pre><code>-- Check binary log status\nSHOW VARIABLES LIKE 'log_bin';\n\n-- List binary log files\nSHOW BINARY LOGS;\n\n-- View events in a specific log file\nSHOW BINLOG EVENTS IN 'binlog.000042' LIMIT 20;\n</code></pre> <p>Binary logging is enabled by default in MySQL 8.0. The <code>binlog_format</code> variable controls how changes are recorded:</p> Format Description Use Case <code>ROW</code> Logs actual row changes Default in 8.0. Safest for replication <code>STATEMENT</code> Logs the SQL statements Smaller logs, but non-deterministic functions can cause drift <code>MIXED</code> Statement by default, row when needed Compromise, but adds complexity <pre><code>-- Check current format\nSHOW VARIABLES LIKE 'binlog_format';\n\n-- Purge old binary logs\nPURGE BINARY LOGS BEFORE '2024-01-01 00:00:00';\n\n-- Or keep only the last 7 days\nSET GLOBAL binlog_expire_logs_seconds = 604800;\n</code></pre>"},{"location":"Databases/mysql-administration/#table-maintenance","title":"Table Maintenance","text":"<p>Tables degrade over time. Deletes leave gaps in data files, index statistics drift from reality, and occasional corruption happens. Regular maintenance keeps things running smoothly.</p>"},{"location":"Databases/mysql-administration/#mysqlcheck","title":"<code>mysqlcheck</code>","text":"<p><code>mysqlcheck</code> is a command-line utility that performs table maintenance without entering the <code>mysql</code> client. It supports four operations:</p> Operation Flag Purpose <code>CHECK</code> <code>-c</code> or <code>--check</code> Verify table integrity <code>REPAIR</code> <code>-r</code> or <code>--repair</code> Fix corrupted MyISAM tables <code>ANALYZE</code> <code>-a</code> or <code>--analyze</code> Update index statistics <code>OPTIMIZE</code> <code>-o</code> or <code>--optimize</code> Reclaim space, defragment <pre><code># Check all tables in a database\nmysqlcheck -u root -p --check myapp\n\n# Check all databases\nmysqlcheck -u root -p --check --all-databases\n\n# Analyze all tables (update statistics)\nmysqlcheck -u root -p --analyze --all-databases\n\n# Optimize a specific table\nmysqlcheck -u root -p --optimize myapp orders\n</code></pre>"},{"location":"Databases/mysql-administration/#optimize-table-for-innodb","title":"<code>OPTIMIZE TABLE</code> for InnoDB","text":"<p>For InnoDB tables, <code>OPTIMIZE TABLE</code> performs a full table rebuild. It recreates the table and its indexes, which:</p> <ul> <li>Reclaims space from deleted rows</li> <li>Rebuilds indexes for better efficiency</li> <li>Resets fragmentation</li> </ul> <pre><code>OPTIMIZE TABLE myapp.orders;\n</code></pre> <p>InnoDB translates <code>OPTIMIZE TABLE</code> into <code>ALTER TABLE ... FORCE</code> internally. This means it creates a temporary copy of the table, which requires free disk space equal to the table's size. On a 50 GB table, you need 50 GB of free space.</p> <p>OPTIMIZE TABLE locks</p> <p>While MySQL 8.0 performs the rebuild as an online DDL operation (allowing reads and writes), there are brief periods at the start and end where it acquires metadata locks. On very large tables, plan this for maintenance windows.</p>"},{"location":"Databases/mysql-administration/#analyze-table","title":"<code>ANALYZE TABLE</code>","text":"<p><code>ANALYZE TABLE</code> updates the statistics that the query optimizer uses to choose execution plans. Stale statistics can cause the optimizer to pick poor index choices:</p> <pre><code>ANALYZE TABLE myapp.orders;\nANALYZE TABLE myapp.users, myapp.sessions;\n</code></pre> <p>Unlike <code>OPTIMIZE</code>, <code>ANALYZE TABLE</code> is fast. It reads a sample of index pages and updates the cardinality estimates stored in <code>mysql.innodb_index_stats</code> and <code>mysql.innodb_table_stats</code>. Run it after bulk inserts or deletes that significantly change the data distribution.</p> <p>What does OPTIMIZE TABLE do on an InnoDB table? (requires JavaScript)</p>"},{"location":"Databases/mysql-administration/#information_schema","title":"<code>information_schema</code>","text":"<p>The <code>information_schema</code> database is a read-only set of views that expose metadata about every database, table, column, index, and active process on the server. It is your programmatic interface to MySQL's internals.</p>"},{"location":"Databases/mysql-administration/#key-tables","title":"Key Tables","text":""},{"location":"Databases/mysql-administration/#tables","title":"<code>TABLES</code>","text":"<p>Size, row count, engine, and creation time for every table:</p> <pre><code>-- Find the largest tables\nSELECT\n    TABLE_SCHEMA,\n    TABLE_NAME,\n    ENGINE,\n    TABLE_ROWS,\n    ROUND(DATA_LENGTH / 1024 / 1024, 2) AS data_mb,\n    ROUND(INDEX_LENGTH / 1024 / 1024, 2) AS index_mb,\n    ROUND((DATA_LENGTH + INDEX_LENGTH) / 1024 / 1024, 2) AS total_mb\nFROM information_schema.TABLES\nWHERE TABLE_SCHEMA NOT IN ('mysql', 'information_schema', 'performance_schema', 'sys')\nORDER BY (DATA_LENGTH + INDEX_LENGTH) DESC\nLIMIT 10;\n</code></pre>"},{"location":"Databases/mysql-administration/#columns","title":"<code>COLUMNS</code>","text":"<p>Schema details for every column across all tables:</p> <pre><code>-- Find all columns of a certain type\nSELECT TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME, COLUMN_TYPE\nFROM information_schema.COLUMNS\nWHERE DATA_TYPE = 'enum'\n  AND TABLE_SCHEMA = 'myapp';\n</code></pre>"},{"location":"Databases/mysql-administration/#statistics","title":"<code>STATISTICS</code>","text":"<p>Index metadata including cardinality, uniqueness, and column order:</p> <pre><code>-- Show indexes for a table\nSELECT INDEX_NAME, COLUMN_NAME, SEQ_IN_INDEX, CARDINALITY, NON_UNIQUE\nFROM information_schema.STATISTICS\nWHERE TABLE_SCHEMA = 'myapp' AND TABLE_NAME = 'orders'\nORDER BY INDEX_NAME, SEQ_IN_INDEX;\n</code></pre>"},{"location":"Databases/mysql-administration/#processlist","title":"<code>PROCESSLIST</code>","text":"<p>All active connections and their current state:</p> <pre><code>-- Show active queries (not sleeping)\nSELECT ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO\nFROM information_schema.PROCESSLIST\nWHERE COMMAND != 'Sleep'\nORDER BY TIME DESC;\n</code></pre> <p>performance_schema.threads</p> <p>In MySQL 8.0, <code>performance_schema.threads</code> provides more detail than <code>information_schema.PROCESSLIST</code> and does not require a mutex lock to read. For monitoring scripts, prefer <code>performance_schema.threads</code> or <code>SHOW PROCESSLIST</code> over querying <code>information_schema.PROCESSLIST</code> directly.</p>"},{"location":"Databases/mysql-administration/#innodb_trx","title":"<code>INNODB_TRX</code>","text":"<p>Active InnoDB transactions, including how long they have been running and whether they are waiting on locks:</p> <pre><code>-- Find long-running transactions\nSELECT\n    trx_id,\n    trx_state,\n    trx_started,\n    TIMESTAMPDIFF(SECOND, trx_started, NOW()) AS age_seconds,\n    trx_rows_locked,\n    trx_rows_modified,\n    trx_query\nFROM information_schema.INNODB_TRX\nORDER BY trx_started ASC;\n</code></pre>"},{"location":"Databases/mysql-administration/#routine-maintenance-tasks","title":"Routine Maintenance Tasks","text":"<p>These are the tasks you will perform repeatedly as a MySQL administrator. Build them into your operational routine.</p>"},{"location":"Databases/mysql-administration/#checking-disk-usage","title":"Checking Disk Usage","text":"<p>Monitor data directory size and individual table growth:</p> <pre><code># Total MySQL data directory size\ndu -sh /var/lib/mysql\n\n# Size per database\ndu -sh /var/lib/mysql/*/\n</code></pre> <p>From within MySQL, use the <code>TABLES</code> view for more detail:</p> <pre><code>-- Database sizes\nSELECT\n    TABLE_SCHEMA AS db,\n    ROUND(SUM(DATA_LENGTH + INDEX_LENGTH) / 1024 / 1024, 2) AS size_mb\nFROM information_schema.TABLES\nGROUP BY TABLE_SCHEMA\nORDER BY size_mb DESC;\n\n-- Tables larger than 1 GB\nSELECT TABLE_SCHEMA, TABLE_NAME,\n    ROUND((DATA_LENGTH + INDEX_LENGTH) / 1024 / 1024 / 1024, 2) AS size_gb\nFROM information_schema.TABLES\nWHERE (DATA_LENGTH + INDEX_LENGTH) &gt; 1073741824\nORDER BY (DATA_LENGTH + INDEX_LENGTH) DESC;\n</code></pre>"},{"location":"Databases/mysql-administration/#monitoring-connections","title":"Monitoring Connections","text":"<pre><code>-- Current connection count vs. limit\nSHOW VARIABLES LIKE 'max_connections';\nSHOW STATUS LIKE 'Threads_connected';\n\n-- Connections by user\nSELECT USER, COUNT(*) AS connections\nFROM information_schema.PROCESSLIST\nGROUP BY USER\nORDER BY connections DESC;\n\n-- Connections by host\nSELECT SUBSTRING_INDEX(HOST, ':', 1) AS client_host, COUNT(*) AS connections\nFROM information_schema.PROCESSLIST\nGROUP BY client_host\nORDER BY connections DESC;\n</code></pre>"},{"location":"Databases/mysql-administration/#killing-long-queries","title":"Killing Long Queries","text":"<p>When a runaway query is consuming resources or holding locks:</p> <pre><code>-- Find queries running longer than 60 seconds\nSELECT ID, USER, HOST, DB, TIME, INFO\nFROM information_schema.PROCESSLIST\nWHERE COMMAND = 'Query' AND TIME &gt; 60;\n\n-- Kill a specific connection\nKILL 12345;\n\n-- Kill only the query (keep the connection)\nKILL QUERY 12345;\n</code></pre> <p><code>KILL</code> terminates the entire connection. <code>KILL QUERY</code> cancels just the running statement and leaves the connection open - prefer this for application connections that should stay alive.</p>"},{"location":"Databases/mysql-administration/#checking-replication-status","title":"Checking Replication Status","text":"<p>If your server is a replica, verify replication health:</p> <pre><code>-- MySQL 8.0.22+\nSHOW REPLICA STATUS\\G\n\n-- Older versions\nSHOW SLAVE STATUS\\G\n</code></pre> <p>Key fields to check:</p> Field Healthy Value <code>Replica_IO_Running</code> <code>Yes</code> <code>Replica_SQL_Running</code> <code>Yes</code> <code>Seconds_Behind_Source</code> <code>0</code> (or a low, stable number) <code>Last_Error</code> Empty <pre><code>-- Quick replication check (MySQL 8.0.22+)\nSELECT\n    CHANNEL_NAME,\n    SERVICE_STATE AS io_state\nFROM performance_schema.replication_connection_status;\n\nSELECT\n    CHANNEL_NAME,\n    SERVICE_STATE AS sql_state,\n    LAST_ERROR_MESSAGE\nFROM performance_schema.replication_applier_status;\n</code></pre> <p>Routine Maintenance Tasks (requires JavaScript)</p>"},{"location":"Databases/mysql-administration/#scheduled-maintenance-script","title":"Scheduled Maintenance Script","text":"<p>A basic maintenance routine you can schedule via cron:</p> <pre><code>#!/bin/bash\n# /usr/local/bin/mysql-maintenance.sh\n\nLOG=\"/var/log/mysql-maintenance.log\"\nDATE=$(date '+%Y-%m-%d %H:%M:%S')\n\necho \"[$DATE] Starting maintenance\" &gt;&gt; \"$LOG\"\n\n# Update statistics on all databases\nmysqlcheck -u root --analyze --all-databases &gt;&gt; \"$LOG\" 2&gt;&amp;1\n\n# Check tables for errors\nmysqlcheck -u root --check --all-databases &gt;&gt; \"$LOG\" 2&gt;&amp;1\n\n# Purge binary logs older than 7 days\nmysql -u root -e \"PURGE BINARY LOGS BEFORE DATE_SUB(NOW(), INTERVAL 7 DAY)\" &gt;&gt; \"$LOG\" 2&gt;&amp;1\n\necho \"[$DATE] Maintenance complete\" &gt;&gt; \"$LOG\"\n</code></pre> <pre><code># Run weekly at 3 AM Sunday\necho \"0 3 * * 0 /usr/local/bin/mysql-maintenance.sh\" | crontab -\n</code></pre>"},{"location":"Databases/mysql-administration/#exercises","title":"Exercises","text":"<p>Diagnose and Fix a Stalled MySQL Server (requires JavaScript)</p>"},{"location":"Databases/mysql-administration/#further-reading","title":"Further Reading","text":"<ul> <li>MySQL 8.0 Reference Manual: mysql Client - complete CLI reference including all options and built-in commands</li> <li>MySQL 8.0 Reference Manual: Access Control and Account Management - privilege system, roles, and password policy details</li> <li>MySQL 8.0 Reference Manual: MySQL Server Logs - configuration and format for all log types</li> <li>MySQL 8.0 Reference Manual: information_schema - complete reference for all metadata tables</li> <li>Percona Toolkit - production-grade tools for MySQL including <code>pt-query-digest</code>, <code>pt-kill</code>, and <code>pt-online-schema-change</code></li> </ul> <p>Previous: MySQL Installation &amp; Configuration | Next: MySQL Performance &amp; Optimization | Back to Index</p>"},{"location":"Databases/mysql-installation-and-configuration/","title":"MySQL Installation &amp; Configuration","text":"<p>Getting a database server running is the easy part. Getting it configured correctly - so it performs well, stores your data safely, and doesn't fall over under load - takes understanding. This guide covers installing MySQL on every major platform, understanding the configuration file that controls its behavior, choosing between storage engines, sizing the buffer pool, getting character sets right, and locking down the installation before anything touches production.</p>"},{"location":"Databases/mysql-installation-and-configuration/#installing-mysql","title":"Installing MySQL","text":""},{"location":"Databases/mysql-installation-and-configuration/#debian-and-ubuntu-apt","title":"Debian and Ubuntu (apt)","text":"<p>The MySQL APT Repository provides the latest packages for Debian-based distributions. You can also install the version bundled with your distribution's default repositories, though it may be older.</p> <pre><code># Install from the default Ubuntu/Debian repos\nsudo apt update\nsudo apt install mysql-server\n\n# Check the installed version\nmysql --version\n</code></pre> <p>On Ubuntu 22.04+, this installs MySQL 8.0. The service starts automatically after installation.</p> <pre><code># Verify the service is running\nsudo systemctl status mysql\n\n# Enable MySQL to start on boot (usually enabled by default)\nsudo systemctl enable mysql\n</code></pre>"},{"location":"Databases/mysql-installation-and-configuration/#rhel-fedora-and-centos-dnfyum","title":"RHEL, Fedora, and CentOS (dnf/yum)","text":"<p>Red Hat-based distributions often ship MariaDB by default instead of MySQL. To install MySQL itself, add the official MySQL repository first.</p> <pre><code># Fedora / RHEL 8+ / CentOS Stream (dnf)\nsudo dnf install https://dev.mysql.com/get/mysql80-community-release-el8-9.noarch.rpm\nsudo dnf install mysql-community-server\n\n# CentOS 7 (yum)\nsudo yum install https://dev.mysql.com/get/mysql80-community-release-el7-11.noarch.rpm\nsudo yum install mysql-community-server\n</code></pre> <p>Start and enable the service:</p> <pre><code>sudo systemctl start mysqld\nsudo systemctl enable mysqld\n</code></pre> <p>Temporary root password on RHEL-based installs</p> <p>MySQL on RHEL/CentOS generates a temporary root password during installation. Retrieve it before doing anything else:</p> <pre><code>sudo grep 'temporary password' /var/log/mysqld.log\n</code></pre> <p>You must change this password on first login. The <code>mysql_secure_installation</code> script (covered later) handles this.</p>"},{"location":"Databases/mysql-installation-and-configuration/#macos-homebrew","title":"macOS (Homebrew)","text":"<p>Homebrew is the simplest path on macOS:</p> <pre><code>brew install mysql\n\n# Start the service\nbrew services start mysql\n</code></pre> <p>Homebrew installs MySQL with no root password by default. You should run <code>mysql_secure_installation</code> immediately after starting the service.</p>"},{"location":"Databases/mysql-installation-and-configuration/#docker","title":"Docker","text":"<p>Running MySQL in Docker is the fastest way to get a disposable instance for development or testing. The official <code>mysql</code> image supports all 8.0.x releases.</p> <pre><code># Pull and run MySQL 8.0\ndocker run --name mysql-dev \\\n  -e MYSQL_ROOT_PASSWORD=changeme \\\n  -p 3306:3306 \\\n  -d mysql:8.0\n</code></pre> <p>For a more reproducible setup, use a <code>docker-compose.yml</code> file:</p> <pre><code>services:\n  mysql:\n    image: mysql:8.0\n    container_name: mysql-dev\n    restart: unless-stopped\n    environment:\n      MYSQL_ROOT_PASSWORD: changeme\n      MYSQL_DATABASE: appdb\n      MYSQL_USER: appuser\n      MYSQL_PASSWORD: apppassword\n    ports:\n      - \"3306:3306\"\n    volumes:\n      - mysql_data:/var/lib/mysql\n      - ./my-custom.cnf:/etc/mysql/conf.d/custom.cnf\n\nvolumes:\n  mysql_data:\n</code></pre> <p>The <code>volumes</code> entry for <code>mysql_data</code> ensures your data survives container restarts. The second volume mount lets you inject a custom configuration file.</p> <pre><code># Start the stack\ndocker compose up -d\n\n# Connect to the running instance\ndocker exec -it mysql-dev mysql -u root -p\n</code></pre> <p>Installing MySQL on Ubuntu and Verifying the Service (requires JavaScript)</p> <p>On a fresh MySQL installation on RHEL/CentOS, where do you find the temporary root password? (requires JavaScript)</p>"},{"location":"Databases/mysql-installation-and-configuration/#the-mycnf-configuration-file","title":"The <code>my.cnf</code> Configuration File","text":"<p>MySQL reads its configuration from <code>my.cnf</code> (or <code>my.ini</code> on Windows). This file controls everything from memory allocation to networking behavior to storage engine defaults.</p>"},{"location":"Databases/mysql-installation-and-configuration/#file-locations","title":"File Locations","text":"<p>MySQL checks multiple locations in a specific order, with later files overriding earlier ones:</p> Order Path Typical Use 1 <code>/etc/my.cnf</code> System-wide defaults (RHEL/CentOS) 2 <code>/etc/mysql/my.cnf</code> System-wide defaults (Debian/Ubuntu) 3 <code>~/.my.cnf</code> Per-user client settings 4 Files in <code>conf.d/</code> directories Drop-in overrides <p>To see exactly which files your MySQL installation reads and in what order:</p> <pre><code>mysqld --verbose --help 2&gt;/dev/null | head -20\n</code></pre> <p>Or check the compiled-in defaults:</p> <pre><code>mysql --help | grep -A 1 \"Default options\"\n</code></pre>"},{"location":"Databases/mysql-installation-and-configuration/#section-structure","title":"Section Structure","text":"<p><code>my.cnf</code> uses an INI-style format with sections (also called groups) that target different programs:</p> <pre><code>[mysqld]\n# Settings for the MySQL server daemon\ndatadir = /var/lib/mysql\nport = 3306\n\n[mysql]\n# Settings for the mysql command-line client\nprompt = \\\\u@\\\\h [\\\\d]&gt;\\_\n\n[client]\n# Settings for ALL MySQL client programs (mysql, mysqldump, etc.)\nport = 3306\nsocket = /var/run/mysqld/mysqld.sock\n</code></pre> <p>The <code>[mysqld]</code> section is where the server tuning happens. The <code>[client]</code> section applies to every client tool. The <code>[mysql]</code> section applies only to the interactive <code>mysql</code> CLI.</p>"},{"location":"Databases/mysql-installation-and-configuration/#option-precedence","title":"Option Precedence","text":"<p>When the same option appears in multiple places, MySQL follows this precedence (highest to lowest):</p> <ol> <li>Command-line arguments - <code>mysqld --innodb-buffer-pool-size=2G</code></li> <li>Last file read in the config file chain</li> <li>Within a file, the last occurrence of an option wins</li> </ol> <p>This means a setting in <code>/etc/mysql/conf.d/custom.cnf</code> overrides the same setting in <code>/etc/mysql/my.cnf</code>, and a command-line flag overrides everything.</p> <p>Use conf.d directories for custom settings</p> <p>Instead of editing the main <code>my.cnf</code> directly, drop a file into <code>/etc/mysql/conf.d/</code> (Debian/Ubuntu) or <code>/etc/my.cnf.d/</code> (RHEL/CentOS). This keeps your customizations separate from package-managed defaults and survives package upgrades cleanly.</p> <p>A Production my.cnf Configuration (requires JavaScript)</p>"},{"location":"Databases/mysql-installation-and-configuration/#storage-engines-innodb-vs-myisam","title":"Storage Engines: InnoDB vs MyISAM","text":"<p>A storage engine is the component that handles how data is physically stored, retrieved, and indexed. MySQL's pluggable architecture supports multiple storage engines, but in practice you will use one of two: InnoDB or MyISAM.</p>"},{"location":"Databases/mysql-installation-and-configuration/#innodb","title":"InnoDB","text":"<p>InnoDB has been the default storage engine since MySQL 5.5, and for good reason. It provides:</p> <ul> <li>ACID transactions - full commit, rollback, and crash recovery</li> <li>Row-level locking - concurrent reads and writes without blocking each other</li> <li>Foreign key constraints - referential integrity enforced by the engine</li> <li>Crash recovery - the redo log (write-ahead log) ensures committed data survives power failures</li> <li>MVCC (Multi-Version Concurrency Control) - readers don't block writers and vice versa</li> <li>Clustered indexes - data is physically ordered by the primary key, making primary key lookups extremely fast</li> </ul>"},{"location":"Databases/mysql-installation-and-configuration/#myisam","title":"MyISAM","text":"<p>MyISAM was the default engine before MySQL 5.5. It still exists but is rarely the right choice:</p> <ul> <li>Table-level locking - a write to any row locks the entire table</li> <li>No transactions - no commit, no rollback, no crash recovery guarantees</li> <li>No foreign keys - referential integrity must be enforced by your application</li> <li>Full-text indexing - MyISAM had this first, but InnoDB supports it since MySQL 5.6</li> <li>Smaller disk footprint - MyISAM tables are slightly more compact for read-heavy, append-only workloads</li> <li>REPAIR TABLE - MyISAM tables can be repaired after corruption, but they corrupt more easily than InnoDB</li> </ul>"},{"location":"Databases/mysql-installation-and-configuration/#feature-comparison","title":"Feature Comparison","text":"Feature InnoDB MyISAM Transactions Yes (ACID) No Locking Row-level Table-level Foreign keys Yes No Crash recovery Automatic (redo log) Manual (<code>REPAIR TABLE</code>) Full-text search Yes (5.6+) Yes MVCC Yes No Clustered index Yes No Compressed tables Yes Yes Geospatial indexes Yes (5.7+) Yes Default since MySQL 5.5 Before 5.5 <p>Use InnoDB unless you have a specific, documented reason not to</p> <p>MyISAM's table-level locking makes it unsuitable for any workload with concurrent writes. A single slow <code>UPDATE</code> blocks every other query on that table. InnoDB's row-level locking handles concurrency far better. The only remaining niche for MyISAM is read-only data loaded in bulk and queried by full-text search, and even that use case is shrinking as InnoDB's full-text implementation matures.</p>"},{"location":"Databases/mysql-installation-and-configuration/#innodb_file_per_table","title":"<code>innodb_file_per_table</code>","text":"<p>By default (since MySQL 5.6.6), InnoDB stores each table in its own tablespace file - a <code>.ibd</code> file in the data directory. This setting is controlled by <code>innodb_file_per_table</code>.</p> <pre><code>[mysqld]\ninnodb_file_per_table = ON   # default in MySQL 8.0\n</code></pre> <p>When <code>innodb_file_per_table = ON</code>:</p> <ul> <li>Each table gets its own <code>schema_name/table_name.ibd</code> file</li> <li>Dropping a table or running <code>OPTIMIZE TABLE</code> reclaims disk space to the OS</li> <li>You can place individual tables on different storage devices using <code>DATA DIRECTORY</code></li> <li>Backups can target individual tables (Percona XtraBackup supports partial backups)</li> </ul> <p>When <code>innodb_file_per_table = OFF</code>:</p> <ul> <li>All table data goes into the shared system tablespace (<code>ibdata1</code>)</li> <li><code>ibdata1</code> grows but never shrinks - dropped tables leave empty space inside the file, but the file itself stays large</li> <li>The only way to reclaim that space is to dump all data, delete <code>ibdata1</code>, and reimport</li> </ul> <p>There is almost never a reason to disable this setting on MySQL 8.0.</p> <p>What happens when you DROP a large table and innodb_file_per_table is OFF? (requires JavaScript)</p>"},{"location":"Databases/mysql-installation-and-configuration/#buffer-pool-sizing","title":"Buffer Pool Sizing","text":"<p>The InnoDB buffer pool is a memory cache that holds table data and index pages. When MySQL reads a row, it loads the page containing that row into the buffer pool. Subsequent reads of the same data come from memory instead of disk. The buffer pool is the single most impactful tuning parameter in MySQL.</p>"},{"location":"Databases/mysql-installation-and-configuration/#how-it-works","title":"How It Works","text":"<p>The buffer pool uses an LRU (Least Recently Used) algorithm with a twist: InnoDB splits the list into a \"young\" sublist and an \"old\" sublist. Pages enter at the midpoint of the old sublist and only get promoted to the young sublist if they are accessed again after a short delay. This prevents a one-time table scan from flushing frequently accessed pages out of cache.</p>"},{"location":"Databases/mysql-installation-and-configuration/#sizing-guidelines","title":"Sizing Guidelines","text":"<p>The general rule: set <code>innodb_buffer_pool_size</code> to 50-70% of the total RAM on a dedicated database server.</p> Server RAM Buffer Pool Size Reasoning 2 GB 1 GB Small dev/staging server, leave room for OS and connections 8 GB 5-6 GB Typical production server 32 GB 20-24 GB Large production workload 128 GB 80-100 GB High-performance dedicated database <pre><code>[mysqld]\n# For an 8 GB server\ninnodb_buffer_pool_size = 5G\n\n# For large buffer pools (&gt; 1 GB), use multiple instances\n# Each instance gets its own mutex, reducing contention\ninnodb_buffer_pool_instances = 4\n</code></pre> <p>Check your buffer pool hit ratio</p> <p>After running under production load, check how effectively the buffer pool is serving reads:</p> <pre><code>SHOW GLOBAL STATUS LIKE 'Innodb_buffer_pool_read%';\n</code></pre> <p>Calculate the hit ratio: <code>1 - (Innodb_buffer_pool_reads / Innodb_buffer_pool_read_requests)</code>. A ratio below 99% on a warmed-up server suggests the buffer pool is too small - data is being read from disk when it should be in memory.</p> <p>Don't over-allocate. The operating system needs memory for its own page cache, for per-connection buffers (<code>sort_buffer_size</code>, <code>join_buffer_size</code>, <code>read_buffer_size</code> multiplied by <code>max_connections</code>), and for the OS itself. A MySQL server that uses all available RAM will trigger the OOM killer, and a dead database is worse than a slow one.</p>"},{"location":"Databases/mysql-installation-and-configuration/#character-sets-and-collations","title":"Character Sets and Collations","text":"<p>A character set defines which characters can be stored. A collation defines how those characters are sorted and compared. Getting this wrong causes data loss, broken searches, or silent corruption.</p>"},{"location":"Databases/mysql-installation-and-configuration/#utf8mb4-vs-utf8-utf8mb3","title":"utf8mb4 vs utf8 (utf8mb3)","text":"<p>MySQL's <code>utf8</code> character set is not real UTF-8. It uses a maximum of 3 bytes per character, which means it cannot store any character that requires 4 bytes in UTF-8 encoding. This includes:</p> <ul> <li>Emoji (U+1F600 and above)</li> <li>Many CJK unified ideographs</li> <li>Musical symbols</li> <li>Mathematical symbols beyond the Basic Multilingual Plane</li> </ul> <p><code>utf8mb4</code> is MySQL's true UTF-8 implementation, supporting the full Unicode range with up to 4 bytes per character. It has been the default character set since MySQL 8.0.</p> <pre><code>-- Check current server defaults\nSHOW VARIABLES LIKE 'character_set%';\nSHOW VARIABLES LIKE 'collation%';\n\n-- Set at the server level in my.cnf\n-- [mysqld]\n-- character_set_server = utf8mb4\n-- collation_server = utf8mb4_0900_ai_ci\n\n-- Set at the database level\nCREATE DATABASE myapp CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci;\n\n-- Set at the table level\nCREATE TABLE users (\n    id INT PRIMARY KEY AUTO_INCREMENT,\n    name VARCHAR(100)\n) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci;\n</code></pre>"},{"location":"Databases/mysql-installation-and-configuration/#choosing-a-collation","title":"Choosing a Collation","text":"<p>MySQL 8.0 ships with over 40 collations for <code>utf8mb4</code>. The most commonly used:</p> Collation Behavior Use When <code>utf8mb4_0900_ai_ci</code> Accent-insensitive, case-insensitive (Unicode 9.0) Default for most applications <code>utf8mb4_0900_as_cs</code> Accent-sensitive, case-sensitive Usernames, codes where \"cafe\" and \"cafe\" must differ <code>utf8mb4_unicode_ci</code> Case-insensitive (older Unicode standard) Legacy applications migrating from MySQL 5.7 <code>utf8mb4_bin</code> Binary comparison (byte-by-byte) Exact matching, hash values, case-sensitive without linguistic rules <code>utf8mb4_general_ci</code> Simplified case-insensitive Avoid - faster than unicode_ci on old hardware but less correct <p>The <code>_ai_</code> and <code>_as_</code> segments mean accent-insensitive and accent-sensitive. The <code>_ci</code> and <code>_cs</code> suffixes mean case-insensitive and case-sensitive.</p> <p>Migrating from utf8 to utf8mb4</p> <p>If you have existing tables using <code>utf8</code> (utf8mb3), converting to <code>utf8mb4</code> increases the maximum byte length of each character from 3 to 4. This can push <code>VARCHAR(255)</code> columns past the 767-byte index prefix limit on older row formats. On MySQL 8.0 with <code>DYNAMIC</code> row format (the default), the index prefix limit is 3072 bytes, so this is rarely a problem. Test the conversion on a copy of your data first:</p> <pre><code>ALTER TABLE users CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci;\n</code></pre> <p>Why should you use utf8mb4 instead of utf8 in MySQL? (requires JavaScript)</p>"},{"location":"Databases/mysql-installation-and-configuration/#mariadb-vs-mysql","title":"MariaDB vs MySQL","text":"<p>MariaDB is a fork of MySQL created in 2009 by Michael \"Monty\" Widenius (MySQL's original author) after Oracle acquired Sun Microsystems. MariaDB aims for MySQL compatibility but has diverged in several areas.</p>"},{"location":"Databases/mysql-installation-and-configuration/#where-mariadb-diverges","title":"Where MariaDB Diverges","text":"<p>JSON support: MySQL 8.0 stores JSON in an optimized binary format with a dedicated <code>JSON</code> data type that validates on insert. MariaDB's <code>JSON</code> is an alias for <code>LONGTEXT</code> - it accepts JSON syntax but stores it as plain text without binary optimization. MySQL's JSON path expressions and functions (<code>JSON_TABLE</code>, <code>JSON_ARRAYAGG</code>) are generally more mature.</p> <p>Authentication plugins: MySQL 8.0 defaults to <code>caching_sha2_password</code>, which requires TLS or RSA key exchange. MariaDB uses <code>mysql_native_password</code> by default, which is less secure but more compatible with older client libraries. This difference breaks some client connections when switching between servers.</p> <p>Optimizer: MariaDB includes optimizer features that MySQL lacks, such as condition pushdown for derived tables (before MySQL 8.0.22 added it), and the <code>optimizer_switch</code> settings differ. MariaDB's cost model and join algorithms diverge enough that the same query can choose different execution plans on each.</p> <p>Window functions: MariaDB added window functions in version 10.2 (2017). MySQL added them in 8.0 (2018). The implementations are functionally similar but differ in edge cases around frame specifications.</p> <p>System-versioned tables: MariaDB supports temporal tables natively with <code>WITH SYSTEM VERSIONING</code>, allowing you to query historical row states. MySQL has no equivalent built-in feature.</p> <p>Thread pool: MariaDB includes a thread pool in the community edition. MySQL's thread pool is an Enterprise-only feature.</p>"},{"location":"Databases/mysql-installation-and-configuration/#compatibility","title":"Compatibility","text":"<p>For most applications, MariaDB 10.x is a drop-in replacement for MySQL 5.7. The divergence widens with MySQL 8.0 features:</p> Feature MySQL 8.0 MariaDB 10.x/11.x Native JSON type Yes No (alias for LONGTEXT) <code>caching_sha2_password</code> Default Not supported CTEs (WITH clause) Yes Yes Window functions Yes Yes CHECK constraints Yes (enforced) Yes (enforced since 10.2) Roles Yes Yes Invisible columns Yes (8.0.23+) Yes (10.3+) System versioning No Yes <code>EXCEPT</code> / <code>INTERSECT</code> Yes (8.0.31+) Yes Atomic DDL Yes Partial <p>If you are starting a new project, choose one and stay with it. Migrating between MySQL and MariaDB becomes harder as you use features specific to either fork.</p>"},{"location":"Databases/mysql-installation-and-configuration/#securing-the-installation","title":"Securing the Installation","text":"<p>A fresh MySQL installation ships with settings optimized for getting started, not for security. Before the server accepts any real data, run through these steps.</p>"},{"location":"Databases/mysql-installation-and-configuration/#mysql_secure_installation","title":"<code>mysql_secure_installation</code>","text":"<p>MySQL ships with a script that handles the most critical initial hardening:</p> <pre><code>sudo mysql_secure_installation\n</code></pre> <p>The script walks through these prompts:</p> <ol> <li>Set/change the root password - required on RHEL-based installs, recommended everywhere</li> <li>Remove anonymous users - MySQL creates an anonymous user that can connect without authentication</li> <li>Disallow remote root login - the root account should only connect from <code>localhost</code></li> <li>Remove the test database - a publicly accessible <code>test</code> database ships by default</li> <li>Reload privilege tables - applies the changes immediately</li> </ol> <p>Answer \"yes\" to all of these in any environment that isn't a throwaway development container.</p> <p>Running mysql_secure_installation (requires JavaScript)</p>"},{"location":"Databases/mysql-installation-and-configuration/#additional-hardening","title":"Additional Hardening","text":"<p>Beyond <code>mysql_secure_installation</code>, consider these configuration changes:</p> <pre><code>[mysqld]\n# Disable LOAD DATA LOCAL INFILE (prevents reading server-side files via SQL)\nlocal_infile = OFF\n\n# Require secure transport for all connections (MySQL 8.0+)\nrequire_secure_transport = ON\n\n# Disable symbolic links (prevents symlinking data files to sensitive locations)\nsymbolic-links = 0\n\n# Log all connection attempts\nlog_error_verbosity = 3\n</code></pre> <p>Create application-specific user accounts with minimal privileges instead of sharing the root account:</p> <pre><code>-- Create a user for your application\nCREATE USER 'appuser'@'192.168.1.%' IDENTIFIED BY 'strong_password_here';\n\n-- Grant only the privileges the application needs\nGRANT SELECT, INSERT, UPDATE, DELETE ON appdb.* TO 'appuser'@'192.168.1.%';\n\n-- Never grant ALL PRIVILEGES to application accounts\n-- Never grant SUPER, FILE, or PROCESS to application accounts\n\nFLUSH PRIVILEGES;\n</code></pre>"},{"location":"Databases/mysql-installation-and-configuration/#putting-it-all-together","title":"Putting It All Together","text":"<p>You now have the pieces: a running MySQL installation, a configuration file tuned for your hardware, InnoDB as the storage engine with per-table tablespace and a properly sized buffer pool, utf8mb4 as the character set, and a secured installation with strong authentication.</p> <p>The configuration choices in this guide are conservative defaults that work well for most workloads. As your data grows and your query patterns become clear, the MySQL Performance &amp; Optimization guide covers the tools and techniques for measuring what needs tuning and adjusting these settings based on real data rather than guesswork.</p> <p>Configure MySQL for a New Application (requires JavaScript)</p>"},{"location":"Databases/mysql-installation-and-configuration/#further-reading","title":"Further Reading","text":"<ul> <li>MySQL 8.0 Reference Manual - the complete official documentation</li> <li>InnoDB Storage Engine - architecture, buffer pool, redo log, and configuration</li> <li>MySQL Server System Variables - every my.cnf option documented</li> <li>MariaDB Knowledge Base - MariaDB-specific documentation and compatibility notes</li> <li>Percona Blog: InnoDB Buffer Pool - practical tuning advice from the Percona team</li> <li>MySQL Docker Official Image - environment variables, initialization scripts, and volume configuration</li> </ul> <p>Previous: Database Design &amp; Modeling | Next: MySQL Administration | Back to Index</p>"},{"location":"Databases/mysql-performance/","title":"MySQL Performance &amp; Optimization","text":"<p>Performance problems in MySQL almost always trace back to one of three causes: missing or misused indexes, queries that scan more rows than necessary, or server settings mismatched to the workload. This guide covers the tools and techniques for finding and fixing those problems - from reading <code>EXPLAIN</code> output and building the right indexes, to tuning the buffer pool and analyzing the slow query log.</p>"},{"location":"Databases/mysql-performance/#understanding-explain","title":"Understanding EXPLAIN","text":"<p>The <code>EXPLAIN</code> statement is your primary tool for understanding how MySQL executes a query. Prefix any <code>SELECT</code>, <code>INSERT</code>, <code>UPDATE</code>, or <code>DELETE</code> with <code>EXPLAIN</code> and MySQL returns the execution plan - the sequence of steps the optimizer chose, which indexes it uses, and how many rows it estimates it needs to examine.</p> <pre><code>EXPLAIN SELECT c.name, o.total\nFROM customers c\nJOIN orders o ON o.customer_id = c.id\nWHERE c.country = 'US'\nAND o.created_at &gt; '2025-01-01';\n</code></pre>"},{"location":"Databases/mysql-performance/#key-columns-in-explain-output","title":"Key Columns in EXPLAIN Output","text":"Column What it tells you <code>id</code> The query step number. Same <code>id</code> means tables are joined; different <code>id</code> values indicate subqueries. <code>select_type</code> Whether this is a <code>SIMPLE</code> query, <code>PRIMARY</code>, <code>SUBQUERY</code>, <code>DERIVED</code> table, or <code>UNION</code>. <code>table</code> Which table this row of the plan applies to. <code>type</code> The access type - how MySQL finds rows in this table. The most important column. <code>possible_keys</code> Indexes the optimizer considered. <code>key</code> The index the optimizer actually chose. <code>NULL</code> means no index is used. <code>key_len</code> How many bytes of the index are used. Helps you tell whether the full composite index is used or just a prefix. <code>ref</code> What is compared against the index - a constant, a column from another table, or <code>NULL</code>. <code>rows</code> Estimated number of rows MySQL must examine. This is an estimate, not an exact count. <code>filtered</code> Percentage of rows that survive after applying the <code>WHERE</code> conditions. Lower means more rows are discarded. <code>Extra</code> Additional execution details. Watch for <code>Using filesort</code>, <code>Using temporary</code>, and <code>Using where</code>."},{"location":"Databases/mysql-performance/#access-types-best-to-worst","title":"Access Types (Best to Worst)","text":"<p>The <code>type</code> column is the single most important indicator of query efficiency. From best to worst:</p> Access Type Meaning Typical Scenario <code>system</code> Table has exactly one row System tables <code>const</code> At most one matching row, read at optimization time <code>WHERE id = 1</code> on a primary key <code>eq_ref</code> One row per combination from the previous table JOIN on a primary key or unique index <code>ref</code> All rows matching a single value in the index JOIN or WHERE on a non-unique index <code>range</code> Index scan over a range of values <code>WHERE created_at &gt; '2025-01-01'</code> <code>index</code> Full scan of the index tree (every entry) Covering index query, but scans all rows <code>ALL</code> Full table scan - reads every row No usable index found <p>Anything from <code>const</code> through <code>range</code> is generally acceptable for production queries. Seeing <code>index</code> or <code>ALL</code> on a large table is a red flag.</p>"},{"location":"Databases/mysql-performance/#the-extra-column","title":"The Extra Column","text":"<p>Some values in the <code>Extra</code> column deserve immediate attention:</p> <ul> <li><code>Using index</code> - the query is satisfied entirely from the index without touching the table data (a covering index). This is ideal.</li> <li><code>Using where</code> - MySQL applies a <code>WHERE</code> filter after reading rows. Normal, but check whether an index could push the filter earlier.</li> <li><code>Using temporary</code> - MySQL created an internal temporary table, often for <code>GROUP BY</code> or <code>DISTINCT</code> on columns that aren't indexed.</li> <li><code>Using filesort</code> - MySQL performs an extra sort pass because the result order doesn't match any index order. Expensive on large result sets.</li> <li><code>Using index condition</code> - Index Condition Pushdown (ICP) is active. MySQL evaluates part of the <code>WHERE</code> clause at the storage engine level, reducing the number of full-row reads.</li> </ul> <p>Reading EXPLAIN Output (requires JavaScript)</p>"},{"location":"Databases/mysql-performance/#explain-analyze","title":"EXPLAIN ANALYZE","text":"<p>MySQL 8.0.18 introduced <code>EXPLAIN ANALYZE</code>, which actually runs the query and reports real execution statistics alongside the plan. The output includes actual row counts, loop counts, and execution time per step.</p> <pre><code>EXPLAIN ANALYZE\nSELECT c.name, COUNT(*) AS order_count\nFROM customers c\nJOIN orders o ON o.customer_id = c.id\nWHERE c.country = 'US'\nGROUP BY c.id;\n</code></pre> <p>The output uses a tree format showing nested operations with actual timing:</p> <pre><code>-&gt; Group aggregate: count(0)  (cost=1856 rows=4520) (actual time=0.127..45.3 rows=4210 loops=1)\n    -&gt; Nested loop join  (cost=1412 rows=36160) (actual time=0.089..38.7 rows=33680 loops=1)\n        -&gt; Index lookup on c using idx_country (country='US')  (cost=502 rows=4520) (actual time=0.067..4.2 rows=4520 loops=1)\n        -&gt; Index lookup on o using idx_customer_id (customer_id=c.id)  (cost=0.25 rows=8) (actual time=0.005..0.007 rows=7.45 loops=4520)\n</code></pre> <p>The key difference: <code>rows=4520</code> is the estimate, while <code>actual ... rows=4520</code> is what actually happened. When these numbers diverge significantly, the optimizer may be making poor decisions based on stale statistics. Run <code>ANALYZE TABLE</code> to refresh them.</p> <p>Use EXPLAIN ANALYZE on development, not production</p> <p><code>EXPLAIN ANALYZE</code> executes the full query. On a slow query against a large table, that means waiting for the full execution. Use regular <code>EXPLAIN</code> for initial investigation, then <code>EXPLAIN ANALYZE</code> on a staging environment to validate.</p> <p>In EXPLAIN output, what does access type 'ALL' indicate? (requires JavaScript)</p>"},{"location":"Databases/mysql-performance/#index-strategies","title":"Index Strategies","text":"<p>Indexes are the single biggest lever you have for query performance. A well-chosen index can turn a query from scanning millions of rows to reading a handful. A missing or poorly designed index is behind most slow queries in production.</p>"},{"location":"Databases/mysql-performance/#single-column-indexes","title":"Single-Column Indexes","text":"<p>The simplest index covers one column:</p> <pre><code>CREATE INDEX idx_email ON users (email);\n</code></pre> <p>This speeds up queries that filter or sort by <code>email</code>. MySQL can use it for exact matches (<code>WHERE email = 'user@example.com'</code>), prefix matches (<code>WHERE email LIKE 'user@%'</code>), and range scans (<code>WHERE email BETWEEN 'a' AND 'm'</code>).</p>"},{"location":"Databases/mysql-performance/#composite-multi-column-indexes","title":"Composite (Multi-Column) Indexes","text":"<p>A composite index covers multiple columns in a defined order:</p> <pre><code>CREATE INDEX idx_country_city ON customers (country, city);\n</code></pre> <p>MySQL can use this index for:</p> <ul> <li><code>WHERE country = 'US'</code> - uses the first column</li> <li><code>WHERE country = 'US' AND city = 'Austin'</code> - uses both columns</li> <li><code>WHERE country = 'US' AND city &gt; 'M'</code> - uses first column for equality, second for range</li> </ul> <p>MySQL cannot use this index for:</p> <ul> <li><code>WHERE city = 'Austin'</code> - skips the first column (violates the leftmost prefix rule)</li> </ul> <p>This is the leftmost prefix rule: a composite index on <code>(a, b, c)</code> can satisfy queries that filter on <code>(a)</code>, <code>(a, b)</code>, or <code>(a, b, c)</code>, but not <code>(b)</code>, <code>(c)</code>, or <code>(b, c)</code>. The columns must be used in order from left to right, though MySQL does not require all columns to be present.</p> <p>Column order matters</p> <p>When designing composite indexes, put the most selective equality column first, followed by additional equality columns, and put any range condition column last. Once MySQL hits a range condition in the index, it stops using subsequent index columns for filtering.</p>"},{"location":"Databases/mysql-performance/#covering-indexes","title":"Covering Indexes","text":"<p>A covering index contains all the columns a query needs - both for filtering and for the result set. When MySQL can satisfy a query entirely from the index, it never reads the actual table rows. You see <code>Using index</code> in the <code>Extra</code> column of <code>EXPLAIN</code>.</p> <pre><code>-- Query\nSELECT customer_id, total FROM orders WHERE customer_id = 42;\n\n-- Covering index for this query\nCREATE INDEX idx_covering ON orders (customer_id, total);\n</code></pre> <p>The index contains both <code>customer_id</code> (for filtering) and <code>total</code> (for the result), so MySQL reads only the index. This avoids the random I/O cost of looking up each row in the table data.</p>"},{"location":"Databases/mysql-performance/#prefix-indexes","title":"Prefix Indexes","text":"<p>For long string columns, you can index only the first N characters:</p> <pre><code>CREATE INDEX idx_email_prefix ON users (email(10));\n</code></pre> <p>Prefix indexes use less storage and memory but cannot be used for <code>ORDER BY</code> or as covering indexes. Choose a prefix length that provides good selectivity:</p> <pre><code>-- Check selectivity at different prefix lengths\nSELECT\n  COUNT(DISTINCT LEFT(email, 5)) / COUNT(*) AS sel_5,\n  COUNT(DISTINCT LEFT(email, 10)) / COUNT(*) AS sel_10,\n  COUNT(DISTINCT LEFT(email, 15)) / COUNT(*) AS sel_15,\n  COUNT(DISTINCT email) / COUNT(*) AS sel_full\nFROM users;\n</code></pre> <p>Pick the shortest prefix where selectivity is close to the full-column selectivity.</p>"},{"location":"Databases/mysql-performance/#index-condition-pushdown-icp","title":"Index Condition Pushdown (ICP)","text":"<p>Index Condition Pushdown is an optimization where MySQL evaluates <code>WHERE</code> conditions at the storage engine level during an index scan, before reading the full row. Without ICP, the server reads the full row for every index match and then applies the filter. With ICP, rows that fail the condition are skipped at the index level.</p> <p>ICP is enabled by default in MySQL 5.6+ and appears as <code>Using index condition</code> in <code>EXPLAIN</code> output. You rarely need to think about it, but it explains why some queries are faster than their <code>EXPLAIN</code> row estimates suggest.</p> <p>Given a composite index on (country, city, zip_code), which WHERE clause can fully use all three index columns? (requires JavaScript)</p>"},{"location":"Databases/mysql-performance/#buffer-pool-tuning","title":"Buffer Pool Tuning","text":"<p>The InnoDB buffer pool is MySQL's main memory cache. It holds table data pages, index pages, the change buffer, and the adaptive hash index. Every read and write to InnoDB data goes through the buffer pool. If the data you need is already in memory, MySQL avoids a disk read. If it isn't, MySQL reads from disk and caches the page for future use.</p>"},{"location":"Databases/mysql-performance/#sizing-the-buffer-pool","title":"Sizing the Buffer Pool","text":"<p>The <code>innodb_buffer_pool_size</code> setting controls the buffer pool's total size. The general guidance:</p> <ul> <li>Dedicated database server: 70-80% of total RAM. A server with 64 GB of RAM should have a buffer pool of 45-50 GB.</li> <li>Shared server (running web server, application, and database): 40-50% of RAM, leaving room for the OS, application processes, and filesystem cache.</li> </ul> <pre><code>-- Check current buffer pool size\nSHOW VARIABLES LIKE 'innodb_buffer_pool_size';\n\n-- Set dynamically (MySQL 5.7+)\nSET GLOBAL innodb_buffer_pool_size = 48 * 1024 * 1024 * 1024;  -- 48 GB\n</code></pre> <p>For permanent changes, set it in <code>my.cnf</code>:</p> <pre><code>[mysqld]\ninnodb_buffer_pool_size = 48G\n</code></pre>"},{"location":"Databases/mysql-performance/#buffer-pool-instances","title":"Buffer Pool Instances","text":"<p>The <code>innodb_buffer_pool_instances</code> setting splits the buffer pool into multiple independent regions, reducing contention on the buffer pool mutex when many threads read and write concurrently. MySQL 8.0 defaults to 8 instances when the buffer pool is 1 GB or larger.</p> <pre><code>[mysqld]\ninnodb_buffer_pool_size = 48G\ninnodb_buffer_pool_instances = 16\n</code></pre> <p>Each instance should be at least 1 GB. A pool of 48 GB with 16 instances gives 3 GB per instance.</p>"},{"location":"Databases/mysql-performance/#monitoring-the-buffer-pool-hit-ratio","title":"Monitoring the Buffer Pool Hit Ratio","text":"<p>The hit ratio tells you what percentage of read requests are served from memory rather than disk. A healthy buffer pool has a hit ratio above 99%.</p> <pre><code>SHOW GLOBAL STATUS LIKE 'Innodb_buffer_pool_read%';\n</code></pre> <p>Calculate the ratio:</p> <pre><code>SELECT\n  (1 - (\n    (SELECT VARIABLE_VALUE FROM performance_schema.global_status WHERE VARIABLE_NAME = 'Innodb_buffer_pool_reads')\n    /\n    (SELECT VARIABLE_VALUE FROM performance_schema.global_status WHERE VARIABLE_NAME = 'Innodb_buffer_pool_read_requests')\n  )) * 100 AS buffer_pool_hit_ratio;\n</code></pre> <ul> <li><code>Innodb_buffer_pool_read_requests</code> - total logical reads (satisfied from buffer pool or disk)</li> <li><code>Innodb_buffer_pool_reads</code> - reads that required a disk I/O</li> </ul> <p>If the hit ratio is below 99%, your buffer pool is likely too small for the working set, or your queries are scanning large amounts of data that don't fit in memory.</p> <p>Warm up the buffer pool after restart</p> <p>MySQL 5.6+ can dump and reload the buffer pool contents across restarts. Enable <code>innodb_buffer_pool_dump_at_shutdown</code> and <code>innodb_buffer_pool_load_at_startup</code> in <code>my.cnf</code> to avoid a cold cache after a restart.</p> <p>For a dedicated MySQL server with 64 GB of RAM, what is a reasonable innodb_buffer_pool_size? (requires JavaScript)</p>"},{"location":"Databases/mysql-performance/#query-profiling","title":"Query Profiling","text":"<p>When <code>EXPLAIN</code> tells you what the optimizer plans to do, profiling tells you what actually happened and where time was spent.</p>"},{"location":"Databases/mysql-performance/#set-profiling-legacy","title":"SET profiling (Legacy)","text":"<p>The <code>SET profiling</code> approach is deprecated but still available in MySQL 8.0 and commonly referenced in older documentation:</p> <pre><code>SET profiling = 1;\n\nSELECT * FROM orders WHERE customer_id = 42 AND status = 'shipped';\n\nSHOW PROFILES;\n</code></pre> <p><code>SHOW PROFILES</code> lists recent queries with their total execution time. For a breakdown by stage:</p> <pre><code>SHOW PROFILE FOR QUERY 1;\n</code></pre> <p>This outputs each stage (opening tables, executing, sending data, cleaning up) with its duration. Useful for a quick check, but the Performance Schema provides more detail.</p>"},{"location":"Databases/mysql-performance/#performance-schema","title":"Performance Schema","text":"<p>The Performance Schema is MySQL's instrumentation framework. It records detailed timing and statistics for every stage of query execution with minimal overhead.</p> <p>Enable statement instrumentation (usually on by default):</p> <pre><code>-- Check if events_statements instrumentation is enabled\nSELECT * FROM performance_schema.setup_instruments\nWHERE NAME LIKE 'statement/%' AND ENABLED = 'YES';\n</code></pre> <p>Query recent statement execution statistics:</p> <pre><code>SELECT\n  DIGEST_TEXT,\n  COUNT_STAR AS exec_count,\n  ROUND(AVG_TIMER_WAIT / 1e12, 3) AS avg_seconds,\n  ROUND(SUM_TIMER_WAIT / 1e12, 3) AS total_seconds,\n  SUM_ROWS_EXAMINED,\n  SUM_ROWS_SENT\nFROM performance_schema.events_statements_summary_by_digest\nORDER BY SUM_TIMER_WAIT DESC\nLIMIT 10;\n</code></pre> <p>This query surfaces the 10 most time-consuming query patterns in your workload. The <code>DIGEST_TEXT</code> column normalizes queries (replacing literal values with <code>?</code>), so all executions of the same query pattern are grouped together.</p> <p>For per-stage profiling of a specific query:</p> <pre><code>-- Run the query\nSELECT * FROM orders WHERE customer_id = 42;\n\n-- Find its event ID\nSELECT EVENT_ID, SQL_TEXT, TIMER_WAIT / 1e12 AS seconds\nFROM performance_schema.events_statements_history\nWHERE SQL_TEXT LIKE '%customer_id = 42%'\nORDER BY EVENT_ID DESC LIMIT 1;\n\n-- Get stage-level breakdown\nSELECT EVENT_NAME, TIMER_WAIT / 1e12 AS seconds\nFROM performance_schema.events_stages_history_long\nWHERE NESTING_EVENT_ID = &lt;event_id&gt;\nORDER BY TIMER_START;\n</code></pre>"},{"location":"Databases/mysql-performance/#slow-query-log","title":"Slow Query Log","text":"<p>The slow query log captures every query that exceeds a configurable time threshold. It's the primary way to find problematic queries in production without actively monitoring the server.</p>"},{"location":"Databases/mysql-performance/#enabling-the-slow-query-log","title":"Enabling the Slow Query Log","text":"<pre><code>-- Enable dynamically\nSET GLOBAL slow_query_log = 'ON';\nSET GLOBAL slow_query_log_file = '/var/log/mysql/slow.log';\nSET GLOBAL long_query_time = 1;    -- Log queries taking more than 1 second\nSET GLOBAL log_queries_not_using_indexes = 1;  -- Also log queries without indexes\n</code></pre> <p>For persistent configuration in <code>my.cnf</code>:</p> <pre><code>[mysqld]\nslow_query_log = 1\nslow_query_log_file = /var/log/mysql/slow.log\nlong_query_time = 1\nlog_queries_not_using_indexes = 1\n</code></pre> <p>The <code>long_query_time</code> accepts decimal values. Setting it to <code>0.5</code> captures queries over 500 milliseconds. On a busy system, start with <code>1</code> or <code>2</code> seconds to avoid filling the log, then lower the threshold as you fix the worst offenders.</p> <p>Disk space</p> <p>The slow query log can grow quickly on a busy server. Monitor its size and rotate it regularly with <code>logrotate</code> or by running <code>FLUSH SLOW LOGS</code> after rotating the file.</p>"},{"location":"Databases/mysql-performance/#analyzing-with-pt-query-digest","title":"Analyzing with pt-query-digest","text":"<p>Reading raw slow query logs is impractical on a busy server - you might have thousands of entries. <code>pt-query-digest</code> from Percona Toolkit aggregates the log into a ranked summary.</p> <pre><code>pt-query-digest /var/log/mysql/slow.log\n</code></pre> <p>The output ranks query patterns by total execution time and shows:</p> <ul> <li>Response time - total and average per execution</li> <li>Calls - how many times this query pattern ran</li> <li>Rows examined vs. rows sent - the ratio between rows MySQL read and rows returned to the client. A high ratio indicates the query is reading far more data than needed.</li> <li>Query fingerprint - the normalized query with literal values replaced</li> </ul> <pre><code># Filter to a specific time window\npt-query-digest --since '2025-06-01' --until '2025-06-02' /var/log/mysql/slow.log\n\n# Filter to queries affecting a specific table\npt-query-digest --filter '$event-&gt;{arg} =~ m/orders/' /var/log/mysql/slow.log\n\n# Output as JSON for automated processing\npt-query-digest --output json /var/log/mysql/slow.log\n</code></pre> <p>Diagnosing a Slow Query with EXPLAIN (requires JavaScript)</p> <p>Analyzing the Slow Query Log with pt-query-digest (requires JavaScript)</p>"},{"location":"Databases/mysql-performance/#optimizer-hints","title":"Optimizer Hints","text":"<p>Sometimes the optimizer makes a suboptimal choice - it picks the wrong index, or avoids an index you know is faster. MySQL provides two levels of influence over the optimizer.</p>"},{"location":"Databases/mysql-performance/#table-level-index-hints","title":"Table-Level Index Hints","text":"<p>These legacy hints are placed after the table name in the <code>FROM</code> clause:</p> <pre><code>-- Suggest an index (optimizer can still ignore it)\nSELECT * FROM orders USE INDEX (idx_status) WHERE status = 'pending';\n\n-- Force an index (optimizer must use it or do a full scan)\nSELECT * FROM orders FORCE INDEX (idx_created_at)\nWHERE created_at &gt; '2025-01-01' ORDER BY created_at;\n\n-- Exclude an index from consideration\nSELECT * FROM orders IGNORE INDEX (idx_status)\nWHERE status = 'pending' AND created_at &gt; '2025-01-01';\n</code></pre> <p><code>USE INDEX</code> gives the optimizer a suggestion. <code>FORCE INDEX</code> removes all other index options, leaving only the specified index and a full table scan. <code>IGNORE INDEX</code> removes a specific index from consideration.</p>"},{"location":"Databases/mysql-performance/#mysql-80-optimizer-hints","title":"MySQL 8.0 Optimizer Hints","text":"<p>MySQL 8.0 introduced a more granular hint syntax using <code>/*+ ... */</code> comments placed immediately after the <code>SELECT</code> keyword:</p> <pre><code>-- Force a join order\nSELECT /*+ JOIN_ORDER(c, o) */ c.name, o.total\nFROM customers c JOIN orders o ON o.customer_id = c.id\nWHERE c.country = 'US';\n\n-- Disable index merge optimization\nSELECT /*+ NO_INDEX_MERGE(orders) */ *\nFROM orders WHERE status = 'pending' OR customer_id = 42;\n\n-- Set a per-query resource limit\nSELECT /*+ MAX_EXECUTION_TIME(5000) */ *\nFROM orders WHERE status = 'pending';\n\n-- Hint the optimizer to use a specific index\nSELECT /*+ INDEX(o idx_status_created) */ o.order_id\nFROM orders o WHERE o.status = 'pending';\n</code></pre> <p>Common optimizer hints:</p> Hint Effect <code>JOIN_ORDER(t1, t2)</code> Force a specific join order <code>NO_JOIN_ORDER()</code> Let the optimizer choose join order freely <code>INDEX(tbl idx)</code> Use a specific index <code>NO_INDEX(tbl idx)</code> Avoid a specific index <code>MERGE(tbl)</code> / <code>NO_MERGE(tbl)</code> Force or prevent merging a derived table into the outer query <code>MAX_EXECUTION_TIME(ms)</code> Kill the query if it exceeds the time limit <code>SET_VAR(var=val)</code> Set a session variable for the duration of this query <p>Hints are a last resort</p> <p>In most cases, the optimizer makes correct decisions when it has accurate statistics and well-designed indexes. If you find yourself relying on hints, first check whether <code>ANALYZE TABLE</code> would fix stale statistics, whether a better index exists, or whether you can rewrite the query. Hints create maintenance burden because they bypass the optimizer's ability to adapt to data changes.</p> <p>What is the difference between USE INDEX and FORCE INDEX? (requires JavaScript)</p>"},{"location":"Databases/mysql-performance/#common-anti-patterns","title":"Common Anti-Patterns","text":"<p>These mistakes account for the majority of MySQL performance problems in application code.</p>"},{"location":"Databases/mysql-performance/#select","title":"SELECT *","text":"<pre><code>-- Bad: reads every column, even those you don't need\nSELECT * FROM orders WHERE customer_id = 42;\n\n-- Good: read only what you need\nSELECT order_id, total, status FROM orders WHERE customer_id = 42;\n</code></pre> <p><code>SELECT *</code> prevents the use of covering indexes, transfers unnecessary data over the network, and breaks if columns are added or removed. Always specify the columns you need.</p>"},{"location":"Databases/mysql-performance/#missing-indexes-on-join-columns","title":"Missing Indexes on JOIN Columns","text":"<p>Every column used in a <code>JOIN ... ON</code> condition should have an index. Without one, MySQL scans the entire table for each row from the driving table.</p> <pre><code>-- If orders.customer_id has no index, this is a full scan of orders for each customer\nSELECT c.name, o.total\nFROM customers c\nJOIN orders o ON o.customer_id = c.id;\n</code></pre> <p>Check for missing indexes with:</p> <pre><code>EXPLAIN SELECT ... ;  -- Look for type=ALL on joined tables\n</code></pre>"},{"location":"Databases/mysql-performance/#n1-queries","title":"N+1 Queries","text":"<p>The N+1 problem happens when application code fetches a list of records, then makes a separate query for each row's related data:</p> <pre><code># N+1 anti-pattern\ncustomers = db.query(\"SELECT * FROM customers WHERE country = 'US'\")\nfor customer in customers:\n    orders = db.query(f\"SELECT * FROM orders WHERE customer_id = {customer.id}\")\n</code></pre> <p>This fires one query for the customer list, then N queries for orders - one per customer. If there are 10,000 US customers, that's 10,001 queries.</p> <p>Fix it with a single JOIN or a batch <code>IN</code> query:</p> <pre><code>-- Single query with JOIN\nSELECT c.id, c.name, o.order_id, o.total\nFROM customers c\nJOIN orders o ON o.customer_id = c.id\nWHERE c.country = 'US';\n\n-- Or batch the IDs\nSELECT * FROM orders WHERE customer_id IN (1, 2, 3, ...);\n</code></pre>"},{"location":"Databases/mysql-performance/#functions-on-indexed-columns","title":"Functions on Indexed Columns","text":"<p>Applying a function to an indexed column prevents MySQL from using the index:</p> <pre><code>-- Bad: function on the indexed column prevents index usage\nSELECT * FROM orders WHERE YEAR(created_at) = 2025;\n\n-- Good: rewrite as a range that preserves the index\nSELECT * FROM orders\nWHERE created_at &gt;= '2025-01-01' AND created_at &lt; '2026-01-01';\n</code></pre> <pre><code>-- Bad: LOWER() prevents index usage\nSELECT * FROM users WHERE LOWER(email) = 'user@example.com';\n\n-- Good: use a case-insensitive collation, or a generated column with an index\nALTER TABLE users ADD email_lower VARCHAR(255) GENERATED ALWAYS AS (LOWER(email)) STORED;\nCREATE INDEX idx_email_lower ON users (email_lower);\n</code></pre> <p>The optimizer cannot \"see through\" function calls to know that the result corresponds to index entries. Any transformation - <code>YEAR()</code>, <code>DATE()</code>, <code>LOWER()</code>, <code>CAST()</code>, arithmetic - makes the index invisible to that query.</p> <p>MySQL 8.0 functional indexes</p> <p>MySQL 8.0.13+ supports functional indexes that index expression results directly:</p> <pre><code>CREATE INDEX idx_year ON orders ((YEAR(created_at)));\n</code></pre> <p>This creates a hidden generated column and indexes it. Now <code>WHERE YEAR(created_at) = 2025</code> can use the index.</p>"},{"location":"Databases/mysql-performance/#implicit-type-conversions","title":"Implicit Type Conversions","text":"<p>When the <code>WHERE</code> clause compares a column to a value of a different type, MySQL silently converts one side. If it converts the column, the index becomes unusable:</p> <pre><code>-- orders.order_ref is VARCHAR(20), indexed\n-- Bad: comparing a string column to an integer triggers implicit conversion\nSELECT * FROM orders WHERE order_ref = 12345;\n\n-- Good: match the type\nSELECT * FROM orders WHERE order_ref = '12345';\n</code></pre> <p>MySQL converts the <code>VARCHAR</code> column to a number for every row, turning an index lookup into a full table scan. Always match the literal type to the column type.</p>"},{"location":"Databases/mysql-performance/#optimization-workflow","title":"Optimization Workflow","text":"<p>When a query is slow, follow this sequence:</p> <ol> <li>Get the query - from the slow query log, <code>pt-query-digest</code>, or application logs</li> <li>Run EXPLAIN - identify full table scans, missing indexes, filesorts, and temporary tables</li> <li>Check the indexes - does the table have appropriate indexes for the <code>WHERE</code>, <code>JOIN</code>, <code>ORDER BY</code>, and <code>GROUP BY</code> clauses?</li> <li>Check for anti-patterns - functions on indexed columns, implicit conversions, <code>SELECT *</code>, N+1 queries</li> <li>Add or adjust indexes - create composite indexes that cover the query's filtering and sorting needs</li> <li>Verify with EXPLAIN again - confirm the new index is used and row estimates dropped</li> <li>Measure - run the query and compare before/after execution times</li> </ol> <p>Optimize a Slow Query (requires JavaScript)</p>"},{"location":"Databases/mysql-performance/#further-reading","title":"Further Reading","text":"<ul> <li>MySQL EXPLAIN Output Format - official reference for every column and value in EXPLAIN output</li> <li>MySQL Optimization Guide - the full MySQL optimization reference covering indexes, query tuning, and server configuration</li> <li>Percona Toolkit Documentation - documentation for <code>pt-query-digest</code> and the full Percona Toolkit suite</li> <li>Use The Index, Luke - a site dedicated to SQL indexing and tuning, with database-agnostic explanations of how indexes work</li> <li>MySQL Performance Schema - official documentation for the Performance Schema instrumentation framework</li> <li>High Performance MySQL, 4th Edition - comprehensive book covering MySQL performance from schema design to server tuning</li> </ul> <p>Previous: MySQL Administration | Next: MySQL Replication &amp; High Availability | Back to Index</p>"},{"location":"Databases/mysql-replication/","title":"MySQL Replication &amp; High Availability","text":"<p>A single MySQL server is a single point of failure. Every production database needs a plan for scaling reads, surviving hardware failures, and minimizing downtime during maintenance. MySQL provides several replication mechanisms - from basic binary log replication to fully automated clustering - and the right choice depends on your availability requirements, data consistency needs, and operational complexity budget.</p> <p>This guide covers the replication landscape from the ground up: how binary log replication works, modern GTID-based topologies, semi-synchronous durability guarantees, Group Replication for automatic failover, and the tooling that ties it all together.</p>"},{"location":"Databases/mysql-replication/#binary-log-replication","title":"Binary Log Replication","text":"<p>Binary log replication is the foundation of all MySQL replication. Every other replication method builds on this mechanism. The concept is straightforward: the source server writes every data-modifying event to a binary log, and one or more replica servers read those events and replay them locally.</p>"},{"location":"Databases/mysql-replication/#how-it-works","title":"How It Works","text":"<p>The replication process involves three threads:</p> <ol> <li>Binlog dump thread (on the source) - reads the binary log and sends events to the replica</li> <li>I/O receiver thread (on the replica) - connects to the source, receives events, and writes them to the relay log</li> <li>SQL applier thread (on the replica) - reads events from the relay log and executes them against the local database</li> </ol> <pre><code>flowchart LR\n    subgraph Source[\"Source Server\"]\n        direction TB\n        C[\"Client Writes\"]\n        BL[\"Binary Log\"]\n        BD[\"Binlog Dump Thread\"]\n    end\n\n    subgraph Replica[\"Replica Server\"]\n        direction TB\n        IO[\"I/O Receiver Thread\"]\n        RL[\"Relay Log\"]\n        SQL[\"SQL Applier Thread\"]\n        D[\"Local Data\"]\n    end\n\n    C --&gt; BL\n    BL --&gt; BD\n    BD --&gt;|\"Network\"| IO\n    IO --&gt; RL\n    RL --&gt; SQL\n    SQL --&gt; D</code></pre> <p>The relay log acts as a buffer. If the SQL applier thread falls behind, the I/O thread can continue receiving events without blocking the source. This separation means a slow replica does not slow down the source server.</p>"},{"location":"Databases/mysql-replication/#binary-log-formats","title":"Binary Log Formats","text":"<p>MySQL supports three binlog formats, controlled by the <code>binlog_format</code> variable:</p> Format How It Logs Pros Cons <code>STATEMENT</code> The SQL statement itself Small log size, human-readable Non-deterministic functions (<code>NOW()</code>, <code>UUID()</code>) can produce different results on replicas <code>ROW</code> The before/after image of each modified row Deterministic, safe for all queries Larger log size, especially for bulk updates <code>MIXED</code> Statement-based by default, switches to row-based when needed Balances size and safety Harder to predict which format is used <p>Use ROW format</p> <p><code>ROW</code> is the default since MySQL 5.7.7 and the recommended format for all new deployments. The deterministic behavior eliminates an entire class of replication bugs. The increased log size is a worthwhile trade-off for reliability.</p>"},{"location":"Databases/mysql-replication/#configuring-the-source","title":"Configuring the Source","text":"<p>On the source server, enable the binary log and assign a unique server ID in <code>my.cnf</code>:</p> <pre><code>[mysqld]\nserver-id         = 1\nlog-bin           = mysql-bin\nbinlog-format     = ROW\nsync-binlog       = 1\n</code></pre> <p>Setting <code>sync_binlog = 1</code> forces the binary log to disk on every commit. This prevents data loss if the source crashes, at the cost of additional disk I/O.</p> <p>Create a replication user with the required privilege:</p> <pre><code>CREATE USER 'repl'@'10.0.0.%' IDENTIFIED BY 'strong_password_here';\nGRANT REPLICATION SLAVE ON *.* TO 'repl'@'10.0.0.%';\n</code></pre>"},{"location":"Databases/mysql-replication/#configuring-the-replica","title":"Configuring the Replica","text":"<p>On the replica, set a unique server ID and configure the connection to the source:</p> <pre><code>[mysqld]\nserver-id         = 2\nrelay-log         = relay-bin\nread-only         = ON\n</code></pre> <p>Then point the replica at the source using the binary log file and position:</p> <pre><code>CHANGE REPLICATION SOURCE TO\n    SOURCE_HOST = '10.0.0.1',\n    SOURCE_USER = 'repl',\n    SOURCE_PASSWORD = 'strong_password_here',\n    SOURCE_LOG_FILE = 'mysql-bin.000001',\n    SOURCE_LOG_POS = 154;\n\nSTART REPLICA;\n</code></pre> <p>The <code>SOURCE_LOG_FILE</code> and <code>SOURCE_LOG_POS</code> values come from <code>SHOW MASTER STATUS</code> on the source (or from a backup's metadata). Getting these wrong means the replica starts reading from the wrong position - it will either skip transactions or try to replay events that have already been applied.</p> <p>File-and-position pitfalls</p> <p>Binary log file names and positions are fragile. They change after log rotation, are specific to a single source, and make it difficult to re-point a replica to a new source after failover. GTID replication (covered next) eliminates these problems.</p> <p>In MySQL binary log replication, what is the role of the relay log on the replica? (requires JavaScript)</p>"},{"location":"Databases/mysql-replication/#gtid-replication","title":"GTID Replication","text":"<p>Global Transaction Identifiers (GTIDs) replace file-and-position tracking with a globally unique identifier for every transaction. Each GTID has the format <code>server_uuid:transaction_id</code> - for example, <code>3E11FA47-71CA-11E1-9E33-C80AA9429562:42</code>.</p> <p>The <code>server_uuid</code> is automatically generated when MySQL initializes and stored in <code>auto.cnf</code>. The <code>transaction_id</code> is a monotonically increasing integer per server. Together, they guarantee that every transaction across the entire replication topology has a unique identity.</p>"},{"location":"Databases/mysql-replication/#why-gtids-matter","title":"Why GTIDs Matter","text":"<p>With file-and-position replication, each replica tracks where it is reading from a specific source's binary log. If that source fails and you need to point the replica at a different server, you have to manually calculate the equivalent position in the new server's binary log. This is error-prone and often requires tools like <code>mysqlbinlog</code> to correlate positions.</p> <p>With GTIDs, every server knows exactly which transactions it has executed (tracked in <code>gtid_executed</code>). When you point a replica at a new source, it simply says \"I've executed these GTIDs, send me everything I'm missing.\" The source computes the difference and starts streaming.</p>"},{"location":"Databases/mysql-replication/#enabling-gtid-replication","title":"Enabling GTID Replication","text":"<p>On both source and replica, add to <code>my.cnf</code>:</p> <pre><code>[mysqld]\ngtid-mode                = ON\nenforce-gtid-consistency = ON\nlog-bin                  = mysql-bin\nlog-replica-updates      = ON\n</code></pre> <p><code>enforce_gtid_consistency</code> blocks statements that cannot be safely represented as a single GTID (such as <code>CREATE TABLE ... SELECT</code> or transactions mixing InnoDB and non-transactional engines). This restriction is the trade-off for GTID's reliability.</p> <p><code>log_replica_updates</code> makes replicas write received events to their own binary log, which is required for chained replication (replica of a replica) and for any replica to serve as a failover candidate.</p>"},{"location":"Databases/mysql-replication/#configuring-a-gtid-replica","title":"Configuring a GTID Replica","text":"<p>With GTIDs, the <code>CHANGE REPLICATION SOURCE</code> command is simpler because you do not need file names or positions:</p> <pre><code>CHANGE REPLICATION SOURCE TO\n    SOURCE_HOST = '10.0.0.1',\n    SOURCE_USER = 'repl',\n    SOURCE_PASSWORD = 'strong_password_here',\n    SOURCE_AUTO_POSITION = 1;\n\nSTART REPLICA;\n</code></pre> <p><code>SOURCE_AUTO_POSITION = 1</code> tells the replica to use GTID-based auto-positioning. The replica sends its <code>gtid_executed</code> set to the source, and the source determines what to send.</p> <p>GTID Replication Configuration (requires JavaScript)</p>"},{"location":"Databases/mysql-replication/#inspecting-gtid-state","title":"Inspecting GTID State","text":"<p>Check which transactions have been executed:</p> <pre><code>SELECT @@gtid_executed;\n-- 3E11FA47-71CA-11E1-9E33-C80AA9429562:1-42\n</code></pre> <p>Check which transactions have been received but not yet applied:</p> <pre><code>SELECT RECEIVED_TRANSACTION_SET\nFROM performance_schema.replication_connection_status;\n</code></pre> <p>What is the primary advantage of GTID replication over traditional file-and-position replication? (requires JavaScript)</p> <p>A replica's gtid_executed variable shows 3E11FA47-71CA-11E1-9E33-C80AA9429562:1-42. What does the '1-42' portion represent? (requires JavaScript)</p>"},{"location":"Databases/mysql-replication/#semi-synchronous-replication","title":"Semi-Synchronous Replication","text":"<p>Standard MySQL replication is asynchronous - the source commits a transaction and returns success to the client without waiting for any replica to acknowledge receipt. If the source crashes immediately after committing, committed transactions may be lost because no replica received them.</p> <p>Semi-synchronous replication addresses this gap. With semi-sync enabled, the source waits after each commit until at least one replica acknowledges that it has received and written the event to its relay log. Only then does the source return success to the client.</p>"},{"location":"Databases/mysql-replication/#the-durability-guarantee","title":"The Durability Guarantee","text":"<p>Semi-sync does not guarantee that replicas have applied the transaction - only that they have received and persisted it in the relay log. This is enough to guarantee that the transaction survives a source crash, because at least one replica has the data and can be promoted.</p>"},{"location":"Databases/mysql-replication/#the-performance-trade-off","title":"The Performance Trade-off","text":"<p>Every transaction now includes a network round trip to at least one replica. On a local network, this adds sub-millisecond latency. Across data centers, it can add tens of milliseconds per commit. For write-heavy workloads, this overhead can be significant.</p> <p>If no replica acknowledges within <code>rpl_semi_sync_source_timeout</code> milliseconds (default: 10000), the source falls back to asynchronous replication. This prevents a replica failure from hanging the source indefinitely.</p>"},{"location":"Databases/mysql-replication/#configuration","title":"Configuration","text":"<p>Install and enable the semi-sync plugins on the source:</p> <pre><code>INSTALL PLUGIN rpl_semi_sync_source SONAME 'semisync_source.so';\nSET GLOBAL rpl_semi_sync_source_enabled = 1;\nSET GLOBAL rpl_semi_sync_source_timeout = 5000;  -- 5 seconds before fallback\n</code></pre> <p>On each replica:</p> <pre><code>INSTALL PLUGIN rpl_semi_sync_replica SONAME 'semisync_replica.so';\nSET GLOBAL rpl_semi_sync_replica_enabled = 1;\n</code></pre> <p>Make these persistent by adding to <code>my.cnf</code>:</p> <pre><code># Source\n[mysqld]\nrpl_semi_sync_source_enabled = 1\nrpl_semi_sync_source_timeout = 5000\n\n# Replica\n[mysqld]\nrpl_semi_sync_replica_enabled = 1\n</code></pre> <p>Monitor semi-sync status:</p> <pre><code>SHOW STATUS LIKE 'Rpl_semi_sync%';\n</code></pre> <p>Key variables to watch:</p> Variable Meaning <code>Rpl_semi_sync_source_status</code> <code>ON</code> if semi-sync is active, <code>OFF</code> if fallen back to async <code>Rpl_semi_sync_source_no_tx</code> Transactions committed without semi-sync acknowledgment <code>Rpl_semi_sync_source_yes_tx</code> Transactions committed with semi-sync acknowledgment <code>Rpl_semi_sync_source_avg_wait_time</code> Average time waiting for replica acknowledgment <p>MySQL 8.0.26+ naming</p> <p>MySQL 8.0.26 renamed the semi-sync plugin from <code>rpl_semi_sync_master</code>/<code>rpl_semi_sync_slave</code> to <code>rpl_semi_sync_source</code>/<code>rpl_semi_sync_replica</code>. The old names still work as aliases but are deprecated.</p>"},{"location":"Databases/mysql-replication/#group-replication","title":"Group Replication","text":"<p>MySQL Group Replication goes beyond traditional source-replica topologies. It provides a fault-tolerant replication mechanism where a group of servers coordinate through a consensus protocol (based on Paxos). If a server fails, the remaining members automatically reconfigure the group - no external failover tool needed.</p>"},{"location":"Databases/mysql-replication/#how-it-works_1","title":"How It Works","text":"<p>Every server in a Group Replication cluster is a full member of the group. When a member commits a transaction, the transaction is broadcast to all members through the group communication system. The group uses a consensus protocol to agree on the ordering of transactions. Only after a majority of members acknowledge the transaction is it considered committed.</p> <p>This majority-based consensus means a group of <code>N</code> members can tolerate up to <code>(N-1)/2</code> failures. A 3-member group tolerates 1 failure; a 5-member group tolerates 2.</p>"},{"location":"Databases/mysql-replication/#single-primary-vs-multi-primary","title":"Single-Primary vs Multi-Primary","text":"<p>Group Replication supports two modes:</p> <p>Single-primary mode (default and recommended): One member is elected as the primary and accepts writes. All other members are read-only secondaries. If the primary fails, the group automatically elects a new primary from the remaining members.</p> <p>Multi-primary mode: All members accept writes simultaneously. The group uses certification-based conflict detection to identify conflicting transactions. If two members modify the same row concurrently, one transaction is rolled back. This mode is useful for write-heavy workloads distributed across multiple locations, but conflict handling adds complexity.</p> Feature Single-Primary Multi-Primary Write endpoint One server Any server Conflict handling Not needed Certification-based rollback Complexity Lower Higher Best for Most workloads Geographically distributed writes"},{"location":"Databases/mysql-replication/#basic-configuration","title":"Basic Configuration","text":"<p>Each member needs Group Replication configured in <code>my.cnf</code>:</p> <pre><code>[mysqld]\nserver-id                          = 1\ngtid-mode                          = ON\nenforce-gtid-consistency           = ON\nlog-bin                            = mysql-bin\nlog-replica-updates                = ON\nbinlog-checksum                    = NONE\nrelay-log-info-repository          = TABLE\ntransaction-write-set-extraction   = XXHASH64\n\n# Group Replication settings\nplugin-load-add                    = group_replication.so\ngroup_replication_group_name       = \"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee\"\ngroup_replication_start_on_boot    = OFF\ngroup_replication_local_address    = \"10.0.0.1:33061\"\ngroup_replication_group_seeds      = \"10.0.0.1:33061,10.0.0.2:33061,10.0.0.3:33061\"\ngroup_replication_single_primary_mode = ON\n</code></pre> <p>The <code>group_replication_group_name</code> is a UUID that identifies the group. All members must use the same value. Generate one with <code>SELECT UUID()</code>.</p> <p>Bootstrap the group on the first member:</p> <pre><code>SET GLOBAL group_replication_bootstrap_group = ON;\nSTART GROUP_REPLICATION;\nSET GLOBAL group_replication_bootstrap_group = OFF;\n</code></pre> <p>Then join additional members:</p> <pre><code>START GROUP_REPLICATION;\n</code></pre> <p>Never bootstrap twice</p> <p>Only bootstrap the group once, on the first member. Bootstrapping a second time creates a split-brain situation - two separate groups with the same name, each believing it is authoritative.</p> <p>In a 5-member MySQL Group Replication cluster, how many member failures can the cluster tolerate while still accepting writes? (requires JavaScript)</p>"},{"location":"Databases/mysql-replication/#innodb-cluster","title":"InnoDB Cluster","text":"<p>InnoDB Cluster is MySQL's integrated high-availability solution. It combines three components into a managed stack:</p> <ol> <li>Group Replication - provides the underlying data replication and automatic failover</li> <li>MySQL Shell - provides the administration interface for creating and managing the cluster</li> <li>MySQL Router - provides transparent connection routing so applications do not need to know which server is the primary</li> </ol> <pre><code>flowchart TB\n    subgraph App[\"Application Layer\"]\n        A1[\"App Server 1\"]\n        A2[\"App Server 2\"]\n    end\n\n    subgraph Router[\"MySQL Router\"]\n        RW[\"Read-Write Port 6446\"]\n        RO[\"Read-Only Port 6447\"]\n    end\n\n    subgraph Cluster[\"InnoDB Cluster (Group Replication)\"]\n        P[\"Primary (R/W)\"]\n        S1[\"Secondary (R/O)\"]\n        S2[\"Secondary (R/O)\"]\n        P &lt;--&gt; S1\n        P &lt;--&gt; S2\n        S1 &lt;--&gt; S2\n    end\n\n    A1 --&gt; RW\n    A1 --&gt; RO\n    A2 --&gt; RW\n    A2 --&gt; RO\n    RW --&gt; P\n    RO --&gt; S1\n    RO --&gt; S2</code></pre>"},{"location":"Databases/mysql-replication/#setup-workflow","title":"Setup Workflow","text":"<p>The typical setup uses MySQL Shell's AdminAPI:</p> <p>Step 1: Prepare each server instance:</p> <pre><code>// In MySQL Shell (mysqlsh)\ndba.configureInstance('root@10.0.0.1:3306');\ndba.configureInstance('root@10.0.0.2:3306');\ndba.configureInstance('root@10.0.0.3:3306');\n</code></pre> <p>This checks and applies the configuration requirements (GTID mode, binary logging, etc.).</p> <p>Step 2: Create the cluster on the first instance:</p> <pre><code>shell.connect('root@10.0.0.1:3306');\nvar cluster = dba.createCluster('production');\n</code></pre> <p>Step 3: Add the remaining instances:</p> <pre><code>cluster.addInstance('root@10.0.0.2:3306');\ncluster.addInstance('root@10.0.0.3:3306');\n</code></pre> <p>Step 4: Check cluster status:</p> <pre><code>cluster.status();\n</code></pre> <p>Step 5: Bootstrap MySQL Router on each application server:</p> <pre><code>mysqlrouter --bootstrap root@10.0.0.1:3306 --directory /opt/mysqlrouter\n/opt/mysqlrouter/start.sh\n</code></pre> <p>MySQL Router automatically discovers the cluster topology and routes read-write connections to the primary and read-only connections to the secondaries. When the primary fails and Group Replication elects a new one, Router detects the change and reroutes traffic - no application changes needed.</p>"},{"location":"Databases/mysql-replication/#checking-cluster-health","title":"Checking Cluster Health","text":"<pre><code>// In MySQL Shell\nshell.connect('root@10.0.0.1:3306');\nvar cluster = dba.getCluster();\ncluster.status();\n</code></pre> <p>The output shows the status of each member, its role (PRIMARY or SECONDARY), and any replication issues.</p>"},{"location":"Databases/mysql-replication/#proxysql","title":"ProxySQL","text":"<p>ProxySQL is a high-performance proxy that sits between your application and MySQL. It provides connection pooling, read/write splitting, query routing, and query caching without modifying application code.</p>"},{"location":"Databases/mysql-replication/#connection-routing","title":"Connection Routing","text":"<p>ProxySQL maintains two hostgroups: one for writes (the primary) and one for reads (the replicas). Applications connect to ProxySQL on a single port, and ProxySQL routes queries to the appropriate hostgroup based on configurable rules.</p>"},{"location":"Databases/mysql-replication/#basic-setup","title":"Basic Setup","text":"<p>ProxySQL uses its own admin interface (default port 6032) for configuration. All configuration is stored in a SQLite database and applied at runtime.</p> <p>Add your MySQL servers:</p> <pre><code>-- Connect to ProxySQL admin interface\n-- mysql -u admin -padmin -h 127.0.0.1 -P 6032\n\n-- Hostgroup 0 = writes (primary), Hostgroup 1 = reads (replicas)\nINSERT INTO mysql_servers (hostgroup_id, hostname, port) VALUES\n    (0, '10.0.0.1', 3306),\n    (1, '10.0.0.2', 3306),\n    (1, '10.0.0.3', 3306);\n\nLOAD MYSQL SERVERS TO RUNTIME;\nSAVE MYSQL SERVERS TO DISK;\n</code></pre>"},{"location":"Databases/mysql-replication/#readwrite-splitting","title":"Read/Write Splitting","text":"<p>Configure query rules to route <code>SELECT</code> queries to the read hostgroup:</p> <pre><code>-- Route all SELECTs to hostgroup 1 (replicas)\nINSERT INTO mysql_query_rules (rule_id, active, match_pattern, destination_hostgroup)\n    VALUES (1, 1, '^SELECT.*', 1);\n\n-- Route SELECT ... FOR UPDATE to hostgroup 0 (primary) since it acquires locks\nINSERT INTO mysql_query_rules (rule_id, active, match_pattern, destination_hostgroup)\n    VALUES (2, 1, '^SELECT.*FOR UPDATE', 0);\n\nLOAD MYSQL QUERY RULES TO RUNTIME;\nSAVE MYSQL QUERY RULES TO DISK;\n</code></pre> <p>Rules are evaluated in <code>rule_id</code> order. More specific rules should have lower <code>rule_id</code> values if you want them evaluated first, or use the <code>flagIN</code>/<code>flagOUT</code> chaining mechanism for complex routing logic.</p>"},{"location":"Databases/mysql-replication/#user-configuration","title":"User Configuration","text":"<p>Add MySQL users that ProxySQL will use:</p> <pre><code>INSERT INTO mysql_users (username, password, default_hostgroup)\n    VALUES ('app_user', 'app_password', 0);\n\nLOAD MYSQL USERS TO RUNTIME;\nSAVE MYSQL USERS TO DISK;\n</code></pre> <p>The <code>default_hostgroup</code> is used when no query rule matches (typically set to the write hostgroup so unclassified queries go to the primary).</p> <p>ProxySQL vs MySQL Router</p> <p>MySQL Router is the simpler choice when using InnoDB Cluster - it integrates tightly and requires minimal configuration. ProxySQL is more powerful for custom routing, query caching, connection multiplexing, and mixed environments where you need fine-grained control over how traffic reaches your databases.</p>"},{"location":"Databases/mysql-replication/#failover-patterns","title":"Failover Patterns","text":"<p>When a source server fails, you need a plan to promote a replica. The complexity of this process depends on your replication topology and tooling.</p>"},{"location":"Databases/mysql-replication/#manual-failover","title":"Manual Failover","text":"<p>With basic binary log replication, failover is a manual process:</p> <ol> <li>Identify the most up-to-date replica (the one with the highest executed GTID set, or the most recent relay log position)</li> <li>Ensure the replica has applied all events in its relay log</li> <li>Promote the replica: <code>STOP REPLICA; RESET REPLICA ALL;</code></li> <li>Disable read-only mode: <code>SET GLOBAL read_only = OFF;</code></li> <li>Re-point remaining replicas to the new source</li> <li>Update application connection strings or DNS records</li> </ol> <p>With GTID replication, step 5 is straightforward - each replica uses <code>SOURCE_AUTO_POSITION = 1</code> and automatically catches up from the new source. Without GTIDs, you must calculate the correct binary log position on the new source for each remaining replica.</p>"},{"location":"Databases/mysql-replication/#automatic-failover-with-innodb-cluster","title":"Automatic Failover with InnoDB Cluster","text":"<p>InnoDB Cluster handles failover automatically. When the primary fails:</p> <ol> <li>Group Replication detects the failure (typically within seconds)</li> <li>The remaining members elect a new primary based on a configurable weight</li> <li>MySQL Router detects the topology change and reroutes write traffic</li> <li>Applications continue without intervention</li> </ol>"},{"location":"Databases/mysql-replication/#automatic-failover-with-orchestrator","title":"Automatic Failover with Orchestrator","text":"<p>Orchestrator is an external topology manager for MySQL replication. It continuously monitors the replication topology, detects failures, and can perform automated failover. Orchestrator is widely used in environments that run traditional (non-Group Replication) topologies and need automated recovery.</p> <p>Orchestrator works by:</p> <ul> <li>Periodically polling each MySQL instance for its replication status</li> <li>Maintaining a topology map in its own backend database</li> <li>Running configurable recovery hooks (promotion scripts, DNS updates, VIP failover) when a failure is detected</li> </ul>"},{"location":"Databases/mysql-replication/#split-brain-prevention","title":"Split-Brain Prevention","text":"<p>Split-brain occurs when two servers both believe they are the primary and accept writes simultaneously. This causes divergent data that is difficult or impossible to reconcile.</p> <p>Prevention strategies:</p> Strategy How It Works Quorum-based consensus Group Replication requires a majority of members to agree before accepting writes. A partitioned minority cannot form quorum and stops accepting writes. Fencing Before promoting a new primary, ensure the old primary is truly down (STONITH - \"Shoot The Other Node In The Head\"). This can mean powering off the old server via IPMI/iLO or revoking its network access. Lease-based leadership The primary holds a time-limited lease (in a consensus store like etcd). If it cannot renew the lease, it stops accepting writes. Only a server holding a valid lease can serve as primary."},{"location":"Databases/mysql-replication/#monitoring-replication-lag","title":"Monitoring Replication Lag","text":"<p>Replication lag - the delay between a transaction committing on the source and being applied on the replica - is the most important replication health metric. A lagging replica serves stale data, which can cause application bugs that are difficult to diagnose.</p>"},{"location":"Databases/mysql-replication/#seconds_behind_master","title":"Seconds_Behind_Master","text":"<p>The built-in metric from <code>SHOW REPLICA STATUS</code>:</p> <pre><code>SHOW REPLICA STATUS\\G\n</code></pre> <p>Look for the <code>Seconds_Behind_Source</code> field (historically <code>Seconds_Behind_Master</code>). This value represents the difference between the replica's clock and the timestamp of the event it is currently applying.</p> <p>The problem: this metric is unreliable. It resets to <code>0</code> when the SQL thread is idle (even if the I/O thread is behind), jumps erratically during large transactions, and does not account for clock drift between servers.</p>"},{"location":"Databases/mysql-replication/#heartbeat-tables","title":"Heartbeat Tables","text":"<p>A more reliable approach: write a timestamp to a table on the source at regular intervals and measure the difference on the replica.</p> <p>Create the heartbeat table:</p> <pre><code>CREATE TABLE IF NOT EXISTS replication_heartbeat (\n    id        INT PRIMARY KEY,\n    ts        TIMESTAMP(6) NOT NULL,\n    server_id INT NOT NULL\n);\n</code></pre> <p>On the source, update the timestamp periodically (via a cron job or daemon):</p> <pre><code>REPLACE INTO replication_heartbeat (id, ts, server_id) VALUES (1, NOW(6), @@server_id);\n</code></pre> <p>On the replica, measure the lag:</p> <pre><code>SELECT TIMESTAMPDIFF(MICROSECOND, ts, NOW(6)) / 1000000 AS lag_seconds\nFROM replication_heartbeat\nWHERE id = 1;\n</code></pre>"},{"location":"Databases/mysql-replication/#pt-heartbeat","title":"pt-heartbeat","text":"<p><code>pt-heartbeat</code> from Percona Toolkit automates the heartbeat approach. It is the industry standard for measuring replication lag:</p> <p>Start the daemon on the source:</p> <pre><code>pt-heartbeat --update --database percona --create-table --daemonize\n</code></pre> <p>Monitor lag from the replica:</p> <pre><code>pt-heartbeat --monitor --database percona --master-server-id 1\n</code></pre> <p>Or get a single reading:</p> <pre><code>pt-heartbeat --check --database percona --master-server-id 1\n# Output: 0.02 (seconds of lag)\n</code></pre> <p>Checking Replication Status and Lag (requires JavaScript)</p>"},{"location":"Databases/mysql-replication/#choosing-a-topology","title":"Choosing a Topology","text":"<p>The right replication topology depends on your requirements:</p> Requirement Recommended Topology Read scaling for a single application Source with 1-3 async replicas Zero committed-transaction loss Semi-synchronous replication Automatic failover without external tools Group Replication or InnoDB Cluster Application-transparent routing InnoDB Cluster (Router) or ProxySQL Fine-grained query routing and caching ProxySQL Multi-data-center writes Group Replication in multi-primary mode (with careful conflict handling) Maximum simplicity InnoDB Cluster (manages Group Replication, Router, and administration in one stack) <p>For most production deployments, start with InnoDB Cluster if you are running MySQL 8.0+. It provides automatic failover, transparent routing, and a management interface out of the box. Add ProxySQL if you need advanced query routing, connection multiplexing, or query caching on top.</p> <p>Design a Replication Topology (requires JavaScript)</p>"},{"location":"Databases/mysql-replication/#further-reading","title":"Further Reading","text":"<ul> <li>MySQL Replication Documentation - Official reference covering all replication modes, configuration, and troubleshooting</li> <li>MySQL Group Replication - Detailed documentation on consensus protocol, conflict detection, and operational procedures</li> <li>MySQL InnoDB Cluster - MySQL Shell AdminAPI reference for cluster setup and management</li> <li>ProxySQL Documentation - Configuration reference for query rules, hostgroups, and connection pooling</li> <li>Percona Toolkit: pt-heartbeat - Accurate replication lag measurement tool documentation</li> <li>Orchestrator - Topology management and automated failover for MySQL replication</li> </ul> <p>Previous: MySQL Performance &amp; Optimization | Next: PostgreSQL Fundamentals | Back to Index</p>"},{"location":"Databases/nosql-concepts/","title":"NoSQL Concepts &amp; Architecture","text":"<p>Relational databases dominated for decades, and for good reason - they provide strong guarantees, a mature query language, and well-understood operational patterns. But the web changed what \"normal\" looks like for data. Social networks generating billions of events per day, e-commerce catalogs with wildly different product attributes, real-time gaming leaderboards, IoT sensor streams - these workloads pushed relational systems to their limits and created demand for alternatives.</p> <p>NoSQL (\"Not Only SQL\") is not a single technology. It is a family of database systems that relax one or more relational constraints - fixed schemas, JOIN-heavy queries, single-node scaling, ACID transactions - in exchange for benefits like horizontal scalability, schema flexibility, or specialized data models. The term emerged around 2009, but the ideas go back further: key-value stores like BerkeleyDB existed in the 1990s.</p> <p>The core motivations behind NoSQL adoption:</p> <ul> <li>Horizontal scaling - distribute data across commodity servers instead of buying increasingly expensive hardware</li> <li>Schema flexibility - store documents with varying structures without ALTER TABLE migrations</li> <li>Developer productivity - work with data models that map directly to application objects</li> <li>Specialized query patterns - graph traversals, time-series aggregations, and full-text search that relational systems handle poorly</li> <li>High availability - stay operational even when individual nodes fail</li> </ul> <p>NoSQL does not mean \"no SQL\" or \"better than SQL\"</p> <p>NoSQL databases are not replacements for relational systems. They are alternatives optimized for specific access patterns. Most production applications use both relational and NoSQL databases together. Choosing the wrong database type for your workload causes more pain than the problem you were trying to solve.</p>"},{"location":"Databases/nosql-concepts/#nosql-database-categories","title":"NoSQL Database Categories","text":"<p>NoSQL databases fall into four major categories, each built around a different data model and optimized for different access patterns.</p> <pre><code>flowchart TD\n    A[NoSQL Databases] --&gt; B[Document Stores]\n    A --&gt; C[Key-Value Stores]\n    A --&gt; D[Wide-Column Stores]\n    A --&gt; E[Graph Databases]\n    B --&gt; B1[MongoDB]\n    B --&gt; B2[CouchDB]\n    B --&gt; B3[Amazon DocumentDB]\n    C --&gt; C1[Redis]\n    C --&gt; C2[Memcached]\n    C --&gt; C3[Amazon DynamoDB]\n    D --&gt; D1[Apache Cassandra]\n    D --&gt; D2[Apache HBase]\n    D --&gt; D3[ScyllaDB]\n    E --&gt; E1[Neo4j]\n    E --&gt; E2[Amazon Neptune]\n    E --&gt; E3[ArangoDB]</code></pre>"},{"location":"Databases/nosql-concepts/#document-stores","title":"Document Stores","text":"<p>A document store organizes data as self-contained documents - typically JSON, BSON (binary JSON), or XML. Each document holds all the data for a single entity, including nested objects and arrays. Documents are grouped into collections (analogous to tables), but unlike rows in a relational table, documents in the same collection can have completely different fields.</p>"},{"location":"Databases/nosql-concepts/#the-document-model","title":"The Document Model","text":"<p>Consider a product catalog. In a relational database, you might need a <code>products</code> table, a <code>product_attributes</code> table, a <code>product_images</code> table, and JOINs to reassemble them. In a document store, each product is a single document:</p> <pre><code>{\n  \"_id\": \"prod_8842\",\n  \"name\": \"Mechanical Keyboard\",\n  \"brand\": \"KeyCraft\",\n  \"price\": 149.99,\n  \"specs\": {\n    \"switch_type\": \"Cherry MX Brown\",\n    \"layout\": \"TKL\",\n    \"backlight\": \"RGB\",\n    \"connectivity\": [\"USB-C\", \"Bluetooth 5.0\"]\n  },\n  \"images\": [\n    {\"url\": \"/img/kb-front.jpg\", \"alt\": \"Front view\"},\n    {\"url\": \"/img/kb-side.jpg\", \"alt\": \"Side profile\"}\n  ],\n  \"reviews_count\": 342,\n  \"avg_rating\": 4.6\n}\n</code></pre> <p>This document embeds everything the application needs in a single read. No JOINs, no multi-table lookups. The <code>specs</code> object has fields specific to keyboards - a document for headphones in the same collection would have entirely different spec fields, and that is perfectly valid.</p>"},{"location":"Databases/nosql-concepts/#strengths-and-trade-offs","title":"Strengths and Trade-offs","text":"Strength Trade-off Schema flexibility - add fields without migrations No enforced schema means the application must handle inconsistencies Nested data reduces JOINs Deep nesting creates update complexity (changing a nested value requires rewriting the document) Developer-friendly JSON mapping Cross-document queries (equivalent to JOINs) are expensive or unsupported Horizontal scaling via sharding Denormalized data means updates may need to touch multiple documents"},{"location":"Databases/nosql-concepts/#major-document-stores","title":"Major Document Stores","text":"<p>MongoDB is the most widely used document database. It stores BSON documents, supports rich queries with an aggregation pipeline, and provides replica sets for high availability and sharding for horizontal scaling. You will work with MongoDB in depth in the next guide.</p> <p>CouchDB uses a RESTful HTTP API and stores plain JSON. It pioneered multi-master replication with eventual consistency - useful for offline-first applications that sync when connectivity returns.</p> <p>What is the primary advantage of the document data model over a relational model for a product catalog with varying attributes? (requires JavaScript)</p>"},{"location":"Databases/nosql-concepts/#key-value-stores","title":"Key-Value Stores","text":"<p>A key-value store is the simplest NoSQL model. You store a value (a string, a number, a serialized object, a blob) under a unique key, and you retrieve it by that key. The database treats the value as opaque - it does not parse or index the contents. This simplicity enables extreme performance.</p>"},{"location":"Databases/nosql-concepts/#the-data-model","title":"The Data Model","text":"<pre><code>Key                          Value\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nsession:abc123               {\"user_id\": 7042, \"role\": \"admin\", \"expires\": 1735689600}\ncache:product:8842           {\"name\": \"Mechanical Keyboard\", \"price\": 149.99, ...}\nfeature:dark-mode            \"enabled\"\nrate-limit:192.168.1.50      \"47\"\n</code></pre> <p>Every operation is a <code>GET(key)</code>, <code>SET(key, value)</code>, or <code>DELETE(key)</code>. Some key-value stores add atomic counters, expiration (TTL), and basic data structures, but the core model stays the same.</p>"},{"location":"Databases/nosql-concepts/#use-cases","title":"Use Cases","text":"<ul> <li>Caching - store expensive query results or rendered page fragments with a TTL</li> <li>Session management - fast read/write for user session data</li> <li>Feature flags - simple key lookups to toggle application behavior</li> <li>Rate limiting - atomic increment counters per IP or API key</li> <li>Leaderboards - sorted sets (in stores like Redis that support them)</li> </ul>"},{"location":"Databases/nosql-concepts/#major-key-value-stores","title":"Major Key-Value Stores","text":"<p>Redis goes well beyond simple key-value. It supports strings, hashes, lists, sets, sorted sets, streams, and more - all held in memory for sub-millisecond latency. Redis is covered in its own guide later in this course.</p> <p>Memcached is a pure in-memory cache with a simpler feature set than Redis. It excels at distributed caching for web applications and has been a staple since the mid-2000s.</p> <p>Amazon DynamoDB is a managed key-value and document database. Despite supporting richer queries than a pure key-value store, its core access pattern is key-based lookup, and it scales horizontally with minimal operational overhead.</p> <p>Key design matters</p> <p>In key-value stores, the key is your only query mechanism. Design keys with namespaces and hierarchies (<code>user:7042:preferences</code>, <code>cache:v2:product:8842</code>) so you can reason about your data and implement expiration policies per namespace.</p>"},{"location":"Databases/nosql-concepts/#wide-column-stores","title":"Wide-Column Stores","text":"<p>A wide-column store (sometimes called a column-family store) organizes data into rows and columns, but unlike a relational table, each row can have a different set of columns. Columns are grouped into column families, and the database is optimized for reading and writing entire column families efficiently.</p>"},{"location":"Databases/nosql-concepts/#the-data-model_1","title":"The Data Model","text":"<p>Think of a wide-column store as a two-dimensional key-value store: a row key maps to a set of column-family entries, and each column family contains a dynamic set of columns.</p> Row Key <code>profile</code> (column family) <code>activity</code> (column family) <code>user:1001</code> <code>name: \"Alice\"</code>, <code>email: \"alice@example.com\"</code> <code>last_login: \"2025-03-15\"</code>, <code>page_views: 4821</code> <code>user:1002</code> <code>name: \"Bob\"</code>, <code>org: \"Acme Corp\"</code> <code>last_login: \"2025-03-14\"</code> <code>user:1003</code> <code>name: \"Charlie\"</code>, <code>email: \"charlie@dev.io\"</code>, <code>phone: \"+1-555-0199\"</code> <code>last_login: \"2025-03-15\"</code>, <code>page_views: 127</code>, <code>api_calls: 9483</code> <p>Notice that <code>user:1001</code> has no <code>phone</code> column, <code>user:1002</code> has an <code>org</code> column the others lack, and <code>user:1003</code> has <code>api_calls</code> that the others do not. This sparsity is the defining characteristic - you do not waste storage on NULL columns, and adding a new column does not require a schema migration.</p>"},{"location":"Databases/nosql-concepts/#use-cases_1","title":"Use Cases","text":"<ul> <li>Time-series data - sensor readings, application metrics, event logs (row key = sensor ID + time bucket)</li> <li>Analytics - aggregation queries over large datasets where you read specific column families</li> <li>Content management - storing metadata alongside content with varying attributes</li> <li>IoT data ingestion - high write throughput for millions of devices</li> </ul>"},{"location":"Databases/nosql-concepts/#major-wide-column-stores","title":"Major Wide-Column Stores","text":"<p>Apache Cassandra is designed for massive write throughput and high availability across multiple data centers. It uses a peer-to-peer architecture with no single point of failure, tunable consistency per query, and a SQL-like query language called CQL.</p> <p>Apache HBase runs on top of Hadoop's HDFS and provides strong consistency with a single-master architecture. It is commonly used for random read/write access to large datasets in the Hadoop ecosystem.</p> <p>ScyllaDB is a Cassandra-compatible database rewritten in C++ for lower latency. It provides the same CQL interface with significantly better performance per node.</p> <p>What distinguishes a wide-column store from a relational database? (requires JavaScript)</p>"},{"location":"Databases/nosql-concepts/#graph-databases","title":"Graph Databases","text":"<p>A graph database stores data as nodes (entities), edges (relationships), and properties (attributes on both nodes and edges). The relationships are first-class citizens - stored directly as pointers between nodes rather than computed via JOINs at query time. This makes traversing connections fast regardless of dataset size.</p>"},{"location":"Databases/nosql-concepts/#the-data-model_2","title":"The Data Model","text":"<p>In a social network:</p> <ul> <li>Nodes: <code>User(\"Alice\")</code>, <code>User(\"Bob\")</code>, <code>Post(\"Graph databases are underrated\")</code></li> <li>Edges: <code>Alice -[:FOLLOWS]-&gt; Bob</code>, <code>Alice -[:AUTHORED]-&gt; Post</code>, <code>Bob -[:LIKED]-&gt; Post</code></li> <li>Properties: <code>FOLLOWS {since: \"2024-01-15\"}</code>, <code>User {name: \"Alice\", joined: \"2023-06-01\"}</code></li> </ul> <p>A query like \"find all users who follow someone who liked a post authored by Alice\" requires three JOINs in a relational database but is a direct graph traversal:</p> <pre><code>(Alice)-[:AUTHORED]-&gt;(post)&lt;-[:LIKED]-(liker)&lt;-[:FOLLOWS]-(follower)\n</code></pre> <p>This traversal stays fast because following an edge is a pointer lookup, not a table scan. In relational databases, each additional JOIN multiplies the cost. In graph databases, each additional hop is roughly constant time.</p>"},{"location":"Databases/nosql-concepts/#use-cases_2","title":"Use Cases","text":"<ul> <li>Social networks - friend recommendations, mutual connections, influence analysis</li> <li>Recommendation engines - \"users who bought X also bought Y\" via shared purchase patterns</li> <li>Fraud detection - identifying clusters of accounts with suspicious relationship patterns</li> <li>Knowledge graphs - connecting entities across domains (products, categories, suppliers, regulations)</li> <li>Network topology - modeling infrastructure dependencies, routing, impact analysis</li> </ul>"},{"location":"Databases/nosql-concepts/#major-graph-databases","title":"Major Graph Databases","text":"<p>Neo4j is the most established graph database. Its query language, Cypher, reads almost like English: <code>MATCH (a:User)-[:FOLLOWS]-&gt;(b:User) WHERE a.name = \"Alice\" RETURN b.name</code>. Neo4j uses native graph storage - relationships are stored as direct pointers, not looked up in an index.</p> <p>Amazon Neptune is a managed graph database supporting both property graph (Gremlin) and RDF (SPARQL) query models.</p> <p>When to use a graph database</p> <p>If your queries are primarily about relationships between entities - especially multi-hop traversals - a graph database will dramatically outperform a relational database. If your queries are primarily about filtering and aggregating attributes of individual entities, a relational or document database is the better choice.</p>"},{"location":"Databases/nosql-concepts/#sql-vs-document-query-comparison","title":"SQL vs. Document Query Comparison","text":"<p>To make the difference concrete, here is the same data retrieval in SQL (relational) and in a MongoDB-style document query. The scenario: find all orders for a specific customer, including product details.</p> <p>Querying Relational vs. Document Data Models (requires JavaScript)</p>"},{"location":"Databases/nosql-concepts/#the-cap-theorem","title":"The CAP Theorem","text":"<p>The CAP theorem (formalized by Eric Brewer in 2000, proven by Seth Gilbert and Nancy Lynch in 2002) states that a distributed data store can provide at most two of these three guarantees simultaneously:</p> <ul> <li>Consistency (C) - every read receives the most recent write or an error</li> <li>Availability (A) - every request receives a non-error response (though it may not contain the most recent write)</li> <li>Partition tolerance (P) - the system continues operating despite network partitions between nodes</li> </ul>"},{"location":"Databases/nosql-concepts/#why-only-two","title":"Why Only Two?","text":"<p>In any distributed system, network partitions will happen - switches fail, cables get cut, cloud availability zones lose connectivity. During a partition, the system must make a choice:</p> <p>Choose CP (Consistency + Partition Tolerance): When a partition occurs, the system blocks or returns errors for requests it cannot guarantee are consistent. Nodes that cannot confirm they have the latest data refuse to serve reads. You get correct data or no data.</p> <p>Choose AP (Availability + Partition Tolerance): When a partition occurs, every node continues serving requests using whatever data it has locally. Reads might return stale data, but the system never goes down. Nodes reconcile differences after the partition heals.</p> <p>CA (Consistency + Availability): Only possible when there are no partitions - meaning a single-node system. The moment you distribute data across a network, you must handle partitions, and CA is off the table.</p>"},{"location":"Databases/nosql-concepts/#real-world-cap-classifications","title":"Real-World CAP Classifications","text":"System CAP Choice Behavior During Partition HBase CP Regions on the partitioned side become unavailable until partition heals MongoDB (majority write concern) CP Writes require acknowledgment from a majority of replica set members; partitioned minority cannot accept writes Cassandra AP All nodes continue accepting reads and writes; conflicts resolved by timestamp after partition heals DynamoDB AP Continues serving requests across partitions; eventual consistency by default PostgreSQL (single node) CA No partitions possible on a single node; both consistent and available <p>CAP is about partitions, not normal operation</p> <p>CAP choices only matter during a network partition. Under normal operation, well-designed distributed databases provide all three properties. The CAP classification describes what the system sacrifices when things go wrong, not its everyday behavior.</p> <p>A distributed database is configured to always return the most recent write, even if that means some requests fail during a network partition. Which CAP properties does it guarantee? (requires JavaScript)</p>"},{"location":"Databases/nosql-concepts/#consistency-models","title":"Consistency Models","text":"<p>CAP describes a binary choice during failures. In practice, distributed databases offer a spectrum of consistency models that determine how up-to-date your reads are relative to writes, even during normal operation.</p>"},{"location":"Databases/nosql-concepts/#strong-consistency","title":"Strong Consistency","text":"<p>Every read sees the result of the most recent write. After a write completes, all subsequent reads from any node return that value. This is what relational databases provide by default within a transaction.</p> <p>Cost: Higher latency (writes must propagate before reads can proceed), lower throughput, reduced availability during partitions.</p> <p>Example: A bank transfer. After moving $500 from checking to savings, reading either account balance from any node must reflect the transfer.</p>"},{"location":"Databases/nosql-concepts/#eventual-consistency","title":"Eventual Consistency","text":"<p>If no new writes occur, all nodes will eventually converge to the same value. There is no guarantee about how long \"eventually\" takes - it could be milliseconds or seconds.</p> <p>Cost: Reads may return stale data. The application must tolerate temporarily inconsistent views.</p> <p>Example: A social media \"like\" count. If Alice likes a post, Bob might see the old count for a few seconds. This is acceptable because the counter will converge, and the brief inconsistency has no real consequence.</p>"},{"location":"Databases/nosql-concepts/#causal-consistency","title":"Causal Consistency","text":"<p>If operation A causally precedes operation B (B depends on or was influenced by A), then every node sees A before B. Operations with no causal relationship may appear in different orders on different nodes.</p> <p>Example: In a comment thread, a reply always appears after the message it replies to, but independent top-level comments may appear in different orders on different nodes.</p>"},{"location":"Databases/nosql-concepts/#read-your-writes-consistency","title":"Read-Your-Writes Consistency","text":"<p>After you perform a write, your subsequent reads are guaranteed to see that write. Other users may still see stale data.</p> <p>Example: After updating your profile picture, you immediately see the new picture. Other users might briefly see your old picture. This is usually implemented by routing your reads to the same node that accepted your write.</p>"},{"location":"Databases/nosql-concepts/#monotonic-reads","title":"Monotonic Reads","text":"<p>Once you read a value, subsequent reads will never return an older value. Your view of the data only moves forward in time, never backward.</p> <p>Example: If you read a post that has 47 likes, the next time you read it you will see 47 or more likes, never 45. Without monotonic reads, load-balanced reads across replicas with different lag could show the count jumping backward.</p>"},{"location":"Databases/nosql-concepts/#choosing-a-consistency-level","title":"Choosing a Consistency Level","text":"<p>Most distributed databases let you choose consistency per operation. Cassandra, for example, lets you specify <code>ONE</code>, <code>QUORUM</code>, or <code>ALL</code> for each read or write:</p> Cassandra Level Behavior Latency Consistency <code>ONE</code> Read/write acknowledged by a single node Lowest Weakest (eventual) <code>QUORUM</code> Acknowledged by majority of replicas Medium Strong if read + write quorum &gt; replication factor <code>ALL</code> Acknowledged by every replica Highest Strongest, but one node failure blocks the operation <p>The formula for strong consistency: if <code>R + W &gt; N</code> (read replicas + write replicas &gt; total replicas), reads are guaranteed to overlap with at least one node that has the latest write.</p>"},{"location":"Databases/nosql-concepts/#polyglot-persistence","title":"Polyglot Persistence","text":"<p>Polyglot persistence means using multiple database technologies within a single application, choosing each based on the access pattern of the data it stores.</p> <p>Consider an e-commerce platform:</p> Data Database Reasoning Product catalog MongoDB Varying attributes per product category, flexible schema User sessions Redis Sub-millisecond reads, automatic TTL expiration Order transactions PostgreSQL ACID guarantees for financial data, complex reporting queries Product recommendations Neo4j Traversing purchase history graphs for \"also bought\" suggestions Search index Elasticsearch Full-text search with fuzzy matching and faceted navigation Event stream Apache Kafka High-throughput append-only log for analytics pipeline <p>Each database handles the workload it was designed for. The alternative - forcing all data into a single relational database - means fighting against the database for workloads it was not optimized for.</p>"},{"location":"Databases/nosql-concepts/#the-costs-of-polyglot-persistence","title":"The Costs of Polyglot Persistence","text":"<p>This approach is not free:</p> <ul> <li>Operational complexity - every database engine is another system to monitor, patch, back up, and recover</li> <li>Data synchronization - keeping data consistent across systems requires event-driven architecture or change data capture</li> <li>Team expertise - your team needs operational knowledge of each database</li> <li>Transaction boundaries - distributed transactions across different databases are hard; most teams use eventual consistency between systems</li> </ul> <p>Start simple</p> <p>Begin with a single well-chosen database. Add specialized databases only when you have a measurable problem - not because an architecture diagram looks impressive with more boxes. Most applications work fine with one relational database for years.</p> <p>An e-commerce platform stores product catalog data in MongoDB, user sessions in Redis, and financial transactions in PostgreSQL. What is this pattern called? (requires JavaScript)</p>"},{"location":"Databases/nosql-concepts/#decision-framework-relational-vs-nosql","title":"Decision Framework: Relational vs. NoSQL","text":"<p>Choosing a database is not about ideology. It is about matching your workload to the system designed for it. Work through these criteria:</p>"},{"location":"Databases/nosql-concepts/#data-structure","title":"Data Structure","text":"Your data looks like... Consider Highly structured, well-defined relationships between entities Relational (PostgreSQL, MySQL) Semi-structured with varying attributes per record Document store (MongoDB, CouchDB) Simple lookups by identifier, no complex queries Key-value (Redis, DynamoDB) Sparse columns, high-volume append writes Wide-column (Cassandra, HBase) Heavily interconnected with multi-hop relationship queries Graph (Neo4j, Neptune)"},{"location":"Databases/nosql-concepts/#query-patterns","title":"Query Patterns","text":"You mostly need to... Consider Run ad-hoc queries with complex filters, JOINs, aggregations Relational Fetch complete entities by ID or simple filters Document or key-value Write massive volumes with simple reads by partition key Wide-column Traverse relationships between entities Graph"},{"location":"Databases/nosql-concepts/#consistency-requirements","title":"Consistency Requirements","text":"You need... Consider ACID transactions across multiple tables Relational Strong consistency for financial or inventory data Relational or CP-configured NoSQL Eventual consistency is acceptable AP NoSQL (Cassandra, DynamoDB) Tunable consistency per operation Cassandra, DynamoDB, MongoDB"},{"location":"Databases/nosql-concepts/#scale-requirements","title":"Scale Requirements","text":"Your scale is... Consider Single server handles the load Relational (simplest operational model) Read-heavy, needs read replicas Relational with replicas, or any NoSQL Write-heavy, needs horizontal partitioning Wide-column or sharded document store Global distribution across data centers Cassandra, DynamoDB, CockroachDB"},{"location":"Databases/nosql-concepts/#the-default-choice","title":"The Default Choice","text":"<p>If you are unsure, start with PostgreSQL. It handles JSON documents (via JSONB), full-text search, and horizontal read scaling with replicas. You can add specialized databases later when you have concrete evidence that PostgreSQL is not meeting a specific need.</p>"},{"location":"Databases/nosql-concepts/#practical-exercise","title":"Practical Exercise","text":"<p>Choose the Right Database (requires JavaScript)</p>"},{"location":"Databases/nosql-concepts/#further-reading","title":"Further Reading","text":"<ul> <li>Martin Kleppmann - Designing Data-Intensive Applications - the definitive reference on distributed systems, consistency models, and database internals</li> <li>Eric Brewer's CAP Theorem - Brewer's own retrospective on CAP, twelve years after the original conjecture</li> <li>MongoDB Documentation - official docs for the most widely used document database</li> <li>Redis Documentation - official docs covering data structures, persistence, clustering, and use patterns</li> <li>Apache Cassandra Architecture - how Cassandra implements its peer-to-peer, tunable-consistency design</li> <li>Neo4j Graph Database Concepts - introduction to property graphs and the Cypher query language</li> </ul> <p>Previous: PostgreSQL Advanced Features | Next: MongoDB | Back to Index</p>"},{"location":"Databases/postgresql-administration/","title":"PostgreSQL Administration","text":"<p>PostgreSQL gives you more control over your database than almost any other RDBMS - but that control comes with responsibilities. You need to understand how roles and privileges work, how to manage the storage layer with tablespaces, why dead tuples accumulate and how VACUUM reclaims them, what the <code>pg_stat_*</code> views are telling you, which extensions are worth installing, and how WAL keeps your data safe through crashes. This guide covers the daily administration tasks that keep a PostgreSQL instance running well in production.</p>"},{"location":"Databases/postgresql-administration/#roles-and-privileges","title":"Roles and Privileges","text":"<p>PostgreSQL has a single concept for both users and groups: the role. A role can own database objects, hold privileges, and contain other roles as members. There is no separate <code>CREATE USER</code> and <code>CREATE GROUP</code> - those commands exist for convenience, but they all create roles under the hood.</p>"},{"location":"Databases/postgresql-administration/#creating-roles","title":"Creating Roles","text":"<p>Use <code>CREATE ROLE</code> to define a new role:</p> <pre><code>-- A role that can log in (equivalent to CREATE USER)\nCREATE ROLE app_user LOGIN PASSWORD 'strong_password_here';\n\n-- A role that cannot log in (used as a group)\nCREATE ROLE readonly NOLOGIN;\n\n-- A role with specific attributes\nCREATE ROLE admin_user LOGIN PASSWORD 'another_password'\n    CREATEDB CREATEROLE VALID UNTIL '2027-01-01';\n</code></pre> <p>The key attributes you control at creation time:</p> Attribute Description <code>LOGIN</code> / <code>NOLOGIN</code> Whether the role can connect to the database <code>SUPERUSER</code> Bypasses all permission checks (use sparingly) <code>CREATEDB</code> Can create new databases <code>CREATEROLE</code> Can create, alter, and drop other roles <code>REPLICATION</code> Can initiate streaming replication <code>PASSWORD</code> Sets the authentication password <code>VALID UNTIL</code> Password expiration timestamp <code>CONNECTION LIMIT</code> Maximum concurrent connections for this role <p><code>CREATE USER</code> is an alias for <code>CREATE ROLE ... LOGIN</code>. <code>CREATE GROUP</code> is an alias for <code>CREATE ROLE ... NOLOGIN</code>. Use whichever reads more clearly in context.</p>"},{"location":"Databases/postgresql-administration/#grant-and-revoke","title":"GRANT and REVOKE","text":"<p>Privileges in PostgreSQL are granted at the database, schema, and object level. The <code>GRANT</code> and <code>REVOKE</code> statements control access:</p> <pre><code>-- Database-level: allow connecting\nGRANT CONNECT ON DATABASE myapp TO app_user;\n\n-- Schema-level: allow using objects in the schema\nGRANT USAGE ON SCHEMA public TO app_user;\n\n-- Table-level: specific operations\nGRANT SELECT, INSERT, UPDATE ON TABLE orders TO app_user;\nGRANT ALL PRIVILEGES ON TABLE orders TO admin_user;\n\n-- All tables in a schema at once\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO readonly;\n\n-- Set default privileges for future tables\nALTER DEFAULT PRIVILEGES IN SCHEMA public\n    GRANT SELECT ON TABLES TO readonly;\n\n-- Revoke access\nREVOKE INSERT, UPDATE ON TABLE orders FROM app_user;\n</code></pre> <p>Default privileges only affect future objects</p> <p><code>ALTER DEFAULT PRIVILEGES</code> applies to objects created after the statement runs. Existing tables are unaffected. When setting up a read-only role, you need both <code>GRANT SELECT ON ALL TABLES</code> for current tables and <code>ALTER DEFAULT PRIVILEGES</code> for tables created later.</p>"},{"location":"Databases/postgresql-administration/#role-membership-and-inheritance","title":"Role Membership and Inheritance","text":"<p>Roles can be members of other roles, forming a group structure. When a role inherits from another, it automatically gains that role's privileges:</p> <pre><code>-- Create a group role\nCREATE ROLE analysts NOLOGIN;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO analysts;\n\n-- Add members\nGRANT analysts TO alice;\nGRANT analysts TO bob;\n\n-- alice and bob now inherit SELECT on all public tables\n</code></pre> <p>By default, member roles inherit privileges from their parent role. You can control this with <code>INHERIT</code> and <code>NOINHERIT</code>:</p> <pre><code>-- This role must explicitly SET ROLE to gain privileges\nCREATE ROLE operator NOLOGIN NOINHERIT;\nGRANT admin_role TO operator;\n\n-- The user must switch role context to use admin_role privileges\nSET ROLE admin_role;\n-- ... do admin work ...\nRESET ROLE;\n</code></pre> <p><code>NOINHERIT</code> is useful when you want a user to consciously activate elevated privileges rather than having them always active - similar to <code>sudo</code> on Linux.</p>"},{"location":"Databases/postgresql-administration/#row-level-security","title":"Row-Level Security","text":"<p>Row-level security (RLS) lets you control which rows a user can see or modify at the table level, not just which tables they can access. This is powerful for multi-tenant applications where all tenants share a table but should only see their own data.</p> <pre><code>-- Step 1: Enable RLS on the table\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\n\n-- Step 2: Create policies\nCREATE POLICY tenant_isolation ON orders\n    USING (tenant_id = current_setting('app.current_tenant')::int);\n\n-- Step 3: Users now only see rows matching the policy\n-- The application sets the tenant context per connection:\nSET app.current_tenant = '42';\nSELECT * FROM orders;  -- Only returns rows where tenant_id = 42\n</code></pre> <p>You can create separate policies for different operations:</p> <pre><code>-- Users can see their own data\nCREATE POLICY select_own ON orders\n    FOR SELECT\n    USING (user_id = current_user);\n\n-- Users can only insert rows attributed to themselves\nCREATE POLICY insert_own ON orders\n    FOR INSERT\n    WITH CHECK (user_id = current_user);\n\n-- Users can update their own rows, but cannot change the user_id\nCREATE POLICY update_own ON orders\n    FOR UPDATE\n    USING (user_id = current_user)\n    WITH CHECK (user_id = current_user);\n</code></pre> <p>Table owners bypass RLS by default</p> <p>The table owner and superusers are not subject to row-level security policies. If you want the owner to also be restricted, run <code>ALTER TABLE orders FORCE ROW LEVEL SECURITY</code>. This is important for testing - you may not notice broken policies if you're testing as the table owner.</p> <p>You create a role with NOINHERIT and grant it membership in an admin group. What happens when the role connects? (requires JavaScript)</p>"},{"location":"Databases/postgresql-administration/#tablespace-management","title":"Tablespace Management","text":"<p>A tablespace defines a physical location on the filesystem where PostgreSQL stores data files. By default, PostgreSQL uses two tablespaces:</p> <ul> <li><code>pg_default</code> - stores user data (under <code>base/</code> in the data directory)</li> <li><code>pg_global</code> - stores cluster-wide system catalogs (under <code>global/</code> in the data directory)</li> </ul>"},{"location":"Databases/postgresql-administration/#creating-tablespaces","title":"Creating Tablespaces","text":"<p>You can create additional tablespaces to place data on different storage devices - for example, putting frequently accessed indexes on fast SSDs while keeping large archival tables on slower, cheaper disks:</p> <pre><code>-- The directory must exist and be owned by the postgres OS user\n-- On the OS: mkdir -p /mnt/ssd/pgdata &amp;&amp; chown postgres:postgres /mnt/ssd/pgdata\n\nCREATE TABLESPACE fast_storage\n    OWNER postgres\n    LOCATION '/mnt/ssd/pgdata';\n\nCREATE TABLESPACE archive_storage\n    LOCATION '/mnt/hdd/pgdata';\n</code></pre>"},{"location":"Databases/postgresql-administration/#using-tablespaces","title":"Using Tablespaces","text":"<p>Assign tablespaces when creating objects or move existing objects:</p> <pre><code>-- Create a table in a specific tablespace\nCREATE TABLE recent_events (\n    id bigserial PRIMARY KEY,\n    event_type text,\n    created_at timestamptz DEFAULT now()\n) TABLESPACE fast_storage;\n\n-- Create an index in a specific tablespace\nCREATE INDEX idx_events_created\n    ON recent_events (created_at)\n    TABLESPACE fast_storage;\n\n-- Move an existing table to a different tablespace\nALTER TABLE old_logs SET TABLESPACE archive_storage;\n\n-- Set the default tablespace for a database\nALTER DATABASE myapp SET default_tablespace = 'fast_storage';\n</code></pre> <p>Moving tables locks them</p> <p><code>ALTER TABLE ... SET TABLESPACE</code> acquires an <code>ACCESS EXCLUSIVE</code> lock. The table is completely unavailable during the move. For large tables, this can take a long time. Plan tablespace moves during maintenance windows, or use logical replication to migrate data with minimal downtime.</p>"},{"location":"Databases/postgresql-administration/#monitoring-tablespace-usage","title":"Monitoring Tablespace Usage","text":"<pre><code>-- List all tablespaces with size\nSELECT spcname, pg_size_pretty(pg_tablespace_size(spcname))\nFROM pg_tablespace;\n\n-- See which tablespace each table uses\nSELECT tablename, tablespace\nFROM pg_tables\nWHERE schemaname = 'public';\n-- NULL tablespace means pg_default\n</code></pre>"},{"location":"Databases/postgresql-administration/#vacuum-and-analyze","title":"VACUUM and ANALYZE","text":"<p>If there is one PostgreSQL administration concept you must understand, it is VACUUM. PostgreSQL's multi-version concurrency control (MVCC) system never overwrites data in place. When you update a row, PostgreSQL creates a new version and marks the old one as \"dead.\" When you delete a row, PostgreSQL marks it as dead but does not remove it. These dead tuples accumulate over time and waste disk space, slow down sequential scans, and cause table bloat.</p>"},{"location":"Databases/postgresql-administration/#why-vacuum-exists","title":"Why VACUUM Exists","text":"<p>Consider what happens without VACUUM:</p> <ol> <li>You update 1 million rows in a table</li> <li>PostgreSQL creates 1 million new row versions</li> <li>The old 1 million versions are still on disk, marked as dead</li> <li>Sequential scans now read 2 million row versions even though only 1 million are live</li> <li>Indexes still point to both live and dead versions</li> <li>The table has doubled in size on disk</li> </ol> <p>VACUUM reclaims the space occupied by dead tuples, making it available for reuse by future inserts and updates within the same table.</p>"},{"location":"Databases/postgresql-administration/#vacuum-vs-vacuum-full","title":"VACUUM vs VACUUM FULL","text":"<p>PostgreSQL offers two flavors:</p> <code>VACUUM</code> <code>VACUUM FULL</code> Locking Runs alongside reads and writes Locks the table exclusively Space reclamation Marks dead tuple space as reusable within the table Rewrites the entire table, releasing space to the OS Speed Fast, designed to run frequently Slow, especially on large tables When to use Regular maintenance (autovacuum does this) Only when a table has extreme bloat and you need to shrink the file on disk Downtime None Table is unavailable for the duration <p>In practice, you should almost never need <code>VACUUM FULL</code>. Regular <code>VACUUM</code> keeps dead tuple space recyclable. <code>VACUUM FULL</code> is a last resort for tables that grew extremely large due to a bulk operation and you need the disk space back.</p>"},{"location":"Databases/postgresql-administration/#analyze","title":"ANALYZE","text":"<p><code>ANALYZE</code> collects statistics about the distribution of data in a table's columns. The query planner uses these statistics to choose efficient execution plans - deciding whether to use an index scan, sequential scan, hash join, or merge join.</p> <pre><code>-- Analyze a specific table\nANALYZE orders;\n\n-- Analyze specific columns\nANALYZE orders (customer_id, created_at);\n\n-- VACUUM and ANALYZE together\nVACUUM ANALYZE orders;\n</code></pre> <p>Stale statistics lead to bad query plans. If you load a large batch of data or significantly change the data distribution, running <code>ANALYZE</code> immediately afterward helps the planner make good decisions.</p>"},{"location":"Databases/postgresql-administration/#autovacuum-configuration","title":"Autovacuum Configuration","text":"<p>PostgreSQL runs autovacuum as a background process that automatically triggers <code>VACUUM</code> and <code>ANALYZE</code> when tables accumulate enough changes. The key configuration parameters in <code>postgresql.conf</code>:</p> <pre><code># Master switch (leave this on)\nautovacuum = on\n\n# How often the autovacuum launcher checks for work (seconds)\nautovacuum_naptime = 60\n\n# VACUUM triggers when dead tuples exceed:\n#   autovacuum_vacuum_threshold + (autovacuum_vacuum_scale_factor * table rows)\nautovacuum_vacuum_threshold = 50        # base number of dead tuples\nautovacuum_vacuum_scale_factor = 0.2    # fraction of table size\n\n# ANALYZE triggers when changed tuples exceed:\n#   autovacuum_analyze_threshold + (autovacuum_analyze_scale_factor * table rows)\nautovacuum_analyze_threshold = 50\nautovacuum_analyze_scale_factor = 0.1\n\n# Maximum number of concurrent autovacuum workers\nautovacuum_max_workers = 3\n</code></pre> <p>With the defaults, a table with 10,000 rows triggers autovacuum when it accumulates more than 50 + (0.2 * 10,000) = 2,050 dead tuples. For a table with 10 million rows, the threshold is 50 + (0.2 * 10,000,000) = 2,000,050 dead tuples - which means a lot of bloat accumulates before autovacuum kicks in.</p> <p>For large, heavily-updated tables, you often need per-table overrides:</p> <pre><code>-- More aggressive autovacuum for a high-traffic table\nALTER TABLE orders SET (\n    autovacuum_vacuum_scale_factor = 0.01,\n    autovacuum_analyze_scale_factor = 0.005,\n    autovacuum_vacuum_threshold = 100\n);\n</code></pre>"},{"location":"Databases/postgresql-administration/#monitoring-dead-tuples","title":"Monitoring Dead Tuples","text":"<p>The <code>pg_stat_user_tables</code> view tells you how well autovacuum is keeping up:</p> <pre><code>SELECT\n    schemaname,\n    relname,\n    n_live_tup,\n    n_dead_tup,\n    round(n_dead_tup::numeric / NULLIF(n_live_tup, 0) * 100, 2) AS dead_pct,\n    last_vacuum,\n    last_autovacuum,\n    last_analyze,\n    last_autoanalyze\nFROM pg_stat_user_tables\nORDER BY n_dead_tup DESC\nLIMIT 10;\n</code></pre> <p>If <code>n_dead_tup</code> is consistently high and <code>last_autovacuum</code> is far in the past, autovacuum may be falling behind. Common causes: too few <code>autovacuum_max_workers</code>, thresholds set too high, or long-running transactions holding back the oldest transaction horizon (preventing VACUUM from reclaiming tuples that might still be visible to them).</p> <p>A table has 5 million rows and the autovacuum_vacuum_scale_factor is set to the default 0.2. Approximately how many dead tuples must accumulate before autovacuum triggers a VACUUM? (requires JavaScript)</p> <p>Invalid interactive component configuration (terminal)</p>"},{"location":"Databases/postgresql-administration/#pg_stat_-monitoring-views","title":"pg_stat_* Monitoring Views","text":"<p>PostgreSQL provides a rich set of statistics views prefixed with <code>pg_stat_</code> that give you visibility into what the database is doing. These views are your primary tool for understanding performance, diagnosing problems, and capacity planning.</p>"},{"location":"Databases/postgresql-administration/#pg_stat_activity","title":"pg_stat_activity","text":"<p>The <code>pg_stat_activity</code> view shows one row per active server process - every connection, what it is doing, and how long it has been doing it:</p> <pre><code>-- Active queries right now\nSELECT\n    pid,\n    usename,\n    datname,\n    state,\n    wait_event_type,\n    wait_event,\n    now() - query_start AS query_duration,\n    left(query, 80) AS query_preview\nFROM pg_stat_activity\nWHERE state != 'idle'\nORDER BY query_start;\n</code></pre> <p>Key columns:</p> Column What it tells you <code>state</code> <code>active</code>, <code>idle</code>, <code>idle in transaction</code>, <code>fastpath function call</code> <code>wait_event_type</code> Why the process is waiting: <code>Lock</code>, <code>IO</code>, <code>LWLock</code>, <code>BufferPin</code>, etc. <code>wait_event</code> Specific wait: <code>relation</code>, <code>transactionid</code>, <code>DataFileRead</code>, etc. <code>query_start</code> When the current query began executing <code>xact_start</code> When the current transaction started (important for long transactions) <code>backend_type</code> <code>client backend</code>, <code>autovacuum worker</code>, <code>walwriter</code>, etc. <p>Watch for <code>idle in transaction</code> connections - they hold locks and prevent VACUUM from reclaiming dead tuples. If <code>xact_start</code> is far in the past for an <code>idle in transaction</code> session, that connection is a problem.</p> <pre><code>-- Find long-running transactions blocking VACUUM\nSELECT\n    pid,\n    usename,\n    state,\n    now() - xact_start AS transaction_age,\n    left(query, 60) AS last_query\nFROM pg_stat_activity\nWHERE state = 'idle in transaction'\n    AND now() - xact_start &gt; interval '5 minutes'\nORDER BY xact_start;\n</code></pre>"},{"location":"Databases/postgresql-administration/#pg_stat_user_tables","title":"pg_stat_user_tables","text":"<p>This view provides per-table I/O and maintenance statistics:</p> <pre><code>SELECT\n    relname,\n    seq_scan,\n    seq_tup_read,\n    idx_scan,\n    idx_tup_fetch,\n    n_tup_ins,\n    n_tup_upd,\n    n_tup_del,\n    n_dead_tup\nFROM pg_stat_user_tables\nORDER BY seq_scan DESC\nLIMIT 10;\n</code></pre> <p>A table with high <code>seq_scan</code> counts and many rows probably needs an index. Compare <code>seq_scan</code> to <code>idx_scan</code> - if sequential scans dominate on a large table, the planner is not finding useful indexes for common queries.</p>"},{"location":"Databases/postgresql-administration/#pg_stat_user_indexes","title":"pg_stat_user_indexes","text":"<p>Shows how much each index is actually used:</p> <pre><code>-- Find unused indexes (candidates for removal)\nSELECT\n    schemaname,\n    relname AS table_name,\n    indexrelname AS index_name,\n    idx_scan,\n    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size\nFROM pg_stat_user_indexes\nWHERE idx_scan = 0\nORDER BY pg_relation_size(indexrelid) DESC;\n</code></pre> <p>Unused indexes waste disk space and slow down writes (every INSERT, UPDATE, and DELETE must update all indexes). Periodically auditing for zero-scan indexes and dropping them is a valuable maintenance task.</p>"},{"location":"Databases/postgresql-administration/#pg_stat_bgwriter","title":"pg_stat_bgwriter","text":"<p>The background writer and checkpointer flush dirty buffers to disk. This view shows how that work is distributed:</p> <pre><code>SELECT\n    checkpoints_timed,\n    checkpoints_req,\n    buffers_checkpoint,\n    buffers_clean,\n    buffers_backend,\n    buffers_alloc\nFROM pg_stat_bgwriter;\n</code></pre> <p>If <code>buffers_backend</code> is high relative to <code>buffers_checkpoint</code> and <code>buffers_clean</code>, client processes are being forced to write dirty pages themselves instead of the background writer handling it. This indicates the background writer cannot keep up and you may need to tune <code>bgwriter_lru_maxpages</code> or <code>bgwriter_lru_multiplier</code>.</p> <p>If <code>checkpoints_req</code> is high relative to <code>checkpoints_timed</code>, checkpoints are being forced by WAL volume rather than happening on schedule. Increase <code>max_wal_size</code> to reduce forced checkpoints.</p>"},{"location":"Databases/postgresql-administration/#pg_stat_replication","title":"pg_stat_replication","text":"<p>When running streaming replication, this view shows the state of each standby:</p> <pre><code>SELECT\n    client_addr,\n    state,\n    sent_lsn,\n    write_lsn,\n    flush_lsn,\n    replay_lsn,\n    sent_lsn - replay_lsn AS replay_lag_bytes,\n    write_lag,\n    flush_lag,\n    replay_lag\nFROM pg_stat_replication;\n</code></pre> <p>The lag columns (<code>write_lag</code>, <code>flush_lag</code>, <code>replay_lag</code>) show time-based lag at each stage. If <code>replay_lag</code> is growing, the standby is falling behind - often due to heavy write load, slow standby disk, or a long-running query on the standby blocking replay.</p> <p>You notice a table with 2 million rows has 45,000 sequential scans and only 120 index scans in pg_stat_user_tables. What does this most likely indicate? (requires JavaScript)</p>"},{"location":"Databases/postgresql-administration/#extensions","title":"Extensions","text":"<p>One of PostgreSQL's greatest strengths is its extension system. Extensions add data types, functions, operators, index methods, and more - all without modifying the core server. PostgreSQL ships with a set of contrib extensions, and hundreds more are available from the community.</p>"},{"location":"Databases/postgresql-administration/#installing-extensions","title":"Installing Extensions","text":"<p>Extensions must first be available on the filesystem (installed as shared libraries), then loaded into a specific database:</p> <pre><code>-- See what's available\nSELECT name, default_version, comment\nFROM pg_available_extensions\nORDER BY name;\n\n-- Install an extension\nCREATE EXTENSION IF NOT EXISTS pg_stat_statements;\n\n-- Check installed extensions\nSELECT extname, extversion FROM pg_extension;\n\n-- Update an extension to a new version\nALTER EXTENSION pg_stat_statements UPDATE;\n\n-- Remove an extension\nDROP EXTENSION pg_stat_statements;\n</code></pre>"},{"location":"Databases/postgresql-administration/#pg_stat_statements","title":"pg_stat_statements","text":"<p>pg_stat_statements tracks execution statistics for every SQL statement executed by the server. It is the single most valuable extension for performance analysis.</p> <p>To enable it, add to <code>postgresql.conf</code>:</p> <pre><code>shared_preload_libraries = 'pg_stat_statements'\npg_stat_statements.track = all\n</code></pre> <p>Then restart PostgreSQL and create the extension:</p> <pre><code>CREATE EXTENSION pg_stat_statements;\n\n-- Top 10 queries by total execution time\nSELECT\n    left(query, 80) AS query,\n    calls,\n    round(total_exec_time::numeric, 2) AS total_ms,\n    round(mean_exec_time::numeric, 2) AS avg_ms,\n    rows\nFROM pg_stat_statements\nORDER BY total_exec_time DESC\nLIMIT 10;\n\n-- Reset statistics\nSELECT pg_stat_statements_reset();\n</code></pre> <p>This tells you which queries consume the most time in aggregate - not just the slowest individual query, but the ones that are called thousands of times and add up. A query that takes 2 ms but runs 500,000 times per day (1,000 seconds total) matters more than one that takes 5 seconds but runs once.</p>"},{"location":"Databases/postgresql-administration/#pgcrypto","title":"pgcrypto","text":"<p>pgcrypto provides cryptographic functions directly in SQL - hashing, encryption, and random data generation:</p> <pre><code>CREATE EXTENSION pgcrypto;\n\n-- Hash a password with bcrypt (blowfish)\nSELECT crypt('user_password', gen_salt('bf'));\n\n-- Verify a password\nSELECT (stored_hash = crypt('user_input', stored_hash)) AS password_valid;\n\n-- Generate random bytes\nSELECT gen_random_bytes(16);\n\n-- Generate a random UUID (also available via gen_random_uuid() in PG 13+)\nSELECT gen_random_uuid();\n\n-- Symmetric encryption\nSELECT pgp_sym_encrypt('sensitive data', 'encryption_key');\nSELECT pgp_sym_decrypt(encrypted_column, 'encryption_key');\n</code></pre>"},{"location":"Databases/postgresql-administration/#postgis","title":"PostGIS","text":"<p>PostGIS adds geospatial data types, indexes, and functions - turning PostgreSQL into a full geographic information system. It is one of the most capable open-source geospatial databases available:</p> <pre><code>CREATE EXTENSION postgis;\n\n-- Store a point (longitude, latitude in WGS 84)\nCREATE TABLE locations (\n    id serial PRIMARY KEY,\n    name text,\n    geom geometry(Point, 4326)\n);\n\nINSERT INTO locations (name, geom)\nVALUES ('Office', ST_SetSRID(ST_MakePoint(-73.9857, 40.7484), 4326));\n\n-- Find locations within 5 km of a point\nSELECT name, ST_Distance(\n    geom::geography,\n    ST_SetSRID(ST_MakePoint(-73.9851, 40.7580), 4326)::geography\n) AS distance_meters\nFROM locations\nWHERE ST_DWithin(\n    geom::geography,\n    ST_SetSRID(ST_MakePoint(-73.9851, 40.7580), 4326)::geography,\n    5000\n);\n</code></pre>"},{"location":"Databases/postgresql-administration/#pg_trgm","title":"pg_trgm","text":"<p>pg_trgm provides trigram-based text similarity functions and index support. It is excellent for fuzzy text matching, \"did you mean?\" features, and typo-tolerant searches:</p> <pre><code>CREATE EXTENSION pg_trgm;\n\n-- Similarity score (0 to 1)\nSELECT similarity('PostgreSQL', 'PostgerSQL');  -- ~0.6\n\n-- Find similar product names\nSELECT name, similarity(name, 'PostgreSQ') AS sim\nFROM products\nWHERE similarity(name, 'PostgreSQ') &gt; 0.3\nORDER BY sim DESC;\n\n-- Create a GIN index for fast trigram searches\nCREATE INDEX idx_products_name_trgm\n    ON products USING gin (name gin_trgm_ops);\n\n-- Use LIKE/ILIKE with trigram index acceleration\nSELECT * FROM products WHERE name ILIKE '%postgres%';\n</code></pre> <p>Combine pg_trgm with full-text search</p> <p>Use <code>pg_trgm</code> for fuzzy matching on short strings (names, titles) and PostgreSQL's built-in full-text search (<code>tsvector</code>/<code>tsquery</code>) for document-length content. They complement each other well - trigrams catch typos and abbreviations that full-text search misses.</p> <p>PostgreSQL Admin Command Builder (requires JavaScript)</p>"},{"location":"Databases/postgresql-administration/#wal-management","title":"WAL Management","text":"<p>The Write-Ahead Log (WAL) is PostgreSQL's mechanism for ensuring data durability and crash recovery. The core principle: before any change is written to the actual data files, it is first written to the WAL. If PostgreSQL crashes, it replays the WAL on startup to recover any changes that were committed but not yet flushed to the data files.</p>"},{"location":"Databases/postgresql-administration/#how-wal-works","title":"How WAL Works","text":"<p>Every transaction that modifies data generates WAL records. These records are written sequentially to files in the <code>pg_wal/</code> directory (called <code>pg_xlog/</code> before PostgreSQL 10). The sequence of events:</p> <ol> <li>Transaction modifies data in shared buffers (memory)</li> <li>WAL records describing the changes are written to WAL buffers</li> <li>At commit, WAL buffers are flushed to WAL files on disk (<code>fsync</code>)</li> <li>The commit is acknowledged to the client</li> <li>Later, the background writer or checkpointer writes the actual data pages to disk</li> </ol> <p>Because WAL writes are sequential (append-only) and data file writes are random, this design converts expensive random I/O into cheap sequential I/O for the critical commit path. The actual data pages are written asynchronously in the background.</p>"},{"location":"Databases/postgresql-administration/#wal_level-settings","title":"wal_level Settings","text":"<p>The <code>wal_level</code> parameter controls how much information is written to WAL:</p> Level What it includes Use case <code>minimal</code> Only enough for crash recovery Standalone server with no replication or archiving <code>replica</code> Adds data needed for WAL archiving and replication Default since PostgreSQL 10; required for standby servers <code>logical</code> Adds data for logical decoding Required for logical replication, change data capture <pre><code># postgresql.conf\nwal_level = replica    # The safe default for most deployments\n</code></pre> <p>Changing <code>wal_level</code> requires a server restart.</p>"},{"location":"Databases/postgresql-administration/#wal-archiving","title":"WAL Archiving","text":"<p>WAL archiving copies completed WAL files to a safe location before they are recycled. Combined with a base backup, archived WAL files enable point-in-time recovery (PITR) - restoring a database to any moment in time.</p> <pre><code># postgresql.conf\narchive_mode = on\narchive_command = 'cp %p /var/lib/postgresql/wal_archive/%f'\n</code></pre> <p>The <code>%p</code> placeholder is the full path to the WAL file being archived. <code>%f</code> is just the filename. The <code>archive_command</code> must return exit code 0 on success - PostgreSQL will retry on failure and will not recycle a WAL file until it has been successfully archived.</p> <p>For production systems, use a more robust archive command that copies to remote storage:</p> <pre><code># Archive to S3\narchive_command = 'aws s3 cp %p s3://my-wal-archive/%f'\n\n# Archive with pgBackRest\narchive_command = 'pgbackrest --stanza=mydb archive-push %p'\n\n# Archive with Barman\narchive_command = 'barman-wal-archive backup-server mydb %p'\n</code></pre>"},{"location":"Databases/postgresql-administration/#wal-directory-management","title":"WAL Directory Management","text":"<p>WAL files are 16 MB each by default. Under heavy write load, the <code>pg_wal/</code> directory can grow significantly. Key parameters:</p> <pre><code># Soft target for total WAL size (checkpoints try to stay under this)\nmax_wal_size = 1GB\n\n# Minimum WAL to retain (even if not needed for recovery)\nmin_wal_size = 80MB\n</code></pre> <p>Monitor WAL directory size:</p> <pre><code>-- Current WAL LSN (Log Sequence Number) position\nSELECT pg_current_wal_lsn();\n\n-- Size of pg_wal directory\nSELECT pg_size_pretty(sum(size))\nFROM pg_ls_waldir();\n\n-- Number of WAL files\nSELECT count(*) FROM pg_ls_waldir();\n</code></pre> <p>Never manually delete files from pg_wal</p> <p>If <code>pg_wal/</code> is filling your disk, do not delete WAL files by hand. This can cause data loss or prevent crash recovery. Instead, investigate why WAL is accumulating: a failed <code>archive_command</code>, a replication slot preventing WAL recycling, or <code>max_wal_size</code> set too low. Fix the root cause, and PostgreSQL will recycle old WAL files automatically.</p>"},{"location":"Databases/postgresql-administration/#wal-and-replication-slots","title":"WAL and Replication Slots","text":"<p>Replication slots prevent PostgreSQL from recycling WAL files that a standby server or logical consumer still needs. Without slots, a slow standby might fall behind and find that the WAL it needs has been recycled - breaking replication.</p> <pre><code>-- Create a physical replication slot\nSELECT pg_create_physical_replication_slot('standby1');\n\n-- View replication slots and their WAL retention\nSELECT\n    slot_name,\n    slot_type,\n    active,\n    pg_size_pretty(\n        pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn)\n    ) AS retained_wal\nFROM pg_replication_slots;\n\n-- Drop a slot that's no longer needed (releases retained WAL)\nSELECT pg_drop_replication_slot('standby1');\n</code></pre> <p>An inactive replication slot is one of the most common causes of <code>pg_wal/</code> growing unbounded. If a standby is decommissioned without dropping its slot, WAL accumulates indefinitely.</p>"},{"location":"Databases/postgresql-administration/#putting-it-together-diagnosing-bloat","title":"Putting It Together: Diagnosing Bloat","text":"<p>The topics in this guide are interconnected. Dead tuples accumulate because of MVCC. VACUUM cleans them up. <code>pg_stat_user_tables</code> tells you whether VACUUM is keeping up. Long-running transactions (visible in <code>pg_stat_activity</code>) prevent VACUUM from reclaiming tuples. <code>pg_stat_statements</code> reveals the queries causing the most churn. Understanding these relationships is the core of PostgreSQL administration.</p> <p>Diagnose Table Bloat and Plan a VACUUM Strategy (requires JavaScript)</p>"},{"location":"Databases/postgresql-administration/#further-reading","title":"Further Reading","text":"<ul> <li>PostgreSQL Documentation: Database Roles - official reference for role creation, membership, and privilege management</li> <li>PostgreSQL Documentation: Routine Vacuuming - comprehensive coverage of VACUUM, autovacuum, and transaction ID wraparound prevention</li> <li>PostgreSQL Documentation: Monitoring Database Activity - full reference for all <code>pg_stat_*</code> views and what each column means</li> <li>PostgreSQL Documentation: Write-Ahead Logging - WAL internals, configuration, and reliability guarantees</li> <li>pg_stat_statements Documentation - setup, configuration, and query performance analysis</li> <li>PostgreSQL Wiki: Don't Do This - common PostgreSQL anti-patterns and mistakes to avoid</li> </ul> <p>Previous: PostgreSQL Fundamentals | Next: PostgreSQL Advanced Features | Back to Index</p>"},{"location":"Databases/postgresql-advanced/","title":"PostgreSQL Advanced Features","text":"<p>PostgreSQL extends well beyond standard SQL. Features like recursive CTEs, window functions, native JSON support, and built-in full-text search let you solve problems at the database layer that would otherwise require application code, external search engines, or additional infrastructure. This guide covers the capabilities that make PostgreSQL a practical choice for complex workloads.</p>"},{"location":"Databases/postgresql-advanced/#common-table-expressions-ctes","title":"Common Table Expressions (CTEs)","text":"<p>A Common Table Expression (CTE) is a named temporary result set defined with the <code>WITH</code> clause. It exists only for the duration of a single query, and you reference it by name like a table.</p> <pre><code>WITH active_customers AS (\n    SELECT customer_id, name, email\n    FROM customers\n    WHERE status = 'active'\n)\nSELECT ac.name, o.total\nFROM active_customers ac\nJOIN orders o ON o.customer_id = ac.customer_id\nWHERE o.total &gt; 100;\n</code></pre> <p>CTEs improve readability when a query has multiple logical steps. You can chain them:</p> <pre><code>WITH monthly_totals AS (\n    SELECT customer_id,\n           DATE_TRUNC('month', order_date) AS month,\n           SUM(total) AS month_total\n    FROM orders\n    GROUP BY customer_id, DATE_TRUNC('month', order_date)\n),\nranked AS (\n    SELECT customer_id, month, month_total,\n           RANK() OVER (PARTITION BY month ORDER BY month_total DESC) AS rank\n    FROM monthly_totals\n)\nSELECT * FROM ranked WHERE rank &lt;= 10;\n</code></pre>"},{"location":"Databases/postgresql-advanced/#recursive-ctes","title":"Recursive CTEs","text":"<p>Recursive CTEs use <code>WITH RECURSIVE</code> to traverse hierarchical data - org charts, threaded comments, file trees, bill-of-materials structures. The query has two parts: a base case (the anchor) and a recursive step that references the CTE itself.</p> <pre><code>WITH RECURSIVE org_chart AS (\n    -- Base case: top-level managers (no manager_id)\n    SELECT employee_id, name, manager_id, 1 AS depth\n    FROM employees\n    WHERE manager_id IS NULL\n\n    UNION ALL\n\n    -- Recursive step: find reports of each person found so far\n    SELECT e.employee_id, e.name, e.manager_id, oc.depth + 1\n    FROM employees e\n    JOIN org_chart oc ON e.manager_id = oc.employee_id\n)\nSELECT depth, name, employee_id, manager_id\nFROM org_chart\nORDER BY depth, name;\n</code></pre> <p>PostgreSQL executes this iteratively: it runs the base case first, then repeatedly executes the recursive step using the previous iteration's results until no new rows are produced.</p> <p>Recursive CTE - Building a Category Tree (requires JavaScript)</p> <p>Infinite recursion</p> <p>If your data has cycles (a row that eventually points back to itself), a recursive CTE will run forever. Add a depth limit (<code>WHERE ct.depth &lt; 20</code>) in the recursive step, or use <code>CYCLE</code> detection (PostgreSQL 14+): <code>CYCLE id SET is_cycle USING path</code>.</p>"},{"location":"Databases/postgresql-advanced/#materialized-vs-non-materialized-ctes","title":"Materialized vs Non-Materialized CTEs","text":"<p>Before PostgreSQL 12, CTEs were always materialized - PostgreSQL executed the CTE once, stored the full result, and scanned it for each reference. This prevented the optimizer from pushing predicates into the CTE.</p> <p>PostgreSQL 12+ defaults to inlining (non-materializing) CTEs that are referenced only once, treating them like subqueries. You can control this explicitly:</p> <pre><code>-- Force materialization (useful when the CTE is referenced multiple times)\nWITH expensive_query AS MATERIALIZED (\n    SELECT * FROM large_table WHERE complex_condition\n)\nSELECT * FROM expensive_query WHERE id = 5;\n\n-- Force inlining (lets the planner push filters down)\nWITH simple_filter AS NOT MATERIALIZED (\n    SELECT * FROM orders WHERE status = 'shipped'\n)\nSELECT * FROM simple_filter WHERE customer_id = 42;\n</code></pre> <p>Use <code>MATERIALIZED</code> when the CTE result is reused multiple times. Use <code>NOT MATERIALIZED</code> (or just let the planner decide) when the CTE is referenced once and you want predicate pushdown.</p> <p>What happens if a recursive CTE's data contains a cycle (e.g., employee A reports to B, B reports to C, C reports to A)? (requires JavaScript)</p>"},{"location":"Databases/postgresql-advanced/#window-functions","title":"Window Functions","text":"<p>Window functions perform calculations across a set of rows related to the current row without collapsing them into a single output row. Unlike <code>GROUP BY</code>, which reduces rows, window functions preserve every row and add computed columns.</p> <p>The core syntax is:</p> <pre><code>function_name() OVER (\n    PARTITION BY column     -- divide rows into groups\n    ORDER BY column         -- order within each group\n    frame_specification     -- which rows to include in the calculation\n)\n</code></pre>"},{"location":"Databases/postgresql-advanced/#ranking-functions","title":"Ranking Functions","text":"<p><code>ROW_NUMBER()</code>, <code>RANK()</code>, and <code>DENSE_RANK()</code> assign positions within partitions:</p> <pre><code>SELECT\n    department,\n    employee_name,\n    salary,\n    ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) AS row_num,\n    RANK()       OVER (PARTITION BY department ORDER BY salary DESC) AS rank,\n    DENSE_RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS dense_rank\nFROM employees;\n</code></pre> department employee_name salary row_num rank dense_rank Engineering Alice 150000 1 1 1 Engineering Bob 150000 2 1 1 Engineering Carol 130000 3 3 2 Sales Dave 120000 1 1 1 Sales Eve 110000 2 2 2 <p>The differences: <code>ROW_NUMBER()</code> always assigns unique sequential numbers (ties get arbitrary ordering). <code>RANK()</code> gives tied rows the same rank but skips subsequent numbers (1, 1, 3). <code>DENSE_RANK()</code> gives tied rows the same rank without gaps (1, 1, 2).</p>"},{"location":"Databases/postgresql-advanced/#lag-and-lead","title":"LAG() and LEAD()","text":"<p><code>LAG()</code> and <code>LEAD()</code> access values from previous and subsequent rows:</p> <pre><code>SELECT\n    order_date,\n    total,\n    LAG(total)  OVER (ORDER BY order_date) AS prev_total,\n    LEAD(total) OVER (ORDER BY order_date) AS next_total,\n    total - LAG(total) OVER (ORDER BY order_date) AS change\nFROM orders\nWHERE customer_id = 42;\n</code></pre> <p>This gives you row-by-row comparisons without self-joins. <code>LAG(total, 2)</code> looks two rows back. <code>LAG(total, 1, 0)</code> specifies a default value of 0 when there is no previous row.</p>"},{"location":"Databases/postgresql-advanced/#frame-specification-and-running-totals","title":"Frame Specification and Running Totals","text":"<p>The frame specification controls which rows the window function considers relative to the current row:</p> <pre><code>-- Running total (all rows from start to current row)\nSELECT\n    order_date,\n    total,\n    SUM(total) OVER (ORDER BY order_date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS running_total\nFROM orders;\n\n-- 3-day moving average\nSELECT\n    order_date,\n    total,\n    AVG(total) OVER (ORDER BY order_date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS moving_avg_3\nFROM orders;\n\n-- Percentage of department total\nSELECT\n    department,\n    employee_name,\n    salary,\n    ROUND(salary * 100.0 / SUM(salary) OVER (PARTITION BY department), 1) AS pct_of_dept\nFROM employees;\n</code></pre> <p>Common frame options:</p> Frame Meaning <code>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW</code> All rows from start to current (default with <code>ORDER BY</code>) <code>ROWS BETWEEN 2 PRECEDING AND CURRENT ROW</code> Current row plus the two before it <code>ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING</code> Current row to the end <code>ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING</code> All rows in the partition <p>Invalid interactive component configuration (terminal)</p> <p>What is the difference between RANK() and DENSE_RANK() when two rows tie? (requires JavaScript)</p>"},{"location":"Databases/postgresql-advanced/#jsonb","title":"JSONB","text":"<p>PostgreSQL's JSONB type stores JSON data in a decomposed binary format. Unlike the <code>JSON</code> type (which stores the exact text), JSONB is parsed on input, supports indexing, and allows efficient querying of nested structures.</p>"},{"location":"Databases/postgresql-advanced/#operators","title":"Operators","text":"<pre><code>-- Create a table with JSONB\nCREATE TABLE products (\n    id SERIAL PRIMARY KEY,\n    name TEXT,\n    attributes JSONB\n);\n\nINSERT INTO products (name, attributes) VALUES\n('Laptop', '{\"brand\": \"ThinkPad\", \"specs\": {\"ram\": 16, \"storage\": 512}, \"tags\": [\"business\", \"portable\"]}'),\n('Monitor', '{\"brand\": \"Dell\", \"specs\": {\"size\": 27, \"resolution\": \"4K\"}, \"tags\": [\"display\"]}');\n</code></pre> <p>The key operators:</p> Operator Returns Example Result <code>-&gt;</code> JSONB element <code>attributes -&gt; 'brand'</code> <code>\"ThinkPad\"</code> (JSONB) <code>-&gt;&gt;</code> Text value <code>attributes -&gt;&gt; 'brand'</code> <code>ThinkPad</code> (text) <code>#&gt;</code> JSONB at path <code>attributes #&gt; '{specs,ram}'</code> <code>16</code> (JSONB) <code>#&gt;&gt;</code> Text at path <code>attributes #&gt;&gt; '{specs,ram}'</code> <code>16</code> (text) <code>@&gt;</code> Contains <code>attributes @&gt; '{\"brand\":\"Dell\"}'</code> <code>true</code> <code>?</code> Key exists <code>attributes ? 'brand'</code> <code>true</code> <code>?|</code> Any key exists <code>attributes ?| array['brand','color']</code> <code>true</code> <code>?&amp;</code> All keys exist <code>attributes ?&amp; array['brand','color']</code> <code>false</code> <pre><code>-- Find products with 16GB RAM\nSELECT name FROM products\nWHERE attributes #&gt;&gt; '{specs,ram}' = '16';\n\n-- Find products that contain a specific key-value pair\nSELECT name FROM products\nWHERE attributes @&gt; '{\"brand\": \"Dell\"}';\n\n-- Find products tagged as \"business\"\nSELECT name FROM products\nWHERE attributes -&gt; 'tags' ? 'business';\n</code></pre>"},{"location":"Databases/postgresql-advanced/#jsonb-functions","title":"JSONB Functions","text":"<p><code>jsonb_each()</code> and <code>jsonb_array_elements()</code> expand JSONB into rows:</p> <pre><code>-- Expand top-level keys into rows\nSELECT p.name, kv.key, kv.value\nFROM products p, jsonb_each(p.attributes) AS kv\nWHERE kv.key != 'tags';\n\n-- Expand array elements\nSELECT p.name, tag\nFROM products p, jsonb_array_elements_text(p.attributes -&gt; 'tags') AS tag;\n</code></pre>"},{"location":"Databases/postgresql-advanced/#indexing-jsonb-with-gin","title":"Indexing JSONB with GIN","text":"<p>A GIN index (Generalized Inverted Index) on a JSONB column indexes every key and value in the document, making containment (<code>@&gt;</code>) and existence (<code>?</code>) queries fast:</p> <pre><code>CREATE INDEX idx_product_attrs ON products USING GIN (attributes);\n\n-- These queries use the GIN index\nSELECT * FROM products WHERE attributes @&gt; '{\"brand\": \"Dell\"}';\nSELECT * FROM products WHERE attributes ? 'specs';\n</code></pre> <p>For queries on a specific key path, a btree index on an expression is more efficient than a full GIN index:</p> <pre><code>CREATE INDEX idx_product_brand ON products ((attributes -&gt;&gt; 'brand'));\n\n-- Uses the btree expression index\nSELECT * FROM products WHERE attributes -&gt;&gt; 'brand' = 'ThinkPad';\n</code></pre> <p>JSONB vs normalized tables</p> <p>Use JSONB for truly variable attributes - product specifications, user preferences, API responses where the schema differs per record. If you find yourself querying the same JSONB keys repeatedly with <code>WHERE</code> clauses, those keys should probably be proper columns. JSONB is flexible but slower to query than indexed columns on normalized tables.</p> <p>Invalid interactive component configuration (terminal)</p>"},{"location":"Databases/postgresql-advanced/#table-partitioning","title":"Table Partitioning","text":"<p>Table partitioning splits a large table into smaller physical pieces (partitions) while presenting a single logical table to queries. PostgreSQL supports declarative partitioning (10+), which handles partition routing and constraint enforcement automatically.</p>"},{"location":"Databases/postgresql-advanced/#partition-strategies","title":"Partition Strategies","text":"<p>PostgreSQL supports three partitioning methods:</p> <ul> <li>RANGE - partition by value ranges (dates, IDs). The most common strategy for time-series data.</li> <li>LIST - partition by discrete values (region, status, category).</li> <li>HASH - distribute rows evenly across partitions using a hash function.</li> </ul>"},{"location":"Databases/postgresql-advanced/#range-partitioning-example","title":"Range Partitioning Example","text":"<pre><code>-- Create the partitioned parent table\nCREATE TABLE sensor_readings (\n    sensor_id   INTEGER NOT NULL,\n    reading_time TIMESTAMPTZ NOT NULL,\n    value       DOUBLE PRECISION,\n    quality     TEXT\n) PARTITION BY RANGE (reading_time);\n\n-- Create monthly partitions\nCREATE TABLE sensor_readings_2024_01 PARTITION OF sensor_readings\n    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');\n\nCREATE TABLE sensor_readings_2024_02 PARTITION OF sensor_readings\n    FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');\n\nCREATE TABLE sensor_readings_2024_03 PARTITION OF sensor_readings\n    FOR VALUES FROM ('2024-03-01') TO ('2024-04-01');\n\n-- Create a default partition for data outside defined ranges\nCREATE TABLE sensor_readings_default PARTITION OF sensor_readings DEFAULT;\n</code></pre> <p>Queries that filter on <code>reading_time</code> automatically skip irrelevant partitions - this is partition pruning:</p> <pre><code>-- Only scans sensor_readings_2024_02\nSELECT AVG(value)\nFROM sensor_readings\nWHERE reading_time &gt;= '2024-02-01' AND reading_time &lt; '2024-03-01';\n\n-- EXPLAIN confirms partition pruning\nEXPLAIN SELECT * FROM sensor_readings WHERE reading_time = '2024-02-15';\n--  Append\n--    -&gt;  Seq Scan on sensor_readings_2024_02\n--          Filter: (reading_time = '2024-02-15')\n</code></pre>"},{"location":"Databases/postgresql-advanced/#list-partitioning","title":"List Partitioning","text":"<pre><code>CREATE TABLE orders (\n    id          SERIAL,\n    region      TEXT NOT NULL,\n    order_date  DATE NOT NULL,\n    total       NUMERIC\n) PARTITION BY LIST (region);\n\nCREATE TABLE orders_us PARTITION OF orders FOR VALUES IN ('us-east', 'us-west');\nCREATE TABLE orders_eu PARTITION OF orders FOR VALUES IN ('eu-west', 'eu-central');\nCREATE TABLE orders_apac PARTITION OF orders FOR VALUES IN ('ap-southeast', 'ap-northeast');\n</code></pre>"},{"location":"Databases/postgresql-advanced/#managing-partitions","title":"Managing Partitions","text":"<p>Partitions are regular tables. You can attach existing tables and detach partitions without downtime:</p> <pre><code>-- Create the new partition as a regular table\nCREATE TABLE sensor_readings_2024_04 (LIKE sensor_readings INCLUDING ALL);\n\n-- Load historical data into it (no partition routing overhead)\nCOPY sensor_readings_2024_04 FROM '/data/april_readings.csv' CSV HEADER;\n\n-- Attach it to the partitioned table\nALTER TABLE sensor_readings\n    ATTACH PARTITION sensor_readings_2024_04\n    FOR VALUES FROM ('2024-04-01') TO ('2024-05-01');\n\n-- Detach an old partition (for archiving or deletion)\nALTER TABLE sensor_readings\n    DETACH PARTITION sensor_readings_2024_01;\n</code></pre> <p>Indexes on partitioned tables</p> <p>When you create an index on the parent table, PostgreSQL automatically creates matching indexes on all existing partitions and any future partitions. Each partition's index is independent, which keeps index operations fast.</p> <pre><code>-- Creates an index on every partition\nCREATE INDEX idx_sensor_time ON sensor_readings (reading_time);\n</code></pre>"},{"location":"Databases/postgresql-advanced/#full-text-search","title":"Full-Text Search","text":"<p>PostgreSQL has built-in full-text search (FTS) that eliminates the need for external tools like Elasticsearch for many use cases. FTS uses two core types: <code>tsvector</code> (a processed document) and <code>tsquery</code> (a search query).</p>"},{"location":"Databases/postgresql-advanced/#tsvector-and-tsquery","title":"tsvector and tsquery","text":"<p>A tsvector is a sorted list of normalized words (lexemes) with position information. A tsquery is a search expression with boolean operators.</p> <pre><code>-- Convert text to tsvector\nSELECT to_tsvector('english', 'The quick brown foxes jumped over lazy dogs');\n-- 'brown':3 'dog':9 'fox':4 'jump':5 'lazi':8 'quick':2\n\n-- Convert text to tsquery\nSELECT to_tsquery('english', 'quick &amp; foxes');\n-- 'quick' &amp; 'fox'\n\n-- plainto_tsquery handles plain text (no operators needed)\nSELECT plainto_tsquery('english', 'quick brown fox');\n-- 'quick' &amp; 'brown' &amp; 'fox'\n</code></pre> <p>Notice that FTS normalizes words: \"foxes\" becomes \"fox\", \"jumped\" becomes \"jump\", \"lazy\" becomes \"lazi\". The <code>english</code> dictionary handles stemming, stop word removal (\"the\" is dropped), and case normalization.</p>"},{"location":"Databases/postgresql-advanced/#the-match-operator","title":"The @@ Match Operator","text":"<p>The <code>@@</code> operator matches a <code>tsvector</code> against a <code>tsquery</code>:</p> <pre><code>-- Search articles\nSELECT title, body\nFROM articles\nWHERE to_tsvector('english', title || ' ' || body) @@ to_tsquery('english', 'database &amp; replication');\n</code></pre>"},{"location":"Databases/postgresql-advanced/#stored-tsvector-columns-and-gin-indexes","title":"Stored tsvector Columns and GIN Indexes","text":"<p>Calling <code>to_tsvector()</code> on every query is expensive. Store the processed vector in a column and index it:</p> <pre><code>-- Add a tsvector column\nALTER TABLE articles ADD COLUMN search_vector tsvector;\n\n-- Populate it\nUPDATE articles\nSET search_vector = to_tsvector('english', COALESCE(title, '') || ' ' || COALESCE(body, ''));\n\n-- Keep it updated with a trigger\nCREATE FUNCTION articles_search_update() RETURNS trigger AS $$\nBEGIN\n    NEW.search_vector := to_tsvector('english', COALESCE(NEW.title, '') || ' ' || COALESCE(NEW.body, ''));\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER trg_articles_search\n    BEFORE INSERT OR UPDATE ON articles\n    FOR EACH ROW EXECUTE FUNCTION articles_search_update();\n\n-- Create a GIN index on the stored vector\nCREATE INDEX idx_articles_search ON articles USING GIN (search_vector);\n\n-- Fast searches using the index\nSELECT title\nFROM articles\nWHERE search_vector @@ to_tsquery('english', 'postgresql &amp; partitioning');\n</code></pre>"},{"location":"Databases/postgresql-advanced/#relevance-ranking","title":"Relevance Ranking","text":"<p><code>ts_rank()</code> scores how well a document matches a query, based on lexeme frequency and position:</p> <pre><code>SELECT\n    title,\n    ts_rank(search_vector, query) AS rank\nFROM\n    articles,\n    to_tsquery('english', 'replication | failover') AS query\nWHERE search_vector @@ query\nORDER BY rank DESC\nLIMIT 10;\n</code></pre> <p>You can weight different parts of a document by assigning labels (A, B, C, D) to <code>tsvector</code> components:</p> <pre><code>UPDATE articles\nSET search_vector =\n    setweight(to_tsvector('english', COALESCE(title, '')), 'A') ||\n    setweight(to_tsvector('english', COALESCE(body, '')), 'B');\n</code></pre> <p>Titles (weight A) now contribute more to the rank score than body text (weight B). The default weights are A=1.0, B=0.4, C=0.2, D=0.1.</p> <p>Why should you store a tsvector column and index it with GIN, rather than calling to_tsvector() inline in every query? (requires JavaScript)</p>"},{"location":"Databases/postgresql-advanced/#connection-pooling-with-pgbouncer","title":"Connection Pooling with PgBouncer","text":"<p>PostgreSQL forks a new OS process for every client connection. Each process consumes roughly 5-10 MB of RAM. At 500 connections, that is 2.5-5 GB dedicated just to connection overhead - and context switching between hundreds of processes degrades performance. Most of those connections are idle most of the time.</p> <p>PgBouncer sits between your application and PostgreSQL, maintaining a small pool of actual database connections and multiplexing client connections across them.</p>"},{"location":"Databases/postgresql-advanced/#pooling-modes","title":"Pooling Modes","text":"Mode Behavior Use case Transaction pooling Connection returned to pool after each transaction completes Most applications. Default choice. Session pooling Connection held until client disconnects Applications using session-level features (prepared statements, temp tables, <code>SET</code> commands) Statement pooling Connection returned after each statement Simple autocommit workloads only. Breaks multi-statement transactions. <p>Transaction pooling is the right choice for most web applications. A pool of 20-30 server connections can serve hundreds of application connections because each request only holds a connection for the duration of its transaction.</p>"},{"location":"Databases/postgresql-advanced/#basic-configuration","title":"Basic Configuration","text":"<p>The main configuration file is <code>pgbouncer.ini</code>:</p> <pre><code>[databases]\nmyapp = host=127.0.0.1 port=5432 dbname=myapp_production\n\n[pgbouncer]\nlisten_addr = 0.0.0.0\nlisten_port = 6432\nauth_type = md5\nauth_file = /etc/pgbouncer/userlist.txt\n\n; Pool sizing\npool_mode = transaction\ndefault_pool_size = 20\nmax_client_conn = 200\nmin_pool_size = 5\n\n; Timeouts\nserver_idle_timeout = 300\nclient_idle_timeout = 0\nquery_timeout = 30\n\n; Logging\nlog_connections = 1\nlog_disconnections = 1\n</code></pre> <p>The <code>userlist.txt</code> file maps usernames to passwords:</p> <pre><code>\"myapp_user\" \"md5encrypted_password_here\"\n</code></pre> <p>Applications connect to PgBouncer on port 6432 instead of PostgreSQL on port 5432. No application code changes are needed beyond updating the connection port.</p> <p>Transaction pooling limitations</p> <p>In transaction pooling mode, session-level state does not persist between transactions. This breaks: prepared statements (use <code>DEALLOCATE ALL</code> after each transaction or enable <code>max_prepared_statements</code> in PgBouncer 1.21+), <code>SET</code> commands, <code>LISTEN</code>/<code>NOTIFY</code>, advisory locks, and temporary tables. If your application relies on these, use session pooling mode or refactor the application.</p>"},{"location":"Databases/postgresql-advanced/#foreign-data-wrappers","title":"Foreign Data Wrappers","text":"<p>Foreign Data Wrappers (FDWs) let you query external data sources - other PostgreSQL servers, CSV files, MySQL databases, REST APIs - as if they were local tables. PostgreSQL handles the connection, query translation, and data transfer.</p>"},{"location":"Databases/postgresql-advanced/#postgres_fdw-cross-server-queries","title":"postgres_fdw: Cross-Server Queries","text":"<p><code>postgres_fdw</code> connects to other PostgreSQL instances. This is useful for querying reporting replicas, accessing data in separate microservice databases, or migrating data between servers.</p> <pre><code>-- Install the extension\nCREATE EXTENSION postgres_fdw;\n\n-- Define the remote server\nCREATE SERVER reporting_server\n    FOREIGN DATA WRAPPER postgres_fdw\n    OPTIONS (host 'reporting.internal', port '5432', dbname 'analytics');\n\n-- Map local user to remote credentials\nCREATE USER MAPPING FOR app_user\n    SERVER reporting_server\n    OPTIONS (user 'readonly', password 'secret');\n\n-- Create a foreign table that mirrors the remote table\nCREATE FOREIGN TABLE remote_daily_stats (\n    stat_date   DATE,\n    page_views  BIGINT,\n    unique_users BIGINT,\n    avg_session_seconds NUMERIC\n) SERVER reporting_server\n  OPTIONS (schema_name 'public', table_name 'daily_stats');\n\n-- Query it like a local table\nSELECT stat_date, page_views\nFROM remote_daily_stats\nWHERE stat_date &gt;= CURRENT_DATE - INTERVAL '7 days';\n</code></pre> <p>You can join local and foreign tables in the same query. PostgreSQL pushes down <code>WHERE</code> clauses, <code>ORDER BY</code>, and aggregates to the remote server when possible, minimizing data transfer.</p> <pre><code>-- Import all tables from a remote schema at once\nIMPORT FOREIGN SCHEMA public\n    FROM SERVER reporting_server\n    INTO local_reporting;\n</code></pre>"},{"location":"Databases/postgresql-advanced/#file_fdw-querying-csv-files","title":"file_fdw: Querying CSV Files","text":"<p><code>file_fdw</code> reads flat files as tables, which is useful for log analysis or importing data without <code>COPY</code>:</p> <pre><code>CREATE EXTENSION file_fdw;\n\nCREATE SERVER csv_files FOREIGN DATA WRAPPER file_fdw;\n\nCREATE FOREIGN TABLE access_log (\n    ip          TEXT,\n    request_time TIMESTAMPTZ,\n    method      TEXT,\n    path        TEXT,\n    status      INTEGER,\n    bytes       BIGINT\n) SERVER csv_files\n  OPTIONS (filename '/var/log/app/access.csv', format 'csv', header 'true');\n\n-- Query the CSV file directly with SQL\nSELECT ip, COUNT(*) AS requests\nFROM access_log\nWHERE status &gt;= 500\nGROUP BY ip\nORDER BY requests DESC\nLIMIT 10;\n</code></pre> <p>Security with FDWs</p> <p>User mappings store credentials in the <code>pg_user_mappings</code> catalog. Only superusers and the mapped user can see the password. Use <code>pg_read_server_files</code> role to control who can access <code>file_fdw</code>. In production, store FDW credentials in a secrets manager and rotate them regularly.</p>"},{"location":"Databases/postgresql-advanced/#practical-exercise","title":"Practical Exercise","text":"<p>Building an Analytics Query with Advanced Features (requires JavaScript)</p>"},{"location":"Databases/postgresql-advanced/#further-reading","title":"Further Reading","text":"<ul> <li>PostgreSQL Documentation: WITH Queries (CTEs) - official reference for CTEs including recursive queries</li> <li>PostgreSQL Documentation: Window Functions - tutorial and reference for window function syntax and frame specifications</li> <li>PostgreSQL Documentation: JSON Types - JSONB types, operators, functions, and indexing strategies</li> <li>PostgreSQL Documentation: Table Partitioning - declarative partitioning, partition pruning, and management operations</li> <li>PostgreSQL Documentation: Full Text Search - tsvector, tsquery, ranking, dictionaries, and configuration</li> <li>PgBouncer Documentation - configuration reference for connection pooling modes, limits, and authentication</li> </ul> <p>Previous: PostgreSQL Administration | Next: NoSQL Concepts &amp; Architecture | Back to Index</p>"},{"location":"Databases/postgresql-fundamentals/","title":"PostgreSQL Fundamentals","text":"<p>PostgreSQL began in 1986 as the POSTGRES project at UC Berkeley, led by Michael Stonebraker. The original goal was to push beyond the limitations of existing relational databases by adding support for complex data types, user-defined functions, and extensible type systems. In 1996, the project adopted SQL as its query language and was renamed PostgreSQL to reflect that change. The project has been community-driven ever since, with no single corporate owner.</p> <p>Three design principles distinguish PostgreSQL from other relational databases:</p> <ul> <li>Object-relational design - tables can inherit from other tables, custom types can be defined with operators and functions, and the type system is extensible at runtime</li> <li>Standards compliance - PostgreSQL implements more of the SQL standard than any other open-source database, including window functions, CTEs, lateral joins, and MERGE (added in version 15)</li> <li>Extensibility - you can add custom data types, operators, index methods, procedural languages, and foreign data wrappers without modifying core source code</li> </ul> <p>PostgreSQL uses a multi-version concurrency control (MVCC) model that allows readers and writers to operate without blocking each other. Every transaction sees a consistent snapshot of the database, and old row versions are cleaned up later by the VACUUM process. This architecture trades some write overhead and storage for excellent concurrent read performance.</p>"},{"location":"Databases/postgresql-fundamentals/#installation","title":"Installation","text":""},{"location":"Databases/postgresql-fundamentals/#package-managers","title":"Package Managers","text":"<p>On Debian and Ubuntu, the official PostgreSQL repository provides the latest versions. Add the repository and install:</p> <pre><code># Add the PostgreSQL APT repository\nsudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" &gt; /etc/apt/sources.list.d/pgdg.list'\ncurl -fsSL https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/postgresql.gpg\nsudo apt update\nsudo apt install postgresql-16\n</code></pre> <p>On RHEL, Fedora, and Rocky Linux:</p> <pre><code>sudo dnf install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-9-x86_64/pgdg-redhat-repo-latest.noarch.rpm\nsudo dnf install -y postgresql16-server postgresql16\n</code></pre> <p>On macOS with Homebrew:</p> <pre><code>brew install postgresql@16\nbrew services start postgresql@16\n</code></pre>"},{"location":"Databases/postgresql-fundamentals/#cluster-initialization-with-initdb","title":"Cluster Initialization with initdb","text":"<p>On Linux, after installing the packages, you need to initialize a database cluster - the directory structure where PostgreSQL stores all its data. The <code>initdb</code> command creates this structure:</p> <pre><code># RHEL/Fedora (the Debian package runs initdb automatically)\nsudo /usr/pgsql-16/bin/postgresql-16-setup initdb\n</code></pre> <p>Under the hood, <code>initdb</code> creates:</p> <ul> <li>The <code>base/</code> directory for database files</li> <li>The <code>global/</code> directory for cluster-wide tables</li> <li>Default configuration files (<code>postgresql.conf</code>, <code>pg_hba.conf</code>, <code>pg_ident.conf</code>)</li> <li>The <code>template0</code> and <code>template1</code> template databases used as blueprints when you create new databases</li> <li>The initial <code>postgres</code> database and <code>postgres</code> superuser role</li> </ul> <p>Data directory location</p> <p>The default data directory varies by platform: <code>/var/lib/pgsql/16/data</code> on RHEL, <code>/var/lib/postgresql/16/main</code> on Debian/Ubuntu. Homebrew uses <code>/opt/homebrew/var/postgresql@16</code> on Apple Silicon. You can always find it with <code>SHOW data_directory;</code> from within <code>psql</code>.</p>"},{"location":"Databases/postgresql-fundamentals/#starting-and-stopping","title":"Starting and Stopping","text":"<p>Use <code>systemctl</code> on Linux systems with systemd:</p> <pre><code>sudo systemctl start postgresql-16\nsudo systemctl enable postgresql-16\nsudo systemctl status postgresql-16\n</code></pre> <p>For manual control, <code>pg_ctl</code> operates directly on the data directory:</p> <pre><code>pg_ctl -D /var/lib/pgsql/16/data start\npg_ctl -D /var/lib/pgsql/16/data stop -m fast\npg_ctl -D /var/lib/pgsql/16/data restart\npg_ctl -D /var/lib/pgsql/16/data status\n</code></pre> <p>The <code>-m fast</code> flag sends SIGTERM to active connections and shuts down without waiting for clients to disconnect. The alternative <code>-m smart</code> (default) waits for all connections to close on their own, and <code>-m immediate</code> simulates a crash recovery scenario.</p> <p>Invalid interactive component configuration (terminal)</p> <p>What does initdb create when initializing a PostgreSQL cluster? (requires JavaScript)</p>"},{"location":"Databases/postgresql-fundamentals/#the-psql-cli","title":"The psql CLI","text":"<p>psql is the interactive terminal for PostgreSQL. It supports SQL execution, meta-commands (backslash commands that query system catalogs), tab completion, command history, and scripting.</p>"},{"location":"Databases/postgresql-fundamentals/#connecting","title":"Connecting","text":"<pre><code># Connect as the postgres superuser to the postgres database\nsudo -u postgres psql\n\n# Connect to a specific database as a specific user\npsql -h localhost -p 5432 -U myuser -d mydb\n\n# Connection string format\npsql \"host=db.example.com port=5432 dbname=mydb user=myuser sslmode=require\"\n\n# Switch databases within psql\n\\c mydb\n</code></pre> <p>The <code>\\c</code> (or <code>\\connect</code>) meta-command switches to a different database without leaving the <code>psql</code> session. You can also change the user at the same time with <code>\\c dbname username</code>.</p>"},{"location":"Databases/postgresql-fundamentals/#essential-meta-commands","title":"Essential Meta-Commands","text":"<p>Meta-commands start with a backslash and are processed by <code>psql</code> itself, not sent to the server. These are the ones you will use daily:</p> Command Description <code>\\l</code> List all databases <code>\\dt</code> List tables in the current schema <code>\\dt+</code> List tables with size and description <code>\\d tablename</code> Describe a table (columns, types, indexes, constraints) <code>\\di</code> List indexes <code>\\du</code> List roles (users and groups) <code>\\df</code> List functions <code>\\dn</code> List schemas <code>\\x</code> Toggle expanded display (vertical output for wide rows) <code>\\timing</code> Toggle query execution time display <code>\\e</code> Open the last query in your <code>$EDITOR</code> <code>\\i filename</code> Execute commands from a file <code>\\o filename</code> Send query output to a file <code>\\!</code> Execute a shell command <p>Pattern filtering</p> <p>Most meta-commands accept a pattern argument. <code>\\dt public.*</code> lists tables in the <code>public</code> schema only. <code>\\df *json*</code> lists functions with \"json\" in the name. The pattern follows SQL LIKE syntax.</p>"},{"location":"Databases/postgresql-fundamentals/#copy-vs-copy","title":"COPY vs \\copy","text":"<p><code>COPY</code> is a server-side SQL command that reads and writes files on the server's filesystem, running as the <code>postgres</code> OS user:</p> <pre><code>COPY customers TO '/tmp/customers.csv' WITH (FORMAT csv, HEADER);\nCOPY customers FROM '/tmp/import.csv' WITH (FORMAT csv, HEADER);\n</code></pre> <p><code>\\copy</code> is a <code>psql</code> meta-command that transfers data between the server and the local client machine:</p> <pre><code>\\copy customers TO 'customers.csv' WITH (FORMAT csv, HEADER)\n\\copy customers FROM 'import.csv' WITH (FORMAT csv, HEADER)\n</code></pre> <p>Use <code>\\copy</code> when you are connecting to a remote server and need files on your local machine, or when the <code>postgres</code> user does not have filesystem access to the target path. Use <code>COPY</code> for server-local bulk operations - it is faster because data does not travel over the client connection.</p>"},{"location":"Databases/postgresql-fundamentals/#customizing-with-psqlrc","title":"Customizing with .psqlrc","text":"<p>Create a <code>~/.psqlrc</code> file to customize your <code>psql</code> sessions:</p> <pre><code>-- Quiet mode during startup\n\\set QUIET 1\n\n-- Better NULL display\n\\pset null '(null)'\n\n-- Show query timing\n\\timing\n\n-- Expanded auto mode (vertical display when rows are wide)\n\\x auto\n\n-- History per database\n\\set HISTFILE ~/.psql_history-:DBNAME\n\n-- Verbose error messages\n\\set VERBOSITY verbose\n\n-- Prompt: user@host:port/dbname\n\\set PROMPT1 '%n@%M:%&gt;/%/ %# '\n\n-- Restore normal output\n\\set QUIET 0\n</code></pre> <p>Invalid interactive component configuration (terminal)</p> <p>What is the difference between COPY and \\copy in PostgreSQL? (requires JavaScript)</p>"},{"location":"Databases/postgresql-fundamentals/#configuring-postgresqlconf","title":"Configuring postgresql.conf","text":"<p>The main server configuration file is <code>postgresql.conf</code>, located in the data directory. Changes to most parameters require a server reload (<code>SELECT pg_reload_conf();</code> or <code>pg_ctl reload</code>), though some require a full restart.</p>"},{"location":"Databases/postgresql-fundamentals/#connection-settings","title":"Connection Settings","text":"<pre><code>listen_addresses = 'localhost'    # Which interfaces to listen on ('*' for all)\nport = 5432                       # TCP port\nmax_connections = 100             # Maximum concurrent connections\n</code></pre> <p><code>listen_addresses</code> controls which network interfaces PostgreSQL binds to. The default <code>localhost</code> only accepts local connections. Set it to <code>'*'</code> to listen on all interfaces, but always pair this with proper <code>pg_hba.conf</code> rules.</p> <p><code>max_connections</code> has a direct impact on memory usage because each connection consumes <code>work_mem</code> and other per-session resources. Connection poolers like PgBouncer allow hundreds of application connections to share a smaller pool of database connections.</p>"},{"location":"Databases/postgresql-fundamentals/#memory-settings","title":"Memory Settings","text":"<pre><code>shared_buffers = '256MB'          # Shared memory for caching data pages\nwork_mem = '4MB'                  # Per-operation memory for sorts and hashes\nmaintenance_work_mem = '64MB'     # Memory for VACUUM, CREATE INDEX, etc.\neffective_cache_size = '1GB'      # Planner's estimate of available OS cache\n</code></pre> <p>shared_buffers is the most important memory parameter. It controls how much memory PostgreSQL allocates for caching table and index data in shared memory. Start with 25% of total RAM and adjust based on your workload. Setting it too high wastes memory that the OS file cache could use more effectively.</p> <p>work_mem is allocated per sort or hash operation within a query - a complex query with multiple sorts can allocate <code>work_mem</code> multiple times. Keep this conservative (4-16 MB) for OLTP workloads with many connections. Increase it for analytical queries that need large in-memory sorts.</p> <p>maintenance_work_mem affects operations like <code>VACUUM</code>, <code>CREATE INDEX</code>, and <code>ALTER TABLE ADD FOREIGN KEY</code>. Setting this higher (256 MB - 1 GB) speeds up these maintenance operations. It only matters during maintenance, so higher values are safe.</p> <p>effective_cache_size does not allocate any memory. It tells the query planner how much memory is available for disk caching (shared_buffers plus OS file cache). Set it to approximately 75% of total RAM on a dedicated database server. A higher value makes the planner more likely to choose index scans over sequential scans.</p>"},{"location":"Databases/postgresql-fundamentals/#wal-settings","title":"WAL Settings","text":"<pre><code>wal_level = 'replica'             # Logging level: minimal, replica, or logical\n</code></pre> <p>Write-Ahead Logging (WAL) ensures crash recovery by writing changes to a log before applying them to data files. The <code>wal_level</code> parameter controls how much information is recorded:</p> <ul> <li><code>minimal</code> - enough for crash recovery only; no replication support</li> <li><code>replica</code> - adds information needed for streaming replication and point-in-time recovery (the default since PostgreSQL 10)</li> <li><code>logical</code> - adds information needed for logical decoding and logical replication</li> </ul> <p>WAL level changes require restart</p> <p>Changing <code>wal_level</code> requires a full server restart. Set it to <code>replica</code> from the start if you might ever need replication or point-in-time recovery. Upgrading from <code>minimal</code> later means a restart window.</p>"},{"location":"Databases/postgresql-fundamentals/#transaction-isolation-levels","title":"Transaction Isolation Levels","text":"<p>PostgreSQL's MVCC engine supports four transaction isolation levels, defined by the SQL standard. Each level determines which concurrency anomalies a transaction can observe. Higher isolation provides stronger consistency guarantees but may increase contention between concurrent transactions.</p> <p>The three anomalies are:</p> <ul> <li>Dirty read - a transaction reads data written by another transaction that has not yet committed. If that transaction rolls back, the reader saw data that never existed.</li> <li>Non-repeatable read - a transaction reads the same row twice and gets different values because another transaction modified and committed the row between the two reads.</li> <li>Phantom read - a transaction re-executes a query with a range condition and gets a different set of rows because another transaction inserted or deleted rows that match the condition.</li> </ul> <pre><code>flowchart TD\n    RU[\"Read Uncommitted&lt;br/&gt;(treated as Read Committed&lt;br/&gt;in PostgreSQL)\"] --&gt; DR[\"Dirty Reads\"]\n    RU --&gt; NR[\"Non-repeatable Reads\"]\n    RU --&gt; PH[\"Phantom Reads\"]\n\n    RC[\"Read Committed&lt;br/&gt;(default)\"] --&gt; NR\n    RC --&gt; PH\n\n    RR[\"Repeatable Read\"] --&gt; PH\n\n    SR[\"Serializable\"] --&gt; NONE[\"No Anomalies\"]</code></pre> <p>PostgreSQL does not actually implement Read Uncommitted - if you request it, you get Read Committed behavior instead. The default isolation level is Read Committed, which prevents dirty reads but allows non-repeatable reads and phantom reads. Most applications never need to change this.</p> <p>Set the isolation level per transaction:</p> <pre><code>BEGIN TRANSACTION ISOLATION LEVEL SERIALIZABLE;\n-- All reads within this transaction see a consistent snapshot\n-- Conflicting concurrent transactions will be rolled back\nCOMMIT;\n</code></pre> <p>Or set the default for all transactions on a session or role:</p> <pre><code>SET default_transaction_isolation = 'repeatable read';\nALTER ROLE app_user SET default_transaction_isolation = 'serializable';\n</code></pre> <p>Serializable isolation in PostgreSQL uses Serializable Snapshot Isolation (SSI), which detects dependency cycles between concurrent transactions and aborts one of them. Unlike traditional locking-based serializable implementations, SSI does not block reads - it detects conflicts at commit time. Applications using Serializable must be prepared to retry transactions that fail with a serialization error (<code>SQLSTATE 40001</code>).</p>"},{"location":"Databases/postgresql-fundamentals/#configuring-pg_hbaconf","title":"Configuring pg_hba.conf","text":"<p>The pg_hba.conf file (Host-Based Authentication) controls who can connect to the database, from where, and how they must authenticate. PostgreSQL evaluates rules top to bottom and uses the first matching rule. If no rule matches, the connection is rejected.</p>"},{"location":"Databases/postgresql-fundamentals/#record-format","title":"Record Format","text":"<p>Each line in <code>pg_hba.conf</code> has the format:</p> <pre><code>TYPE  DATABASE  USER  ADDRESS  METHOD\n</code></pre> Field Values TYPE <code>local</code> (Unix socket), <code>host</code> (TCP/IP with or without SSL), <code>hostssl</code> (SSL only), <code>hostnossl</code> (non-SSL only) DATABASE <code>all</code>, a database name, a comma-separated list, or <code>sameuser</code> USER <code>all</code>, a role name, a comma-separated list, or <code>+groupname</code> for group membership ADDRESS IP address with CIDR mask (for <code>host</code> types), omitted for <code>local</code> METHOD <code>trust</code>, <code>peer</code>, <code>md5</code>, <code>scram-sha-256</code>, <code>cert</code>, <code>reject</code>, and others"},{"location":"Databases/postgresql-fundamentals/#authentication-methods","title":"Authentication Methods","text":"<p>trust - allows the connection unconditionally without a password. Only appropriate for local development on single-user machines:</p> <pre><code>local   all   all   trust\n</code></pre> <p>peer - uses the operating system username to authenticate. The connecting OS user must match the PostgreSQL role name. This is the default for local Unix socket connections:</p> <pre><code>local   all   all   peer\n</code></pre> <p>md5 - requires an MD5-hashed password. Supported by all clients but less secure than <code>scram-sha-256</code>:</p> <pre><code>host    all   all   192.168.1.0/24   md5\n</code></pre> <p>scram-sha-256 - the strongest password-based authentication method. Uses the SCRAM-SHA-256 challenge-response protocol. Recommended for all password-authenticated connections:</p> <pre><code>host    all   all   10.0.0.0/8   scram-sha-256\n</code></pre> <p>cert - authenticates using client SSL certificates. The client must present a valid certificate signed by a trusted CA:</p> <pre><code>hostssl   all   all   0.0.0.0/0   cert\n</code></pre> <p>reject - unconditionally rejects the connection. Useful as a catch-all at the end of the file or to block specific networks:</p> <pre><code>host    all   all   0.0.0.0/0   reject\n</code></pre> <p>Order matters</p> <p>PostgreSQL evaluates pg_hba.conf rules top to bottom and uses the first match. A <code>trust</code> rule above a <code>scram-sha-256</code> rule will override the stronger method. Always place more restrictive rules before less restrictive ones, and end with a <code>reject</code> rule for defense in depth.</p> <p>Understanding pg_hba.conf (requires JavaScript)</p> <p>After editing <code>pg_hba.conf</code>, reload the configuration:</p> <pre><code>sudo -u postgres psql -c \"SELECT pg_reload_conf();\"\n# Or\npg_ctl -D /var/lib/pgsql/16/data reload\n</code></pre>"},{"location":"Databases/postgresql-fundamentals/#schemas-vs-databases","title":"Schemas vs Databases","text":"<p>PostgreSQL organizes objects in a three-level hierarchy: cluster &gt; database &gt; schema &gt; objects (tables, views, functions, indexes). Understanding this hierarchy is essential for organizing your data and managing access.</p>"},{"location":"Databases/postgresql-fundamentals/#the-catalog-hierarchy","title":"The Catalog Hierarchy","text":"<p>A cluster is the entire PostgreSQL instance managed by one <code>postmaster</code> process. It contains multiple databases, but cross-database queries are not possible in a single SQL statement (unlike MySQL, where you can <code>SELECT</code> across databases freely).</p> <p>A database is an isolated container of schemas and objects. Each connection targets exactly one database. You create databases with:</p> <pre><code>CREATE DATABASE analytics OWNER app_user;\n</code></pre> <p>A schema is a namespace within a database. It allows you to group related objects and control access at the namespace level. Multiple schemas can contain tables with the same name without conflict:</p> <pre><code>CREATE SCHEMA inventory;\nCREATE TABLE inventory.products (id serial PRIMARY KEY, name text);\n\nCREATE SCHEMA reporting;\nCREATE TABLE reporting.products (id serial PRIMARY KEY, name text, summary text);\n</code></pre>"},{"location":"Databases/postgresql-fundamentals/#the-public-schema","title":"The public Schema","text":"<p>Every new database contains a public schema. When you create objects without specifying a schema, they go into <code>public</code>. When you query without specifying a schema, PostgreSQL searches according to the search_path:</p> <pre><code>-- These are equivalent when search_path includes 'public'\nSELECT * FROM users;\nSELECT * FROM public.users;\n\n-- Check the current search path\nSHOW search_path;\n-- \"$user\", public\n\n-- Modify the search path for the session\nSET search_path TO myapp, public;\n</code></pre> <p>The default <code>search_path</code> is <code>\"$user\", public</code>. PostgreSQL first looks for a schema matching the current role name, then falls back to <code>public</code>. Setting a custom <code>search_path</code> at the role or database level organizes multi-tenant or multi-module applications:</p> <pre><code>-- Set a default search_path for a role\nALTER ROLE app_user SET search_path TO myapp, public;\n\n-- Set a default search_path for a database\nALTER DATABASE mydb SET search_path TO myapp, public;\n</code></pre>"},{"location":"Databases/postgresql-fundamentals/#when-to-use-schemas-vs-databases","title":"When to Use Schemas vs Databases","text":"Use Case Choose Fully isolated applications with different users and permissions Separate databases Logical grouping within one application (e.g., <code>auth</code>, <code>billing</code>, <code>inventory</code>) Schemas within one database Multi-tenant applications where tenants share the same table structure One schema per tenant Extensions that need to be shared (e.g., <code>pg_stat_statements</code>) Schemas within one database (extensions are per-database) <p>In PostgreSQL, what happens when you run CREATE TABLE products (...) without specifying a schema? (requires JavaScript)</p>"},{"location":"Databases/postgresql-fundamentals/#system-catalogs","title":"System Catalogs","text":"<p>PostgreSQL stores all metadata - table definitions, column types, indexes, functions, permissions, and statistics - in system catalogs. These are regular tables that you can query with SQL.</p>"},{"location":"Databases/postgresql-fundamentals/#the-pg_catalog-schema","title":"The pg_catalog Schema","text":"<p>Every database contains a <code>pg_catalog</code> schema with the catalog tables. The most commonly queried catalogs:</p> Catalog Contents <code>pg_class</code> All relations (tables, indexes, sequences, views) <code>pg_attribute</code> All columns of all relations <code>pg_index</code> Index definitions and properties <code>pg_namespace</code> Schemas <code>pg_roles</code> Roles (users and groups) <code>pg_database</code> Databases in the cluster <code>pg_proc</code> Functions and procedures <code>pg_type</code> Data types <code>pg_settings</code> Server configuration parameters (queryable view of <code>postgresql.conf</code>) <p>You can query these directly:</p> <pre><code>-- Find all tables in the public schema\nSELECT relname, relkind, reltuples::bigint AS row_estimate\nFROM pg_class\nWHERE relnamespace = 'public'::regnamespace\n  AND relkind = 'r'\nORDER BY reltuples DESC;\n\n-- Find all indexes on a table\nSELECT indexrelid::regclass AS index_name,\n       indisunique AS is_unique,\n       indisprimary AS is_primary\nFROM pg_index\nWHERE indrelid = 'users'::regclass;\n</code></pre>"},{"location":"Databases/postgresql-fundamentals/#statistical-views","title":"Statistical Views","text":"<p>PostgreSQL collects runtime statistics through the statistics collector. The <code>pg_stat_*</code> views expose this data:</p> <p>pg_stat_activity - shows every current connection, its query, and its state:</p> <pre><code>SELECT pid, usename, datname, state, query, query_start\nFROM pg_stat_activity\nWHERE state != 'idle'\nORDER BY query_start;\n</code></pre> <p>This is your first stop when diagnosing slow queries or connection issues. The <code>state</code> column shows <code>active</code> (running a query), <code>idle</code> (waiting for a command), <code>idle in transaction</code> (inside an open transaction but not executing), and <code>idle in transaction (aborted)</code>.</p> <p>Idle in transaction</p> <p>Connections in <code>idle in transaction</code> state hold locks and prevent VACUUM from cleaning up dead rows. If you see long-running idle-in-transaction sessions, investigate the application code. Set <code>idle_in_transaction_session_timeout</code> to automatically terminate sessions that sit idle inside a transaction too long.</p> <p>pg_stat_user_tables - per-table statistics including sequential scan counts, index scan counts, and dead tuple counts:</p> <pre><code>SELECT schemaname, relname,\n       seq_scan, seq_tup_read,\n       idx_scan, idx_tup_fetch,\n       n_tup_ins, n_tup_upd, n_tup_del,\n       n_dead_tup, last_autovacuum\nFROM pg_stat_user_tables\nORDER BY n_dead_tup DESC;\n</code></pre> <p>Tables with high <code>seq_scan</code> counts and low <code>idx_scan</code> counts may need better indexes. Tables with high <code>n_dead_tup</code> counts may need more aggressive autovacuum settings.</p>"},{"location":"Databases/postgresql-fundamentals/#the-information_schema","title":"The information_schema","text":"<p>PostgreSQL also provides the SQL-standard information_schema - a set of views that present catalog data in a standardized, portable format:</p> <pre><code>-- List all columns in a table (portable across databases)\nSELECT column_name, data_type, is_nullable, column_default\nFROM information_schema.columns\nWHERE table_schema = 'public'\n  AND table_name = 'users'\nORDER BY ordinal_position;\n\n-- List all tables\nSELECT table_schema, table_name\nFROM information_schema.tables\nWHERE table_schema NOT IN ('pg_catalog', 'information_schema')\nORDER BY table_schema, table_name;\n</code></pre> <p>The <code>information_schema</code> is portable across SQL databases, but it is slower than querying <code>pg_catalog</code> directly because it uses complex views. For PostgreSQL-specific metadata (like tablespace, storage parameters, or toast tables), you need the <code>pg_catalog</code> tables.</p> <p>PostgreSQL Catalog Exploration (requires JavaScript)</p>"},{"location":"Databases/postgresql-fundamentals/#further-reading","title":"Further Reading","text":"<ul> <li>PostgreSQL Official Documentation - the complete reference for every feature, function, and configuration parameter</li> <li>PostgreSQL Wiki - Tuning Your PostgreSQL Server - community-maintained tuning guidance with workload-specific recommendations</li> <li>pgTune - web-based configuration calculator that generates postgresql.conf settings based on your hardware</li> <li>The Internals of PostgreSQL - free online book covering MVCC, WAL, query processing, and buffer management internals</li> <li>pg_hba.conf Documentation - full reference for authentication configuration with examples for every method</li> </ul> <p>Previous: MySQL Replication &amp; High Availability | Next: PostgreSQL Administration | Back to Index</p>"},{"location":"Databases/redis/","title":"Redis","text":"<p>Redis is an in-memory data structure store that operates as a database, cache, message broker, and streaming engine. Unlike disk-based databases that optimize for storage capacity, Redis keeps all data in RAM and uses a single-threaded event loop to process commands sequentially - eliminating the need for locks and delivering sub-millisecond latency at hundreds of thousands of operations per second.</p> <p>Redis is not just a key-value store. It provides native data structures - strings, hashes, lists, sets, sorted sets, and streams - each with purpose-built commands that execute atomically. This means operations like \"increment a counter,\" \"add to a sorted leaderboard,\" or \"push onto a queue\" happen in a single command rather than a read-modify-write cycle.</p> <p>Redis Versions</p> <p>Redis 7.x is the current stable release. The core concepts in this guide apply across versions, but specific features like Redis Functions (replacing Lua <code>EVAL</code> in some use cases) arrived in Redis 7.0. Redis was relicensed under SSPL in 2024, which led to the Valkey fork under the Linux Foundation.</p>"},{"location":"Databases/redis/#data-structures","title":"Data Structures","text":"<p>Every value in Redis is stored under a string key. What makes Redis powerful is the variety of value types and the atomic operations available for each.</p>"},{"location":"Databases/redis/#strings","title":"Strings","text":"<p>The simplest type. A string in Redis can hold text, integers, floating-point numbers, or binary data up to 512 MB.</p> <pre><code># Basic set and get\nSET user:1:name \"Alice\"\nGET user:1:name\n# \"Alice\"\n\n# Atomic increment - no read-modify-write race condition\nSET page:views 0\nINCR page:views\nINCR page:views\nGET page:views\n# \"2\"\n\n# Append to existing value\nAPPEND user:1:name \" Chen\"\nGET user:1:name\n# \"Alice Chen\"\n\n# Set with expiration (seconds)\nSET session:abc123 '{\"user_id\": 1}' EX 3600\n\n# Set only if key does not exist (distributed lock primitive)\nSET lock:order:42 \"worker-1\" NX EX 30\n</code></pre> <p><code>INCR</code>, <code>DECR</code>, <code>INCRBY</code>, and <code>INCRBYFLOAT</code> all operate atomically. Two clients calling <code>INCR</code> on the same key simultaneously will never lose an update - the single-threaded model guarantees serial execution.</p>"},{"location":"Databases/redis/#hashes","title":"Hashes","text":"<p>A hash stores field-value pairs under a single key - similar to a row in a relational table or a JSON object. Hashes are memory-efficient for objects with many fields because Redis uses a compact encoding for small hashes.</p> <pre><code># Set individual fields\nHSET user:1 name \"Alice\" email \"alice@example.com\" role \"admin\"\n\n# Get a single field\nHGET user:1 email\n# \"alice@example.com\"\n\n# Get all fields and values\nHGETALL user:1\n# 1) \"name\"\n# 2) \"Alice\"\n# 3) \"email\"\n# 4) \"alice@example.com\"\n# 5) \"role\"\n# 6) \"admin\"\n\n# Atomic field increment\nHINCRBY user:1 login_count 1\n\n# Check field existence\nHEXISTS user:1 phone\n# (integer) 0\n</code></pre>"},{"location":"Databases/redis/#lists","title":"Lists","text":"<p>A list is a doubly-linked list of strings. Push and pop operations on both ends run in O(1), making lists ideal for queues, stacks, and recent-activity feeds.</p> <pre><code># Push to the left (head) and right (tail)\nLPUSH notifications:user:1 \"Order shipped\" \"Payment received\"\nRPUSH notifications:user:1 \"Review requested\"\n\n# Pop from the left\nLPOP notifications:user:1\n# \"Payment received\"\n\n# Range query (0-indexed, -1 means last element)\nLRANGE notifications:user:1 0 -1\n# 1) \"Order shipped\"\n# 2) \"Review requested\"\n\n# Blocking pop - waits up to 30 seconds for an element\nBRPOP task:queue 30\n</code></pre> <p><code>BRPOP</code> and <code>BLPOP</code> turn lists into reliable work queues. A worker calls <code>BRPOP</code> and blocks until a producer pushes a task.</p>"},{"location":"Databases/redis/#sets","title":"Sets","text":"<p>A set is an unordered collection of unique strings. Sets support membership tests, intersections, unions, and differences - all server-side.</p> <pre><code># Add members\nSADD tags:article:1 \"redis\" \"database\" \"caching\"\nSADD tags:article:2 \"redis\" \"performance\" \"caching\"\n\n# List all members\nSMEMBERS tags:article:1\n# 1) \"redis\"\n# 2) \"database\"\n# 3) \"caching\"\n\n# Intersection - tags shared by both articles\nSINTER tags:article:1 tags:article:2\n# 1) \"redis\"\n# 2) \"caching\"\n\n# Union - all tags across both articles\nSUNION tags:article:1 tags:article:2\n# 1) \"redis\"\n# 2) \"database\"\n# 3) \"caching\"\n# 4) \"performance\"\n\n# Membership test\nSISMEMBER tags:article:1 \"redis\"\n# (integer) 1\n</code></pre>"},{"location":"Databases/redis/#sorted-sets","title":"Sorted Sets","text":"<p>A sorted set associates each member with a floating-point score. Members are unique, and the set is ordered by score. This makes sorted sets the go-to structure for leaderboards, priority queues, and time-series indexes.</p> <pre><code># Add members with scores\nZADD leaderboard 1500 \"alice\" 1200 \"bob\" 1800 \"charlie\"\n\n# Range by rank (0-indexed, lowest score first)\nZRANGE leaderboard 0 -1 WITHSCORES\n# 1) \"bob\"\n# 2) \"1200\"\n# 3) \"alice\"\n# 4) \"1500\"\n# 5) \"charlie\"\n# 6) \"1800\"\n\n# Range by score\nZRANGEBYSCORE leaderboard 1300 1600\n# 1) \"alice\"\n\n# Rank of a member (0-indexed from lowest score)\nZRANK leaderboard \"charlie\"\n# (integer) 2\n\n# Reverse rank (highest score = rank 0)\nZREVRANK leaderboard \"charlie\"\n# (integer) 0\n\n# Increment a score atomically\nZINCRBY leaderboard 200 \"bob\"\n</code></pre>"},{"location":"Databases/redis/#streams","title":"Streams","text":"<p>A stream is an append-only log with consumer group support - Redis's answer to Kafka-style messaging. Each entry has an auto-generated ID based on the timestamp and a sequence number.</p> <pre><code># Append entries\nXADD events * action \"login\" user \"alice\"\n# \"1700000000000-0\"\nXADD events * action \"purchase\" user \"bob\" amount \"49.99\"\n# \"1700000000001-0\"\n\n# Read entries by range\nXRANGE events - +\n# Returns all entries from earliest (-) to latest (+)\n\n# Read new entries (blocking, consumer pattern)\nXREAD BLOCK 5000 STREAMS events $\n\n# Consumer groups for parallel processing\nXGROUP CREATE events analytics-group 0\nXREADGROUP GROUP analytics-group worker-1 COUNT 10 STREAMS events &gt;\nXACK events analytics-group \"1700000000000-0\"\n</code></pre> <p>Consumer groups allow multiple workers to divide stream entries among themselves. Each entry is delivered to exactly one consumer in the group, and <code>XACK</code> confirms processing - unacknowledged entries can be reclaimed by other consumers if a worker crashes.</p> <p>Which Redis data structure would you use to build a real-time leaderboard that ranks players by score? (requires JavaScript)</p> <p>Exploring Redis Data Structures (requires JavaScript)</p>"},{"location":"Databases/redis/#caching-patterns","title":"Caching Patterns","text":"<p>Redis is most commonly deployed as a cache layer between an application and a slower primary database. The pattern you choose determines how data flows between the three layers.</p>"},{"location":"Databases/redis/#cache-aside-lazy-loading","title":"Cache-Aside (Lazy Loading)","text":"<p>The application manages the cache explicitly. On a read, the application checks Redis first. On a miss, it queries the primary database and writes the result to Redis.</p> <pre><code>Read path:\n1. App checks Redis         \u2192 cache hit?  \u2192 return data\n2. Cache miss               \u2192 query database\n3. Write result to Redis    \u2192 return data\n</code></pre> <p>Advantages: only requested data is cached; the cache naturally fills with hot data. Disadvantage: the first request for any key always hits the database (cold start), and cached data can become stale if the database is updated independently.</p>"},{"location":"Databases/redis/#read-through","title":"Read-Through","text":"<p>The cache sits between the application and the database. The application always reads from the cache, and the cache itself fetches from the database on a miss.</p> <p>This simplifies application code but requires a cache layer that knows how to query the backing store - typically implemented with a caching library or proxy, not raw Redis commands.</p>"},{"location":"Databases/redis/#write-through","title":"Write-Through","text":"<p>Every write goes to both the cache and the database synchronously. This guarantees the cache is always consistent with the database at the cost of higher write latency.</p> <pre><code>Write path:\n1. App writes to Redis\n2. App writes to database (or cache layer writes to database)\n3. Both confirmed \u2192 return success\n</code></pre>"},{"location":"Databases/redis/#write-behind-write-back","title":"Write-Behind (Write-Back)","text":"<p>Writes go to Redis immediately, and a background process asynchronously flushes changes to the database. This gives the lowest write latency but risks data loss if Redis crashes before the flush completes.</p> <p>Choose Your Consistency Trade-off</p> <p>Cache-aside is the most common pattern because it balances simplicity with efficiency. Write-through adds consistency guarantees but increases write latency. Write-behind maximizes write speed but can lose data. Pick based on whether your system tolerates stale reads (cache-aside), slow writes (write-through), or potential data loss (write-behind).</p> <p>In the cache-aside pattern, what happens on a cache miss? (requires JavaScript)</p>"},{"location":"Databases/redis/#ttl-and-eviction","title":"TTL and Eviction","text":"<p>Redis runs in RAM, and RAM is finite. TTL (time-to-live) and eviction policies control what happens when memory fills up.</p>"},{"location":"Databases/redis/#setting-and-managing-ttl","title":"Setting and Managing TTL","text":"<pre><code># Set a key with a 60-second TTL\nSET session:abc \"data\" EX 60\n\n# Set TTL on an existing key\nEXPIRE user:cache:1 300\n\n# Check remaining TTL (seconds)\nTTL user:cache:1\n# (integer) 297\n\n# Check remaining TTL (milliseconds)\nPTTL user:cache:1\n# (integer) 297421\n\n# Remove expiration (key persists indefinitely)\nPERSIST user:cache:1\nTTL user:cache:1\n# (integer) -1  (no expiration)\n</code></pre> <p>A TTL of <code>-1</code> means the key has no expiration. A TTL of <code>-2</code> means the key does not exist.</p>"},{"location":"Databases/redis/#eviction-policies","title":"Eviction Policies","text":"<p>When Redis reaches <code>maxmemory</code>, it must decide what to remove. The <code>maxmemory-policy</code> setting controls this.</p> Policy Behavior <code>noeviction</code> Return errors on writes when memory is full. Reads still work. Default policy. <code>allkeys-lru</code> Evict the least recently used key across all keys. Best general-purpose caching policy. <code>volatile-lru</code> Evict the least recently used key among keys with an expiration set. <code>allkeys-random</code> Evict a random key. Simple but less efficient than LRU. <code>volatile-random</code> Evict a random key among keys with an expiration set. <code>volatile-ttl</code> Evict the key with the shortest remaining TTL. <code>allkeys-lfu</code> Evict the least frequently used key. Better than LRU for access patterns with popular items. <code>volatile-lfu</code> Evict the least frequently used key among keys with an expiration set. <pre><code># Set maximum memory to 256 MB\nCONFIG SET maxmemory 256mb\n\n# Set eviction policy\nCONFIG SET maxmemory-policy allkeys-lru\n\n# Verify current settings\nCONFIG GET maxmemory\nCONFIG GET maxmemory-policy\n</code></pre> <p>LRU vs LFU</p> <p>LRU (least recently used) evicts keys that have not been accessed recently - good for general caching. LFU (least frequently used) tracks access frequency and evicts keys that are rarely accessed - better when you have a small set of extremely popular keys that should never be evicted alongside a long tail of infrequent keys.</p>"},{"location":"Databases/redis/#pubsub","title":"Pub/Sub","text":"<p>Redis pub/sub provides fire-and-forget messaging. Publishers send messages to channels, and all subscribers listening on that channel receive the message in real time.</p> <pre><code># Terminal 1: Subscribe to a channel\nSUBSCRIBE alerts:system\n# Reading messages... (press Ctrl-C to quit)\n\n# Terminal 2: Publish a message\nPUBLISH alerts:system \"CPU usage above 90%\"\n# (integer) 1  (number of subscribers who received the message)\n\n# Pattern-based subscription (receive from all alert channels)\nPSUBSCRIBE alerts:*\n</code></pre>"},{"location":"Databases/redis/#use-cases-and-limitations","title":"Use Cases and Limitations","text":"<p>Pub/sub works well for real-time notifications, chat systems, and broadcasting configuration changes. However, it has significant limitations:</p> <ul> <li>No persistence: messages are not stored. If a subscriber is offline when a message is published, that message is lost forever.</li> <li>No replay: there is no way to read historical messages. Unlike streams, pub/sub has no concept of message IDs or consumer offsets.</li> <li>No acknowledgment: the publisher knows how many subscribers received the message but has no way to confirm processing.</li> <li>Scales with subscribers: every subscriber receives every message on the channel. There is no consumer-group-style load distribution.</li> </ul> <p>If you need persistent, replayable messaging with consumer groups, use Redis Streams instead of pub/sub.</p>"},{"location":"Databases/redis/#lua-scripting","title":"Lua Scripting","text":"<p>Redis executes Lua scripts atomically - the entire script runs without any other command being interleaved. This solves the problem of multi-step operations that would otherwise require a transaction or external locking.</p>"},{"location":"Databases/redis/#eval-basics","title":"EVAL Basics","text":"<pre><code># Simple script: get, increment, and return\nEVAL \"local val = redis.call('GET', KEYS[1]) or 0; redis.call('SET', KEYS[1], val + ARGV[1]); return val + ARGV[1]\" 1 mycounter 5\n</code></pre> <p>The <code>EVAL</code> command takes the script, the number of keys, the key names, and any additional arguments. Inside the script, <code>KEYS[1]</code> refers to the first key and <code>ARGV[1]</code> to the first argument.</p>"},{"location":"Databases/redis/#practical-example-rate-limiting","title":"Practical Example: Rate Limiting","text":"<p>A common use case is implementing a sliding-window rate limiter that checks and updates the counter in a single atomic operation:</p> <pre><code>EVAL \"\n  local key = KEYS[1]\n  local limit = tonumber(ARGV[1])\n  local window = tonumber(ARGV[2])\n  local current = tonumber(redis.call('GET', key) or 0)\n  if current &gt;= limit then\n    return 0\n  end\n  current = redis.call('INCR', key)\n  if current == 1 then\n    redis.call('EXPIRE', key, window)\n  end\n  return 1\n\" 1 ratelimit:api:user:42 100 60\n</code></pre> <p>This script checks if <code>user:42</code> has exceeded 100 requests in the current 60-second window. If not, it increments the counter and sets the TTL on the first request. The entire check-and-increment is atomic - no race condition between two concurrent requests.</p>"},{"location":"Databases/redis/#practical-example-conditional-update","title":"Practical Example: Conditional Update","text":"<p>Update a value only if the current value matches an expected value (compare-and-swap):</p> <pre><code>EVAL \"\n  local current = redis.call('GET', KEYS[1])\n  if current == ARGV[1] then\n    redis.call('SET', KEYS[1], ARGV[2])\n    return 1\n  end\n  return 0\n\" 1 config:feature_flag \"disabled\" \"enabled\"\n</code></pre> <p>Atomic Rate Limiting with Lua (requires JavaScript)</p> <p>Why is Lua scripting important for Redis operations like rate limiting? (requires JavaScript)</p>"},{"location":"Databases/redis/#persistence","title":"Persistence","text":"<p>Redis keeps data in memory, but that does not mean data disappears on restart. Redis offers two persistence mechanisms and a hybrid mode.</p>"},{"location":"Databases/redis/#rdb-snapshots","title":"RDB Snapshots","text":"<p>RDB (Redis Database) persistence creates point-in-time snapshots of the entire dataset. Redis forks the process and the child writes the snapshot to disk while the parent continues serving requests.</p> <pre><code># Trigger a snapshot manually (blocks until complete)\nSAVE\n\n# Trigger a background snapshot (non-blocking)\nBGSAVE\n\n# Check last successful save\nLASTSAVE\n</code></pre> <p>Configure automatic snapshots in <code>redis.conf</code>:</p> <pre><code># Save after 3600 seconds if at least 1 key changed\nsave 3600 1\n# Save after 300 seconds if at least 100 keys changed\nsave 300 100\n# Save after 60 seconds if at least 10000 keys changed\nsave 60 10000\n\n# Enable checksum verification on RDB load\nrdbchecksum yes\n\n# RDB filename\ndbfilename dump.rdb\n</code></pre> <p>Advantages: compact single-file backups, fast restarts. Disadvantage: you lose all changes since the last snapshot if Redis crashes.</p>"},{"location":"Databases/redis/#aof-append-only-file","title":"AOF (Append-Only File)","text":"<p>AOF logs every write operation. On restart, Redis replays the log to reconstruct the dataset.</p> <pre><code># Enable AOF\nappendonly yes\n\n# Sync policy\nappendfsync always    # Fsync after every write - safest, slowest\nappendfsync everysec  # Fsync once per second - good balance (default)\nappendfsync no        # Let the OS decide when to fsync - fastest, riskiest\n</code></pre> <code>appendfsync</code> Durability Performance <code>always</code> Lose at most one command Significant latency impact <code>everysec</code> Lose at most one second of data Minimal latency impact <code>no</code> OS-dependent (typically up to 30 seconds) Best throughput <p>AOF files grow over time as every command is appended. Redis automatically rewrites the AOF in the background to compact it (<code>BGREWRITEAOF</code>), replacing the command log with the minimal set of commands to reproduce the current state.</p>"},{"location":"Databases/redis/#rdb-aof-hybrid","title":"RDB + AOF Hybrid","text":"<p>Since Redis 4.0, you can enable both RDB and AOF. When <code>aof-use-rdb-preamble yes</code> is set, the AOF rewrite produces a file that starts with an RDB snapshot followed by AOF commands for changes since the snapshot. This combines fast loading (RDB) with minimal data loss (AOF).</p> <pre><code># Recommended production persistence configuration\nappendonly yes\nappendfsync everysec\naof-use-rdb-preamble yes\nsave 3600 1\n</code></pre> <p>No Persistence = Data Loss</p> <p>Running Redis with both RDB and AOF disabled means all data is lost on restart. This is acceptable for pure caching use cases where the backing database is the source of truth, but never for primary data storage.</p>"},{"location":"Databases/redis/#redis-sentinel","title":"Redis Sentinel","text":"<p>Redis Sentinel provides high availability for Redis deployments without using Redis Cluster. Sentinel monitors Redis instances, detects failures, and performs automatic failover.</p> <p>Sentinel provides four capabilities:</p> <ul> <li>Monitoring: continuously checks whether master and replica instances are working as expected</li> <li>Notification: sends alerts (via API or scripts) when a monitored instance fails</li> <li>Automatic failover: promotes a replica to master when the master is unreachable, reconfigures other replicas to use the new master</li> <li>Configuration provider: clients query Sentinel for the current master address, so they reconnect automatically after failover</li> </ul>"},{"location":"Databases/redis/#architecture","title":"Architecture","text":"<p>A typical Sentinel deployment uses three Sentinel processes (for quorum) monitoring one master and two or more replicas:</p> <pre><code>Sentinel 1 \u2500\u2500\u2500\u2500\u2500\u2510\nSentinel 2 \u2500\u2500\u2500\u2500\u2500\u2524\u2500\u2500\u2192 Master (read/write)\nSentinel 3 \u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n                         \u251c\u2500\u2500\u2192 Replica 1 (read-only)\n                         \u2514\u2500\u2500\u2192 Replica 2 (read-only)\n</code></pre>"},{"location":"Databases/redis/#configuration","title":"Configuration","text":"<pre><code># sentinel.conf\nsentinel monitor mymaster 192.168.1.10 6379 2\nsentinel down-after-milliseconds mymaster 5000\nsentinel failover-timeout mymaster 60000\nsentinel parallel-syncs mymaster 1\n</code></pre> Setting Purpose <code>sentinel monitor</code> Name, host, port, and quorum (number of Sentinels that must agree the master is down) <code>down-after-milliseconds</code> Time in ms before an unresponsive instance is considered subjectively down <code>failover-timeout</code> Maximum time for the failover process <code>parallel-syncs</code> How many replicas can sync with the new master simultaneously during failover <p>The quorum of 2 means at least two of three Sentinels must agree the master is unreachable before triggering failover. This prevents a single Sentinel's network partition from causing an unnecessary failover.</p>"},{"location":"Databases/redis/#redis-cluster","title":"Redis Cluster","text":"<p>Redis Cluster provides automatic sharding across multiple nodes, allowing you to scale beyond the memory of a single server. Unlike Sentinel (which provides HA for a single dataset), Cluster distributes data across multiple masters.</p>"},{"location":"Databases/redis/#hash-slots","title":"Hash Slots","text":"<p>Redis Cluster divides the keyspace into 16,384 hash slots. Each key is mapped to a slot using <code>CRC16(key) mod 16384</code>. Each master node in the cluster is responsible for a subset of these slots.</p> <pre><code>Node A: slots 0-5460\nNode B: slots 5461-10922\nNode C: slots 10923-16383\n</code></pre> <p>When you send a command to a node that does not own the key's slot, the node responds with a <code>MOVED</code> redirect telling the client which node to contact. Cluster-aware clients (like <code>redis-py-cluster</code> or Jedis in cluster mode) learn the slot mapping and route commands directly.</p>"},{"location":"Databases/redis/#setting-up-a-cluster","title":"Setting Up a Cluster","text":"<pre><code># Create a 6-node cluster (3 masters + 3 replicas)\nredis-cli --cluster create \\\n  192.168.1.1:6379 192.168.1.2:6379 192.168.1.3:6379 \\\n  192.168.1.4:6379 192.168.1.5:6379 192.168.1.6:6379 \\\n  --cluster-replicas 1\n\n# Check cluster status\nredis-cli -c CLUSTER INFO\n\n# View slot distribution\nredis-cli -c CLUSTER SLOTS\n</code></pre>"},{"location":"Databases/redis/#adding-and-removing-nodes","title":"Adding and Removing Nodes","text":"<pre><code># Add a new node to the cluster\nredis-cli --cluster add-node 192.168.1.7:6379 192.168.1.1:6379\n\n# Reshard slots to the new node\nredis-cli --cluster reshard 192.168.1.1:6379\n\n# Remove a node (must have zero slots first)\nredis-cli --cluster del-node 192.168.1.1:6379 &lt;node-id&gt;\n</code></pre> <p>Multi-Key Operations in Cluster</p> <p>Commands that operate on multiple keys (like <code>MGET</code>, <code>SINTER</code>, or <code>EVAL</code> with multiple keys) only work when all keys hash to the same slot. Use hash tags - <code>{user:1}:profile</code> and <code>{user:1}:settings</code> - to force related keys into the same slot. The hash is computed only on the content between <code>{</code> and <code>}</code>.</p>"},{"location":"Databases/redis/#redis-cli","title":"redis-cli","text":"<p><code>redis-cli</code> is the standard command-line interface for interacting with Redis. Beyond running commands, it has built-in tools for monitoring, benchmarking, and diagnostics.</p>"},{"location":"Databases/redis/#connecting","title":"Connecting","text":"<pre><code># Connect to local instance (default 127.0.0.1:6379)\nredis-cli\n\n# Connect to a remote instance with auth\nredis-cli -h redis.example.com -p 6379 -a yourpassword\n\n# Connect to a specific database (0-15)\nredis-cli -n 2\n\n# Connect in cluster mode (follows MOVED redirects)\nredis-cli -c\n\n# Run a single command without entering interactive mode\nredis-cli GET mykey\n</code></pre>"},{"location":"Databases/redis/#monitoring-and-diagnostics","title":"Monitoring and Diagnostics","text":"<pre><code># Real-time feed of every command processed\nredis-cli MONITOR\n\n# Continuous latency measurement\nredis-cli --latency\n\n# Latency history (one sample per 15 seconds)\nredis-cli --latency-history\n\n# Live stats (ops/sec, memory, clients, etc.)\nredis-cli --stat\n\n# Server information (sections: server, clients, memory, stats, replication, etc.)\nredis-cli INFO\nredis-cli INFO memory\nredis-cli INFO replication\n</code></pre> <p><code>MONITOR</code> shows every command hitting the server in real time - invaluable for debugging but adds overhead. Do not leave it running in production.</p>"},{"location":"Databases/redis/#useful-commands","title":"Useful Commands","text":"<pre><code># List all keys matching a pattern (use SCAN in production)\nKEYS user:*\n\n# Iterative key scan (safe for production - does not block)\nSCAN 0 MATCH user:* COUNT 100\n\n# Check key type\nTYPE user:1\n\n# Memory usage of a specific key\nMEMORY USAGE user:1\n\n# Flush the current database\nFLUSHDB\n\n# Flush all databases\nFLUSHALL\n\n# Slow log - queries that exceeded a time threshold\nSLOWLOG GET 10\n</code></pre> <p>SCAN over KEYS</p> <p><code>KEYS</code> blocks the server while it iterates every key in the database. On a production instance with millions of keys, this can freeze Redis for seconds. Always use <code>SCAN</code> with a cursor instead - it returns results incrementally without blocking.</p> <p>redis-cli Command Builder (requires JavaScript)</p>"},{"location":"Databases/redis/#putting-it-together","title":"Putting It Together","text":"<p>Redis Caching Layer Design (requires JavaScript)</p>"},{"location":"Databases/redis/#further-reading","title":"Further Reading","text":"<ul> <li>Redis Documentation - official reference for all commands, data types, and configuration</li> <li>Redis Data Types Tutorial - interactive walkthrough of every data structure</li> <li>Redis Persistence - in-depth coverage of RDB, AOF, and hybrid persistence</li> <li>Redis Sentinel Documentation - complete Sentinel setup and failover configuration</li> <li>Redis Cluster Specification - hash slot mechanics, gossip protocol, and resharding details</li> <li>Valkey Project - open-source Redis fork under the Linux Foundation</li> </ul> <p>Previous: MongoDB | Next: Backup &amp; Recovery Strategies | Back to Index</p>"},{"location":"Databases/scaling-and-architecture/","title":"Scaling &amp; Architecture Patterns","text":"<p>Your single-server database handles the first million users. Then queries slow down, connections pile up, and you start eyeing the credit card limit on your cloud account. Scaling a database is not just about throwing hardware at the problem - it requires understanding which bottleneck you are hitting and choosing the right architectural pattern to address it.</p> <p>This guide covers the spectrum from simple read replicas through sharding, connection pooling, and the distributed data patterns that appear in microservices architectures.</p>"},{"location":"Databases/scaling-and-architecture/#read-replicas","title":"Read Replicas","text":"<p>A read replica is a copy of your primary database that receives changes asynchronously and serves read-only queries. Most production workloads are read-heavy (often 80-90% reads), so offloading those reads to replicas is the first scaling move for nearly every application.</p>"},{"location":"Databases/scaling-and-architecture/#how-replication-works","title":"How Replication Works","text":"<p>The primary server records every write operation in a log - the binary log in MySQL, the write-ahead log (WAL) in PostgreSQL. Replicas connect to the primary, stream the log entries, and apply them locally:</p> <pre><code>flowchart TD\n    App[Application] --&gt;|Writes| Primary[(Primary Database)]\n    App --&gt;|Reads| LB{Load Balancer}\n    Primary --&gt;|Binary Log / WAL| R1[(Replica 1)]\n    Primary --&gt;|Binary Log / WAL| R2[(Replica 2)]\n    Primary --&gt;|Binary Log / WAL| R3[(Replica 3)]\n    LB --&gt; R1\n    LB --&gt; R2\n    LB --&gt; R3</code></pre> <p>In MySQL, binary log replication ships row-level or statement-level changes. In PostgreSQL, streaming replication sends WAL records. Both achieve the same result: replicas converge toward the primary's state.</p>"},{"location":"Databases/scaling-and-architecture/#routing-reads-vs-writes","title":"Routing Reads vs Writes","text":"<p>Your application needs to send writes to the primary and reads to replicas. There are three common approaches:</p> <p>Application-level routing - your code explicitly chooses a connection. Simple but couples routing logic to application code:</p> <pre><code>def get_user(user_id):\n    # Read from replica\n    with replica_pool.connection() as conn:\n        return conn.execute(\"SELECT * FROM users WHERE id = %s\", (user_id,))\n\ndef update_user(user_id, name):\n    # Write to primary\n    with primary_pool.connection() as conn:\n        conn.execute(\"UPDATE users SET name = %s WHERE id = %s\", (name, user_id))\n</code></pre> <p>Proxy-based routing - a middleware layer like ProxySQL or PgBouncer with Pgpool-II inspects queries and routes them automatically. The application connects to a single endpoint.</p> <p>DNS-based routing - separate DNS entries for read and write endpoints (e.g., <code>db-write.internal</code> and <code>db-read.internal</code>). AWS RDS and Aurora use this pattern with their cluster endpoints.</p>"},{"location":"Databases/scaling-and-architecture/#consistency-trade-offs","title":"Consistency Trade-offs","text":"<p>Replicas lag behind the primary. A user writes a comment, the next page load hits a replica that has not received the change yet, and the comment appears missing. This is replication lag, and you need a strategy for it:</p> <ul> <li>Read-your-writes consistency - after a write, route that user's subsequent reads to the primary for a short window (a few seconds)</li> <li>Monotonic reads - pin a user's session to a single replica so they never see data go backward</li> <li>Semi-synchronous replication - the primary waits for at least one replica to acknowledge the write before confirming to the client. Reduces lag at the cost of write latency</li> </ul>"},{"location":"Databases/scaling-and-architecture/#monitoring-replica-lag","title":"Monitoring Replica Lag","text":"<p>In MySQL, check <code>Seconds_Behind_Master</code> (or <code>Seconds_Behind_Source</code> in MySQL 8.0.22+):</p> <pre><code>SHOW REPLICA STATUS\\G\n-- Look for: Seconds_Behind_Source: 0\n</code></pre> <p>In PostgreSQL, compare WAL positions:</p> <pre><code>-- On the primary\nSELECT client_addr,\n       state,\n       pg_wal_lsn_diff(pg_current_wal_lsn(), replay_lsn) AS replay_lag_bytes\nFROM pg_stat_replication;\n</code></pre> <p>Lag is not just a number</p> <p><code>Seconds_Behind_Source</code> measures the timestamp difference between the event on the primary and when the replica processed it. It does not account for events still in transit. A replica can show 0 seconds of lag while being thousands of transactions behind if it is processing them quickly but the primary is generating them faster.</p> <p>A user posts a comment on your site, then immediately refreshes the page and sees their comment missing. What is the most likely cause? (requires JavaScript)</p> <p>Invalid interactive component configuration (terminal)</p>"},{"location":"Databases/scaling-and-architecture/#connection-pooling","title":"Connection Pooling","text":"<p>Every database connection consumes memory - typically 5-10 MB per connection in PostgreSQL, somewhat less in MySQL. A server with 4 GB of RAM dedicated to connections supports a few hundred at most. When your application spawns thousands of short-lived connections (common with PHP, serverless functions, or microservices), the overhead of creating and tearing down connections becomes a bottleneck before you run out of query capacity.</p> <p>A connection pooler sits between your application and the database, maintaining a pool of persistent connections and multiplexing client requests across them. One hundred application instances sharing twenty database connections is common.</p> <pre><code>flowchart LR\n    A1[App Instance 1] --&gt; P[Connection Pool&lt;br/&gt;PgBouncer / ProxySQL]\n    A2[App Instance 2] --&gt; P\n    A3[App Instance 3] --&gt; P\n    A4[App Instance 4] --&gt; P\n    A5[App Instance N] --&gt; P\n    P --&gt; |\"Fixed pool&lt;br/&gt;of connections\"| DB[(Database Server)]</code></pre>"},{"location":"Databases/scaling-and-architecture/#proxysql-for-mysql","title":"ProxySQL for MySQL","text":"<p>ProxySQL handles connection pooling, query routing, and query caching for MySQL. It operates as a transparent proxy - your application connects to ProxySQL on port 6033 instead of MySQL on port 3306.</p> <p>Key capabilities:</p> <ul> <li>Connection multiplexing - thousands of client connections share a smaller pool of backend connections</li> <li>Query routing - sends reads to replicas and writes to the primary based on query rules</li> <li>Query caching - caches SELECT results for configurable TTLs</li> <li>Query rewriting - modifies queries in-flight (adding hints, rewriting patterns)</li> </ul> <p>Configuration is stored in ProxySQL's internal SQLite database and managed through its admin interface on port 6032:</p> <pre><code>-- Add backend servers\nINSERT INTO mysql_servers (hostgroup_id, hostname, port, weight)\nVALUES (10, 'primary.db.internal', 3306, 1000);\n\nINSERT INTO mysql_servers (hostgroup_id, hostname, port, weight)\nVALUES (20, 'replica1.db.internal', 3306, 500);\n\nINSERT INTO mysql_servers (hostgroup_id, hostname, port, weight)\nVALUES (20, 'replica2.db.internal', 3306, 500);\n\n-- Route reads to hostgroup 20, writes to hostgroup 10\nINSERT INTO mysql_query_rules (rule_id, active, match_pattern, destination_hostgroup)\nVALUES (1, 1, '^SELECT.*FOR UPDATE', 10);\n\nINSERT INTO mysql_query_rules (rule_id, active, match_pattern, destination_hostgroup)\nVALUES (2, 1, '^SELECT', 20);\n\n-- Apply changes\nLOAD MYSQL SERVERS TO RUNTIME;\nLOAD MYSQL QUERY RULES TO RUNTIME;\nSAVE MYSQL SERVERS TO DISK;\nSAVE MYSQL QUERY RULES TO DISK;\n</code></pre>"},{"location":"Databases/scaling-and-architecture/#pgbouncer-for-postgresql","title":"PgBouncer for PostgreSQL","text":"<p>PgBouncer is a lightweight connection pooler for PostgreSQL. It supports three pooling modes:</p> Mode Behavior Use Case Session Connection assigned for the full client session Applications using session-level features (prepared statements, temp tables) Transaction Connection assigned per transaction Most web applications - best multiplexing ratio Statement Connection assigned per query Simple query patterns with no multi-statement transactions <p>Transaction pooling gives the best multiplexing ratio. A pool of 20 PostgreSQL connections can serve hundreds of application instances because each instance only holds a connection for the duration of a transaction.</p>"},{"location":"Databases/scaling-and-architecture/#pool-sizing","title":"Pool Sizing","text":"<p>The optimal pool size is smaller than you think. PostgreSQL's documentation recommends:</p> <pre><code>pool_size = (core_count * 2) + effective_spindle_count\n</code></pre> <p>For a server with 8 cores and SSDs (effectively 1 spindle), that is <code>(8 * 2) + 1 = 17</code> connections. More connections means more context switching, more lock contention, and worse throughput - not better.</p> <p>The connection pool paradox</p> <p>Adding more connections past the optimal point makes your database slower. If 20 connections can handle your workload with 50ms average query time, 200 connections will compete for the same CPU and disk, increasing average query time to 500ms. The database does more work per second with fewer concurrent connections.</p> <p>Invalid interactive component configuration (terminal)</p> <p>You have a PostgreSQL server with 4 CPU cores and SSD storage. Using the recommended pool sizing formula, what is the optimal connection pool size? (requires JavaScript)</p>"},{"location":"Databases/scaling-and-architecture/#horizontal-vs-vertical-scaling","title":"Horizontal vs Vertical Scaling","text":"<p>When your database hits its limits, you have two directions to grow.</p> <p>Vertical scaling (scaling up) means giving your existing server more resources - more CPU, more RAM, faster storage. It is the simplest path: no code changes, no architectural complexity. You upgrade the instance type, restart, and your database handles more load.</p> <p>Horizontal scaling (scaling out) means spreading data across multiple servers. This includes read replicas (covered above), sharding (covered below), and distributed databases. It introduces significant complexity but removes the ceiling imposed by a single machine's hardware.</p>"},{"location":"Databases/scaling-and-architecture/#trade-offs","title":"Trade-offs","text":"Factor Vertical Horizontal Complexity Minimal - same architecture High - application changes, data routing Downtime Brief restart for upgrades Can be zero-downtime with proper orchestration Cost curve Exponential - top-tier hardware is disproportionately expensive Linear - commodity hardware scales predictably Ceiling Hard limit at largest available instance Theoretically unlimited Consistency Strong - single node Requires careful design for consistency guarantees Operational burden One server to manage Many servers, network partitions, coordination"},{"location":"Databases/scaling-and-architecture/#when-each-is-appropriate","title":"When Each Is Appropriate","text":"<p>Scale vertically first. It buys you time without the operational overhead of distributed systems. A single server with 64 cores, 512 GB RAM, and NVMe storage handles more than most applications need. Switch to horizontal scaling when:</p> <ul> <li>Your dataset exceeds what a single server's storage can hold</li> <li>Your write throughput exceeds what a single server's I/O can handle</li> <li>Your availability requirements demand geographic distribution</li> <li>Your cost curve goes parabolic on the next vertical tier</li> </ul> <p>The single-server sweet spot</p> <p>Many high-traffic applications run on a single primary with read replicas far longer than their engineering teams expect. Stack Overflow served 1.3 billion page views per month from a small cluster of SQL Server instances. Vertical scaling combined with read replicas handles more load than most applications will ever reach.</p>"},{"location":"Databases/scaling-and-architecture/#sharding-strategies","title":"Sharding Strategies","text":"<p>Sharding splits a single logical database across multiple physical databases, each holding a subset of the data. Each subset is a shard. Unlike read replicas (which hold full copies), shards hold partitions - a given row exists on exactly one shard.</p> <p>Sharding is the most operationally complex scaling strategy. It should be your last resort after exhausting vertical scaling, read replicas, caching, and query optimization.</p>"},{"location":"Databases/scaling-and-architecture/#hash-based-sharding","title":"Hash-Based Sharding","text":"<p>Compute a hash of the shard key (typically the primary key or tenant ID) and use modulo arithmetic to assign rows to shards:</p> <pre><code>shard_number = hash(shard_key) % number_of_shards\n</code></pre> <p>This distributes data evenly. The problem arises when you add shards - changing the modulus remaps most keys, requiring a massive data migration.</p> <p>Consistent hashing solves this. Instead of <code>hash % N</code>, consistent hashing maps both keys and shard nodes onto a hash ring. When you add a shard, only the keys between the new node and its neighbor move. This minimizes data migration during resharding.</p>"},{"location":"Databases/scaling-and-architecture/#range-based-sharding","title":"Range-Based Sharding","text":"<p>Assign rows to shards based on value ranges of the shard key:</p> Shard Range Shard 1 user_id 1 - 1,000,000 Shard 2 user_id 1,000,001 - 2,000,000 Shard 3 user_id 2,000,001 - 3,000,000 <p>Range-based sharding supports efficient range queries (all users in a given range hit a single shard). The downside is hotspots - if most activity comes from recently created users, the latest shard handles disproportionate load.</p>"},{"location":"Databases/scaling-and-architecture/#directory-based-sharding","title":"Directory-Based Sharding","text":"<p>A separate lookup service maps each shard key to its shard location. The application queries the directory first, then routes to the appropriate shard.</p> <p>This is the most flexible approach - you can move individual tenants between shards without changing the hashing or range logic. The directory itself becomes a critical dependency and potential bottleneck, so it is typically a fast key-value store like Redis.</p>"},{"location":"Databases/scaling-and-architecture/#choosing-a-shard-key","title":"Choosing a Shard Key","text":"<p>The shard key determines how data is distributed and which queries can be satisfied by a single shard. A good shard key:</p> <ul> <li>Distributes data evenly - avoids hotspot shards</li> <li>Aligns with query patterns - queries that filter on the shard key hit a single shard</li> <li>Has high cardinality - enough distinct values to distribute across many shards</li> <li>Is immutable - changing a shard key requires moving the row between shards</li> </ul> <p>For multi-tenant SaaS applications, <code>tenant_id</code> is often the ideal shard key. All of a tenant's data lives on one shard, so most queries stay within a single shard.</p>"},{"location":"Databases/scaling-and-architecture/#cross-shard-queries","title":"Cross-Shard Queries","text":"<p>Queries that span multiple shards are expensive. A <code>JOIN</code> between a user on Shard 1 and their orders on Shard 3 cannot be executed by either shard alone. The application (or a query coordinator) must:</p> <ol> <li>Query each relevant shard independently</li> <li>Merge results in the application layer</li> <li>Handle pagination, sorting, and aggregation across partial results</li> </ol> <p>This is why shard key selection matters so much - you want the queries your application runs most often to hit a single shard.</p> <pre><code>flowchart TD\n    App[Application] --&gt; Router{Shard Router}\n    Router --&gt;|tenant_id 1-1000| S1[(Shard 1)]\n    Router --&gt;|tenant_id 1001-2000| S2[(Shard 2)]\n    Router --&gt;|tenant_id 2001-3000| S3[(Shard 3)]\n    S1 --&gt; R1[(Replica 1a)]\n    S2 --&gt; R2[(Replica 2a)]\n    S3 --&gt; R3[(Replica 3a)]\n    Dir[(Shard Directory)] -.-&gt;|Lookup| Router</code></pre> <p>Sharding is a one-way door</p> <p>Unsharding (merging shards back into a single database) is significantly harder than sharding in the first place. Before sharding, exhaust every other option: better indexes, query optimization, caching, read replicas, vertical scaling. Each of these is reversible. Sharding is not.</p> <p>You are building a multi-tenant SaaS application where each tenant has users, orders, and invoices. Which shard key gives you the best balance of even distribution and single-shard query efficiency? (requires JavaScript)</p> <p>Your application uses hash-based sharding with 4 shards (shard = hash(user_id) % 4). You need to expand to 6 shards to handle growth. What is the primary problem with this approach? (requires JavaScript)</p>"},{"location":"Databases/scaling-and-architecture/#microservices-data-patterns","title":"Microservices Data Patterns","text":"<p>When your application splits into microservices, the database architecture must split too. Getting data boundaries wrong is the most common source of pain in microservices systems.</p>"},{"location":"Databases/scaling-and-architecture/#database-per-service","title":"Database-per-Service","text":"<p>Each microservice owns its data store. The Orders service has its own database. The Inventory service has its own database. No service directly queries another service's database - it goes through that service's API.</p> <p>This provides:</p> <ul> <li>Independent deployability - schema changes do not ripple across services</li> <li>Technology freedom - the Orders service can use PostgreSQL while the Search service uses Elasticsearch</li> <li>Fault isolation - one service's database problems do not cascade to others</li> <li>Clear ownership - one team owns the data, the schema, and the access patterns</li> </ul> <p>The cost is that operations that previously required a single <code>JOIN</code> now require API calls across services.</p>"},{"location":"Databases/scaling-and-architecture/#the-shared-database-anti-pattern","title":"The Shared Database Anti-Pattern","text":"<p>Multiple services reading and writing the same database tables is the shared database pattern. It feels simpler at first - no API calls, just queries. But it creates tight coupling:</p> <ul> <li>Schema changes in one service break other services</li> <li>Lock contention between services causes unpredictable performance</li> <li>No clear ownership - who is responsible for data consistency?</li> <li>Services cannot be deployed independently if they share migration timelines</li> </ul> <p>Shared databases are appropriate when the \"services\" are really modules of a monolith that happen to deploy separately. For genuinely independent services, database-per-service is the path to operational sanity.</p>"},{"location":"Databases/scaling-and-architecture/#data-ownership-boundaries","title":"Data Ownership Boundaries","text":"<p>Each piece of data should have exactly one authoritative service. The Customers service owns customer profiles. The Orders service owns order records. When the Orders service needs customer data (like a name for an invoice), it has two options:</p> <ul> <li>Synchronous API call - query the Customers service at render time. Simple but creates runtime coupling</li> <li>Event-driven data replication - the Customers service publishes <code>CustomerUpdated</code> events, and the Orders service stores a local copy of the fields it needs. Decoupled but eventually consistent</li> </ul>"},{"location":"Databases/scaling-and-architecture/#api-composition-for-cross-service-queries","title":"API Composition for Cross-Service Queries","text":"<p>A dashboard that shows \"customer name, recent orders, payment status\" spans three services. An API composition layer (sometimes called a BFF - Backend for Frontend) queries each service and assembles the response:</p> <pre><code>async def get_customer_dashboard(customer_id):\n    customer, orders, payments = await asyncio.gather(\n        customer_service.get(customer_id),\n        order_service.list_by_customer(customer_id),\n        payment_service.get_status(customer_id),\n    )\n    return {\n        \"customer\": customer,\n        \"recent_orders\": orders[:5],\n        \"payment_status\": payments,\n    }\n</code></pre> <p>This works for simple aggregations. For complex queries involving filtering and sorting across service boundaries, you need CQRS or a dedicated read model.</p>"},{"location":"Databases/scaling-and-architecture/#cqrs","title":"CQRS","text":"<p>Command Query Responsibility Segregation (CQRS) uses separate models for reading and writing data. The command side handles writes (creates, updates, deletes) and optimizes for data integrity and business rule enforcement. The query side handles reads and optimizes for the specific views your application needs.</p>"},{"location":"Databases/scaling-and-architecture/#why-separate-read-and-write-models","title":"Why Separate Read and Write Models?","text":"<p>In a traditional architecture, the same database schema serves both reads and writes. This forces compromises:</p> <ul> <li>Normalization optimizes writes (no duplicated data to keep in sync) but makes reads expensive (joins across many tables)</li> <li>Denormalization optimizes reads (pre-joined data) but makes writes complex (update multiple places)</li> </ul> <p>CQRS eliminates this compromise. The write model is normalized for integrity. The read model is denormalized - pre-computed views that match exactly what the UI needs.</p>"},{"location":"Databases/scaling-and-architecture/#projections","title":"Projections","text":"<p>A projection transforms command-side events into read-side views. When a new order is placed, the projection updates:</p> <ul> <li>The customer's order list view</li> <li>The product's sales count view</li> <li>The daily revenue summary view</li> </ul> <p>Each view is a read model optimized for a specific query pattern. No joins at read time - the data is pre-aggregated.</p>"},{"location":"Databases/scaling-and-architecture/#eventual-consistency","title":"Eventual Consistency","text":"<p>The read model lags behind the write model. After a write, there is a delay before the projection updates the read views. This is eventual consistency between the command and query sides.</p> <p>For most use cases, this delay is milliseconds to seconds - imperceptible to users. For cases where it matters (like showing a user their just-submitted order), use the same read-your-writes pattern described in the replicas section: route immediately post-write reads to the command side.</p>"},{"location":"Databases/scaling-and-architecture/#when-cqrs-adds-value","title":"When CQRS Adds Value","text":"<p>CQRS is worth the complexity when:</p> <ul> <li>Read and write patterns are dramatically different (many more reads than writes, or reads requiring heavy aggregation)</li> <li>You need different scaling profiles for reads and writes</li> <li>Multiple services need different views of the same data</li> <li>Your read queries require joins across many tables that are expensive at query time</li> </ul> <p>CQRS is unnecessary complexity when:</p> <ul> <li>Your application is a straightforward CRUD app</li> <li>Read and write patterns are similar</li> <li>You have a small team and the operational overhead of maintaining two models outweighs the benefits</li> </ul> <p>CQRS without event sourcing</p> <p>CQRS does not require event sourcing. You can implement CQRS with a standard relational database: writes go to normalized tables, and background processes or database triggers maintain denormalized read views. Start here before introducing an event store.</p>"},{"location":"Databases/scaling-and-architecture/#event-sourcing-overview","title":"Event Sourcing Overview","text":"<p>Event sourcing stores every state change as an immutable event rather than overwriting the current state. Instead of a <code>users</code> table with the current name and email, you have an event stream:</p> <pre><code>UserCreated    { id: 42, name: \"Alice\", email: \"alice@example.com\", timestamp: \"2024-01-15T10:00:00Z\" }\nEmailChanged   { id: 42, email: \"alice@newdomain.com\", timestamp: \"2024-03-22T14:30:00Z\" }\nNameChanged    { id: 42, name: \"Alice Smith\", timestamp: \"2024-06-01T09:15:00Z\" }\nAccountLocked  { id: 42, reason: \"Too many login failures\", timestamp: \"2024-07-10T23:45:00Z\" }\n</code></pre> <p>The current state is derived by replaying all events for that entity. The event store is the system of record - append-only and immutable.</p>"},{"location":"Databases/scaling-and-architecture/#event-store","title":"Event Store","text":"<p>The event store is an append-only log. Events are never updated or deleted. Each event has:</p> <ul> <li>Stream ID - identifies the entity (e.g., user 42)</li> <li>Event type - what happened (e.g., <code>EmailChanged</code>)</li> <li>Payload - the data associated with the event</li> <li>Timestamp - when it happened</li> <li>Sequence number - ordering within the stream</li> </ul> <p>Purpose-built event stores include EventStoreDB and Axon Server. You can also build event sourcing on top of PostgreSQL, Kafka, or DynamoDB, though you lose some purpose-built features like stream subscriptions and projections.</p>"},{"location":"Databases/scaling-and-architecture/#projections-and-materializers","title":"Projections and Materializers","text":"<p>Since the event store holds raw events, you need projections (also called materializers) to build queryable views. A projection subscribes to the event stream and maintains a read-optimized representation:</p> <pre><code>class UserProfileProjection:\n    def handle(self, event):\n        if event.type == \"UserCreated\":\n            db.execute(\n                \"INSERT INTO user_profiles (id, name, email) VALUES (%s, %s, %s)\",\n                (event.data[\"id\"], event.data[\"name\"], event.data[\"email\"])\n            )\n        elif event.type == \"EmailChanged\":\n            db.execute(\n                \"UPDATE user_profiles SET email = %s WHERE id = %s\",\n                (event.data[\"email\"], event.data[\"id\"])\n            )\n        elif event.type == \"NameChanged\":\n            db.execute(\n                \"UPDATE user_profiles SET name = %s WHERE id = %s\",\n                (event.data[\"name\"], event.data[\"id\"])\n            )\n</code></pre>"},{"location":"Databases/scaling-and-architecture/#replay-capability","title":"Replay Capability","text":"<p>The most powerful feature of event sourcing is replay. Because the event store contains the complete history, you can:</p> <ul> <li>Rebuild read models - if a projection has a bug, fix it and replay all events to rebuild the view from scratch</li> <li>Add new projections - need a new report? Create a new projection and replay history to backfill it</li> <li>Debug production issues - replay the exact sequence of events that led to a bug</li> <li>Audit everything - the event log is a complete, immutable audit trail</li> </ul>"},{"location":"Databases/scaling-and-architecture/#relationship-with-cqrs","title":"Relationship with CQRS","text":"<p>Event sourcing and CQRS pair naturally. The event store serves as the write model (command side), and projections build the read models (query side). However, they are independent patterns:</p> <ul> <li>CQRS without event sourcing: writes go to normalized tables, projections build denormalized views</li> <li>Event sourcing without CQRS: events are the source of truth, but you might query by replaying events (impractical at scale - you almost always want projections)</li> </ul> <p>Event sourcing trade-offs</p> <p>Event sourcing adds significant complexity: event schema evolution is harder than database migrations, eventual consistency between the event store and projections requires careful handling, and querying by replaying events is expensive without projections. Use it when the audit trail, replay capability, and temporal queries justify the cost - financial systems, collaborative editing, and compliance-heavy domains.</p>"},{"location":"Databases/scaling-and-architecture/#architecture-topologies","title":"Architecture Topologies","text":"<p>Here are three common production topologies, each suited to different scale and availability requirements.</p>"},{"location":"Databases/scaling-and-architecture/#single-primary-with-replicas","title":"Single Primary with Replicas","text":"<p>The most common starting topology. One primary handles all writes; multiple replicas handle reads. A proxy or load balancer routes traffic:</p> <pre><code>Primary (writes) --&gt; Replica 1 (reads)\n                 --&gt; Replica 2 (reads)\n                 --&gt; Replica 3 (reads)\n</code></pre> <p>Best for: Applications with read-heavy workloads and moderate write volume. Handles the vast majority of production use cases.</p>"},{"location":"Databases/scaling-and-architecture/#multi-region","title":"Multi-Region","text":"<p>For global applications needing low latency and high availability across regions. Each region has a primary or a local read replica, with cross-region replication:</p> Component US-East EU-West AP-Southeast Primary Yes No No Replica Yes Yes Yes Proxy ProxySQL ProxySQL ProxySQL Writes Local Routed to US-East Routed to US-East Reads Local Local Local <p>Write latency for non-primary regions equals the network round-trip to the primary. Some systems (like CockroachDB, YugabyteDB, or Aurora Global Database) support multi-primary writes with conflict resolution, trading consistency for latency.</p>"},{"location":"Databases/scaling-and-architecture/#sharded-with-replicas","title":"Sharded with Replicas","text":"<p>For datasets too large for a single server. Each shard is itself a primary with replicas:</p> <pre><code>Shard 1: Primary --&gt; Replica 1a, Replica 1b\nShard 2: Primary --&gt; Replica 2a, Replica 2b\nShard 3: Primary --&gt; Replica 3a, Replica 3b\n\nShard Router: directs queries to the correct shard\n</code></pre> <p>This is the most operationally complex topology. Each shard needs its own monitoring, backup strategy, and failover configuration. Tools like Vitess (for MySQL) and Citus (for PostgreSQL) provide sharding infrastructure that simplifies management.</p>"},{"location":"Databases/scaling-and-architecture/#putting-it-all-together","title":"Putting It All Together","text":"<p>Scaling decisions should follow a progression, not jump to the most complex solution:</p> <ol> <li>Optimize queries and indexes - free performance on existing hardware</li> <li>Scale vertically - bigger instance, more RAM, faster storage</li> <li>Add read replicas - offload read traffic</li> <li>Add connection pooling - handle more concurrent clients with existing capacity</li> <li>Introduce caching - Redis/Memcached for frequently accessed data</li> <li>Consider CQRS - separate read and write models when query patterns diverge significantly</li> <li>Shard as a last resort - when write throughput or dataset size exceeds single-server limits</li> </ol> <p>Each step up adds operational complexity. Do not skip ahead - every level handles more scale than most applications ever reach.</p> <p>Design a Scaling Strategy (requires JavaScript)</p>"},{"location":"Databases/scaling-and-architecture/#further-reading","title":"Further Reading","text":"<ul> <li>ProxySQL Documentation - connection pooling, query routing, and caching for MySQL</li> <li>PgBouncer Documentation - lightweight connection pooling for PostgreSQL</li> <li>Vitess - Scalable MySQL Clustering - sharding infrastructure used at YouTube, Slack, and Square</li> <li>Martin Kleppmann - Designing Data-Intensive Applications - the definitive reference on distributed data systems</li> <li>CQRS Pattern - Microsoft Architecture Guide - when and how to implement CQRS</li> <li>EventStoreDB Documentation - purpose-built event sourcing database</li> </ul> <p>Previous: Database Security | Next: InnoDB Recovery with PDRT | Back to Index</p>"},{"location":"Databases/sql-essentials/","title":"SQL Essentials","text":"<p>SQL (Structured Query Language) is the standard language for communicating with relational databases. Whether you are running MySQL, PostgreSQL, SQLite, or SQL Server, the core syntax is the same. This guide covers the statements you will use every day - defining tables, querying data, joining tables, aggregating results, and controlling transactions.</p> <p>SQL statements fall into two broad categories: DDL (Data Definition Language) for defining structure, and DML (Data Manipulation Language) for working with data. You will also encounter TCL (Transaction Control Language) for managing transactions. This guide covers all three.</p>"},{"location":"Databases/sql-essentials/#data-definition-language-ddl","title":"Data Definition Language (DDL)","text":"<p>DDL statements define the structure of your database - the tables, columns, data types, and constraints that shape how data is stored.</p>"},{"location":"Databases/sql-essentials/#create-table","title":"CREATE TABLE","text":"<p><code>CREATE TABLE</code> builds a new table with named columns, each assigned a data type and optional constraints.</p> <pre><code>CREATE TABLE employees (\n    id          INT           PRIMARY KEY AUTO_INCREMENT,\n    first_name  VARCHAR(50)   NOT NULL,\n    last_name   VARCHAR(50)   NOT NULL,\n    email       VARCHAR(100)  UNIQUE NOT NULL,\n    department  VARCHAR(50)   DEFAULT 'Unassigned',\n    salary      DECIMAL(10,2) CHECK (salary &gt;= 0),\n    hire_date   DATE          NOT NULL,\n    manager_id  INT,\n    FOREIGN KEY (manager_id) REFERENCES employees(id)\n);\n</code></pre> <p>The most common column types across database systems:</p> Type Description Example <code>INT</code> / <code>INTEGER</code> Whole numbers <code>42</code>, <code>-7</code> <code>BIGINT</code> Large whole numbers <code>9223372036854775807</code> <code>DECIMAL(p,s)</code> Exact fixed-point numbers <code>DECIMAL(10,2)</code> stores <code>12345678.99</code> <code>FLOAT</code> / <code>DOUBLE</code> Approximate floating-point Scientific calculations <code>VARCHAR(n)</code> Variable-length string up to n characters Names, emails <code>TEXT</code> Variable-length string with large capacity Comments, descriptions <code>DATE</code> Calendar date <code>2025-03-15</code> <code>DATETIME</code> / <code>TIMESTAMP</code> Date and time <code>2025-03-15 14:30:00</code> <code>BOOLEAN</code> True/false <code>TRUE</code>, <code>FALSE</code> <p>Constraints enforce rules at the database level:</p> <ul> <li><code>PRIMARY KEY</code> - uniquely identifies each row; implies <code>NOT NULL</code> and <code>UNIQUE</code></li> <li><code>NOT NULL</code> - column cannot contain NULL values</li> <li><code>UNIQUE</code> - all values in the column must be distinct</li> <li><code>DEFAULT</code> - provides a value when none is specified during insert</li> <li><code>CHECK</code> - validates that values meet a condition</li> <li><code>FOREIGN KEY</code> - enforces a reference to a row in another table (or the same table)</li> <li><code>AUTO_INCREMENT</code> (MySQL) / <code>SERIAL</code> (PostgreSQL) - generates sequential values automatically</li> </ul> <p>Primary Key Strategy</p> <p>Every table should have a primary key. Integer auto-increment keys are simple and performant. UUIDs work better in distributed systems where multiple nodes generate IDs independently. Avoid using business data (email, SSN) as primary keys - business rules change, but primary keys should not.</p>"},{"location":"Databases/sql-essentials/#alter-table","title":"ALTER TABLE","text":"<p><code>ALTER TABLE</code> modifies an existing table without dropping and recreating it.</p> <pre><code>-- Add a column\nALTER TABLE employees ADD COLUMN phone VARCHAR(20);\n\n-- Drop a column\nALTER TABLE employees DROP COLUMN phone;\n\n-- Modify a column's data type\nALTER TABLE employees MODIFY COLUMN department VARCHAR(100);\n\n-- Rename a column (MySQL 8.0+)\nALTER TABLE employees RENAME COLUMN department TO dept_name;\n\n-- Add a constraint\nALTER TABLE employees ADD CONSTRAINT chk_salary CHECK (salary &gt;= 0);\n\n-- Drop a constraint\nALTER TABLE employees DROP CONSTRAINT chk_salary;\n</code></pre>"},{"location":"Databases/sql-essentials/#drop-table","title":"DROP TABLE","text":"<p><code>DROP TABLE</code> permanently removes a table and all its data.</p> <pre><code>-- Drop a table (fails if it doesn't exist)\nDROP TABLE employees;\n\n-- Drop only if the table exists\nDROP TABLE IF EXISTS employees;\n</code></pre> <p>DROP is permanent</p> <p><code>DROP TABLE</code> cannot be rolled back in most database systems (MySQL's DDL is auto-committed). Always verify you are connected to the correct database before running destructive DDL. In production, use <code>IF EXISTS</code> to avoid errors in scripts.</p> <p>Which constraint ensures that every value in a column is different from all other values in that same column? (requires JavaScript)</p>"},{"location":"Databases/sql-essentials/#data-manipulation-language-dml","title":"Data Manipulation Language (DML)","text":"<p>DML statements read and modify the data stored in your tables.</p>"},{"location":"Databases/sql-essentials/#insert","title":"INSERT","text":"<p><code>INSERT</code> adds new rows to a table.</p> <pre><code>-- Insert a single row with all columns specified\nINSERT INTO employees (first_name, last_name, email, department, salary, hire_date)\nVALUES ('Ada', 'Lovelace', 'ada@example.com', 'Engineering', 95000.00, '2024-01-15');\n\n-- Insert multiple rows\nINSERT INTO employees (first_name, last_name, email, department, salary, hire_date)\nVALUES\n    ('Grace', 'Hopper', 'grace@example.com', 'Engineering', 105000.00, '2023-06-01'),\n    ('Alan', 'Turing', 'alan@example.com', 'Research', 98000.00, '2023-09-15'),\n    ('Linus', 'Torvalds', 'linus@example.com', 'Engineering', 120000.00, '2022-03-01');\n\n-- Insert from a query\nINSERT INTO engineering_staff (first_name, last_name, email)\nSELECT first_name, last_name, email\nFROM employees\nWHERE department = 'Engineering';\n</code></pre>"},{"location":"Databases/sql-essentials/#select","title":"SELECT","text":"<p><code>SELECT</code> retrieves data from one or more tables. It is the most frequently used SQL statement.</p> <pre><code>-- Select all columns\nSELECT * FROM employees;\n\n-- Select specific columns\nSELECT first_name, last_name, salary FROM employees;\n\n-- Filter with WHERE\nSELECT first_name, last_name, salary\nFROM employees\nWHERE department = 'Engineering' AND salary &gt; 90000;\n\n-- Sort results\nSELECT first_name, last_name, salary\nFROM employees\nORDER BY salary DESC;\n\n-- Limit results\nSELECT first_name, last_name\nFROM employees\nORDER BY hire_date DESC\nLIMIT 5;\n\n-- Remove duplicates\nSELECT DISTINCT department FROM employees;\n\n-- Alias columns and tables\nSELECT e.first_name AS name, e.salary AS annual_pay\nFROM employees AS e\nWHERE e.department = 'Engineering';\n</code></pre> <p>WHERE clause operators:</p> Operator Description Example <code>=</code> Equal <code>WHERE department = 'Engineering'</code> <code>!=</code> or <code>&lt;&gt;</code> Not equal <code>WHERE status &lt;&gt; 'inactive'</code> <code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code> Comparison <code>WHERE salary &gt;= 50000</code> <code>BETWEEN</code> Range (inclusive) <code>WHERE salary BETWEEN 50000 AND 100000</code> <code>IN</code> Match any value in a list <code>WHERE department IN ('Engineering', 'Research')</code> <code>LIKE</code> Pattern matching <code>WHERE last_name LIKE 'T%'</code> <code>IS NULL</code> / <code>IS NOT NULL</code> NULL check <code>WHERE manager_id IS NULL</code> <code>AND</code>, <code>OR</code>, <code>NOT</code> Logical operators <code>WHERE salary &gt; 80000 AND department = 'Engineering'</code> <p>LIKE patterns</p> <p><code>%</code> matches any sequence of characters (including none). <code>_</code> matches exactly one character. So <code>LIKE 'A%'</code> matches anything starting with A, and <code>LIKE '_a%'</code> matches anything with <code>a</code> as the second character.</p>"},{"location":"Databases/sql-essentials/#update","title":"UPDATE","text":"<p><code>UPDATE</code> modifies existing rows.</p> <pre><code>-- Update specific rows\nUPDATE employees\nSET salary = salary * 1.10\nWHERE department = 'Engineering';\n\n-- Update multiple columns\nUPDATE employees\nSET department = 'R&amp;D',\n    salary = 110000\nWHERE email = 'alan@example.com';\n</code></pre> <p>Always use WHERE with UPDATE and DELETE</p> <p>Running <code>UPDATE employees SET salary = 0</code> without a <code>WHERE</code> clause sets every employee's salary to zero. The same applies to <code>DELETE</code>. Always include a <code>WHERE</code> clause unless you genuinely intend to affect every row. In critical environments, run a <code>SELECT</code> with the same <code>WHERE</code> clause first to verify which rows will be affected.</p>"},{"location":"Databases/sql-essentials/#delete","title":"DELETE","text":"<p><code>DELETE</code> removes rows from a table.</p> <pre><code>-- Delete specific rows\nDELETE FROM employees\nWHERE department = 'Inactive';\n\n-- Delete all rows (keeps table structure)\nDELETE FROM employees;\n\n-- TRUNCATE is faster for removing all rows (resets auto-increment)\nTRUNCATE TABLE employees;\n</code></pre> <p>DDL and DML Basics (requires JavaScript)</p> <p>What happens if you run `DELETE FROM employees;` without a WHERE clause? (requires JavaScript)</p>"},{"location":"Databases/sql-essentials/#joins","title":"JOINs","text":"<p>JOINs combine rows from two or more tables based on a related column. They are fundamental to working with relational data, where information is split across normalized tables.</p> <p>For the examples below, consider these two tables:</p> <pre><code>CREATE TABLE departments (\n    id   INT PRIMARY KEY,\n    name VARCHAR(50) NOT NULL\n);\n\nINSERT INTO departments VALUES\n    (1, 'Engineering'), (2, 'Marketing'), (3, 'Finance'), (4, 'Legal');\n\nCREATE TABLE staff (\n    id      INT PRIMARY KEY,\n    name    VARCHAR(50) NOT NULL,\n    dept_id INT\n);\n\nINSERT INTO staff VALUES\n    (1, 'Ada', 1), (2, 'Grace', 1), (3, 'Alan', 2),\n    (4, 'Claude', 3), (5, 'Dijkstra', NULL);\n</code></pre> <p>Notice that the <code>Legal</code> department (id=4) has no staff, and <code>Dijkstra</code> (id=5) has no department (<code>dept_id</code> is NULL).</p>"},{"location":"Databases/sql-essentials/#inner-join","title":"INNER JOIN","text":"<p>Returns only rows where there is a match in both tables. Unmatched rows from either side are excluded.</p> <pre><code>SELECT s.name AS employee, d.name AS department\nFROM staff s\nINNER JOIN departments d ON s.dept_id = d.id;\n</code></pre> employee department Ada Engineering Grace Engineering Alan Marketing Claude Finance <p>Dijkstra is excluded (no matching department). Legal is excluded (no matching staff).</p>"},{"location":"Databases/sql-essentials/#left-join-left-outer-join","title":"LEFT JOIN (LEFT OUTER JOIN)","text":"<p>Returns all rows from the left table and matching rows from the right table. Where there is no match, right-side columns are NULL.</p> <pre><code>SELECT s.name AS employee, d.name AS department\nFROM staff s\nLEFT JOIN departments d ON s.dept_id = d.id;\n</code></pre> employee department Ada Engineering Grace Engineering Alan Marketing Claude Finance Dijkstra NULL <p>Every staff member appears. Dijkstra has NULL for department because there is no matching row.</p>"},{"location":"Databases/sql-essentials/#right-join-right-outer-join","title":"RIGHT JOIN (RIGHT OUTER JOIN)","text":"<p>Returns all rows from the right table and matching rows from the left table. Where there is no match, left-side columns are NULL.</p> <pre><code>SELECT s.name AS employee, d.name AS department\nFROM staff s\nRIGHT JOIN departments d ON s.dept_id = d.id;\n</code></pre> employee department Ada Engineering Grace Engineering Alan Marketing Claude Finance NULL Legal <p>Every department appears. Legal shows NULL for employee because no one is assigned to it.</p>"},{"location":"Databases/sql-essentials/#full-outer-join","title":"FULL OUTER JOIN","text":"<p>Returns all rows from both tables. Where there is no match on either side, the missing columns are NULL. MySQL does not support <code>FULL OUTER JOIN</code> directly - you simulate it with a <code>UNION</code> of <code>LEFT JOIN</code> and <code>RIGHT JOIN</code>.</p> <pre><code>-- PostgreSQL, SQL Server\nSELECT s.name AS employee, d.name AS department\nFROM staff s\nFULL OUTER JOIN departments d ON s.dept_id = d.id;\n\n-- MySQL equivalent\nSELECT s.name AS employee, d.name AS department\nFROM staff s\nLEFT JOIN departments d ON s.dept_id = d.id\nUNION\nSELECT s.name AS employee, d.name AS department\nFROM staff s\nRIGHT JOIN departments d ON s.dept_id = d.id;\n</code></pre> employee department Ada Engineering Grace Engineering Alan Marketing Claude Finance Dijkstra NULL NULL Legal <p>Both unmatched sides appear: Dijkstra (no department) and Legal (no staff).</p>"},{"location":"Databases/sql-essentials/#cross-join","title":"CROSS JOIN","text":"<p>Returns the Cartesian product - every row from the left table combined with every row from the right table. If the left table has 5 rows and the right has 4, the result has 20 rows. Use CROSS JOIN when you genuinely need all combinations, such as generating a schedule grid.</p> <pre><code>SELECT s.name, d.name\nFROM staff s\nCROSS JOIN departments d;\n-- Returns 5 * 4 = 20 rows\n</code></pre> <p>JOIN Types in Action (requires JavaScript)</p> <p>You have a `customers` table and an `orders` table. You want a list of ALL customers, including those who have never placed an order. Which JOIN should you use? (requires JavaScript)</p>"},{"location":"Databases/sql-essentials/#subqueries","title":"Subqueries","text":"<p>A subquery is a <code>SELECT</code> statement nested inside another statement. Subqueries run first, and their result is used by the outer query. They can appear in <code>WHERE</code>, <code>FROM</code>, <code>SELECT</code>, and <code>HAVING</code> clauses.</p>"},{"location":"Databases/sql-essentials/#scalar-subquery","title":"Scalar Subquery","text":"<p>Returns a single value. Used anywhere a single value is expected.</p> <pre><code>-- Find employees who earn more than the average salary\nSELECT first_name, last_name, salary\nFROM employees\nWHERE salary &gt; (SELECT AVG(salary) FROM employees);\n</code></pre>"},{"location":"Databases/sql-essentials/#column-subquery","title":"Column Subquery","text":"<p>Returns a single column of values. Used with <code>IN</code>, <code>ANY</code>, or <code>ALL</code>.</p> <pre><code>-- Find employees in departments that are located in New York\nSELECT first_name, last_name\nFROM employees\nWHERE department_id IN (\n    SELECT id FROM departments WHERE location = 'New York'\n);\n</code></pre>"},{"location":"Databases/sql-essentials/#row-subquery","title":"Row Subquery","text":"<p>Returns a single row with multiple columns. Used for multi-column comparisons.</p> <pre><code>-- Find employees with the same department and job title as employee 101\nSELECT first_name, last_name\nFROM employees\nWHERE (department_id, job_title) = (\n    SELECT department_id, job_title FROM employees WHERE id = 101\n);\n</code></pre>"},{"location":"Databases/sql-essentials/#table-subquery-derived-table","title":"Table Subquery (Derived Table)","text":"<p>Returns a full result set used as a temporary table in the <code>FROM</code> clause.</p> <pre><code>-- Average salary by department, then find departments above the global average\nSELECT dept_avg.department, dept_avg.avg_salary\nFROM (\n    SELECT department, AVG(salary) AS avg_salary\n    FROM employees\n    GROUP BY department\n) AS dept_avg\nWHERE dept_avg.avg_salary &gt; (SELECT AVG(salary) FROM employees);\n</code></pre>"},{"location":"Databases/sql-essentials/#correlated-subquery","title":"Correlated Subquery","text":"<p>A subquery that references the outer query. It runs once per row of the outer query, which makes it powerful but potentially slow on large datasets.</p> <pre><code>-- Find employees who earn more than the average for their department\nSELECT first_name, last_name, department, salary\nFROM employees e1\nWHERE salary &gt; (\n    SELECT AVG(salary)\n    FROM employees e2\n    WHERE e2.department = e1.department\n);\n</code></pre> <p>The inner query references <code>e1.department</code> from the outer query, so it recalculates the average for each employee's own department.</p> <pre><code>-- EXISTS: check if related rows exist\nSELECT d.name\nFROM departments d\nWHERE EXISTS (\n    SELECT 1 FROM employees e WHERE e.department_id = d.id\n);\n</code></pre> <p><code>EXISTS</code> returns TRUE if the subquery produces any rows. It is typically faster than <code>IN</code> for large datasets because it can stop as soon as it finds one match.</p>"},{"location":"Databases/sql-essentials/#aggregations","title":"Aggregations","text":"<p>Aggregate functions compute a single result from a set of rows. They are the backbone of reporting queries.</p> Function Description Example <code>COUNT(*)</code> Number of rows <code>COUNT(*)</code> counts all rows; <code>COUNT(column)</code> counts non-NULL values <code>SUM(column)</code> Total of numeric values <code>SUM(salary)</code> <code>AVG(column)</code> Arithmetic mean <code>AVG(salary)</code> <code>MIN(column)</code> Smallest value <code>MIN(hire_date)</code> <code>MAX(column)</code> Largest value <code>MAX(salary)</code>"},{"location":"Databases/sql-essentials/#group-by","title":"GROUP BY","text":"<p><code>GROUP BY</code> splits the result set into groups, one per distinct value (or combination of values) in the grouped columns. Aggregate functions then operate on each group independently.</p> <pre><code>-- Count employees per department\nSELECT department, COUNT(*) AS employee_count\nFROM employees\nGROUP BY department;\n\n-- Average salary by department, sorted highest first\nSELECT department, AVG(salary) AS avg_salary\nFROM employees\nGROUP BY department\nORDER BY avg_salary DESC;\n\n-- Group by multiple columns\nSELECT department, job_title, COUNT(*) AS headcount\nFROM employees\nGROUP BY department, job_title;\n</code></pre>"},{"location":"Databases/sql-essentials/#having","title":"HAVING","text":"<p><code>HAVING</code> filters groups after aggregation, the way <code>WHERE</code> filters individual rows before aggregation. You cannot use aggregate functions in a <code>WHERE</code> clause - that is what <code>HAVING</code> is for.</p> <pre><code>-- Departments with more than 5 employees\nSELECT department, COUNT(*) AS employee_count\nFROM employees\nGROUP BY department\nHAVING COUNT(*) &gt; 5;\n\n-- Departments where the average salary exceeds 80,000\nSELECT department, AVG(salary) AS avg_salary\nFROM employees\nGROUP BY department\nHAVING AVG(salary) &gt; 80000;\n</code></pre> <p>The full logical execution order of a <code>SELECT</code> statement:</p> <ol> <li><code>FROM</code> (and JOINs) - identify the source tables</li> <li><code>WHERE</code> - filter individual rows</li> <li><code>GROUP BY</code> - group the remaining rows</li> <li><code>HAVING</code> - filter groups</li> <li><code>SELECT</code> - compute output columns and expressions</li> <li><code>DISTINCT</code> - remove duplicates</li> <li><code>ORDER BY</code> - sort results</li> <li><code>LIMIT</code> / <code>OFFSET</code> - restrict how many rows to return</li> </ol> <p>This execution order explains why you can use column aliases in <code>ORDER BY</code> but not in <code>WHERE</code> - aliases are defined in step 5, after <code>WHERE</code> runs in step 2.</p> <p>What is the difference between WHERE and HAVING? (requires JavaScript)</p>"},{"location":"Databases/sql-essentials/#transactions","title":"Transactions","text":"<p>A transaction groups multiple SQL statements into a single atomic unit. Either all statements succeed (commit), or all are rolled back as if nothing happened. Transactions enforce the ACID properties covered in the Database Fundamentals guide.</p>"},{"location":"Databases/sql-essentials/#basic-transaction-control","title":"Basic Transaction Control","text":"<pre><code>-- Start a transaction\nSTART TRANSACTION;  -- or BEGIN in PostgreSQL\n\n-- Perform operations\nUPDATE accounts SET balance = balance - 500 WHERE id = 1;\nUPDATE accounts SET balance = balance + 500 WHERE id = 2;\n\n-- If everything succeeded, make it permanent\nCOMMIT;\n\n-- If something went wrong, undo everything\n-- ROLLBACK;\n</code></pre> <p>If the database crashes between the two <code>UPDATE</code> statements, the transaction is automatically rolled back on recovery. Money does not vanish from account 1 without appearing in account 2.</p>"},{"location":"Databases/sql-essentials/#savepoint","title":"SAVEPOINT","text":"<p>Savepoints create checkpoints within a transaction, allowing you to roll back to a specific point without aborting the entire transaction.</p> <pre><code>START TRANSACTION;\n\nINSERT INTO orders (customer_id, total) VALUES (42, 150.00);\nSAVEPOINT after_order;\n\nINSERT INTO order_items (order_id, product_id, qty) VALUES (LAST_INSERT_ID(), 7, 2);\n-- Oops, wrong product\nROLLBACK TO after_order;\n\n-- Correct the mistake\nINSERT INTO order_items (order_id, product_id, qty) VALUES (LAST_INSERT_ID(), 12, 2);\n\nCOMMIT;\n</code></pre> <p>The order row survives because you only rolled back to the savepoint, not the entire transaction.</p>"},{"location":"Databases/sql-essentials/#auto-commit","title":"Auto-commit","text":"<p>By default, most database systems auto-commit each individual statement. When auto-commit is on, every <code>INSERT</code>, <code>UPDATE</code>, and <code>DELETE</code> is immediately permanent. Starting an explicit transaction with <code>START TRANSACTION</code> or <code>BEGIN</code> disables auto-commit until you <code>COMMIT</code> or <code>ROLLBACK</code>.</p> <pre><code>-- Check auto-commit status (MySQL)\nSELECT @@autocommit;\n\n-- Disable auto-commit for the session\nSET autocommit = 0;\n</code></pre> <p>DDL and Transactions</p> <p>In MySQL, DDL statements (<code>CREATE TABLE</code>, <code>ALTER TABLE</code>, <code>DROP TABLE</code>) cause an implicit commit - any open transaction is automatically committed before the DDL runs. PostgreSQL supports transactional DDL, meaning you can roll back a <code>CREATE TABLE</code> inside a transaction. Know which behavior your database uses.</p>"},{"location":"Databases/sql-essentials/#common-functions","title":"Common Functions","text":"<p>SQL provides built-in functions for transforming data within queries. Function names and behavior are mostly consistent across databases, with some variations noted below.</p>"},{"location":"Databases/sql-essentials/#string-functions","title":"String Functions","text":"<pre><code>-- Concatenate strings\nSELECT CONCAT(first_name, ' ', last_name) AS full_name FROM employees;\n-- PostgreSQL also supports: first_name || ' ' || last_name\n\n-- Extract a portion of a string (1-indexed)\nSELECT SUBSTRING(email, 1, POSITION('@' IN email) - 1) AS username FROM employees;\n-- MySQL alternative: SUBSTR(email, 1, LOCATE('@', email) - 1)\n\n-- Remove leading/trailing whitespace\nSELECT TRIM('   hello   ');          -- 'hello'\nSELECT LTRIM('   hello');            -- 'hello'\nSELECT RTRIM('hello   ');            -- 'hello'\n\n-- Change case\nSELECT UPPER(last_name) FROM employees;    -- 'TORVALDS'\nSELECT LOWER(email) FROM employees;        -- 'linus@example.com'\n\n-- String length\nSELECT LENGTH(first_name) FROM employees;  -- character count\n\n-- Replace occurrences\nSELECT REPLACE(email, '@example.com', '@company.com') FROM employees;\n</code></pre>"},{"location":"Databases/sql-essentials/#date-functions","title":"Date Functions","text":"<pre><code>-- Current date and time\nSELECT NOW();              -- 2025-03-15 14:30:00\nSELECT CURRENT_DATE;       -- 2025-03-15\nSELECT CURRENT_TIMESTAMP;  -- 2025-03-15 14:30:00\n\n-- Add/subtract intervals\nSELECT DATE_ADD(hire_date, INTERVAL 90 DAY) AS review_date FROM employees;\n-- PostgreSQL: hire_date + INTERVAL '90 days'\n\n-- Difference between dates\nSELECT DATEDIFF(NOW(), hire_date) AS days_employed FROM employees;\n-- PostgreSQL: NOW() - hire_date (returns an interval)\n\n-- Extract parts of a date\nSELECT EXTRACT(YEAR FROM hire_date) AS hire_year FROM employees;\nSELECT EXTRACT(MONTH FROM hire_date) AS hire_month FROM employees;\n\n-- Format a date (MySQL)\nSELECT DATE_FORMAT(hire_date, '%M %d, %Y') FROM employees;\n-- PostgreSQL: TO_CHAR(hire_date, 'Month DD, YYYY')\n</code></pre>"},{"location":"Databases/sql-essentials/#numeric-functions","title":"Numeric Functions","text":"<pre><code>-- Rounding\nSELECT ROUND(salary / 12, 2) AS monthly_salary FROM employees;   -- 2 decimal places\nSELECT CEIL(4.1);    -- 5 (round up)\nSELECT FLOOR(4.9);   -- 4 (round down)\n\n-- Absolute value\nSELECT ABS(-42);     -- 42\n\n-- Modulo\nSELECT MOD(17, 5);   -- 2\n\n-- Power and square root\nSELECT POWER(2, 10); -- 1024\nSELECT SQRT(144);    -- 12\n</code></pre>"},{"location":"Databases/sql-essentials/#null-handling","title":"NULL Handling","text":"<p>NULL represents the absence of a value. It is not zero, not an empty string, not false. Any arithmetic or comparison with NULL produces NULL (except <code>IS NULL</code>). These functions help you handle NULLs explicitly.</p> <pre><code>-- COALESCE: returns the first non-NULL argument\nSELECT COALESCE(phone, mobile, 'No phone on file') AS contact_number\nFROM employees;\n\n-- IFNULL (MySQL) / COALESCE for two arguments\nSELECT IFNULL(manager_id, 0) AS mgr FROM employees;\n\n-- NULLIF: returns NULL if both arguments are equal, otherwise the first\nSELECT NULLIF(department, 'Unassigned') AS dept FROM employees;\n-- Returns NULL if department is 'Unassigned', otherwise returns the department name\n\n-- IS NULL / IS NOT NULL in conditions\nSELECT * FROM employees WHERE manager_id IS NULL;\n</code></pre> <p><code>COALESCE</code> is the most portable function for NULL handling - it works identically across MySQL, PostgreSQL, SQL Server, and SQLite.</p>"},{"location":"Databases/sql-essentials/#select-command-builder","title":"SELECT Command Builder","text":"<p>Build a <code>SELECT</code> statement by combining clauses. Each group represents a part of the query you can customize.</p> <p>SELECT Statement Builder (requires JavaScript)</p>"},{"location":"Databases/sql-essentials/#putting-it-all-together","title":"Putting It All Together","text":"<p>Sales Database Query Challenge (requires JavaScript)</p>"},{"location":"Databases/sql-essentials/#further-reading","title":"Further Reading","text":"<ul> <li>MySQL 8.0 Reference Manual - complete syntax reference for all MySQL statements</li> <li>PostgreSQL Documentation - SQL Commands - authoritative reference for PostgreSQL SQL syntax</li> <li>SQLBolt - interactive SQL tutorials with exercises you can run in the browser</li> <li>Use The Index, Luke - how SQL indexing works and why your queries are slow</li> <li>SQL Style Guide by Simon Holywell - formatting conventions for readable SQL</li> </ul> <p>Previous: Database Fundamentals | Next: Database Design &amp; Modeling | Back to Index</p>"},{"location":"Dev%20Zero/Perl/","title":"Perl: Zero to Expert","text":"<p>A comprehensive course that takes you from Unix foundations through professional Perl development. Each guide builds on the last, with interactive quizzes, terminal simulations, code walkthroughs, and hands-on exercises throughout.</p>"},{"location":"Dev%20Zero/Perl/#foundations","title":"Foundations","text":""},{"location":"Dev%20Zero/Perl/#introduction-why-perl-and-why-unix-first","title":"Introduction: Why Perl, and Why Unix First","text":"<p>The operating system Perl was born on. Covers Unix processes, file descriptors, signals, and why understanding the kernel layer makes Perl's I/O model intuitive. Includes your first Perl commands and a walkthrough of a basic script's anatomy.</p>"},{"location":"Dev%20Zero/Perl/#scalars-strings-and-numbers","title":"Scalars, Strings, and Numbers","text":"<p>Perl's fundamental data type. Covers the <code>$</code> sigil, string interpolation, heredocs, numeric types, scalar vs. list context, string manipulation functions, <code>undef</code> and truthiness, and the special variables (<code>$_</code>, <code>$!</code>, <code>$@</code>) that make Perl concise.</p>"},{"location":"Dev%20Zero/Perl/#arrays-hashes-and-lists","title":"Arrays, Hashes, and Lists","text":"<p>Perl's aggregate data types. Covers <code>@arrays</code> and <code>%hashes</code>, list operations, slices, iteration patterns, sorting with custom comparators, <code>map</code>/<code>grep</code>/<code>join</code>/<code>split</code>, and an introduction to nested data structures.</p>"},{"location":"Dev%20Zero/Perl/#control-flow","title":"Control Flow","text":"<p>Directing program logic. Covers <code>if</code>/<code>elsif</code>/<code>else</code>, <code>unless</code>, loops (<code>while</code>/<code>until</code>/<code>for</code>/<code>foreach</code>), statement modifiers, loop control with labels (<code>next</code>/<code>last</code>/<code>redo</code>), and short-circuit operators.</p>"},{"location":"Dev%20Zero/Perl/#core-language","title":"Core Language","text":""},{"location":"Dev%20Zero/Perl/#regular-expressions","title":"Regular Expressions","text":"<p>Perl's signature feature. Covers matching (<code>m//</code>), substitution (<code>s///</code>), quantifiers, character classes, anchors, captures, backreferences, lookahead/lookbehind, the <code>/g</code>, <code>/x</code>, <code>/e</code> modifiers, and <code>split</code> with regex.</p>"},{"location":"Dev%20Zero/Perl/#subroutines-and-references","title":"Subroutines and References","text":"<p>Building reusable code and complex data. Covers <code>sub</code> declarations, <code>@_</code> and argument handling, return values, references and dereferencing, anonymous data structures, closures, and <code>sort</code> with custom subroutines.</p>"},{"location":"Dev%20Zero/Perl/#file-io-and-system-interaction","title":"File I/O and System Interaction","text":"<p>Reading, writing, and interacting with the OS. Covers <code>open</code>/<code>close</code>, file modes, the diamond operator (<code>&lt;&gt;</code>), directory operations, file tests (<code>-e</code>, <code>-f</code>, <code>-d</code>), <code>stat</code>, <code>system</code>/backticks/<code>open</code>-pipe, and <code>fork</code>/<code>exec</code>/<code>wait</code>.</p>"},{"location":"Dev%20Zero/Perl/#professional-practice","title":"Professional Practice","text":""},{"location":"Dev%20Zero/Perl/#modules-and-cpan","title":"Modules and CPAN","text":"<p>Code organization and the Perl ecosystem. Covers <code>use</code>/<code>require</code>, writing modules, <code>@INC</code> and module paths, namespaces, Exporter, <code>cpanm</code>, finding and evaluating CPAN modules, and <code>Dist::Zilla</code> for distribution management.</p>"},{"location":"Dev%20Zero/Perl/#object-oriented-perl","title":"Object-Oriented Perl","text":"<p>Perl's OOP model. Covers <code>bless</code>, constructors, methods, inheritance (<code>@ISA</code>/<code>use parent</code>), accessor generation, <code>Moose</code>/<code>Moo</code> for modern OOP, roles, type constraints, and when to use OOP vs. procedural Perl.</p>"},{"location":"Dev%20Zero/Perl/#error-handling-and-debugging","title":"Error Handling and Debugging","text":"<p>Writing resilient code and finding bugs. Covers <code>die</code>/<code>warn</code>/<code>eval</code>, <code>Try::Tiny</code>, <code>$@</code> and error propagation, <code>use strict</code>/<code>use warnings</code>, the Perl debugger (<code>perl -d</code>), <code>Devel::</code> modules, and logging strategies.</p>"},{"location":"Dev%20Zero/Perl/#applied-perl","title":"Applied Perl","text":""},{"location":"Dev%20Zero/Perl/#testing","title":"Testing","text":"<p>Perl's testing culture. Covers <code>Test::More</code>, <code>prove</code>, TAP protocol, test organization, <code>Test2::Suite</code>, mocking, test coverage with <code>Devel::Cover</code>, and integrating tests with CI/CD pipelines.</p>"},{"location":"Dev%20Zero/Perl/#text-processing-and-one-liners","title":"Text Processing and One-Liners","text":"<p>Perl as a command-line power tool. Covers <code>-n</code>, <code>-p</code>, <code>-l</code>, <code>-a</code>, <code>-e</code> flags, field processing, in-place editing (<code>-i</code>), log parsing, CSV/TSV manipulation, and building complex one-liners incrementally.</p>"},{"location":"Dev%20Zero/Perl/#networking-and-daemons","title":"Networking and Daemons","text":"<p>Network programming and background services. Covers <code>IO::Socket</code>, client-server patterns, HTTP with <code>HTTP::Tiny</code> and <code>LWP</code>, <code>Mojolicious::UserAgent</code>, writing daemons, PID management, signal handling, and process supervision.</p>"},{"location":"Dev%20Zero/Perl/#web-frameworks-and-apis","title":"Web Frameworks and APIs","text":"<p>Building web applications. Covers PSGI/Plack, <code>Mojolicious</code> (routes, templates, WebSockets), <code>Dancer2</code>, RESTful API design, JSON handling, middleware, authentication patterns, and deployment.</p>"},{"location":"Dev%20Zero/Perl/#reference","title":"Reference","text":""},{"location":"Dev%20Zero/Perl/#developer-roadmap","title":"Developer Roadmap","text":"<p>The full learning path from operating system fundamentals through professional Perl development. Phase-by-phase progression with book recommendations, community resources, and career milestones.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/","title":"Arrays, Hashes, and Lists","text":""},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#collecting-and-organizing-data","title":"Collecting and Organizing Data","text":"<p>Version: 1.0 Year: 2026</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#copyright-notice","title":"Copyright Notice","text":"<p>Copyright (c) 2025-2026 Ryan Thomas Robson / Robworks Software LLC. Licensed under CC BY-NC-ND 4.0. You may share this material for non-commercial purposes with attribution, but you may not distribute modified versions.</p> <p>Scalars hold a single value. That gets you surprisingly far, but real programs need collections - ordered lists of items, and named lookups that map keys to values. Perl gives you two aggregate data types for this: arrays and hashes. Everything else - lists, slices, iteration patterns, and nested structures - builds on top of these two.</p> <p>This guide covers the tools you need to collect, organize, filter, transform, and iterate over data in Perl.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#arrays","title":"Arrays","text":"<p>An array is an ordered collection of scalars, identified by the <code>@</code> sigil. Each element has a numeric index starting at 0.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#creating-arrays","title":"Creating Arrays","text":"<pre><code># List assignment\nmy @colors = ('red', 'green', 'blue');\n\n# qw() - quote words, splits on whitespace\nmy @days = qw(Monday Tuesday Wednesday Thursday Friday);\n\n# Range operator\nmy @nums = (1..10);       # 1, 2, 3, ... 10\nmy @letters = ('a'..'z'); # a, b, c, ... z\n\n# Empty array\nmy @empty = ();\n</code></pre> <p>The <code>qw()</code> operator is a shorthand for quoting a list of words. It splits on whitespace and returns a list of strings - no commas or quotes needed.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#accessing-elements","title":"Accessing Elements","text":"<p>When you access a single element from an array, you use the <code>$</code> sigil because a single element is a scalar:</p> <pre><code>my @fruits = ('apple', 'banana', 'cherry', 'date');\n\nprint $fruits[0];   # apple  (first element)\nprint $fruits[2];   # cherry (third element)\nprint $fruits[-1];  # date   (last element)\nprint $fruits[-2];  # cherry (second from last)\n</code></pre> <p>Negative indices count backwards from the end. <code>-1</code> is the last element, <code>-2</code> is second-to-last, and so on.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#array-length-and-last-index","title":"Array Length and Last Index","text":"<p>Two common operations: getting the number of elements and getting the last valid index.</p> <pre><code>my @arr = ('a', 'b', 'c', 'd');\n\n# Last index: $#array\nprint $#arr;           # 3 (indices 0..3)\n\n# Element count: scalar @array\nprint scalar @arr;     # 4\n\n# These are equivalent ways to access the last element\nprint $arr[$#arr];     # d\nprint $arr[-1];        # d\n</code></pre> <p><code>$#array</code> returns the index of the last element (one less than the count). <code>scalar @array</code> forces the array into scalar context, where an array evaluates to its element count. Any situation that expects a single value triggers scalar context automatically - <code>if (@array)</code> is true when the array is non-empty.</p> <p>Array in Boolean Context</p> <p>An empty array evaluates to <code>0</code> (false) in boolean context, and a non-empty array evaluates to its element count (true). This means <code>if (@array) { ... }</code> is the idiomatic way to check whether an array has elements.</p> <p>Working with Arrays (requires JavaScript)</p> <p>Given my @arr = ('a', 'b', 'c', 'd'); what is the value of $#arr? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#array-operations","title":"Array Operations","text":"<p>Perl provides built-in functions to add, remove, and rearrange array elements without manual index management.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#adding-and-removing-elements","title":"Adding and Removing Elements","text":"<pre><code>my @stack = (1, 2, 3);\n\n# push/pop: work on the END of the array\npush @stack, 4;          # @stack = (1, 2, 3, 4)\npush @stack, 5, 6;       # @stack = (1, 2, 3, 4, 5, 6)\nmy $top = pop @stack;    # $top = 6, @stack = (1, 2, 3, 4, 5)\n\n# shift/unshift: work on the BEGINNING of the array\nmy $first = shift @stack;    # $first = 1, @stack = (2, 3, 4, 5)\nunshift @stack, 0;           # @stack = (0, 2, 3, 4, 5)\nunshift @stack, -2, -1;      # @stack = (-2, -1, 0, 2, 3, 4, 5)\n</code></pre> <p><code>push</code> and <code>pop</code> make the array behave like a stack (last-in, first-out). <code>shift</code> and <code>unshift</code> work from the front - useful for queues (first-in, first-out). All four modify the array in place.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#splice-the-swiss-army-knife","title":"Splice: The Swiss Army Knife","text":"<p><code>splice</code> can insert, remove, or replace elements at any position:</p> <pre><code>my @arr = ('a', 'b', 'c', 'd', 'e');\n\n# splice(ARRAY, OFFSET, LENGTH, REPLACEMENT_LIST)\n\n# Remove 2 elements starting at index 1\nmy @removed = splice(@arr, 1, 2);\n# @removed = ('b', 'c'), @arr = ('a', 'd', 'e')\n\n# Insert without removing (LENGTH = 0)\nsplice(@arr, 1, 0, 'x', 'y');\n# @arr = ('a', 'x', 'y', 'd', 'e')\n\n# Replace 1 element at index 2\nsplice(@arr, 2, 1, 'z');\n# @arr = ('a', 'x', 'z', 'd', 'e')\n</code></pre> <p>Think of <code>splice</code> as the generalized form. <code>push</code>, <code>pop</code>, <code>shift</code>, and <code>unshift</code> are just convenient shortcuts for common splice operations.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#reverse-and-sort","title":"Reverse and Sort","text":"<pre><code>my @letters = ('c', 'a', 'd', 'b');\n\n# reverse returns a new list in reversed order\nmy @backwards = reverse @letters;  # ('b', 'd', 'a', 'c')\n\n# sort returns a new list in sorted order (alphabetical by default)\nmy @sorted = sort @letters;        # ('a', 'b', 'c', 'd')\n\n# Neither modifies the original\nprint \"@letters\\n\";  # c a d b\n</code></pre> <p><code>reverse</code> and <code>sort</code> return new lists - the original array stays untouched unless you assign back to it: <code>@letters = sort @letters;</code></p> <p>Sort Is Alphabetical by Default</p> <p><code>sort</code> compares elements as strings by default. This means <code>sort (10, 2, 30)</code> produces <code>(10, 2, 30)</code> because the string <code>\"10\"</code> comes before <code>\"2\"</code> in ASCII order. You need a custom comparator for numeric sorting - covered in the Sorting section below.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#lists-and-list-assignment","title":"Lists and List Assignment","text":"<p>A list is an ordered sequence of scalars. It is not a data type - it is a temporary construct that exists during evaluation. Arrays store lists, but a list and an array are not the same thing.</p> <pre><code># This is a list literal\n(1, 2, 3)\n\n# Assigned to an array, it becomes the array's contents\nmy @nums = (1, 2, 3);\n\n# List assignment to scalars\nmy ($x, $y, $z) = (10, 20, 30);\nprint \"$x $y $z\\n\";  # 10 20 30\n</code></pre>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#swapping-variables","title":"Swapping Variables","text":"<p>List assignment makes variable swapping trivial - no temporary variable needed:</p> <pre><code>my ($a, $b) = ('first', 'second');\n($a, $b) = ($b, $a);\nprint \"$a $b\\n\";  # second first\n</code></pre> <p>Perl evaluates the right side completely before assigning to the left side, so the swap works without a temp.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#array-slices","title":"Array Slices","text":"<p>An array slice extracts multiple elements at once, returning a list. Use the <code>@</code> sigil because you are getting multiple values:</p> <pre><code>my @arr = ('zero', 'one', 'two', 'three', 'four');\n\n# Slice: specific indices\nmy @subset = @arr[1, 3];       # ('one', 'three')\n\n# Slice: range\nmy @middle = @arr[1..3];       # ('one', 'two', 'three')\n\n# Slice assignment\n@arr[0, 4] = ('ZERO', 'FOUR');\nprint \"@arr\\n\";  # ZERO one two three FOUR\n</code></pre> <p>The sigil change is a common source of confusion: <code>$arr[0]</code> (one element, scalar) vs. <code>@arr[0,2]</code> (multiple elements, list). The sigil indicates what you are getting back, not what the variable is.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#context-in-assignment","title":"Context in Assignment","text":"<p>When a list has more values than variables, extras are discarded. When it has fewer, remaining variables get <code>undef</code>:</p> <pre><code># Extra values discarded\nmy ($first, $second) = (10, 20, 30, 40);\n# $first = 10, $second = 20 (30 and 40 are lost)\n\n# Missing values become undef\nmy ($a, $b, $c) = (1, 2);\n# $a = 1, $b = 2, $c = undef\n\n# An array in a list absorbs everything remaining\nmy ($head, @rest) = (1, 2, 3, 4, 5);\n# $head = 1, @rest = (2, 3, 4, 5)\n</code></pre> <p>Array Absorbs All Remaining Values</p> <p>If you put an array in the middle of a list assignment, it will consume all remaining values and leave subsequent variables as <code>undef</code>:</p> <pre><code>my (@arr, $last) = (1, 2, 3, 4);\n# @arr = (1, 2, 3, 4), $last = undef  &lt;-- probably not what you wanted\n</code></pre> <p>Always put arrays and hashes at the end of a list assignment.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#iteration","title":"Iteration","text":""},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#foreach-for","title":"foreach / for","text":"<p>The most common way to iterate over an array is <code>foreach</code>:</p> <pre><code>my @names = qw(Alice Bob Charlie);\n\nforeach my $name (@names) {\n    print \"Hello, $name!\\n\";\n}\n</code></pre> <p><code>foreach</code> and <code>for</code> are interchangeable when used with a list - Perl treats them identically:</p> <pre><code># These are the same\nforeach my $item (@list) { ... }\nfor my $item (@list) { ... }\n</code></pre> <p>Most Perl programmers use <code>for</code> (shorter) for list iteration and reserve the C-style syntax when they actually need it.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#the-default-variable-_","title":"The Default Variable: $_","text":"<p>If you omit the loop variable, Perl uses <code>$_</code>, the default variable:</p> <pre><code>my @words = qw(hello world);\n\nfor (@words) {\n    print \"Word: $_\\n\";\n}\n</code></pre> <p>Many built-in functions operate on <code>$_</code> by default: <code>print</code>, <code>chomp</code>, <code>split</code>, <code>lc</code>, <code>uc</code>, and others. This makes <code>$_</code> the implicit \"it\" that flows through your code.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#c-style-for-loop","title":"C-Style for Loop","text":"<p>When you need an index counter, use the C-style form:</p> <pre><code>my @items = qw(alpha beta gamma);\n\nfor (my $i = 0; $i &lt;= $#items; $i++) {\n    print \"$i: $items[$i]\\n\";\n}\n</code></pre> <p>Output:</p> <pre><code>0: alpha\n1: beta\n2: gamma\n</code></pre>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#modifying-elements-during-iteration","title":"Modifying Elements During Iteration","text":"<p>The loop variable in <code>for</code> is an alias to the actual array element, not a copy. Modifying it changes the array:</p> <pre><code>my @nums = (1, 2, 3, 4);\n\nfor my $n (@nums) {\n    $n *= 10;\n}\n\nprint \"@nums\\n\";  # 10 20 30 40\n</code></pre> <p>Alias Behavior</p> <p>This alias behavior is intentional and efficient - Perl does not copy the element. But it also means you can accidentally modify your data. If you need a copy, assign to a new variable inside the loop: <code>my $copy = $n;</code></p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#while-with-arrays","title":"while with Arrays","text":"<p>You can drain an array with <code>while</code> and <code>shift</code>:</p> <pre><code>my @queue = qw(first second third);\n\nwhile (my $item = shift @queue) {\n    print \"Processing: $item\\n\";\n}\n# @queue is now empty\n</code></pre> <p>This pattern treats the array as a queue: process the front element, then remove it.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#sorting","title":"Sorting","text":"<p>Default sort compares elements as strings using Perl's <code>cmp</code> operator. For anything else, you supply a comparison block.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#default-sort-alphabetical","title":"Default Sort (Alphabetical)","text":"<pre><code>my @words = qw(banana cherry apple date);\nmy @sorted = sort @words;\nprint \"@sorted\\n\";  # apple banana cherry date\n</code></pre>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#numeric-sort","title":"Numeric Sort","text":"<p>The spaceship operator <code>&lt;=&gt;</code> compares two numbers and returns -1, 0, or 1:</p> <pre><code>my @nums = (42, 5, 17, 100, 3);\nmy @sorted = sort { $a &lt;=&gt; $b } @nums;\nprint \"@sorted\\n\";  # 3 5 17 42 100\n\n# Descending: swap $a and $b\nmy @desc = sort { $b &lt;=&gt; $a } @nums;\nprint \"@desc\\n\";  # 100 42 17 5 3\n</code></pre> <p>Inside the sort block, <code>$a</code> and <code>$b</code> are special package variables that Perl sets to the two elements being compared. The block must return a negative number, zero, or positive number to indicate ordering.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#case-insensitive-sort","title":"Case-Insensitive Sort","text":"<pre><code>my @names = qw(Charlie alice Bob);\nmy @sorted = sort { lc($a) cmp lc($b) } @names;\nprint \"@sorted\\n\";  # alice Bob Charlie\n</code></pre> <p>The <code>cmp</code> operator does string comparison (like <code>&lt;=&gt;</code> does numeric comparison). Wrapping in <code>lc()</code> normalizes case before comparing.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#multi-key-sort","title":"Multi-Key Sort","text":"<p>Chain comparisons with <code>||</code> (or) to break ties:</p> <pre><code>my @people = (\n    { name =&gt; 'Alice',   age =&gt; 30 },\n    { name =&gt; 'Bob',     age =&gt; 25 },\n    { name =&gt; 'Charlie', age =&gt; 30 },\n);\n\nmy @sorted = sort {\n    $a-&gt;{age} &lt;=&gt; $b-&gt;{age}       # primary: age ascending\n    ||\n    $a-&gt;{name} cmp $b-&gt;{name}     # secondary: name ascending\n} @people;\n\nfor my $p (@sorted) {\n    print \"$p-&gt;{name}: $p-&gt;{age}\\n\";\n}\n# Bob: 25\n# Alice: 30\n# Charlie: 30\n</code></pre> <p>The <code>||</code> short-circuits: if the first comparison returns non-zero, that result is used. If it returns zero (equal ages), the second comparison breaks the tie.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#the-schwartzian-transform","title":"The Schwartzian Transform","text":"<p>When sorting by a computed value, you do not want to recompute it on every comparison. The Schwartzian transform caches the sort key:</p> <pre><code># Sort files by size (compute stat only once per file)\nmy @sorted =\n    map  { $_-&gt;[0] }                    # 3. extract filename\n    sort { $a-&gt;[1] &lt;=&gt; $b-&gt;[1] }        # 2. sort by size\n    map  { [$_, -s $_] }                # 1. pair filename with size\n    @files;\n</code></pre> <p>This reads bottom-to-top: wrap each element with its computed key, sort by the key, then unwrap. Named after Randal Schwartz, who popularized the pattern on Usenet.</p> <p>Sorting Arrays (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#hashes","title":"Hashes","text":"<p>A hash (also called an associative array) is an unordered collection of key-value pairs, identified by the <code>%</code> sigil. Keys are always strings. Values are scalars.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#creating-hashes","title":"Creating Hashes","text":"<pre><code># Fat comma (=&gt;) auto-quotes the left side\nmy %person = (\n    name =&gt; 'Alice',\n    age  =&gt; 30,\n    city =&gt; 'Portland',\n);\n\n# Equivalent using plain commas (but less readable)\nmy %person = ('name', 'Alice', 'age', 30, 'city', 'Portland');\n</code></pre> <p>The fat comma <code>=&gt;</code> is syntactically identical to a comma, but it auto-quotes the word on its left side. This makes hash initialization readable and eliminates the need for quotes on simple keys.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#hash-structure","title":"Hash Structure","text":"<p>A hash maps string keys to scalar values. Unlike arrays, there is no defined order:</p> <pre><code>flowchart LR\n    H[\"%colors\"] --&gt; K1[\"'red'\"] --&gt; V1[\"'#FF0000'\"]\n    H --&gt; K2[\"'green'\"] --&gt; V2[\"'#00FF00'\"]\n    H --&gt; K3[\"'blue'\"] --&gt; V3[\"'#0000FF'\"]</code></pre>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#accessing-values","title":"Accessing Values","text":"<p>Use the <code>$</code> sigil (you are retrieving a single scalar value) with curly braces:</p> <pre><code>my %data = (name =&gt; 'Alice', age =&gt; 30, city =&gt; 'Portland');\n\nprint $data{name};    # Alice\nprint $data{age};     # 30\n\n# Assigning a new key-value pair\n$data{email} = 'alice@example.com';\n\n# Overwriting an existing value\n$data{age} = 31;\n</code></pre> <p>Sigil Summary</p> <p>The sigil tells you what you are getting back, not what the variable is:</p> Expression Sigil Meaning <code>%hash</code> <code>%</code> The entire hash <code>$hash{key}</code> <code>$</code> One scalar value <code>@hash{@keys}</code> <code>@</code> A list of values (hash slice)"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#testing-and-deleting-keys","title":"Testing and Deleting Keys","text":"<pre><code>my %config = (debug =&gt; 1, verbose =&gt; 0, timeout =&gt; 30);\n\n# exists: does the key exist? (regardless of value)\nif (exists $config{debug}) {\n    print \"debug key exists\\n\";\n}\n\n# defined: is the value defined? (not undef)\nif (defined $config{verbose}) {\n    print \"verbose is defined (value: $config{verbose})\\n\";  # prints, value is 0\n}\n\n# delete: remove a key-value pair\ndelete $config{timeout};\nprint exists $config{timeout} ? \"yes\" : \"no\";  # no\n</code></pre> <p><code>exists</code> checks whether a key is present. <code>defined</code> checks whether the value is not <code>undef</code>. <code>delete</code> removes the key-value pair entirely. The distinction matters: a key can exist with a value of <code>undef</code>, <code>0</code>, or an empty string - all of which are valid.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#hash-slices","title":"Hash Slices","text":"<p>Like array slices, you can extract multiple values at once:</p> <pre><code>my %scores = (math =&gt; 95, english =&gt; 88, science =&gt; 92, history =&gt; 78);\n\n# Hash slice: returns a list of values\nmy @subset = @scores{qw(math science)};\nprint \"@subset\\n\";  # 95 92\n\n# Hash slice assignment\n@scores{qw(art music)} = (85, 90);\n</code></pre> <p>Note the <code>@</code> sigil on the slice - you are getting back a list of values, not a single scalar.</p> <p>Given my %data = (name =&gt; 'Alice', age =&gt; 30); how do you access the age value? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#hash-iteration","title":"Hash Iteration","text":"<p>Hashes have no inherent order, but Perl gives you several ways to walk through all key-value pairs.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#keys-values-each","title":"keys, values, each","text":"<pre><code>my %inventory = (apples =&gt; 12, bananas =&gt; 6, cherries =&gt; 50);\n\n# keys: returns a list of all keys\nmy @fruits = keys %inventory;\nprint \"@fruits\\n\";  # (order not guaranteed)\n\n# values: returns a list of all values\nmy @counts = values %inventory;\nprint \"@counts\\n\";  # (order not guaranteed)\n\n# each: returns the next (key, value) pair\nwhile (my ($fruit, $count) = each %inventory) {\n    print \"$fruit: $count\\n\";\n}\n</code></pre> <p><code>keys</code> and <code>values</code> return complete lists. <code>each</code> returns one pair at a time and maintains an internal iterator - useful for very large hashes where you do not want to build the entire key list in memory.</p> <p>Avoid <code>each</code> in Most Code</p> <p>The <code>each</code> function has a hidden internal iterator tied to the hash. If you exit a <code>while (each)</code> loop early, the iterator is not reset, and the next call to <code>each</code> resumes where you left off. This causes subtle bugs. Prefer <code>for my $key (keys %hash)</code> unless you have a specific reason to use <code>each</code>.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#sorted-iteration","title":"Sorted Iteration","text":"<p>Since hash order is unpredictable, sort the keys when you need consistent output:</p> <pre><code>my %capitals = (\n    France  =&gt; 'Paris',\n    Germany =&gt; 'Berlin',\n    Japan   =&gt; 'Tokyo',\n    Brazil  =&gt; 'Brasilia',\n);\n\nfor my $country (sort keys %capitals) {\n    printf \"%-10s =&gt; %s\\n\", $country, $capitals{$country};\n}\n</code></pre> <p>Output:</p> <pre><code>Brazil     =&gt; Brasilia\nFrance     =&gt; Paris\nGermany    =&gt; Berlin\nJapan      =&gt; Tokyo\n</code></pre> <p>You can also sort by value:</p> <pre><code># Sort by value (alphabetical)\nfor my $key (sort { $capitals{$a} cmp $capitals{$b} } keys %capitals) {\n    print \"$key: $capitals{$key}\\n\";\n}\n</code></pre> <p>Word Frequency Counter (requires JavaScript)</p> <p>Simple CSV Processor (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#powerful-list-operations","title":"Powerful List Operations","text":"<p>Perl provides several built-in functions for transforming and filtering lists without explicit loops. These are the workhorses of idiomatic Perl code.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#grep-filter-a-list","title":"grep: Filter a List","text":"<p><code>grep</code> returns elements where the block evaluates to true:</p> <pre><code>my @numbers = (1..20);\n\n# Keep only even numbers\nmy @even = grep { $_ % 2 == 0 } @numbers;\nprint \"@even\\n\";  # 2 4 6 8 10 12 14 16 18 20\n\n# Keep non-empty strings\nmy @words = ('hello', '', 'world', '', 'perl');\nmy @nonempty = grep { length $_ } @words;\nprint \"@nonempty\\n\";  # hello world perl\n\n# grep with a regex\nmy @errors = grep { /error/i } @log_lines;\n</code></pre> <p><code>grep</code> is Perl's list filter. The block runs once for each element, with <code>$_</code> set to the current element. Elements where the block returns true are included in the result.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#map-transform-a-list","title":"map: Transform a List","text":"<p><code>map</code> applies a transformation to every element and returns the results:</p> <pre><code>my @names = qw(alice bob charlie);\n\n# Capitalize each name\nmy @upper = map { ucfirst $_ } @names;\nprint \"@upper\\n\";  # Alice Bob Charlie\n\n# Square each number\nmy @nums = (1, 2, 3, 4, 5);\nmy @squares = map { $_ ** 2 } @nums;\nprint \"@squares\\n\";  # 1 4 9 16 25\n\n# map can return multiple values per element\nmy @pairs = map { ($_, $_ * 2) } (1, 2, 3);\nprint \"@pairs\\n\";  # 1 2 2 4 3 6\n</code></pre> <p><code>map</code> is the transformation counterpart to <code>grep</code>. The block runs for each element and returns whatever it produces. If the block returns a list, those values are flattened into the result.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#split-and-join","title":"split and join","text":"<p><code>split</code> breaks a string into a list. <code>join</code> combines a list into a string:</p> <pre><code># split: string -&gt; list\nmy $csv = \"Alice,30,Portland\";\nmy @fields = split /,/, $csv;\nprint \"$fields[0]\\n\";   # Alice\nprint \"$fields[1]\\n\";   # 30\n\n# join: list -&gt; string\nmy @parts = ('usr', 'local', 'bin');\nmy $path = join '/', @parts;\nprint \"$path\\n\";         # usr/local/bin\n\n# Split on whitespace (default behavior)\nmy $line = \"  hello   world  \";\nmy @words = split ' ', $line;    # ('hello', 'world')\n</code></pre> <p>split with a String vs. Regex</p> <p><code>split ' ', $str</code> (with a literal space string, not a regex) has special behavior: it splits on any whitespace and discards leading whitespace. This matches <code>awk</code>'s default field splitting. <code>split /\\s+/, $str</code> does not discard leading whitespace and may produce an empty first field.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#chaining-operations","title":"Chaining Operations","text":"<p>Combine <code>map</code>, <code>grep</code>, <code>sort</code>, and <code>join</code> to build data pipelines:</p> <pre><code>my @raw = qw(banana APPLE cherry apple BANANA Cherry);\n\n# Normalize, deduplicate, sort, and format\nmy @result =\n    sort\n    grep { !$seen{$_}++ }\n    map  { lc $_ }\n    @raw;\n\nprint join(', ', @result), \"\\n\";\n# apple, banana, cherry\n</code></pre> <p>Read from bottom to top: <code>map</code> lowercases everything, <code>grep</code> keeps only the first occurrence of each value (using <code>%seen</code> to track duplicates), <code>sort</code> orders alphabetically.</p> <pre><code># Sum of squares of even numbers from 1-20\nmy $sum = 0;\n$sum += $_ for\n    map  { $_ ** 2 }\n    grep { $_ % 2 == 0 }\n    (1..20);\n\nprint \"$sum\\n\";  # 1540\n</code></pre> <p>What does this code produce? my @nums = (1..10); my @even = grep { $_ % 2 == 0 } @nums; print \"@even\"; (requires JavaScript)</p> <p>Command Builder (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#nested-data-structures","title":"Nested Data Structures","text":"<p>So far, arrays hold scalars and hashes map strings to scalars. But what if you need an array of arrays, or a hash whose values are arrays? Perl handles this with references - scalar values that point to other data structures. A full treatment of references comes in a later guide, but you need the basics here to build real-world data structures.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#references-in-brief","title":"References in Brief","text":"<p>A reference is a scalar that holds the memory address of another variable. Create one with <code>\\</code> and dereference with the appropriate sigil or arrow notation:</p> <pre><code>my @colors = ('red', 'green', 'blue');\nmy $ref = \\@colors;       # $ref is a reference to @colors\n\nprint $ref-&gt;[0];           # red (arrow notation)\nprint ${$ref}[1];          # green (block dereference)\n</code></pre> <p>Anonymous references skip the named variable:</p> <pre><code>my $arr_ref = ['red', 'green', 'blue'];   # anonymous array ref\nmy $hash_ref = { name =&gt; 'Alice', age =&gt; 30 };  # anonymous hash ref\n</code></pre> <p>Square brackets <code>[]</code> create an anonymous array reference. Curly braces <code>{}</code> (in a value context) create an anonymous hash reference.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#arrays-of-arrays","title":"Arrays of Arrays","text":"<pre><code>my @matrix = (\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9],\n);\n\n# Access element at row 1, column 2\nprint $matrix[1][2], \"\\n\";    # 6\nprint $matrix[1]-&gt;[2], \"\\n\";  # 6 (same thing - arrow is optional between brackets)\n\n# Iterate\nfor my $row (@matrix) {\n    print join(', ', @$row), \"\\n\";\n}\n# 1, 2, 3\n# 4, 5, 6\n# 7, 8, 9\n</code></pre> <p>Each element of <code>@matrix</code> is a reference to an anonymous array. <code>$matrix[1]</code> gives you the reference, and <code>[2]</code> indexes into the array it points to.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#hashes-of-arrays","title":"Hashes of Arrays","text":"<p>Group related lists under named keys:</p> <pre><code>my %departments = (\n    Engineering =&gt; ['Alice', 'Charlie', 'Eve'],\n    Marketing   =&gt; ['Bob', 'Diana'],\n    Support     =&gt; ['Frank', 'Grace', 'Hank'],\n);\n\n# Access a specific person\nprint $departments{Engineering}[0], \"\\n\";  # Alice\n\n# Add to a department\npush @{$departments{Marketing}}, 'Ivan';\n\n# Iterate\nfor my $dept (sort keys %departments) {\n    my @members = @{$departments{$dept}};\n    print \"$dept: \", join(', ', @members), \"\\n\";\n}\n# Engineering: Alice, Charlie, Eve\n# Marketing: Bob, Diana, Ivan\n# Support: Frank, Grace, Hank\n</code></pre>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#nested-data-structure","title":"Nested Data Structure","text":"<p>Here is how a hash of arrays looks in memory:</p> <pre><code>flowchart TD\n    R[\"%departments\"] --&gt; E[\"'Engineering'\"]\n    R --&gt; M[\"'Marketing'\"]\n    E --&gt; EA[\"['Alice', 'Charlie', 'Eve']\"]\n    M --&gt; MA[\"['Bob', 'Diana']\"]</code></pre> <p>Each key points to a reference, and each reference points to an array of scalars.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#hashes-of-hashes","title":"Hashes of Hashes","text":"<p>The most common nested structure for record-like data:</p> <pre><code>my %users = (\n    alice =&gt; {\n        email =&gt; 'alice@example.com',\n        role  =&gt; 'admin',\n        age   =&gt; 30,\n    },\n    bob =&gt; {\n        email =&gt; 'bob@example.com',\n        role  =&gt; 'user',\n        age   =&gt; 25,\n    },\n);\n\n# Access nested value\nprint $users{alice}{email}, \"\\n\";   # alice@example.com\n\n# Add a new user\n$users{charlie} = {\n    email =&gt; 'charlie@example.com',\n    role  =&gt; 'user',\n    age   =&gt; 28,\n};\n\n# Iterate over all users\nfor my $name (sort keys %users) {\n    my $info = $users{$name};\n    printf \"%-10s %-25s %s\\n\", $name, $info-&gt;{email}, $info-&gt;{role};\n}\n</code></pre> <p>Output:</p> <pre><code>alice      alice@example.com         admin\nbob        bob@example.com           user\ncharlie    charlie@example.com       user\n</code></pre>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#arrow-notation","title":"Arrow Notation","text":"<p>When chaining dereferences, the arrow <code>-&gt;</code> between adjacent brackets is optional:</p> <pre><code>my %data = (\n    servers =&gt; [\n        { name =&gt; 'web1', ip =&gt; '10.0.0.1' },\n        { name =&gt; 'web2', ip =&gt; '10.0.0.2' },\n    ],\n);\n\n# All three are equivalent\nprint $data{servers}-&gt;[0]-&gt;{name};  # web1 (explicit arrows)\nprint $data{servers}[0]{name};      # web1 (arrows optional between brackets)\nprint ${$data{servers}}[0]{name};   # web1 (block dereference)\n</code></pre> <p>The arrow-between-brackets rule applies only between adjacent subscripts. The first arrow after a variable name is still required: <code>$ref-&gt;[0]</code> (not <code>$ref[0]</code>, which would access <code>@ref</code>).</p> <p>Autovivification</p> <p>Perl automatically creates intermediate data structures when you access a nested path that does not exist yet:</p> <pre><code>my %data;\n$data{a}{b}{c} = 'deep';\n# Perl silently created $data{a} as a hashref, and $data{a}{b} as a hashref\n</code></pre> <p>This is convenient but can mask typos. The <code>autovivification</code> pragma on CPAN lets you disable it selectively.</p> <p>Phone Book Application (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#putting-it-all-together","title":"Putting It All Together","text":"<p>Here is a complete example that combines arrays, hashes, list operations, sorting, and nested structures to solve a practical problem:</p> <pre><code>use strict;\nuse warnings;\n\n# Log entries: timestamp, severity, message\nmy @log = (\n    { time =&gt; '09:01', level =&gt; 'INFO',  msg =&gt; 'Server started' },\n    { time =&gt; '09:05', level =&gt; 'WARN',  msg =&gt; 'Disk usage at 80%' },\n    { time =&gt; '09:12', level =&gt; 'ERROR', msg =&gt; 'Connection refused to db1' },\n    { time =&gt; '09:15', level =&gt; 'INFO',  msg =&gt; 'Backup completed' },\n    { time =&gt; '09:22', level =&gt; 'ERROR', msg =&gt; 'Connection refused to db2' },\n    { time =&gt; '09:30', level =&gt; 'WARN',  msg =&gt; 'Memory usage at 75%' },\n    { time =&gt; '09:45', level =&gt; 'ERROR', msg =&gt; 'Timeout on api endpoint' },\n);\n\n# Count entries by severity\nmy %counts;\n$counts{$_-&gt;{level}}++ for @log;\n\nprint \"Summary:\\n\";\nfor my $level (sort keys %counts) {\n    printf \"  %-8s %d entries\\n\", $level, $counts{$level};\n}\n\n# Extract error messages\nmy @errors = map  { \"$_-&gt;{time} - $_-&gt;{msg}\" }\n             grep { $_-&gt;{level} eq 'ERROR' }\n             @log;\n\nprint \"\\nErrors:\\n\";\nprint \"  $_\\n\" for @errors;\n\n# Group messages by severity\nmy %by_level;\nfor my $entry (@log) {\n    push @{$by_level{$entry-&gt;{level}}}, $entry-&gt;{msg};\n}\n\nprint \"\\nGrouped:\\n\";\nfor my $level (sort keys %by_level) {\n    print \"  $level:\\n\";\n    print \"    - $_\\n\" for @{$by_level{$level}};\n}\n</code></pre> <p>Output:</p> <pre><code>Summary:\n  ERROR    3 entries\n  INFO     2 entries\n  WARN     2 entries\n\nErrors:\n  09:12 - Connection refused to db1\n  09:22 - Connection refused to db2\n  09:45 - Timeout on api endpoint\n\nGrouped:\n  ERROR:\n    - Connection refused to db1\n    - Connection refused to db2\n    - Timeout on api endpoint\n  INFO:\n    - Server started\n    - Backup completed\n  WARN:\n    - Disk usage at 80%\n    - Memory usage at 75%\n</code></pre> <p>This pattern - filter with <code>grep</code>, transform with <code>map</code>, group into hashes, sort for output - is the core of practical Perl data processing.</p>"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#quick-reference","title":"Quick Reference","text":"Operation Syntax Description Create array <code>my @arr = (1, 2, 3)</code> Ordered list of scalars Access element <code>$arr[0]</code> Single element (scalar context) Last index <code>$#arr</code> Highest valid index Count <code>scalar @arr</code> Number of elements Add to end <code>push @arr, $val</code> Append element(s) Remove from end <code>pop @arr</code> Remove and return last Add to front <code>unshift @arr, $val</code> Prepend element(s) Remove from front <code>shift @arr</code> Remove and return first Array slice <code>@arr[1,3,5]</code> Multiple elements Create hash <code>my %h = (k =&gt; 'v')</code> Key-value pairs Access value <code>$h{key}</code> Single value (scalar context) Key exists? <code>exists $h{key}</code> Boolean test Delete key <code>delete $h{key}</code> Remove pair All keys <code>keys %h</code> List of keys All values <code>values %h</code> List of values Hash slice <code>@h{qw(a b)}</code> Multiple values Filter list <code>grep { ... } @list</code> Keep matching elements Transform list <code>map { ... } @list</code> Apply block to each Sort list <code>sort { ... } @list</code> Custom ordering Split string <code>split /,/, $str</code> String to list Join list <code>join ',', @list</code> List to string"},{"location":"Dev%20Zero/Perl/arrays-hashes-lists/#further-reading","title":"Further Reading","text":"<ul> <li>perldata - Perl data types: scalars, arrays, and hashes</li> <li>perlfunc - complete list of Perl built-in functions</li> <li>perlref - Perl references and nested data structures</li> <li>perldsc - Perl data structures cookbook (arrays of arrays, hashes of hashes, etc.)</li> <li>perllol - manipulating arrays of arrays</li> <li>perlop - Perl operators including <code>&lt;=&gt;</code>, <code>cmp</code>, <code>=&gt;</code>, and <code>qw()</code></li> <li>Learning Perl, Chapter 3-6 - the \"Llama Book\" coverage of lists, arrays, hashes, and I/O</li> <li>Intermediate Perl - the \"Alpaca Book,\" focused on references and data structures</li> </ul> <p>Previous: Scalars, Strings, and Numbers | Next: Control Flow | Back to Index</p>"},{"location":"Dev%20Zero/Perl/control-flow/","title":"Control Flow","text":""},{"location":"Dev%20Zero/Perl/control-flow/#directing-program-logic","title":"Directing Program Logic","text":"<p>Version: 1.0 Year: 2026</p>"},{"location":"Dev%20Zero/Perl/control-flow/#copyright-notice","title":"Copyright Notice","text":"<p>Copyright (c) 2025-2026 Ryan Thomas Robson / Robworks Software LLC. Licensed under CC BY-NC-ND 4.0. You may share this material for non-commercial purposes with attribution, but you may not distribute modified versions.</p>"},{"location":"Dev%20Zero/Perl/control-flow/#conditionals-ifelsifelse","title":"Conditionals: if/elsif/else","text":"<p>Every program needs to make decisions. In Perl, the primary tool for branching is <code>if</code>:</p> <pre><code>my $temperature = 95;\n\nif ($temperature &gt; 100) {\n    print \"Boiling!\\n\";\n} elsif ($temperature &gt; 80) {\n    print \"Hot\\n\";\n} elsif ($temperature &gt; 60) {\n    print \"Comfortable\\n\";\n} else {\n    print \"Cold\\n\";\n}\n</code></pre> <p>A few things stand out immediately:</p> <ul> <li>The condition must be in parentheses.</li> <li>The body must be in braces, even for a single statement. Perl does not allow braceless <code>if</code> blocks (unlike C or JavaScript).</li> <li><code>elsif</code> has no second <code>e</code>. This trips up every newcomer exactly once.</li> </ul>"},{"location":"Dev%20Zero/Perl/control-flow/#comparison-operators","title":"Comparison Operators","text":"<p>Perl has two complete sets of comparison operators - one for numbers and one for strings. Using the wrong set is one of the most common Perl bugs.</p> Operation Numeric String Equal <code>==</code> <code>eq</code> Not equal <code>!=</code> <code>ne</code> Less than <code>&lt;</code> <code>lt</code> Greater than <code>&gt;</code> <code>gt</code> Less or equal <code>&lt;=</code> <code>le</code> Greater or equal <code>&gt;=</code> <code>ge</code> Comparison (three-way) <code>&lt;=&gt;</code> <code>cmp</code> <p>The three-way operators (<code>&lt;=&gt;</code> and <code>cmp</code>) return -1, 0, or 1. They are essential for custom <code>sort</code> comparators:</p> <pre><code>my @sorted = sort { $a &lt;=&gt; $b } @numbers;    # numeric ascending\nmy @alpha  = sort { $a cmp $b } @strings;     # alphabetical\n</code></pre> <p>String vs. Numeric Comparison</p> <p><code>\"apple\" == \"banana\"</code> evaluates to true because <code>==</code> forces both strings to numeric context, converting them to <code>0</code>. Use <code>eq</code> for string comparison. If <code>use warnings</code> is enabled (and it should be), Perl will warn you about this mistake.</p>"},{"location":"Dev%20Zero/Perl/control-flow/#compound-conditions","title":"Compound Conditions","text":"<p>Combine conditions with logical operators. Perl provides two styles:</p> Meaning Symbol (high precedence) Word (low precedence) AND <code>&amp;&amp;</code> <code>and</code> OR <code>\\|\\|</code> <code>or</code> NOT <code>!</code> <code>not</code> <pre><code># Symbol style - common in conditions\nif ($age &gt;= 18 &amp;&amp; $age &lt;= 65) {\n    print \"Working age\\n\";\n}\n\n# Word style - common in flow control\nopen my $fh, '&lt;', $file or die \"Cannot open $file: $!\";\n</code></pre> <p>The word operators (<code>and</code>, <code>or</code>, <code>not</code>) have lower precedence than assignment, which is why <code>open ... or die</code> works without parentheses. The symbol operators (<code>&amp;&amp;</code>, <code>||</code>, <code>!</code>) bind more tightly and are the standard choice inside conditionals.</p> <p>Precedence Rule of Thumb</p> <p>Use <code>&amp;&amp;</code>/<code>||</code>/<code>!</code> inside <code>if</code> conditions. Use <code>and</code>/<code>or</code>/<code>not</code> for flow control at the statement level (like <code>open ... or die</code>). Mixing them in the same expression leads to surprises.</p> <p>Conditional Expressions (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/control-flow/#unless","title":"unless","text":"<p><code>unless</code> is a negated <code>if</code>. It executes the block when the condition is false:</p> <pre><code>unless ($user_authenticated) {\n    redirect_to_login();\n}\n</code></pre> <p>This reads more naturally than <code>if (!$user_authenticated)</code> in many cases. But there are clear rules about when <code>unless</code> helps readability:</p> <p>Use <code>unless</code> when:</p> <ul> <li>The condition is a simple boolean or single test</li> <li>The English reads naturally: \"unless the user is authenticated, redirect\"</li> </ul> <p>Avoid <code>unless</code> when:</p> <ul> <li>You need an <code>elsif</code> or <code>else</code> branch - <code>unless/else</code> reads backwards and confuses everyone</li> <li>The condition is compound (<code>unless ($a &amp;&amp; !$b)</code> is a logic puzzle)</li> <li>You are using double negatives (<code>unless (!$found)</code> - just use <code>if ($found)</code>)</li> </ul> <pre><code># Good - reads naturally\ndie \"File not found\" unless -e $filename;\n\n# Bad - unless with else is confusing\nunless ($ready) {\n    wait_more();\n} else {\n    proceed();   # Wait, so this runs when $ready is true?\n}\n\n# Better - just use if\nif ($ready) {\n    proceed();\n} else {\n    wait_more();\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/control-flow/#the-ternary-operator","title":"The Ternary Operator","text":"<p>The ternary operator (<code>?:</code>) is an expression-level if/else. It evaluates to one of two values based on a condition:</p> <pre><code>my $status = ($count &gt; 0) ? \"active\" : \"empty\";\n</code></pre> <p>This is equivalent to:</p> <pre><code>my $status;\nif ($count &gt; 0) {\n    $status = \"active\";\n} else {\n    $status = \"empty\";\n}\n</code></pre> <p>The ternary operator shines in assignments, function arguments, and print statements - anywhere you need a value, not a block:</p> <pre><code>printf \"Found %d %s\\n\", $count, ($count == 1) ? \"item\" : \"items\";\n\nmy $label = $is_admin ? \"Administrator\" : \"User\";\n</code></pre> <p>You can nest ternaries, but deep nesting becomes unreadable quickly:</p> <pre><code># One level of nesting is sometimes acceptable\nmy $grade = ($score &gt;= 90) ? \"A\"\n          : ($score &gt;= 80) ? \"B\"\n          : ($score &gt;= 70) ? \"C\"\n          :                  \"F\";\n\n# More than two levels? Use if/elsif instead.\n</code></pre> <p>Tip</p> <p>If a ternary expression does not fit on one line or requires nesting more than two levels deep, switch to <code>if</code>/<code>elsif</code>. The few saved lines are not worth the readability cost.</p>"},{"location":"Dev%20Zero/Perl/control-flow/#statement-modifiers","title":"Statement Modifiers","text":"<p>Perl supports postfix conditionals - you can put <code>if</code>, <code>unless</code>, <code>while</code>, <code>until</code>, <code>for</code>, or <code>foreach</code> after a single statement:</p> <pre><code>print \"Found it!\\n\" if $found;\nwarn \"Disk full\\n\"  unless $space_available;\nprint $_ while &lt;STDIN&gt;;\n$total += $_ for @values;\n</code></pre> <p>Statement modifiers work only with a single statement on the left side. You cannot put a block before a modifier:</p> <pre><code># This is a syntax error\n{\n    print \"hello\\n\";\n    print \"world\\n\";\n} if $greet;\n\n# Do this instead\nif ($greet) {\n    print \"hello\\n\";\n    print \"world\\n\";\n}\n</code></pre> <p>The common modifiers and when to use them:</p> Modifier Meaning Example <code>if</code> Execute when true <code>return $cached if exists $cache{$key};</code> <code>unless</code> Execute when false <code>die \"Required\" unless defined $value;</code> <code>while</code> Loop while true <code>print &lt;$fh&gt; while defined($_ = &lt;$fh&gt;);</code> <code>until</code> Loop until true <code>sleep 1 until -e $lockfile;</code> <code>for</code>/<code>foreach</code> Loop over list <code>print \"$_\\n\" for @names;</code> <p>Statement modifiers read like English and reduce visual clutter for simple operations. The convention is: use the modifier form when the action is more important than the condition, and the whole thing fits on one line.</p> <pre><code># Modifier form - emphasizes the action\nnext if $line =~ /^#/;\n\n# Block form - emphasizes the condition\nif ($line =~ /^#/) {\n    $comment_count++;\n    push @comments, $line;\n    next;\n}\n</code></pre> <p>FizzBuzz in Perl (requires JavaScript)</p> <p>What does this code do? print \"found it!\\n\" and exit if $x == 42; (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/control-flow/#perls-truth-rules","title":"Perl's Truth Rules","text":"<p>Truth in Perl is simple once you learn the four false values. Understanding truthiness is fundamental to every conditional, loop, and short-circuit operator in the language.</p> <p>A value is false if it is:</p> <ol> <li><code>undef</code> - no value at all</li> <li><code>\"\"</code> - the empty string</li> <li><code>0</code> - the number zero</li> <li><code>\"0\"</code> - the string containing just the character zero</li> </ol> <p>Everything else is true. This includes <code>\"0E0\"</code>, <code>\" \"</code> (a space), <code>\"00\"</code>, <code>0.0</code> (which stringifies to <code>\"0\"</code>), empty arrays in scalar context (they evaluate to <code>0</code>), and references (always true).</p> <pre><code># All false\nif (undef)  { ... }   # false\nif (\"\")     { ... }   # false\nif (0)      { ... }   # false\nif (\"0\")    { ... }   # false\n\n# All true (some surprise people)\nif (\"0E0\")  { ... }   # true - non-empty string that isn't \"0\"\nif (\" \")    { ... }   # true - contains a space\nif (\"00\")   { ... }   # true - two characters, not the string \"0\"\nif (0.0)    { ... }   # false - 0.0 is numeric 0\nif (\\0)     { ... }   # true - it's a reference\nif ([])     { ... }   # true - it's a reference to an empty array\n</code></pre> <p>The \\\"0E0\\\" Trick</p> <p>The DBI module returns <code>\"0E0\"</code> for queries that succeed but affect zero rows. This evaluates to true in boolean context (it is a non-empty string that is not <code>\"0\"</code>) but to <code>0</code> in numeric context (it is zero in scientific notation). This lets you distinguish \"zero rows affected\" (true, <code>\"0E0\"</code>) from \"query failed\" (false, <code>undef</code>).</p> <p>The following diagram shows how Perl evaluates truthiness:</p> <pre><code>flowchart TD\n    A[Value to evaluate] --&gt; B{Is it undef?}\n    B --&gt;|Yes| FALSE[FALSE]\n    B --&gt;|No| C{Is it a string?}\n    C --&gt;|Yes| D{Is it empty ''?}\n    D --&gt;|Yes| FALSE\n    D --&gt;|No| E{Is it '0'?}\n    E --&gt;|Yes| FALSE\n    E --&gt;|No| TRUE[TRUE]\n    C --&gt;|No| F{Is it 0?}\n    F --&gt;|Yes| FALSE\n    F --&gt;|No| TRUE</code></pre> <p>In practice, Perl internally converts between strings and numbers freely, so the evaluation is more nuanced. A value like <code>0.0</code> is numeric zero, which is false. A value like <code>\"00\"</code> is the string \"00\", not \"0\", so it is true. The rules above cover every case you will encounter in real code.</p> <p>Which of these values is TRUE in Perl? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/control-flow/#while-and-until-loops","title":"while and until Loops","text":"<p>The <code>while</code> loop repeats a block as long as the condition is true:</p> <pre><code>my $count = 10;\nwhile ($count &gt; 0) {\n    print \"$count...\\n\";\n    $count--;\n}\nprint \"Liftoff!\\n\";\n</code></pre> <p>The <code>until</code> loop is the opposite - it repeats until the condition becomes true:</p> <pre><code>my $response;\nuntil (defined $response &amp;&amp; $response eq \"yes\") {\n    print \"Continue? (yes/no): \";\n    $response = &lt;STDIN&gt;;\n    chomp $response;\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/control-flow/#do-while-and-do-until","title":"do-while and do-until","text":"<p>If you need the body to execute at least once before checking the condition, use <code>do</code>:</p> <pre><code>my $input;\ndo {\n    print \"Enter a number (1-10): \";\n    $input = &lt;STDIN&gt;;\n    chomp $input;\n} while ($input &lt; 1 || $input &gt; 10);\n</code></pre> <p>do-while Is Not a True Loop</p> <p>A <code>do { ... } while</code> block is not a real loop in Perl's eyes. The <code>next</code>, <code>last</code>, and <code>redo</code> loop control statements do not work inside it. If you need loop control, use a regular <code>while</code> loop with the exit condition at the end.</p>"},{"location":"Dev%20Zero/Perl/control-flow/#infinite-loops","title":"Infinite Loops","text":"<p><code>while (1)</code> creates an infinite loop. You break out of it with <code>last</code>:</p> <pre><code>while (1) {\n    print \"Command: \";\n    my $cmd = &lt;STDIN&gt;;\n    chomp $cmd;\n    last if $cmd eq \"quit\";\n    process_command($cmd);\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/control-flow/#reading-input-with-while","title":"Reading Input with while","text":"<p>One of Perl's most common patterns is reading input line by line:</p> <pre><code>while (my $line = &lt;STDIN&gt;) {\n    chomp $line;\n    print \"Got: $line\\n\";\n}\n</code></pre> <p>Perl has special magic here: the <code>while (&lt;STDIN&gt;)</code> construct automatically checks for <code>defined</code> rather than truth, so it correctly handles lines containing just <code>\"0\"</code>. Writing <code>while (defined(my $line = &lt;STDIN&gt;))</code> is equivalent, but the short form is idiomatic.</p> <p>The same pattern works for files:</p> <pre><code>open my $fh, '&lt;', 'data.txt' or die \"Cannot open: $!\";\nwhile (my $line = &lt;$fh&gt;) {\n    chomp $line;\n    # process $line\n}\nclose $fh;\n</code></pre>"},{"location":"Dev%20Zero/Perl/control-flow/#for-and-foreach-loops","title":"for and foreach Loops","text":"<p>Perl has two loop styles: C-style <code>for</code> and list-iteration <code>foreach</code>. In practice, Perl treats <code>for</code> and <code>foreach</code> as interchangeable keywords - you can use either for both styles. Most Perl programmers use <code>for</code> exclusively.</p>"},{"location":"Dev%20Zero/Perl/control-flow/#c-style-for","title":"C-style for","text":"<pre><code>for (my $i = 0; $i &lt; 10; $i++) {\n    print \"$i\\n\";\n}\n</code></pre> <p>The three-part header works exactly like C: initialize, test, increment. The loop variable <code>$i</code> is lexically scoped to the loop when declared with <code>my</code>.</p>"},{"location":"Dev%20Zero/Perl/control-flow/#list-iteration-with-forforeach","title":"List Iteration with for/foreach","text":"<p>The far more common style iterates over a list:</p> <pre><code>my @fruits = (\"apple\", \"banana\", \"cherry\");\n\nfor my $fruit (@fruits) {\n    print \"I like $fruit\\n\";\n}\n</code></pre> <p>If you omit the loop variable, Perl uses <code>$_</code>:</p> <pre><code>for (@fruits) {\n    print \"I like $_\\n\";\n}\n</code></pre> <p>Aliasing, Not Copying</p> <p>The loop variable is an alias for the current element, not a copy. Modifying the loop variable modifies the original array:</p> <pre><code>my @nums = (1, 2, 3);\nfor (@nums) {\n    $_ *= 2;\n}\n# @nums is now (2, 4, 6)\n</code></pre> <p>This is powerful but can cause bugs if you forget about it. Use a named variable (<code>for my $item (@array)</code>) to make the aliasing visible and intentional.</p>"},{"location":"Dev%20Zero/Perl/control-flow/#the-range-operator","title":"The Range Operator","text":"<p>The range operator (<code>..</code>) generates a list of consecutive values:</p> <pre><code>for my $n (1..10) {\n    print \"$n\\n\";\n}\n\n# Also works with characters\nfor my $letter ('a'..'z') {\n    print \"$letter \";\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/control-flow/#reverse-iteration","title":"Reverse Iteration","text":"<p>To iterate in reverse:</p> <pre><code>for my $i (reverse 1..10) {\n    print \"$i...\\n\";\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/control-flow/#iterating-with-index","title":"Iterating with Index","text":"<p>When you need both the index and the value:</p> <pre><code>my @items = (\"first\", \"second\", \"third\");\n\nfor my $i (0..$#items) {\n    print \"$i: $items[$i]\\n\";\n}\n</code></pre> <p><code>$#items</code> gives the last index of the array.</p>"},{"location":"Dev%20Zero/Perl/control-flow/#loop-control","title":"Loop Control","text":"<p>Three statements control loop execution from inside the body: <code>next</code>, <code>last</code>, and <code>redo</code>.</p> Statement Effect Equivalent in C/Python <code>next</code> Skip to the next iteration <code>continue</code> / <code>continue</code> <code>last</code> Exit the loop entirely <code>break</code> / <code>break</code> <code>redo</code> Restart the current iteration (no re-check) No equivalent <pre><code>for my $n (1..20) {\n    next if $n % 2 == 0;    # skip even numbers\n    last if $n &gt; 15;         # stop after 15\n    print \"$n\\n\";            # prints 1, 3, 5, 7, 9, 11, 13, 15\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/control-flow/#redo","title":"redo","text":"<p><code>redo</code> restarts the current iteration without re-evaluating the loop condition or advancing the iterator. It is useful for retry logic:</p> <pre><code>for my $server (@servers) {\n    my $response = ping($server);\n    unless ($response) {\n        warn \"Retrying $server...\\n\";\n        sleep 1;\n        redo;   # try the same $server again\n    }\n    print \"$server is up\\n\";\n}\n</code></pre> <p>Infinite redo</p> <p>Without a counter or other exit condition, <code>redo</code> can create an infinite loop. Always include a way to eventually stop retrying:</p> <pre><code>my $attempts = 0;\nfor my $server (@servers) {\n    $attempts = 0;\n    RETRY: {\n        my $response = ping($server);\n        unless ($response) {\n            $attempts++;\n            redo RETRY if $attempts &lt; 3;\n            warn \"Giving up on $server\\n\";\n            next;\n        }\n    }\n    print \"$server is up\\n\";\n}\n</code></pre> <p>The following diagram shows how each loop control keyword changes the flow within a single loop iteration:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; Condition: Enter loop\n    Condition --&gt; Body: Test passes\n    Condition --&gt; [*]: Test fails (loop ends)\n    Body --&gt; Continue: continue block\n    Continue --&gt; Condition: Next iteration\n    Body --&gt; Condition: next (skip to condition)\n    Body --&gt; Body: redo (restart body, skip condition)\n    Body --&gt; [*]: last (exit loop immediately)</code></pre>"},{"location":"Dev%20Zero/Perl/control-flow/#labeled-loops","title":"Labeled Loops","text":"<p>When you have nested loops, <code>next</code> and <code>last</code> affect the innermost loop by default. Labels let you target an outer loop:</p> <pre><code>OUTER: for my $i (1..10) {\n    for my $j (1..10) {\n        next OUTER if $j == 5;    # skip to next $i\n        last OUTER if $i == 3;    # exit both loops\n        print \"$i.$j \";\n    }\n}\n</code></pre> <p>Labels are uppercase by convention. They go before the loop keyword followed by a colon. You can use any label name, but <code>OUTER</code>, <code>LINE</code>, <code>ROW</code>, and <code>FILE</code> are common choices that describe what the loop iterates over.</p> <p>Loop Control with Labels (requires JavaScript)</p> <p>Log File Processor with Loop Control (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/control-flow/#short-circuit-operators","title":"Short-Circuit Operators","text":"<p>Perl's logical operators do not just return true or false - they return the value that determined the result. This makes them powerful control flow tools beyond simple boolean logic.</p>"},{"location":"Dev%20Zero/Perl/control-flow/#and-as-value-selectors","title":"|| and &amp;&amp; as Value Selectors","text":"<p>The <code>||</code> (logical or) operator evaluates the left side. If true, it returns that value. If false, it evaluates and returns the right side:</p> <pre><code>my $name = $user_input || \"Anonymous\";\n# If $user_input is truthy, $name gets that value\n# If $user_input is false (empty string, undef, 0), $name gets \"Anonymous\"\n</code></pre> <p>The <code>&amp;&amp;</code> (logical and) operator evaluates the left side. If false, it returns that value. If true, it evaluates and returns the right side:</p> <pre><code>my $result = $data &amp;&amp; process($data);\n# Only calls process() if $data is truthy\n</code></pre>"},{"location":"Dev%20Zero/Perl/control-flow/#the-defined-or-operator","title":"The Defined-Or Operator: //","text":"<p>The <code>||</code> operator has a problem: it treats <code>0</code> and <code>\"\"</code> as false, which is often not what you want for defaults. The <code>//</code> (defined-or) operator checks for <code>defined</code> instead of truth:</p> <pre><code>my $port = $config{port} // 8080;\n# Uses 8080 only if $config{port} is undef\n# If $config{port} is 0, it keeps 0 (unlike ||)\n</code></pre> <p>This is critical for numeric defaults where <code>0</code> is a legitimate value:</p> <pre><code>my $count = $args{count} || 10;    # Bug: count of 0 becomes 10\nmy $count = $args{count} // 10;    # Correct: only undef becomes 10\n</code></pre>"},{"location":"Dev%20Zero/Perl/control-flow/#assignment-shortcuts","title":"Assignment Shortcuts","text":"<p>Both <code>||</code> and <code>//</code> have assignment forms:</p> <pre><code>$x ||= \"default\";     # $x = $x || \"default\"\n$x //= \"default\";     # $x = $x // \"default\"\n</code></pre> <p><code>$x //= \"default\"</code> is the standard pattern for setting defaults. It assigns <code>\"default\"</code> only if <code>$x</code> is <code>undef</code>, leaving <code>0</code>, <code>\"\"</code>, and other false-but-defined values alone.</p>"},{"location":"Dev%20Zero/Perl/control-flow/#error-handling-with-or","title":"Error Handling with or","text":"<p>The <code>or</code> operator (low-precedence version of <code>||</code>) is Perl's idiom for error handling:</p> <pre><code>open my $fh, '&lt;', $filename or die \"Cannot open $filename: $!\";\nchdir $directory            or die \"Cannot chdir to $directory: $!\";\nmkdir $path                 or warn \"Cannot create $path: $!\";\n</code></pre> <p>This works because <code>open</code> returns a truthy value on success and false (<code>undef</code>) on failure. When <code>open</code> succeeds, <code>or</code> short-circuits and <code>die</code> never executes. When <code>open</code> fails, <code>or</code> evaluates the right side and <code>die</code> terminates the program with an error message.</p> <p>or vs || for Error Handling</p> <p>Use <code>or</code> (not <code>||</code>) with <code>die</code>/<code>warn</code> because of precedence. <code>open $fh, '&lt;', $file || die ...</code> is parsed as <code>open $fh, '&lt;', ($file || die ...)</code>, which passes the result of <code>$file || die</code> as the filename. The <code>or</code> operator's low precedence ensures the <code>open</code> call completes first.</p> <p>Menu-Driven Calculator (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/control-flow/#givenwhen-experimental","title":"given/when (Experimental)","text":"<p>Perl 5.10 introduced <code>given</code>/<code>when</code> as a switch/case mechanism:</p> <pre><code># Historical syntax - DO NOT USE in new code\nuse feature 'switch';\ngiven ($status) {\n    when (\"active\")   { process_active()   }\n    when (\"pending\")  { process_pending()   }\n    when (\"inactive\") { process_inactive()  }\n    default           { handle_unknown()    }\n}\n</code></pre> <p>Deprecated and Unreliable</p> <p><code>given</code>/<code>when</code> has been experimental since its introduction and produces deprecation warnings in Perl 5.38+. The smartmatch operator (<code>~~</code>) that powers it has complex, surprising behavior that even experienced Perl developers find confusing. The feature is expected to be removed in a future Perl release.</p> <p>Use <code>if</code>/<code>elsif</code> chains instead:</p> <pre><code>if ($status eq \"active\") {\n    process_active();\n} elsif ($status eq \"pending\") {\n    process_pending();\n} elsif ($status eq \"inactive\") {\n    process_inactive();\n} else {\n    handle_unknown();\n}\n</code></pre> <p>For dispatch tables (mapping values to actions), use a hash of code references:</p> <pre><code>my %dispatch = (\n    active   =&gt; \\&amp;process_active,\n    pending  =&gt; \\&amp;process_pending,\n    inactive =&gt; \\&amp;process_inactive,\n);\n\nmy $handler = $dispatch{$status} // \\&amp;handle_unknown;\n$handler-&gt;();\n</code></pre>"},{"location":"Dev%20Zero/Perl/control-flow/#putting-it-all-together","title":"Putting It All Together","text":"<p>Control flow in Perl is about choosing the right tool for each situation:</p> <ul> <li><code>if</code>/<code>elsif</code>/<code>else</code> for multi-way branching</li> <li><code>unless</code> for simple negated conditions</li> <li>Ternary for inline value selection</li> <li>Statement modifiers for concise single-statement conditions</li> <li><code>while</code>/<code>until</code> for condition-driven loops</li> <li><code>for</code>/<code>foreach</code> for iterating over lists</li> <li><code>next</code>/<code>last</code>/<code>redo</code> for fine-grained loop control</li> <li>Labels for controlling nested loops</li> <li><code>||</code>/<code>//</code>/<code>&amp;&amp;</code> for short-circuit logic and defaults</li> <li><code>or</code>/<code>and</code> for statement-level flow control and error handling</li> </ul> <p>The key is readability. Perl gives you many ways to express the same logic. Pick the one that makes intent clearest to someone reading your code six months from now - including yourself.</p>"},{"location":"Dev%20Zero/Perl/control-flow/#further-reading","title":"Further Reading","text":"<ul> <li>perlsyn - Perl Syntax - official documentation for all Perl control structures</li> <li>perlop - Perl Operators - complete operator reference including precedence table</li> <li>Learning Perl, Chapter 10: More Control Structures - the \"Llama Book\" covers control flow in depth</li> <li>Perl Best Practices, Chapter 6: Control Structures - Damian Conway's style recommendations</li> <li>Modern Perl, Chapter 3 - control flow in modern Perl style</li> </ul> <p>Previous: Arrays, Hashes, and Lists | Next: Regular Expressions | Back to Index</p>"},{"location":"Dev%20Zero/Perl/error-handling-debugging/","title":"Error Handling and Debugging","text":""},{"location":"Dev%20Zero/Perl/error-handling-debugging/#writing-resilient-code-and-finding-bugs","title":"Writing Resilient Code and Finding Bugs","text":"<p>Version: 1.1 Year: 2026</p>"},{"location":"Dev%20Zero/Perl/error-handling-debugging/#copyright-notice","title":"Copyright Notice","text":"<p>Copyright (c) 2025-2026 Ryan Thomas Robson / Robworks Software LLC. Licensed under CC BY-NC-ND 4.0. You may share this material for non-commercial purposes with attribution, but you may not distribute modified versions.</p> <p>Perl programs fail. Files go missing, networks drop, users provide garbage input, and code has bugs. The difference between a script that crashes cryptically and one that reports exactly what went wrong comes down to how you handle errors. This guide covers Perl's error handling mechanisms, the debugging toolkit, and the profiling tools that help you write resilient code.</p>"},{"location":"Dev%20Zero/Perl/error-handling-debugging/#die-warn-and-the-carp-family","title":"die, warn, and the Carp Family","text":""},{"location":"Dev%20Zero/Perl/error-handling-debugging/#die-and-warn","title":"die and warn","text":"<p><code>die</code> terminates the program (unless caught by <code>eval</code>) and prints to <code>STDERR</code>. <code>warn</code> prints to <code>STDERR</code> but does not terminate:</p> <pre><code>die \"Configuration file not found\\n\";   # Program stops\nwarn \"Disk usage above 90%\\n\";          # Program continues\n</code></pre> <p>If the string does not end with a newline, Perl appends the filename and line number automatically:</p> <pre><code>die \"Configuration file not found\";\n# Output: Configuration file not found at script.pl line 12.\n</code></pre> <p><code>die</code> can also throw a reference - an object, hash, or any scalar. This is the basis for structured exception handling:</p> <pre><code>die { code =&gt; 404, message =&gt; \"Not found\", path =&gt; $file };\n</code></pre>"},{"location":"Dev%20Zero/Perl/error-handling-debugging/#carp-better-error-reporting","title":"Carp: Better Error Reporting","text":"<p>The built-in <code>Carp</code> module provides versions of <code>warn</code> and <code>die</code> that report errors from the caller's perspective:</p> <pre><code>use Carp;\n\nsub validate_age {\n    my ($age) = @_;\n    croak \"Age must be a positive number\" unless $age &amp;&amp; $age &gt; 0;\n}\n\nvalidate_age(-5);\n# Output: Age must be a positive number at caller.pl line 10.\n</code></pre> Function Behavior <code>carp</code> Warns from caller's perspective <code>croak</code> Dies from caller's perspective <code>cluck</code> Warns with full stack trace <code>confess</code> Dies with full stack trace <p>Use <code>carp</code>/<code>croak</code> in modules - the error should point to the caller's code, not your internal validation. Use <code>confess</code>/<code>cluck</code> when you need the complete call stack.</p> <p>die, warn, and Carp Behavior (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/error-handling-debugging/#eval-blocks-and","title":"eval Blocks and $@","text":"<p>The <code>eval</code> block is Perl's built-in mechanism for catching exceptions. When code inside <code>eval</code> calls <code>die</code>, the error is caught and stored in <code>$@</code>:</p> <pre><code>eval {\n    open my $fh, '&lt;', $file or die \"Cannot open $file: $!\";\n    process(&lt;$fh&gt;);\n    close $fh;\n};\nif ($@) {\n    warn \"Processing failed: $@\";\n}\n</code></pre> <p>eval BLOCK vs. eval STRING</p> <p>The block form (<code>eval { ... }</code>) compiles at compile time and is safe. The string form (<code>eval \"...\"</code>) compiles arbitrary code at runtime - it is a security risk. Never use <code>eval STRING</code> unless you have an extremely specific reason.</p>"},{"location":"Dev%20Zero/Perl/error-handling-debugging/#the-problem","title":"The $@ Problem","text":"<p><code>$@</code> has a well-known flaw: it can be clobbered between the <code>eval</code> block and your error check. Destructors (DESTROY methods) that run when objects go out of scope can reset <code>$@</code> if they use <code>eval</code> internally:</p> <pre><code>eval {\n    my $obj = SomeClass-&gt;new();  # $obj has a DESTROY method\n    die \"Something failed\";\n};\n# $obj's DESTROY runs here - may reset $@ to \"\"\nif ($@) {\n    # This might not execute even though die was called!\n}\n</code></pre> <pre><code>flowchart TD\n    A[Code calls die] --&gt; B{Inside eval block?}\n    B --&gt;|No| C[Program terminates\\nError printed to STDERR]\n    B --&gt;|Yes| D[eval block exits]\n    D --&gt; E[Error stored in $@]\n    E --&gt; F{Destructors run\\nbefore if check}\n    F --&gt;|DESTROY uses eval| G[$@ may be clobbered\\nError lost]\n    F --&gt;|No interference| H[if $@ catches error]\n    H --&gt; I[Error handled]\n    G --&gt; J[Error silently ignored]\n    J --&gt; K[Use Try::Tiny\\nto avoid this]</code></pre> <p>What is the main problem with the common pattern: eval { ... }; if ($@) { handle_error($@) }? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/error-handling-debugging/#trytiny","title":"Try::Tiny","text":"<p><code>Try::Tiny</code> is the standard CPAN solution for safe exception handling. It avoids the <code>$@</code> clobbering problem:</p> <pre><code>use Try::Tiny;\n\ntry {\n    open my $fh, '&lt;', $file or die \"Cannot open $file: $!\";\n    process($fh);\n    close $fh;\n}\ncatch {\n    warn \"Failed to process $file: $_\";    # Error is in $_, not $@\n}\nfinally {\n    cleanup_resources();\n};    # &lt;-- Semicolon required! try/catch is a function call\n</code></pre> <p><code>Try::Tiny</code> works by checking whether <code>eval</code> returned true (via a trailing <code>1</code>) rather than checking <code>$@</code> directly. This reliably detects errors even when <code>$@</code> is clobbered.</p> <p>The Trailing Semicolon</p> <p><code>try</code>/<code>catch</code>/<code>finally</code> are function calls, not language keywords. You must end the chain with a semicolon. Forgetting it produces confusing error messages.</p> Situation Use Simple scripts with no objects <code>eval</code> block is fine Code with DESTROY methods <code>Try::Tiny</code> Libraries and modules <code>Try::Tiny</code> Performance-critical inner loops <code>eval</code> block (slight overhead per call) <p>Error Handling Pipeline with Try::Tiny (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/error-handling-debugging/#exception-objects","title":"Exception Objects","text":"<p>Since <code>die</code> accepts any scalar, you can throw objects that carry error codes and context:</p> <pre><code>package App::Error;\nuse overload '\"\"' =&gt; sub {\n    sprintf \"%s (code %d) at %s line %d\",\n        $_[0]-&gt;{message}, $_[0]-&gt;{code}, $_[0]-&gt;{file}, $_[0]-&gt;{line};\n};\n\nsub new {\n    my ($class, %args) = @_;\n    bless {\n        message =&gt; $args{message} // 'Unknown error',\n        code    =&gt; $args{code}    // 500,\n        file    =&gt; (caller(0))[1],\n        line    =&gt; (caller(0))[2],\n    }, $class;\n}\nsub message { $_[0]-&gt;{message} }\nsub code    { $_[0]-&gt;{code} }\n</code></pre> <p>For larger systems, define a hierarchy and dispatch on type:</p> <pre><code>package App::Error::IO;\nuse parent -norequire, 'App::Error';\n\npackage App::Error::Auth;\nuse parent -norequire, 'App::Error';\n\n# In a catch block:\ncatch {\n    if (ref $_ &amp;&amp; $_-&gt;isa('App::Error::Auth')) {\n        redirect_to_login();\n    } elsif (ref $_ &amp;&amp; $_-&gt;isa('App::Error::IO')) {\n        retry_with_fallback();\n    } else {\n        log_and_show_generic_error($_);\n    }\n};\n</code></pre> <p>Exception Modules on CPAN</p> <p>For production applications, consider <code>Throwable</code> (a Moo role) or <code>Exception::Class</code> (a hierarchy builder). Both handle stack traces and stringification without boilerplate.</p>"},{"location":"Dev%20Zero/Perl/error-handling-debugging/#use-strict-and-use-warnings-deep-dive","title":"use strict and use warnings Deep Dive","text":""},{"location":"Dev%20Zero/Perl/error-handling-debugging/#use-strict","title":"use strict","text":"<p><code>use strict</code> enables three restrictions:</p> Restriction What It Catches <code>strict 'vars'</code> Undeclared variables (typos like <code>$naem</code> instead of <code>$name</code>) <code>strict 'refs'</code> Symbolic references (<code>$$varname</code> where <code>$varname</code> is a string) <code>strict 'subs'</code> Bareword strings used as values without quotes"},{"location":"Dev%20Zero/Perl/error-handling-debugging/#warning-categories","title":"Warning Categories","text":"<p><code>use warnings</code> enables runtime warnings organized into a category hierarchy:</p> <pre><code>flowchart TD\n    A[all] --&gt; B[closure]\n    A --&gt; C[deprecated]\n    A --&gt; D[io]\n    A --&gt; E[misc]\n    A --&gt; F[numeric]\n    A --&gt; G[once]\n    A --&gt; H[overflow]\n    A --&gt; I[redefine]\n    A --&gt; J[recursion]\n    A --&gt; K[uninitialized]\n    A --&gt; L[void]\n    A --&gt; M[syntax]\n    D --&gt; D1[closed]\n    D --&gt; D2[exec]\n    D --&gt; D3[newline]\n    M --&gt; M1[ambiguous]\n    M --&gt; M2[precedence]\n    M --&gt; M3[printf]</code></pre> <p>You can enable or disable specific categories:</p> <pre><code>use warnings;                             # Enable all\nuse warnings qw(uninitialized numeric);   # Specific categories only\nno warnings 'uninitialized';              # Suppress in current scope\n</code></pre> Category Triggered By <code>uninitialized</code> Using <code>undef</code> in an operation <code>numeric</code> Non-numeric string in numeric context <code>once</code> Variable used only once <code>redefine</code> Redefining a subroutine <code>recursion</code> Deep recursion (100+ levels) <code>void</code> Useless expression in void context"},{"location":"Dev%20Zero/Perl/error-handling-debugging/#perl-w-vs-use-warnings","title":"perl -w vs. use warnings","text":"Feature <code>perl -w</code> <code>use warnings</code> Scope Global (entire program + all modules) Lexical (current file/block only) Granularity All or nothing Per-category control <p><code>perl -w</code> enables warnings everywhere, including inside modules that intentionally suppress them. <code>use warnings</code> affects only the current lexical scope. Always prefer <code>use warnings</code>.</p> <p>Making Warnings Fatal</p> <p>Promote warnings to errors with <code>use warnings FATAL =&gt; 'all'</code> or target specific categories with <code>use warnings FATAL =&gt; 'uninitialized'</code>. Useful in test suites but potentially disruptive in production.</p>"},{"location":"Dev%20Zero/Perl/error-handling-debugging/#the-perl-debugger","title":"The Perl Debugger","text":"<p>Perl ships with an interactive debugger invoked with <code>perl -d script.pl</code>. This drops you into a session where you can step through code, set breakpoints, and evaluate expressions.</p>"},{"location":"Dev%20Zero/Perl/error-handling-debugging/#essential-debugger-commands","title":"Essential Debugger Commands","text":"Command Action <code>n</code> Execute next line (step over) <code>s</code> Step into subroutine call <code>c</code> / <code>c LINE</code> Continue to next breakpoint or specific line <code>r</code> Return from current subroutine <code>b LINE</code> / <code>b SUB</code> Set breakpoint at line or subroutine <code>B *</code> Delete all breakpoints <code>p EXPR</code> Print expression value <code>x EXPR</code> Dump expression (like Data::Dumper) <code>l</code> / <code>l SUB</code> List source code <code>T</code> Print stack trace <code>q</code> Quit <p>Conditional breakpoints trigger only when a condition is true: <code>b 42 $count &gt; 100</code>. Watchpoints (<code>w $total</code>) break when a variable changes.</p> <p>Using the Perl Debugger (requires JavaScript)</p> <p>Command Builder (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/error-handling-debugging/#profiling-and-coverage","title":"Profiling and Coverage","text":""},{"location":"Dev%20Zero/Perl/error-handling-debugging/#develnytprof","title":"Devel::NYTProf","text":"<p><code>Devel::NYTProf</code> is the gold standard Perl profiler. It records per-line timings, subroutine call counts, and generates HTML reports:</p> <pre><code>perl -d:NYTProf script.pl    # Profile\nnytprofhtml --open            # Generate HTML report\n</code></pre> <p>For large applications, control profiling programmatically with <code>DB::disable_profile()</code> and <code>DB::enable_profile()</code>.</p> <p>Profile Before Optimizing</p> <p>Intuition about performance is unreliable. Always profile first - the bottleneck is rarely where you expect it.</p>"},{"location":"Dev%20Zero/Perl/error-handling-debugging/#develcover","title":"Devel::Cover","text":"<p><code>Devel::Cover</code> measures which parts of your code are exercised by tests:</p> <pre><code>cover -test            # Run tests with coverage\ncover cover_db         # Generate report\n</code></pre> <p>It reports statement, branch, condition, and subroutine coverage. Target 80-90% for most projects.</p>"},{"location":"Dev%20Zero/Perl/error-handling-debugging/#datadumper-for-inspection","title":"Data::Dumper for Inspection","text":"<p><code>Data::Dumper</code> prints any data structure in a readable format:</p> <pre><code>use Data::Dumper;\nlocal $Data::Dumper::Sortkeys = 1;\nwarn Dumper(\\%config);    # Quick debug: warn goes to STDERR immediately\n</code></pre> <p>For colored, human-friendly output, <code>Data::Printer</code> is an alternative:</p> <pre><code>use DDP;\np %config;    # Colored output to STDERR\n</code></pre>"},{"location":"Dev%20Zero/Perl/error-handling-debugging/#logging-strategies","title":"Logging Strategies","text":""},{"location":"Dev%20Zero/Perl/error-handling-debugging/#logany","title":"Log::Any","text":"<p><code>Log::Any</code> separates the logging interface from the output destination. Modules log through <code>Log::Any</code>; the main script decides where messages go:</p> <pre><code># In your module\nuse Log::Any '$log';\n$log-&gt;info(\"Connecting to $dsn\");\n$log-&gt;error(\"Connection failed: $@\");\n\n# In your main script\nuse Log::Any::Adapter ('File', '/var/log/myapp.log');\n</code></pre>"},{"location":"Dev%20Zero/Perl/error-handling-debugging/#loglog4perl","title":"Log::Log4perl","text":"<p><code>Log::Log4perl</code> provides hierarchical loggers, multiple appenders, and pattern layouts (modeled after Java's Log4j):</p> <pre><code>use Log::Log4perl;\nLog::Log4perl-&gt;init(\\q{\n    log4perl.rootLogger = DEBUG, Screen\n    log4perl.appender.Screen = Log::Log4perl::Appender::Screen\n    log4perl.appender.Screen.layout = PatternLayout\n    log4perl.appender.Screen.layout.ConversionPattern = %d [%p] %F{1}:%L %m%n\n});\nmy $log = Log::Log4perl-&gt;get_logger();\n$log-&gt;info(\"Application started\");\n</code></pre> Need Solution Script debugging <code>warn</code> with <code>Data::Dumper</code> Reusable module <code>Log::Any</code> (no adapter dependency) Complex log routing <code>Log::Log4perl</code>"},{"location":"Dev%20Zero/Perl/error-handling-debugging/#practical-patterns","title":"Practical Patterns","text":""},{"location":"Dev%20Zero/Perl/error-handling-debugging/#the-or-die-idiom","title":"The or die Idiom","text":"<pre><code>open my $fh, '&lt;', $file   or die \"Cannot open $file: $!\\n\";\nchdir $dir                 or die \"Cannot chdir to $dir: $!\\n\";\n</code></pre> <p>Always include <code>$!</code> - it contains the OS error message.</p>"},{"location":"Dev%20Zero/Perl/error-handling-debugging/#layered-error-context","title":"Layered Error Context","text":"<p>Each layer adds context so the final message traces back to the root cause:</p> <pre><code>sub read_user_data {\n    my ($user_id) = @_;\n    my $path = \"/data/users/$user_id.json\";\n    try {\n        open my $fh, '&lt;', $path or die \"Cannot open: $!\";\n        local $/;\n        my $content = &lt;$fh&gt;;\n        close $fh;\n        return decode_json($content);\n    }\n    catch {\n        die \"Failed to read user $user_id: $_\";\n    };\n}\n# Error: \"Failed to read user 42: Cannot open: No such file or directory\"\n</code></pre>"},{"location":"Dev%20Zero/Perl/error-handling-debugging/#retry-with-backoff","title":"Retry with Backoff","text":"<pre><code>sub retry {\n    my (%args) = @_;\n    my $tries = $args{tries} // 3;\n    my $delay = $args{delay} // 1;\n    for my $attempt (1 .. $tries) {\n        my $result;\n        try { $result = $args{code}-&gt;() }\n        catch {\n            die \"Failed after $tries attempts: $_\" if $attempt == $tries;\n            warn \"Attempt $attempt failed, retrying in ${delay}s\\n\";\n            sleep $delay;\n            $delay *= 2;\n        };\n        return $result if defined $result;\n    }\n}\n</code></pre> <p>Add Error Handling to a File Processor (requires JavaScript)</p> <p>Build a Custom Exception Class Hierarchy (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/error-handling-debugging/#further-reading","title":"Further Reading","text":"<ul> <li>perldiag - complete list of Perl diagnostic messages</li> <li>perldebtut - Perl debugging tutorial</li> <li>perldebug - full debugger reference</li> <li>Try::Tiny documentation - safe exception handling</li> <li>Devel::NYTProf documentation - profiling guide</li> <li>Log::Any documentation - logging API</li> <li>Perl Best Practices, Chapter 13 - error handling recommendations</li> </ul> <p>Previous: Object-Oriented Perl | Next: Testing | Back to Index</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/","title":"File I/O and System Interaction","text":""},{"location":"Dev%20Zero/Perl/file-io-and-system/#reading-writing-and-talking-to-the-os","title":"Reading, Writing, and Talking to the OS","text":"<p>Version: 1.1 Year: 2026</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#copyright-notice","title":"Copyright Notice","text":"<p>Copyright (c) 2025-2026 Ryan Thomas Robson / Robworks Software LLC. Licensed under CC BY-NC-ND 4.0. You may share this material for non-commercial purposes with attribution, but you may not distribute modified versions.</p> <p>Perl was built for text processing on Unix systems. Its file I/O is not an afterthought bolted onto a language - it is woven into the core. Reading files, writing output, testing file attributes, running external commands, and managing processes all use simple, consistent syntax that maps directly to the underlying operating system calls.</p> <p>This guide covers everything from opening your first file to forking child processes. By the end, you will be able to read and write files safely, navigate directories, run system commands, and build the kind of file-processing scripts that Perl is famous for.</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#opening-files","title":"Opening Files","text":"<p>The <code>open</code> function connects a filehandle to a file. Modern Perl uses the three-argument form:</p> <pre><code>open my $fh, '&lt;', 'data.txt' or die \"Cannot open data.txt: $!\";\n</code></pre> <p>Three things happen here:</p> <ol> <li><code>my $fh</code> declares a lexical filehandle - a variable that holds the connection to the file.</li> <li><code>'&lt;'</code> is the mode - read-only in this case.</li> <li><code>'data.txt'</code> is the filename, completely separate from the mode.</li> </ol> <p>The <code>or die</code> idiom terminates the program with an error message if <code>open</code> fails. <code>$!</code> contains the operating system's error message (like \"No such file or directory\" or \"Permission denied\").</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#file-modes","title":"File Modes","text":"Mode Meaning Creates file? Truncates? <code>&lt;</code> Read only No No <code>&gt;</code> Write (create/truncate) Yes Yes <code>&gt;&gt;</code> Append Yes No <code>+&lt;</code> Read and write No No <code>+&gt;</code> Read and write (truncate) Yes Yes <code>+&gt;&gt;</code> Read and append Yes No <p>The most common modes by far are <code>&lt;</code> (read), <code>&gt;</code> (write), and <code>&gt;&gt;</code> (append). Read/write modes (<code>+&lt;</code>, <code>+&gt;</code>, <code>+&gt;&gt;</code>) are rare in practice - most programs read from one file and write to another.</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#why-three-argument-open","title":"Why Three-Argument open?","text":"<p>Perl also supports a two-argument form:</p> <pre><code># Two-argument open - DO NOT USE\nopen(FH, $filename);\n</code></pre> <p>This is dangerous because the mode is parsed from the filename string itself. If <code>$filename</code> comes from user input and contains <code>\"&gt; /etc/passwd\"</code>, Perl will happily open that file for writing. The three-argument form keeps mode and filename separate, preventing injection attacks:</p> <pre><code># Safe: mode and filename are separate arguments\nopen my $fh, '&lt;', $filename or die \"Cannot open $filename: $!\";\n</code></pre>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#lexical-vs-bareword-filehandles","title":"Lexical vs. Bareword Filehandles","text":"<p>Older Perl code uses bareword (uppercase) filehandles:</p> <pre><code># Old style - bareword filehandle (global)\nopen(INFILE, '&lt;', 'data.txt') or die \"Cannot open: $!\";\nprint INFILE \"data\";  # Oops: INFILE is open for reading, not writing\nclose INFILE;\n</code></pre> <p>Bareword filehandles are global, meaning they can collide across subroutines and modules. Lexical filehandles (stored in <code>my</code> variables) are scoped to the block where they are declared and automatically close when they go out of scope:</p> <pre><code># Modern style - lexical filehandle (scoped)\n{\n    open my $fh, '&lt;', 'data.txt' or die \"Cannot open: $!\";\n    # ... use $fh ...\n}   # $fh goes out of scope, file is automatically closed\n</code></pre> <p>Always use lexical filehandles. The only bareword filehandles you should use are the built-in ones: <code>STDIN</code>, <code>STDOUT</code>, and <code>STDERR</code>.</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#closing-files","title":"Closing Files","text":"<p>Call <code>close</code> when you are done:</p> <pre><code>close $fh or warn \"Error closing file: $!\";\n</code></pre> <p>Checking the return value of <code>close</code> matters for write handles - it is the point where buffered output is flushed to disk. A full disk will cause <code>close</code> to fail even if all <code>print</code> calls succeeded. Lexical filehandles auto-close when they go out of scope, but explicit <code>close</code> makes intent clear and catches errors.</p> <p>Why is the three-argument form of open safer than the two-argument form? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#reading-files","title":"Reading Files","text":""},{"location":"Dev%20Zero/Perl/file-io-and-system/#line-by-line","title":"Line by Line","text":"<p>The most common pattern reads a file one line at a time using the diamond operator <code>&lt;$fh&gt;</code> inside a <code>while</code> loop:</p> <pre><code>open my $fh, '&lt;', 'server.log' or die \"Cannot open: $!\";\n\nwhile (my $line = &lt;$fh&gt;) {\n    chomp $line;          # Remove trailing newline\n    print \"&gt;&gt; $line\\n\";\n}\n\nclose $fh;\n</code></pre> <p><code>chomp</code> removes the trailing newline from a string. Without it, every line ends with <code>\\n</code>, which causes double-spacing when you <code>print</code> with your own newline.</p> <p>Why <code>while</code> and not <code>for</code>?</p> <p><code>while (&lt;$fh&gt;)</code> reads one line at a time, keeping memory usage constant regardless of file size. <code>for my $line (&lt;$fh&gt;)</code> reads the entire file into memory first, then iterates. For a 10 GB log file, <code>while</code> uses a few kilobytes; <code>for</code> needs 10 GB of RAM.</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#the-default-variable-_","title":"The Default Variable: $_","text":"<p>When you omit the variable in the <code>while</code> loop, Perl reads into <code>$_</code>:</p> <pre><code>while (&lt;$fh&gt;) {\n    chomp;               # chomp operates on $_ by default\n    print if /ERROR/;    # print and regex match also use $_\n}\n</code></pre> <p>This is idiomatic Perl. The <code>$_</code> variable is the default for dozens of built-in functions - <code>chomp</code>, <code>print</code>, <code>split</code>, <code>lc</code>, <code>uc</code>, and most regex operations.</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#reading-into-an-array","title":"Reading into an Array","text":"<p>To load all lines at once:</p> <pre><code>open my $fh, '&lt;', 'names.txt' or die \"Cannot open: $!\";\nmy @lines = &lt;$fh&gt;;\nclose $fh;\n\nchomp @lines;   # chomp works on arrays too - removes newline from each element\nprint \"Read \", scalar @lines, \" lines\\n\";\n</code></pre> <p>Each element in <code>@lines</code> includes the trailing newline until you <code>chomp</code> the array.</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#slurping-an-entire-file","title":"Slurping an Entire File","text":"<p>Sometimes you need the whole file as a single string - for regex matching across lines, for instance:</p> <pre><code># Method 1: local $/\nmy $content;\n{\n    open my $fh, '&lt;', 'config.txt' or die \"Cannot open: $!\";\n    local $/;              # Undefine the input record separator\n    $content = &lt;$fh&gt;;      # Read everything as one string\n    close $fh;\n}\n\n# Method 2: File::Slurper (recommended for production)\nuse File::Slurper 'read_text';\nmy $content = read_text('config.txt');\n</code></pre> <p><code>$/</code> is the input record separator - by default, it is <code>\"\\n\"</code>, which is why <code>&lt;$fh&gt;</code> reads one line at a time. Setting it to <code>undef</code> makes <code>&lt;$fh&gt;</code> read the entire remaining file. The <code>local</code> keyword ensures <code>$/</code> reverts to its original value when the block exits.</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#the-diamond-operator","title":"The Diamond Operator: &lt;&gt;","text":"<p>The bare diamond operator <code>&lt;&gt;</code> (with no filehandle) reads from files named on the command line, or from <code>STDIN</code> if no files are given:</p> <pre><code># script.pl - processes any files passed as arguments\nwhile (&lt;&gt;) {\n    chomp;\n    print \"Line $.: $_\\n\";\n}\n</code></pre> <pre><code># Reads from file1.txt and file2.txt:\nperl script.pl file1.txt file2.txt\n\n# Reads from standard input:\necho \"hello\" | perl script.pl\n</code></pre> <p><code>$.</code> holds the current line number. It resets to 1 at the start of each file when using <code>&lt;&gt;</code>.</p> <p>The diamond operator looks at <code>@ARGV</code>, which contains the command-line arguments. It <code>open</code>s each filename in <code>@ARGV</code> in sequence. If <code>@ARGV</code> is empty, it reads from <code>STDIN</code>. This is exactly how Unix utilities like <code>cat</code>, <code>grep</code>, and <code>sed</code> work - and Perl borrowed the pattern directly.</p> <p>See also</p> <p>Perl's filehandle model mirrors Unix standard streams. For the underlying OS concepts of STDIN, STDOUT, STDERR, and redirection, see Streams and Redirection.</p> <p>Reading Files in Perl (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#writing-files","title":"Writing Files","text":""},{"location":"Dev%20Zero/Perl/file-io-and-system/#print-and-say","title":"print and say","text":"<p>Write to a filehandle with <code>print</code>:</p> <pre><code>open my $fh, '&gt;', 'output.txt' or die \"Cannot open for writing: $!\";\n\nprint $fh \"First line\\n\";\nprint $fh \"Second line\\n\";\n\nclose $fh or die \"Error closing: $!\";\n</code></pre> <p>No Comma After the Filehandle</p> <p><code>print $fh \"text\"</code> has no comma between <code>$fh</code> and <code>\"text\"</code>. Writing <code>print $fh, \"text\"</code> is a common mistake - Perl interprets it as printing both <code>$fh</code> and <code>\"text\"</code> to <code>STDOUT</code>.</p> <p>The <code>say</code> function (requires <code>use feature 'say'</code> or <code>use v5.10</code>) is identical to <code>print</code> but adds a newline automatically:</p> <pre><code>use feature 'say';\n\nopen my $fh, '&gt;', 'output.txt' or die \"Cannot open: $!\";\nsay $fh \"First line\";      # Equivalent to: print $fh \"First line\\n\";\nsay $fh \"Second line\";\nclose $fh;\n</code></pre>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#formatted-output-with-printf","title":"Formatted Output with printf","text":"<p><code>printf</code> works like C's <code>printf</code> - format string with placeholders:</p> <pre><code>printf $fh \"%-20s %8.2f\\n\", $name, $price;\n</code></pre> <p>Common format specifiers:</p> Specifier Meaning Example <code>%s</code> String <code>\"hello\"</code> <code>%d</code> Integer <code>42</code> <code>%f</code> Float <code>3.14</code> <code>%e</code> Scientific <code>3.14e+00</code> <code>%x</code> Hexadecimal <code>2a</code> <code>%%</code> Literal <code>%</code> <code>%</code> <p>Width and precision: <code>%10s</code> (right-align in 10 chars), <code>%-10s</code> (left-align), <code>%.2f</code> (2 decimal places), <code>%08d</code> (zero-pad to 8 digits).</p> <p><code>sprintf</code> returns the formatted string instead of printing it:</p> <pre><code>my $line = sprintf \"%-20s %8.2f\", $item, $cost;\n</code></pre>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#binary-mode","title":"Binary Mode","text":"<p>By default, Perl may translate line endings on some platforms. For binary files (images, compressed data, executables), use <code>binmode</code>:</p> <pre><code>open my $fh, '&lt;', 'image.png' or die \"Cannot open: $!\";\nbinmode $fh;\n\n# Read raw bytes\nmy $bytes_read = read $fh, my $buffer, 1024;\nclose $fh;\n</code></pre> <p>For UTF-8 text files, use the encoding layer:</p> <pre><code>open my $fh, '&lt;:encoding(UTF-8)', 'unicode.txt' or die $!;\n# or:\nopen my $fh, '&lt;', 'unicode.txt' or die $!;\nbinmode $fh, ':encoding(UTF-8)';\n</code></pre> <p>File Word Counter (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#file-tests","title":"File Tests","text":"<p>Perl provides a full set of file test operators - single-character flags prefixed with a hyphen that check properties of files and directories.</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#common-file-tests","title":"Common File Tests","text":"<pre><code>my $file = '/etc/passwd';\n\nif (-e $file) { print \"Exists\\n\" }\nif (-f $file) { print \"Is a regular file\\n\" }\nif (-d $file) { print \"Is a directory\\n\" }\nif (-r $file) { print \"Is readable\\n\" }\nif (-w $file) { print \"Is writable\\n\" }\nif (-x $file) { print \"Is executable\\n\" }\n</code></pre> Operator Tests for <code>-e</code> File exists <code>-f</code> Regular file (not directory or device) <code>-d</code> Directory <code>-l</code> Symbolic link <code>-r</code> Readable by effective uid <code>-w</code> Writable by effective uid <code>-x</code> Executable by effective uid <code>-s</code> File size in bytes (returns size, false if zero) <code>-z</code> File has zero size <code>-T</code> File looks like a text file <code>-B</code> File looks like a binary file"},{"location":"Dev%20Zero/Perl/file-io-and-system/#timestamps","title":"Timestamps","text":"<p>Three operators return file age in days (fractional) since the script started:</p> Operator Measures <code>-M</code> Time since last modification <code>-A</code> Time since last access <code>-C</code> Time since inode change <pre><code>my $age = -M '/var/log/syslog';\nprintf \"Log file last modified %.1f days ago\\n\", $age;\n\n# Find files modified in the last 24 hours\nif (-M $file &lt; 1) {\n    print \"$file was modified today\\n\";\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#the-_-cache-filehandle","title":"The _ Cache Filehandle","text":"<p>Each file test performs a <code>stat</code> system call. When you chain multiple tests on the same file, use the special <code>_</code> filehandle to reuse the cached stat data:</p> <pre><code>if (-f $file &amp;&amp; -r _ &amp;&amp; -s _) {\n    print \"$file is a readable file with non-zero size\\n\";\n}\n</code></pre> <p>The <code>_</code> uses the result from the most recent file test or <code>stat</code> call, avoiding redundant system calls. Without <code>_</code>, the three tests above would <code>stat</code> the file three times.</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#stacking-tests-perl-510","title":"Stacking Tests (Perl 5.10+)","text":"<p>Perl 5.10 and later allow stacking file tests:</p> <pre><code># These are equivalent:\nif (-f $file &amp;&amp; -r $file &amp;&amp; -w $file) { ... }\nif (-f -r -w $file) { ... }   # Stacked - reads right to left\n</code></pre> <p>Stacked tests read right to left: <code>-f -r -w $file</code> tests writable first, then readable, then regular file. They also use the <code>_</code> cache automatically.</p> <p>File Test Operators (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#stat-and-file-information","title":"stat and File Information","text":"<p>The <code>stat</code> function returns a 13-element list with detailed information about a file:</p> <pre><code>my @info = stat('data.txt');\n</code></pre> Index Field Description 0 <code>dev</code> Device number 1 <code>ino</code> Inode number 2 <code>mode</code> File mode (permissions and type) 3 <code>nlink</code> Number of hard links 4 <code>uid</code> User ID of owner 5 <code>gid</code> Group ID of owner 6 <code>rdev</code> Device identifier (special files) 7 <code>size</code> File size in bytes 8 <code>atime</code> Last access time (epoch seconds) 9 <code>mtime</code> Last modification time (epoch seconds) 10 <code>ctime</code> Inode change time (epoch seconds) 11 <code>blksize</code> Preferred block size for I/O 12 <code>blocks</code> Number of blocks allocated <p>Remembering indices is painful. The <code>File::stat</code> module provides named access:</p> <pre><code>use File::stat;\n\nmy $st = stat('data.txt') or die \"Cannot stat: $!\";\nprintf \"Size: %d bytes\\n\", $st-&gt;size;\nprintf \"Owner UID: %d\\n\", $st-&gt;uid;\nprintf \"Modified: %s\\n\", scalar localtime($st-&gt;mtime);\nprintf \"Permissions: %04o\\n\", $st-&gt;mode &amp; 07777;\n</code></pre>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#lstat-for-symbolic-links","title":"lstat for Symbolic Links","text":"<p><code>lstat</code> is identical to <code>stat</code> but returns information about the symlink itself rather than the file it points to:</p> <pre><code>if (-l '/usr/local/bin/python') {\n    my @link_info = lstat('/usr/local/bin/python');\n    my @target_info = stat('/usr/local/bin/python');\n    printf \"Link size: %d, Target size: %d\\n\", $link_info[7], $target_info[7];\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#timestamps-and-formatting","title":"Timestamps and Formatting","text":"<p>Convert epoch timestamps to readable dates with <code>localtime</code> or the <code>POSIX::strftime</code> function:</p> <pre><code>use POSIX 'strftime';\n\nmy $mtime = (stat 'data.txt')[9];\nmy $formatted = strftime \"%Y-%m-%d %H:%M:%S\", localtime($mtime);\nprint \"Last modified: $formatted\\n\";\n</code></pre>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#directory-operations","title":"Directory Operations","text":""},{"location":"Dev%20Zero/Perl/file-io-and-system/#opendirreaddirclosedir","title":"opendir/readdir/closedir","text":"<p>The <code>opendir</code>/<code>readdir</code>/<code>closedir</code> trio works like <code>open</code>/<code>&lt;$fh&gt;</code>/<code>close</code> but for directories:</p> <pre><code>opendir my $dh, '/var/log' or die \"Cannot open directory: $!\";\nmy @entries = readdir $dh;\nclosedir $dh;\n\n# readdir returns ALL entries, including . and ..\nmy @files = grep { $_ ne '.' &amp;&amp; $_ ne '..' } @entries;\n\nprint \"$_\\n\" for sort @files;\n</code></pre> <p>readdir Returns Names, Not Paths</p> <p><code>readdir</code> returns bare filenames, not full paths. To use the results with <code>open</code>, <code>stat</code>, or file tests, prepend the directory path:</p> <pre><code>my $dir = '/var/log';\nopendir my $dh, $dir or die $!;\nwhile (my $entry = readdir $dh) {\n    next if $entry =~ /^\\./;           # Skip dotfiles\n    my $path = \"$dir/$entry\";          # Build full path\n    printf \"%-30s %d bytes\\n\", $entry, -s $path if -f $path;\n}\nclosedir $dh;\n</code></pre>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#filtering-directory-contents","title":"Filtering Directory Contents","text":"<p>Combine <code>readdir</code> with <code>grep</code> and file tests:</p> <pre><code>opendir my $dh, $dir or die $!;\n\n# Only regular files\nmy @files = grep { -f \"$dir/$_\" } readdir $dh;\n\n# Rewind to read again\nrewinddir $dh;\n\n# Only directories (excluding . and ..)\nmy @subdirs = grep { -d \"$dir/$_\" &amp;&amp; $_ !~ /^\\./ } readdir $dh;\n\nclosedir $dh;\n</code></pre>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#glob-and-filename-expansion","title":"glob and Filename Expansion","text":"<p>The <code>glob</code> function expands shell-style wildcards and returns full paths:</p> <pre><code># Find all .txt files in current directory\nmy @txt_files = glob('*.txt');\n\n# Find all .log files in /var/log\nmy @logs = glob('/var/log/*.log');\n\n# Multiple patterns\nmy @code = glob('*.pl *.pm *.t');\n\n# Angle bracket syntax (same as glob)\nmy @configs = &lt;~/.config/*.conf&gt;;\n</code></pre> <p><code>glob</code> returns full paths (or relative to the current directory), unlike <code>readdir</code> which returns bare names. For simple wildcard matching, <code>glob</code> is more convenient than <code>opendir</code>/<code>readdir</code>.</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#recursive-traversal-with-filefind","title":"Recursive Traversal with File::Find","text":"<p>For walking directory trees, use <code>File::Find</code>:</p> <pre><code>use File::Find;\n\nfind(sub {\n    return unless -f;           # Skip non-files\n    return unless /\\.pm$/;      # Only .pm files\n    print \"$File::Find::name\\n\";  # Full path\n}, '/usr/lib/perl5');\n</code></pre> <p><code>File::Find</code> calls your subroutine once for each file and directory found. Inside the callback:</p> <ul> <li><code>$_</code> is the bare filename</li> <li><code>$File::Find::name</code> is the full path</li> <li><code>$File::Find::dir</code> is the current directory</li> </ul>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#creating-and-removing-directories","title":"Creating and Removing Directories","text":"<pre><code>mkdir 'output'        or die \"Cannot create directory: $!\";\nmkdir 'output', 0755  or die \"Cannot create directory: $!\";   # with permissions\n\nrmdir 'output'        or die \"Cannot remove directory: $!\";   # must be empty\n</code></pre> <p><code>rmdir</code> only removes empty directories. For recursive removal, use <code>File::Path</code>:</p> <pre><code>use File::Path qw(make_path remove_tree);\n\nmake_path('a/b/c/d');     # Like mkdir -p\nremove_tree('a');          # Like rm -rf\n</code></pre>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#changing-directory","title":"Changing Directory","text":"<p><code>chdir</code> changes the current working directory:</p> <pre><code>chdir '/tmp' or die \"Cannot chdir: $!\";\n</code></pre> <p>Avoid chdir in Libraries</p> <p><code>chdir</code> affects the entire process. In subroutines and modules, use full paths instead of changing directories. If you must <code>chdir</code>, save and restore the original directory:</p> <pre><code>use Cwd;\nmy $original = getcwd();\nchdir $target or die \"Cannot chdir: $!\";\n# ... do work ...\nchdir $original or die \"Cannot return: $!\";\n</code></pre>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#file-io-decision-tree","title":"File I/O Decision Tree","text":"<p>When choosing how to interact with files and commands, this decision tree covers the most common paths:</p> <pre><code>flowchart TD\n    A[Need to work with a file?] --&gt; B{Read or Write?}\n    B --&gt;|Read| C{Whole file or line by line?}\n    C --&gt;|Line by line| D[\"while (&amp;lt;$fh&amp;gt;) { }\"]\n    C --&gt;|Whole file| E[\"my @lines = &amp;lt;$fh&amp;gt;\\nor slurp\"]\n    B --&gt;|Write| F{New or append?}\n    F --&gt;|New/overwrite| G[\"open $fh, '&gt;', $file\"]\n    F --&gt;|Append| H[\"open $fh, '&gt;&gt;', $file\"]\n    B --&gt;|Both| I[\"open $fh, '+&lt;', $file\"]\n    A --&gt; J{External command?}\n    J --&gt;|Run, ignore output| K[\"system 'cmd'\"]\n    J --&gt;|Capture output| L[\"my $out = `cmd`\"]\n    J --&gt;|Stream output| M[\"open $fh, '-|', 'cmd'\"]</code></pre> <p>Log File Rotation Script (requires JavaScript)</p> <p>Directory Tree Walker (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#running-external-commands","title":"Running External Commands","text":"<p>Perl gives you several ways to run external programs. Each one has a different purpose, and choosing the right one matters.</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#system","title":"system()","text":"<p><code>system</code> runs a command and waits for it to finish. It returns the exit status, not the output:</p> <pre><code>my $status = system('ls', '-la', '/tmp');\n\nif ($status == 0) {\n    print \"Command succeeded\\n\";\n} else {\n    warn \"Command failed with status: \", $status &gt;&gt; 8, \"\\n\";\n}\n</code></pre> <p>The return value is the raw wait status. To get the actual exit code, shift right by 8: <code>$status &gt;&gt; 8</code>. Or check <code>$?</code> after the call:</p> <pre><code>system('make', 'install');\nif ($? == -1) {\n    die \"Failed to execute: $!\";\n} elsif ($? &amp; 127) {\n    die \"Killed by signal \", $? &amp; 127;\n} else {\n    printf \"Exited with value %d\\n\", $? &gt;&gt; 8;\n}\n</code></pre> <p>Shell Injection</p> <p><code>system</code> with a single string argument passes it through the shell:</p> <pre><code>system(\"ls -la $dir\");   # DANGEROUS if $dir contains shell metacharacters\n</code></pre> <p>Always use the list form to bypass the shell:</p> <pre><code>system('ls', '-la', $dir);   # Safe: no shell interpretation\n</code></pre>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#backticks-and-qx","title":"Backticks and qx()","text":"<p>Backticks and <code>qx()</code> capture the command's standard output as a string:</p> <pre><code>my $output = `ls -la /tmp`;\n# or equivalently:\nmy $output = qx(ls -la /tmp);\n\n# In list context, returns one element per line\nmy @lines = `ls -1 /tmp`;\nchomp @lines;\n</code></pre> <p>The exit status is available in <code>$?</code> after the backtick expression.</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#open-with-pipes","title":"open with Pipes","text":"<p>For streaming data to or from a command, use <code>open</code> with pipe modes:</p> <pre><code># Read from a command (like backticks but streaming)\nopen my $reader, '-|', 'find', '/var/log', '-name', '*.log'\n    or die \"Cannot run find: $!\";\n\nwhile (&lt;$reader&gt;) {\n    chomp;\n    print \"Found: $_\\n\";\n}\nclose $reader;\n\n# Write to a command\nopen my $writer, '|-', 'mail', '-s', 'Report', 'admin@example.com'\n    or die \"Cannot run mail: $!\";\n\nprint $writer \"Today's report:\\n\";\nprint $writer \"All systems operational.\\n\";\nclose $writer;\n</code></pre> <p>The <code>-|</code> mode opens a pipe for reading from the command. The <code>|-</code> mode opens a pipe for writing to the command's <code>STDIN</code>. The three-argument form with list arguments avoids shell interpretation.</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#exec","title":"exec()","text":"<p><code>exec</code> replaces the current Perl process entirely with the new command:</p> <pre><code>exec('vim', $filename) or die \"Cannot exec vim: $!\";\n# This line NEVER executes - exec replaces the process\nprint \"You will never see this\\n\";\n</code></pre> <p><code>exec</code> is typically used after <code>fork</code> (covered in the next section) to run a command in the child process. If you just want to run a command and continue your Perl script, use <code>system</code> instead.</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#comparison-table","title":"Comparison Table","text":"Method Captures output? Returns to Perl? Shell involved? <code>system('cmd')</code> No (goes to terminal) Yes (returns exit status) Yes (single string) <code>system('cmd', @args)</code> No Yes No (list form) <code>`cmd`</code> / <code>qx(cmd)</code> Yes (returns stdout) Yes Yes <code>open $fh, '-\\|', 'cmd'</code> Yes (via filehandle) Yes No (list form) <code>exec('cmd')</code> N/A No (replaces process) Yes (single string) <code>exec('cmd', @args)</code> N/A No No (list form) <p>IPC::Open3 for Full Control</p> <p>When you need separate access to a command's STDIN, STDOUT, and STDERR, the <code>IPC::Open3</code> module gives you filehandles for all three streams:</p> <pre><code>use IPC::Open3;\nmy $pid = open3(my $stdin, my $stdout, my $stderr, 'some_command');\n</code></pre> <p>You need to run an external command, capture its output into a variable, and then continue executing your Perl script. Which approach should you use? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#process-control","title":"Process Control","text":""},{"location":"Dev%20Zero/Perl/file-io-and-system/#fork","title":"fork()","text":"<p><code>fork</code> creates a copy of the current process. The parent gets the child's PID; the child gets 0:</p> <pre><code>my $pid = fork();\n\ndie \"Fork failed: $!\" unless defined $pid;\n\nif ($pid) {\n    # Parent process\n    print \"Parent ($$): spawned child $pid\\n\";\n    waitpid($pid, 0);    # Wait for child to finish\n    print \"Child exited with status: \", $? &gt;&gt; 8, \"\\n\";\n} else {\n    # Child process\n    print \"Child ($$): doing work...\\n\";\n    sleep 2;\n    exit 0;     # Child exits - DO NOT continue into parent's code\n}\n</code></pre> <p>Always exit or exec in the Child</p> <p>After <code>fork</code>, the child process has a full copy of the parent's code. If you forget to <code>exit</code> or <code>exec</code>, the child will fall through and execute the parent's remaining code - leading to duplicate output, double database connections, and other chaos.</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#wait-and-waitpid","title":"wait and waitpid","text":"<p><code>wait</code> blocks until any child process exits. <code>waitpid</code> waits for a specific child:</p> <pre><code># Wait for any child\nmy $finished_pid = wait();\n\n# Wait for a specific child\nwaitpid($pid, 0);           # Block until $pid exits\n\n# Non-blocking check\nuse POSIX ':sys_wait_h';\nmy $result = waitpid($pid, WNOHANG);\nif ($result == 0) {\n    print \"Child still running\\n\";\n} elsif ($result &gt; 0) {\n    print \"Child finished\\n\";\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#the-fork-exec-pattern","title":"The fork-exec Pattern","text":"<p>The classic Unix pattern for running a command in a subprocess:</p> <pre><code>my $pid = fork();\ndie \"Fork failed: $!\" unless defined $pid;\n\nif ($pid == 0) {\n    # Child: replace with the desired command\n    exec('sort', '-u', 'data.txt') or die \"Cannot exec: $!\";\n}\n\n# Parent: wait for the child\nwaitpid($pid, 0);\nmy $exit_code = $? &gt;&gt; 8;\n</code></pre>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#signals-with-kill","title":"Signals with kill","text":"<p>Send signals to processes with <code>kill</code>:</p> <pre><code>kill 'TERM', $pid;     # Polite termination request\nkill 'KILL', $pid;     # Forceful kill (cannot be caught)\nkill 'HUP', $pid;      # Hangup (often means \"reload config\")\nkill 0, $pid;          # Check if process exists (signal 0)\n</code></pre> <p>Signal 0 is a useful trick - it does not actually send a signal but returns true if the process exists and you have permission to signal it:</p> <pre><code>if (kill 0, $pid) {\n    print \"Process $pid is alive\\n\";\n} else {\n    print \"Process $pid is gone\\n\";\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#capturing-child-exit-status","title":"Capturing Child Exit Status","text":"<p>The <code>$?</code> variable holds the status of the last child process. It packs three pieces of information:</p> <pre><code>system('some_command');\n\nmy $exit_code = $? &gt;&gt; 8;        # Bits 8-15: exit code (0-255)\nmy $signal    = $? &amp; 127;       # Bits 0-6: signal that killed it\nmy $core_dump = $? &amp; 128;       # Bit 7: core dump flag\n\nif ($? == -1) {\n    print \"Failed to execute: $!\\n\";\n} elsif ($signal) {\n    printf \"Died from signal %d%s\\n\", $signal, $core_dump ? \" (core dumped)\" : \"\";\n} else {\n    printf \"Exited with code %d\\n\", $exit_code;\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#environment-and-filesystem","title":"Environment and Filesystem","text":""},{"location":"Dev%20Zero/Perl/file-io-and-system/#env","title":"%ENV","text":"<p>The <code>%ENV</code> hash gives you direct access to environment variables:</p> <pre><code># Read environment variables\nmy $home = $ENV{HOME};\nmy $path = $ENV{PATH};\nmy $user = $ENV{USER} // 'unknown';\n\n# Set environment variables (affects child processes)\n$ENV{DEBUG} = 1;\n$ENV{DATABASE_URL} = 'postgres://localhost/mydb';\n\n# Remove an environment variable\ndelete $ENV{DEBUG};\n\n# Print all environment variables\nfor my $key (sort keys %ENV) {\n    print \"$key=$ENV{$key}\\n\";\n}\n</code></pre> <p>Changes to <code>%ENV</code> affect any child processes spawned by <code>system</code>, backticks, or <code>fork</code>/<code>exec</code>. They do not affect the parent shell that launched your Perl script.</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#program-identity","title":"Program Identity","text":"<pre><code>print \"Script: $0\\n\";                     # Program name/path\nprint \"PID: $$\\n\";                        # Process ID\nprint \"Effective UID: $&gt;\\n\";              # Effective user ID\nprint \"Effective GID: $)\\n\";              # Effective group ID\n</code></pre> <p><code>$0</code> contains the script name as it was invoked. You can assign to <code>$0</code> to change what shows up in <code>ps</code> output - useful for long-running daemons.</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#portable-path-handling","title":"Portable Path Handling","text":"<p>Hardcoding <code>/</code> as the directory separator breaks on Windows. Use <code>File::Spec</code> for portable path construction:</p> <pre><code>use File::Spec;\n\nmy $path = File::Spec-&gt;catfile('usr', 'local', 'bin', 'perl');\n# Unix: usr/local/bin/perl\n# Windows: usr\\local\\bin\\perl\n\nmy ($volume, $dir, $file) = File::Spec-&gt;splitpath('/usr/local/bin/perl');\n# $volume = ''  $dir = '/usr/local/bin/'  $file = 'perl'\n\nmy $abs = File::Spec-&gt;rel2abs('lib/My/Module.pm');\n# Converts relative path to absolute\n</code></pre>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#filebasename","title":"File::Basename","text":"<p><code>File::Basename</code> extracts filename components:</p> <pre><code>use File::Basename;\n\nmy $path = '/home/user/documents/report.pdf';\n\nmy $name = basename($path);        # report.pdf\nmy $dir  = dirname($path);         # /home/user/documents\nmy ($base, $dirpart, $ext) = fileparse($path, qr/\\.[^.]*/);\n# $base = 'report'  $dirpart = '/home/user/documents/'  $ext = '.pdf'\n</code></pre>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#essential-file-utilities","title":"Essential File Utilities","text":"Module Purpose <code>Cwd</code> Get current working directory (<code>getcwd</code>, <code>abs_path</code>) <code>File::Copy</code> Copy and move files (<code>copy</code>, <code>move</code>) <code>File::Path</code> Create/remove directory trees (<code>make_path</code>, <code>remove_tree</code>) <code>File::Temp</code> Create temporary files and directories <code>File::Spec</code> Portable path manipulation <code>File::Basename</code> Extract path components <pre><code>use Cwd 'getcwd';\nuse File::Copy qw(copy move);\nuse File::Temp 'tempfile';\n\n# Current directory\nmy $cwd = getcwd();\n\n# Copy a file\ncopy('source.txt', 'backup.txt') or die \"Copy failed: $!\";\n\n# Move (rename) a file\nmove('old.txt', 'new.txt') or die \"Move failed: $!\";\n\n# Create a temporary file (auto-deleted when $fh goes out of scope)\nmy ($fh, $tempname) = tempfile(UNLINK =&gt; 1);\nprint $fh \"Temporary data\\n\";\nprint \"Temp file: $tempname\\n\";\n</code></pre>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#practical-patterns","title":"Practical Patterns","text":""},{"location":"Dev%20Zero/Perl/file-io-and-system/#in-place-editing","title":"In-Place Editing","text":"<p>Perl can modify files directly with the <code>-i</code> flag - the same capability <code>sed -i</code> provides:</p> <pre><code># Replace all occurrences of \"foo\" with \"bar\" in file.txt\nperl -i -pe 's/foo/bar/g' file.txt\n\n# Same, but keep a backup with .bak extension\nperl -i.bak -pe 's/foo/bar/g' file.txt\n</code></pre> <p>From within a script, use <code>$^I</code> and <code>@ARGV</code>:</p> <pre><code>local $^I = '.bak';          # Set backup extension (empty string = no backup)\nlocal @ARGV = ('file.txt');  # Files to process\n\nwhile (&lt;&gt;) {\n    s/foo/bar/g;\n    print;                    # Prints to the modified file, not STDOUT\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#file-locking-with-flock","title":"File Locking with flock","text":"<p>When multiple processes access the same file, use <code>flock</code> to prevent corruption:</p> <pre><code>use Fcntl ':flock';    # Import LOCK_SH, LOCK_EX, LOCK_NB, LOCK_UN\n\nopen my $fh, '&gt;&gt;', 'counter.txt' or die \"Cannot open: $!\";\n\n# Exclusive lock - blocks until acquired\nflock($fh, LOCK_EX) or die \"Cannot lock: $!\";\n\n# Now safe to write\nprint $fh \"Entry at \" . localtime() . \"\\n\";\n\n# Lock is released when filehandle is closed\nclose $fh;\n</code></pre> <p>Lock types:</p> Constant Value Meaning <code>LOCK_SH</code> 1 Shared lock (multiple readers) <code>LOCK_EX</code> 2 Exclusive lock (single writer) <code>LOCK_NB</code> 4 Non-blocking (combine with <code>\\|</code>) <code>LOCK_UN</code> 8 Unlock <pre><code># Non-blocking lock attempt\nif (flock($fh, LOCK_EX | LOCK_NB)) {\n    print \"Got the lock\\n\";\n    # ... do work ...\n} else {\n    print \"File is locked by another process\\n\";\n}\n</code></pre> <p>flock Caveats</p> <p><code>flock</code> is advisory - it only works if all processes accessing the file use <code>flock</code>. A process that opens the file without locking will ignore locks entirely. Also, <code>flock</code> does not work over NFS on many systems.</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#atomic-writes","title":"Atomic Writes","text":"<p>Writing directly to a file risks corruption if the script crashes mid-write. The safe pattern is write-to-temp-then-rename:</p> <pre><code>use File::Temp 'tempfile';\nuse File::Copy 'move';\n\nmy $target = 'config.json';\n\n# Write to a temporary file in the same directory\nmy ($tmp_fh, $tmp_name) = tempfile(DIR =&gt; '.', UNLINK =&gt; 0);\nprint $tmp_fh '{\"setting\": \"new_value\"}';\nclose $tmp_fh or die \"Error closing temp file: $!\";\n\n# Atomic rename (on the same filesystem)\nmove($tmp_name, $target) or die \"Cannot rename: $!\";\n</code></pre> <p>The <code>rename</code> (or <code>move</code> on the same filesystem) operation is atomic on Unix - the file either has the old content or the new content, never a partial write. Creating the temp file in the same directory as the target ensures they are on the same filesystem.</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#log-rotation","title":"Log Rotation","text":"<p>A practical pattern combining file tests, directory operations, and file manipulation:</p> <pre><code>use strict;\nuse warnings;\nuse File::Copy 'move';\n\nsub rotate_log {\n    my ($logfile, $max_rotations) = @_;\n    $max_rotations //= 5;\n\n    # Delete the oldest\n    unlink \"$logfile.$max_rotations\" if -f \"$logfile.$max_rotations\";\n\n    # Shift existing rotated files up\n    for my $n (reverse 1 .. $max_rotations - 1) {\n        my $src = \"$logfile.$n\";\n        my $dst = \"$logfile.\" . ($n + 1);\n        move($src, $dst) if -f $src;\n    }\n\n    # Rotate the current log\n    if (-f $logfile) {\n        move($logfile, \"$logfile.1\") or warn \"Cannot rotate $logfile: $!\";\n    }\n\n    # Create fresh log\n    open my $fh, '&gt;', $logfile or die \"Cannot create $logfile: $!\";\n    close $fh;\n}\n\n# Rotate if log exceeds 10 MB\nif (-s '/var/log/app.log' &gt; 10 * 1024 * 1024) {\n    rotate_log('/var/log/app.log', 7);\n}\n</code></pre> <p>Command Builder (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/file-io-and-system/#quick-reference","title":"Quick Reference","text":""},{"location":"Dev%20Zero/Perl/file-io-and-system/#file-open-modes","title":"File Open Modes","text":"Mode Meaning <code>open $fh, '&lt;', $f</code> Read <code>open $fh, '&gt;', $f</code> Write (truncate) <code>open $fh, '&gt;&gt;', $f</code> Append <code>open $fh, '+&lt;', $f</code> Read/write <code>open $fh, '-\\|', @cmd</code> Read from command <code>open $fh, '\\|-', @cmd</code> Write to command"},{"location":"Dev%20Zero/Perl/file-io-and-system/#file-test-operators","title":"File Test Operators","text":"Test Meaning <code>-e</code> Exists <code>-f</code> Regular file <code>-d</code> Directory <code>-r</code>/<code>-w</code>/<code>-x</code> Readable/writable/executable <code>-s</code> Size in bytes <code>-z</code> Zero size <code>-l</code> Symlink <code>-M</code>/<code>-A</code>/<code>-C</code> Modification/access/change age (days)"},{"location":"Dev%20Zero/Perl/file-io-and-system/#external-commands","title":"External Commands","text":"Syntax Purpose <code>system(@cmd)</code> Run command, get exit status <code>`cmd`</code> Run command, capture output <code>open $fh, '-\\|', @cmd</code> Stream output from command <code>open $fh, '\\|-', @cmd</code> Stream input to command <code>exec(@cmd)</code> Replace current process <code>fork()</code> Create child process"},{"location":"Dev%20Zero/Perl/file-io-and-system/#essential-modules","title":"Essential Modules","text":"Module Purpose <code>File::Copy</code> <code>copy</code>, <code>move</code> <code>File::Path</code> <code>make_path</code>, <code>remove_tree</code> <code>File::Temp</code> Temporary files/directories <code>File::Find</code> Recursive directory traversal <code>File::Spec</code> Portable path building <code>File::Basename</code> <code>basename</code>, <code>dirname</code>, <code>fileparse</code> <code>File::Slurper</code> Read/write entire files <code>Fcntl</code> File locking constants <code>Cwd</code> Current working directory <code>IPC::Open3</code> Full subprocess I/O control"},{"location":"Dev%20Zero/Perl/file-io-and-system/#further-reading","title":"Further Reading","text":"<ul> <li>perlopentut - Perl open tutorial with examples for every mode</li> <li>perlio - PerlIO layers (encoding, buffering, compression)</li> <li>perlfunc - complete built-in function reference</li> <li>perlvar - special variables (<code>$!</code>, <code>$?</code>, <code>$/</code>, <code>$.</code>, <code>$_</code>)</li> <li>perlipc - interprocess communication (signals, pipes, sockets, fork)</li> <li>File::Find documentation - recursive directory traversal</li> <li>Learning Perl, Chapter 11: Filehandles and File Tests - the \"Llama Book\" covers file I/O fundamentals</li> <li>Intermediate Perl, Chapter 14: Process Management - fork, exec, signals, and IPC</li> </ul> <p>Previous: Subroutines and References | Next: Modules and CPAN | Back to Index</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/","title":"Modules and CPAN","text":""},{"location":"Dev%20Zero/Perl/modules-and-cpan/#code-organization-and-the-perl-ecosystem","title":"Code Organization and the Perl Ecosystem","text":"<p>Version: 1.1 Year: 2026</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#copyright-notice","title":"Copyright Notice","text":"<p>Copyright (c) 2025-2026 Ryan Thomas Robson / Robworks Software LLC. Licensed under CC BY-NC-ND 4.0. You may share this material for non-commercial purposes with attribution, but you may not distribute modified versions.</p> <p>Modules are Perl's unit of code organization - a file that declares a namespace, exports functions, and can be loaded by any script or other module. The Comprehensive Perl Archive Network (CPAN) hosts over 200,000 modules covering everything from date parsing to web frameworks. Knowing how to write modules, find the right CPAN library, and manage dependencies is what separates scripts from maintainable software.</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#use-vs-require","title":"use vs. require","text":"<p>Perl provides two mechanisms for loading external code: <code>use</code> and <code>require</code>. They look similar but behave differently.</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#use","title":"use","text":"<p><code>use</code> loads a module at compile time - before your program's runtime code executes. Under the hood, <code>use Module</code> is equivalent to <code>BEGIN { require Module; Module-&gt;import(); }</code>:</p> <pre><code>use File::Basename;              # loads and imports at compile time\nuse Carp qw(croak confess);     # imports only croak and confess\n</code></pre> <p>The <code>BEGIN</code> block forces execution at compile time, and <code>import()</code> brings the module's exported symbols into your namespace.</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#require","title":"require","text":"<p><code>require</code> loads a module at runtime without calling <code>import()</code>. You must use fully-qualified names to access its functions:</p> <pre><code>require File::Basename;\nmy $base = File::Basename::basename($path);   # full package name required\n</code></pre> <p>Use <code>require</code> for conditional or optional loading:</p> <pre><code>if ($needs_xml) {\n    require XML::LibXML;\n    my $parser = XML::LibXML-&gt;new();\n}\n\n# Optional dependency with fallback\nmy $has_json_xs = eval { require JSON::XS; 1 };\nmy $json = $has_json_xs ? JSON::XS-&gt;new() : do { require JSON::PP; JSON::PP-&gt;new() };\n</code></pre> <p>What is the key difference between use and require in Perl? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#inc-and-module-search-path","title":"@INC and Module Search Path","text":"<p>When you write <code>use Some::Module</code>, Perl needs to find the file <code>Some/Module.pm</code> on disk. It searches through the directories listed in the special array <code>@INC</code>.</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#default-inc","title":"Default @INC","text":"<p>Perl populates <code>@INC</code> from these sources (searched in order): <code>-I</code> command-line flags, the <code>PERL5LIB</code> environment variable, site-specific directories (where <code>cpanm</code> installs), vendor directories, and core Perl library directories.</p> <pre><code># Print your @INC to see the search path\nperl -e 'print join(\"\\n\", @INC), \"\\n\";'\n</code></pre>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#modifying-inc","title":"Modifying @INC","text":"<p>The <code>use lib</code> pragma is the standard way to add paths inside scripts. It prepends paths to <code>@INC</code> at compile time:</p> <pre><code>use lib '/home/user/lib';       # inside your script\nuse lib './lib';                 # relative path\n</code></pre> <p>Other approaches:</p> <pre><code>perl -I/home/user/lib script.pl          # command-line flag\nexport PERL5LIB=/home/user/lib           # environment variable\n</code></pre> <p>Current directory removed from @INC</p> <p>Since Perl 5.26, <code>.</code> is no longer in <code>@INC</code> by default. If your script loads modules from the current directory, you must add <code>use lib '.'</code> explicitly. This was a security fix to prevent malicious <code>.pm</code> files in the working directory from hijacking module loading.</p> <p>Perl converts <code>::</code> separators to directory separators and appends <code>.pm</code> - so <code>File::Basename</code> becomes <code>File/Basename.pm</code>.</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#the-module-loading-process","title":"The Module Loading Process","text":"<pre><code>flowchart TD\n    A[\"use Some::Module\"] --&gt; B[\"Convert :: to /\\nSome::Module becomes Some/Module.pm\"]\n    B --&gt; C[\"Check %INC\\nAlready loaded?\"]\n    C --&gt;|Yes| D[\"Skip loading\\nReturn cached result\"]\n    C --&gt;|No| E[\"Search @INC directories\\nin order\"]\n    E --&gt; F{Found?}\n    F --&gt;|No| G[\"die: Can't locate\\nSome/Module.pm in @INC\"]\n    F --&gt;|Yes| H[\"Compile and execute\\nthe .pm file\"]\n    H --&gt; I[\"Record in %INC:\\n$INC{'Some/Module.pm'} = '/path/to/Some/Module.pm'\"]\n    I --&gt; J[\"Call Some::Module-&gt;import()\\n(for use only, not require)\"]</code></pre> <p>Perl checks <code>%INC</code> before searching <code>@INC</code>, so each module is loaded only once per interpreter.</p> <p>Since Perl 5.26, what happens if you run `use MyModule;` and MyModule.pm is in the current directory but `use lib '.'` was not specified? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#writing-modules","title":"Writing Modules","text":"<p>A Perl module is a <code>.pm</code> file that declares a package (namespace) and returns a true value when loaded.</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#minimal-module","title":"Minimal Module","text":"<pre><code># File: lib/Greeting.pm\npackage Greeting;\n\nuse strict;\nuse warnings;\n\nsub hello {\n    my $name = shift // 'World';\n    return \"Hello, $name!\";\n}\n\n1;\n</code></pre> <p>Three essential elements: <code>package Greeting</code> declares the namespace (all subs after this line belong to <code>Greeting</code>); <code>1;</code> at the end returns a true value (without it, loading fails); and <code>use strict; use warnings</code> for safety.</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#using-your-module","title":"Using Your Module","text":"<pre><code># File: script.pl\nuse lib './lib';\nuse Greeting;\n\nprint Greeting::hello(\"Alice\"), \"\\n\";   # Hello, Alice!\n</code></pre> <p>Without Exporter (covered below), you must use the fully-qualified name <code>Greeting::hello()</code>.</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#packages-and-namespaces","title":"Packages and Namespaces","text":"<p>A package creates a separate namespace. Identically-named items in different packages do not collide:</p> <pre><code>package Database;\nsub connect { ... }    # Database::connect\n\npackage WebServer;\nsub connect { ... }    # WebServer::connect - no conflict\n</code></pre> <p>A single file can contain multiple <code>package</code> declarations, but the convention is one package per file, with the file path matching the package name.</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#nested-namespaces","title":"Nested Namespaces","text":"<p>Use <code>::</code> to create hierarchy. The file path must match:</p> Package name File path <code>My::App::Config</code> <code>lib/My/App/Config.pm</code> <code>Database::Pool</code> <code>lib/Database/Pool.pm</code> <p>The 1; Return Value</p> <p>The <code>1;</code> at the end of a module is required because Perl evaluates the file and checks whether it returned a true value. If you forget it, you get the error \"Module.pm did not return a true value.\" Some developers use <code>__END__</code> after the code and put documentation there, but <code>1;</code> must come before <code>__END__</code>.</p> <p>Building a Simple Module with Exporter (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#exporter","title":"Exporter","text":"<p>When you write <code>use File::Basename</code>, the <code>basename</code> and <code>dirname</code> functions become available in your script without a package prefix. This happens through the Exporter module, which provides the mechanism for injecting symbols into the caller's namespace.</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#export-vs-export_ok","title":"@EXPORT vs. @EXPORT_OK","text":"<p>Exporter uses two arrays to control what gets exported:</p> Array Behavior Best practice <code>@EXPORT</code> Symbols exported automatically with <code>use Module</code> Avoid - pollutes the caller's namespace without consent <code>@EXPORT_OK</code> Symbols exported only when explicitly requested Preferred - caller chooses what to import <pre><code>package MathUtils;\nuse strict;\nuse warnings;\nuse Exporter 'import';\n\nour @EXPORT    = qw(add);                       # auto-exported (avoid this)\nour @EXPORT_OK = qw(subtract multiply divide);  # exported on request\n\nsub add      { $_[0] + $_[1] }\nsub subtract { $_[0] - $_[1] }\nsub multiply { $_[0] * $_[1] }\nsub divide   { $_[1] != 0 ? $_[0] / $_[1] : undef }\n1;\n</code></pre> <pre><code>use MathUtils;                         # imports add() via @EXPORT\nuse MathUtils qw(subtract multiply);   # imports only subtract() and multiply()\n</code></pre>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#export-tags","title":"Export Tags","text":"<p>Group related exports with <code>%EXPORT_TAGS</code>. Callers import tags with a colon prefix:</p> <pre><code>our @EXPORT_OK = qw(read_file write_file slurp_dir list_files);\nour %EXPORT_TAGS = (\n    io  =&gt; [qw(read_file write_file)],\n    dir =&gt; [qw(slurp_dir list_files)],\n    all =&gt; \\@EXPORT_OK,\n);\n\n# Caller:\nuse FileUtils qw(:io);     # imports read_file, write_file\nuse FileUtils qw(:all);    # imports everything in @EXPORT_OK\n</code></pre> <p>Avoid @EXPORT for new modules</p> <p>Putting symbols in <code>@EXPORT</code> means every user of your module gets those names injected into their namespace whether they want them or not. This can cause name collisions and makes it hard to trace where a function came from. Use <code>@EXPORT_OK</code> and let callers explicitly request what they need.</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#cpanm-installing-cpan-modules","title":"cpanm: Installing CPAN Modules","text":"<p><code>cpanm</code> (also called <code>cpanminus</code>) is the preferred tool for installing modules from CPAN. It is simpler and faster than the older <code>cpan</code> shell.</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#installing-and-using-cpanm","title":"Installing and Using cpanm","text":"<pre><code># Install cpanm itself\ncurl -L https://cpanmin.us | perl - App::cpanminus\n\n# Install a module\ncpanm JSON::XS\n\n# Install a specific version\ncpanm JSON::XS@4.03\n\n# Install without running tests (faster)\ncpanm --notest DBI\n\n# Install from a cpanfile (dependency file)\ncpanm --installdeps .\n\n# Install to a local directory (no root required)\ncpanm -l ~/perl5 DateTime\n</code></pre> <p>Installing and Using CPAN Modules (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#finding-modules-on-metacpan","title":"Finding Modules on MetaCPAN","text":"<p>MetaCPAN is the primary search interface for CPAN. It provides source browsing, documentation rendering, dependency graphs, and quality metrics.</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#evaluating-module-quality","title":"Evaluating Module Quality","text":"<p>Not all CPAN modules are equal. Search by task (\"parse CSV\", \"send email\"), check the <code>Task::Kensho</code> curated collection, and look at reverse dependencies to gauge popularity. Before committing to a dependency, check these indicators:</p> Indicator Where to find it What it tells you CPAN Testers MetaCPAN sidebar Pass/fail across platforms and Perl versions GitHub/GitLab activity Repository link on MetaCPAN Recent commits, open issues, maintainer responsiveness Reverse dependencies MetaCPAN \"Reverse Dependencies\" tab How many other modules rely on this one Last release date MetaCPAN module page Whether the module is actively maintained Documentation quality MetaCPAN POD rendering Clear SYNOPSIS, complete API docs, examples License META file or POD Whether it is compatible with your project <p>The CPAN Testers Matrix</p> <p>CPAN Testers runs automated tests of every CPAN upload across hundreds of platform/Perl-version combinations. A module with all green results is reliable. Sporadic failures on obscure platforms are normal. Widespread failures are a red flag.</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#cpanfile-for-dependency-management","title":"cpanfile for Dependency Management","text":"<p>A <code>cpanfile</code> declares your project's module dependencies in a single file. It serves the same purpose as <code>requirements.txt</code> in Python or <code>package.json</code> in Node.js.</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#basic-cpanfile","title":"Basic cpanfile","text":"<pre><code># cpanfile\nrequires 'perl', '5.020';\nrequires 'Mojolicious', '&gt;= 9.0';\nrequires 'DBI';\nrequires 'JSON::XS';\nrequires 'Try::Tiny';\n\non 'test' =&gt; sub {\n    requires 'Test2::Suite';\n};\n\non 'develop' =&gt; sub {\n    requires 'Perl::Tidy';\n    requires 'Perl::Critic';\n};\n</code></pre> <p>Install with <code>cpanm --installdeps .</code> for runtime deps, add <code>--with-test</code> for test deps, or <code>--with-develop</code> for development tools.</p> <p>Creating a cpanfile and Installing Dependencies (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#locallib-for-user-space-installs","title":"local::lib for User-Space Installs","text":"<p>When you do not have root access or want to keep module installations isolated from the system Perl, <code>local::lib</code> sets up a personal module directory.</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#setup","title":"Setup","text":"<pre><code># Install local::lib\ncpanm local::lib\n\n# Activate it (add to ~/.bashrc or ~/.zshrc)\neval \"$(perl -I$HOME/perl5/lib/perl5 -Mlocal::lib)\"\n</code></pre> <p>Once activated, <code>cpanm</code> installs modules into <code>~/perl5/</code>, Perl adds <code>~/perl5/lib/perl5</code> to <code>@INC</code> automatically, and no <code>sudo</code> is required.</p> <p>You can also create project-specific libraries:</p> <pre><code># Install modules into ./local/ instead of ~/perl5\neval \"$(perl -Mlocal::lib=./local)\"\ncpanm --installdeps .\n</code></pre> <p>Combine with cpanfile</p> <p>The workflow of <code>cpanfile</code> + <code>local::lib</code> + <code>cpanm --installdeps .</code> gives you reproducible, isolated dependency management. New developers clone the repository, run one command, and have everything they need.</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#cpan-ecosystem-relationships","title":"CPAN Ecosystem Relationships","text":"<p>The CPAN toolchain is a set of interconnected components that work together to author, distribute, install, and test Perl modules:</p> <pre><code>flowchart TD\n    subgraph \"Author Side\"\n        A[\"Module Author\"] --&gt; B[\"Distribution Tools\\nDist::Zilla / ExtUtils::MakeMaker\"]\n        B --&gt; C[\"Upload to PAUSE\"]\n    end\n\n    subgraph \"Infrastructure\"\n        C --&gt; D[\"CPAN\\n(master archive)\"]\n        D --&gt; E[\"CPAN Mirrors\\n(worldwide)\"]\n        D --&gt; F[\"MetaCPAN\\n(search and docs)\"]\n        D --&gt; G[\"CPAN Testers\\n(automated testing)\"]\n    end\n\n    subgraph \"User Side\"\n        E --&gt; H[\"cpanm / cpan\\n(install tools)\"]\n        H --&gt; I[\"local::lib / system perl\\n(install targets)\"]\n        F --&gt; J[\"Developer\\n(searches for modules)\"]\n        J --&gt; H\n    end</code></pre> <p>Authors upload to PAUSE, which distributes to CPAN mirrors worldwide. MetaCPAN provides search and documentation. CPAN Testers runs automated cross-platform tests. Users install via cpanm.</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#distzilla-overview","title":"Dist::Zilla Overview","text":"<p>Dist::Zilla (commonly called <code>dzil</code>) is a distribution authoring tool. Packaging a module as a proper CPAN distribution requires <code>Makefile.PL</code>, <code>META.json</code>, <code>MANIFEST</code>, a <code>LICENSE</code> file, and consistent version numbers. Dist::Zilla generates all of this from a <code>dist.ini</code> configuration file.</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#minimal-distini","title":"Minimal dist.ini","text":"<pre><code>name    = My-Utils\nauthor  = Your Name &lt;you@example.com&gt;\nlicense = Perl_5\nversion = 0.001\n\n[@Basic]\n</code></pre>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#common-workflow","title":"Common Workflow","text":"<pre><code>dzil new My::Utils     # create a new distribution\ndzil build             # generate the tarball\ndzil test              # run tests\ndzil release           # upload to CPAN\n</code></pre> <p>The standard project layout is <code>lib/</code> for module code and <code>t/</code> for tests. Dist::Zilla manages everything else.</p> <p>Dist::Zilla vs. Minilla</p> <p>Dist::Zilla is powerful but has a steep learning curve. Minilla is a lighter alternative that follows conventions over configuration. For a first CPAN upload, Minilla is often the faster path.</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#exercises","title":"Exercises","text":"<p>Write a Utility Module (requires JavaScript)</p> <p>Build a Module with Tests and Install It (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#cpanm-command-builder","title":"cpanm Command Builder","text":"<p>Command Builder (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#quick-reference","title":"Quick Reference","text":"Concept Syntax Load at compile time <code>use Module</code> Load at runtime <code>require Module</code> Add search path <code>use lib './lib'</code> Declare package <code>package My::Module;</code> Return true <code>1;</code> (end of every <code>.pm</code> file) Export on request <code>our @EXPORT_OK = qw(func)</code> Export tags <code>our %EXPORT_TAGS = (tag =&gt; [...])</code> Install module <code>cpanm Module::Name</code> Install from cpanfile <code>cpanm --installdeps .</code> Declare dependency <code>requires 'Module'</code> in cpanfile User-space install <code>eval \"$(perl -Mlocal::lib)\"</code>"},{"location":"Dev%20Zero/Perl/modules-and-cpan/#further-reading","title":"Further Reading","text":"<ul> <li>perlmod - Perl modules, packages, and symbol tables</li> <li>perlnewmod - preparing a new module for distribution</li> <li>Exporter - Exporter module documentation</li> <li>cpanfile specification - format reference for dependency files</li> <li>local::lib - create and use a local lib directory</li> <li>MetaCPAN - search and browse the CPAN archive</li> <li>Task::Kensho - curated list of recommended CPAN modules</li> <li>Intermediate Perl - the \"Alpaca Book\" covers modules, references, and distribution authoring</li> </ul> <p>Previous: File I/O and System Interaction | Next: Object-Oriented Perl | Back to Index</p>"},{"location":"Dev%20Zero/Perl/networking-daemons/","title":"Networking and Daemons","text":""},{"location":"Dev%20Zero/Perl/networking-daemons/#system-programming-with-perl","title":"System Programming with Perl","text":"<p>Version: 1.1 Year: 2026</p>"},{"location":"Dev%20Zero/Perl/networking-daemons/#copyright-notice","title":"Copyright Notice","text":"<p>Copyright (c) 2025-2026 Ryan Thomas Robson / Robworks Software LLC. Licensed under CC BY-NC-ND 4.0. You may share this material for non-commercial purposes with attribution, but you may not distribute modified versions.</p> <p>Perl grew up on networked Unix systems. Sockets, forking, and signal handling map directly to system calls that Perl has wrapped since version 1. This guide covers TCP and UDP clients and servers, HTTP requests, JSON encoding, daemon processes, and event-driven frameworks for concurrent I/O.</p>"},{"location":"Dev%20Zero/Perl/networking-daemons/#tcp-sockets-with-iosocketinet","title":"TCP Sockets with IO::Socket::INET","text":"<p><code>IO::Socket::INET</code> provides an object-oriented interface over the raw <code>socket</code>/<code>bind</code>/<code>listen</code>/<code>accept</code> system calls. It ships with core Perl.</p>"},{"location":"Dev%20Zero/Perl/networking-daemons/#tcp-server","title":"TCP Server","text":"<p>A basic TCP server binds to a port, listens for connections, and handles each client:</p> <pre><code>use IO::Socket::INET;\n\nmy $server = IO::Socket::INET-&gt;new(\n    LocalPort =&gt; 9000,\n    Proto     =&gt; 'tcp',\n    Listen    =&gt; 5,\n    ReuseAddr =&gt; 1,\n) or die \"Cannot create server: $!\\n\";\n\nwhile (my $client = $server-&gt;accept()) {\n    while (my $line = &lt;$client&gt;) {\n        chomp $line;\n        print $client \"Echo: $line\\n\";\n    }\n    close $client;\n}\n</code></pre> <p><code>Listen</code> sets the backlog queue size. <code>ReuseAddr</code> lets you restart the server immediately without waiting for the kernel to release the port.</p> <p>Blocking Accept</p> <p>This server handles one client at a time. While serving client A, client B waits in the backlog queue. For concurrent clients, you need <code>fork</code>, threads, or an event loop - covered later in this guide.</p>"},{"location":"Dev%20Zero/Perl/networking-daemons/#tcp-client","title":"TCP Client","text":"<pre><code>use IO::Socket::INET;\n\nmy $sock = IO::Socket::INET-&gt;new(\n    PeerHost =&gt; 'localhost',\n    PeerPort =&gt; 9000,\n    Proto    =&gt; 'tcp',\n) or die \"Cannot connect: $!\\n\";\n\nprint $sock \"Hello, server!\\n\";\nmy $reply = &lt;$sock&gt;;\nprint \"Server says: $reply\";\nclose $sock;\n</code></pre> <p>The socket object is a filehandle. You read from it with <code>&lt;$sock&gt;</code> and write with <code>print $sock</code> - the same I/O model as regular files.</p>"},{"location":"Dev%20Zero/Perl/networking-daemons/#tcp-client-server-flow","title":"TCP Client-Server Flow","text":"<pre><code>sequenceDiagram\n    participant C as Client\n    participant S as Server\n    S-&gt;&gt;S: bind() + listen()\n    C-&gt;&gt;S: connect()\n    S-&gt;&gt;S: accept()\n    S--&gt;&gt;C: Connection established\n    C-&gt;&gt;S: send(\"Hello\")\n    S-&gt;&gt;S: Process data\n    S--&gt;&gt;C: send(\"Echo: Hello\")\n    C-&gt;&gt;S: send(\"quit\")\n    S--&gt;&gt;C: close()\n    C-&gt;&gt;C: close()</code></pre> <p>TCP Echo Server and Client (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/networking-daemons/#udp-sockets","title":"UDP Sockets","text":"<p>UDP is connectionless - no handshake, no guaranteed delivery, no ordering. Each <code>send</code> is an independent datagram. This makes UDP appropriate for DNS queries, logging, metrics, and real-time data where dropped packets are acceptable.</p> <p>A UDP server binds to a port and calls <code>recv</code> in a loop. A UDP client specifies the peer at creation time and sends immediately - no handshake:</p> <pre><code># Server\nmy $srv = IO::Socket::INET-&gt;new(LocalPort =&gt; 9001, Proto =&gt; 'udp')\n    or die \"Cannot bind: $!\\n\";\nwhile (1) {\n    $srv-&gt;recv(my $data, 1024);\n    chomp $data;\n    $srv-&gt;send(\"ACK: $data\\n\");\n}\n\n# Client\nmy $cli = IO::Socket::INET-&gt;new(\n    PeerHost =&gt; 'localhost', PeerPort =&gt; 9001, Proto =&gt; 'udp',\n) or die \"Cannot create socket: $!\\n\";\n$cli-&gt;send(\"ping\\n\");\n$cli-&gt;recv(my $reply, 1024);\nprint \"Got: $reply\";\n</code></pre> <p>When to Use UDP</p> <p>Choose UDP for fire-and-forget scenarios: syslog forwarding, StatsD metrics, DNS lookups, or any situation where retransmission logic lives in your application layer. For everything else, use TCP.</p> <p>What happens when you call IO::Socket::INET-&gt;new(Proto =&gt; 'tcp', PeerHost =&gt; 'example.com', PeerPort =&gt; 80)? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/networking-daemons/#http-clients","title":"HTTP Clients","text":""},{"location":"Dev%20Zero/Perl/networking-daemons/#httptiny","title":"HTTP::Tiny","text":"<p><code>HTTP::Tiny</code> ships with Perl since version 5.14. No CPAN install needed:</p> <pre><code>use HTTP::Tiny;\nmy $http = HTTP::Tiny-&gt;new(timeout =&gt; 10);\nmy $res  = $http-&gt;get('https://httpbin.org/get');\n\nif ($res-&gt;{success}) {\n    print $res-&gt;{content};\n} else {\n    warn \"Failed: $res-&gt;{status} $res-&gt;{reason}\\n\";\n}\n</code></pre> <p>The response is a hash reference with keys <code>success</code>, <code>status</code>, <code>reason</code>, <code>content</code>, and <code>headers</code>. POST requests pass a content body and headers:</p> <pre><code>my $res = $http-&gt;post('https://httpbin.org/post', {\n    content =&gt; '{\"key\": \"value\"}',\n    headers =&gt; { 'Content-Type' =&gt; 'application/json' },\n});\n</code></pre> <p><code>HTTP::Tiny</code> handles redirects, connection keep-alive, and HTTPS if <code>IO::Socket::SSL</code> is installed.</p>"},{"location":"Dev%20Zero/Perl/networking-daemons/#lwpuseragent","title":"LWP::UserAgent","text":"<p><code>LWP::UserAgent</code> is the full-featured HTTP client on CPAN. It supports cookies, authentication, proxies, file uploads, and content negotiation:</p> <pre><code>use LWP::UserAgent;\n\nmy $ua = LWP::UserAgent-&gt;new(timeout =&gt; 10);\nmy $res = $ua-&gt;get('https://api.github.com/zen');\n\nif ($res-&gt;is_success) {\n    print $res-&gt;decoded_content, \"\\n\";\n} else {\n    warn \"Error: \", $res-&gt;status_line, \"\\n\";\n}\n</code></pre> <p>The response is an <code>HTTP::Response</code> object with methods like <code>is_success</code>, <code>status_line</code>, and <code>decoded_content</code>.</p> Feature HTTP::Tiny LWP::UserAgent Core Perl Yes (5.14+) No (CPAN) HTTPS Needs IO::Socket::SSL Needs LWP::Protocol::https Cookies Manual Built-in File upload Manual Built-in"},{"location":"Dev%20Zero/Perl/networking-daemons/#mojouseragent","title":"Mojo::UserAgent","text":"<p><code>Mojo::UserAgent</code> is part of the Mojolicious framework and supports async HTTP with promises:</p> <pre><code>use Mojo::UserAgent;\nmy $ua = Mojo::UserAgent-&gt;new;\n\n# Synchronous\nmy $res = $ua-&gt;get('https://httpbin.org/get')-&gt;result;\nprint $res-&gt;json-&gt;{origin}, \"\\n\";\n\n# Asynchronous - two requests concurrently\nmy $p1 = $ua-&gt;get_p('https://httpbin.org/delay/1');\nmy $p2 = $ua-&gt;get_p('https://httpbin.org/delay/1');\n\nMojo::Promise-&gt;all($p1, $p2)-&gt;then(sub {\n    print \"Both requests complete\\n\";\n})-&gt;catch(sub { warn \"Failed: @_\\n\" })-&gt;wait;\n</code></pre> <p><code>Promise-&gt;all</code> runs both requests concurrently - two 1-second requests finish in roughly 1 second instead of 2.</p> <p>HTTP API Client with Error Handling (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/networking-daemons/#json-handling","title":"JSON Handling","text":"<p><code>JSON::MaybeXS</code> auto-detects the fastest available JSON backend (Cpanel::JSON::XS, JSON::XS, or pure-Perl JSON::PP):</p> <pre><code>use JSON::MaybeXS qw(encode_json decode_json);\n\nmy $json = encode_json({ name =&gt; 'Perl', year =&gt; 1987 });\nmy $data = decode_json($json);\nprint $data-&gt;{name}, \"\\n\";    # Perl\n</code></pre> <p>For pretty-printed output, use the OO interface with <code>pretty =&gt; 1</code> and <code>canonical =&gt; 1</code> (sorted keys).</p> <p>JSON Boolean Values</p> <p>JSON's <code>true</code>/<code>false</code> map to <code>JSON::PP::Boolean</code> objects (behave like <code>1</code>/<code>0</code>). To create JSON booleans from Perl, use <code>JSON::MaybeXS::true</code>/<code>JSON::MaybeXS::false</code>, or <code>\\1</code>/<code>\\0</code>.</p>"},{"location":"Dev%20Zero/Perl/networking-daemons/#writing-daemons","title":"Writing Daemons","text":"<p>A daemon is a long-running background process with no controlling terminal. The classic Unix double-fork ensures the daemon cannot reacquire a controlling terminal:</p> <pre><code>use POSIX qw(setsid);\n\nsub daemonize {\n    my $pid = fork();\n    die \"First fork failed: $!\\n\" unless defined $pid;\n    exit 0 if $pid;\n\n    setsid() or die \"setsid failed: $!\\n\";\n\n    $pid = fork();\n    die \"Second fork failed: $!\\n\" unless defined $pid;\n    exit 0 if $pid;\n\n    chdir '/' or die \"Cannot chdir to /: $!\\n\";\n    umask 0;\n\n    open STDIN,  '&lt;', '/dev/null' or die \"Cannot redirect STDIN: $!\\n\";\n    open STDOUT, '&gt;', '/dev/null' or die \"Cannot redirect STDOUT: $!\\n\";\n    open STDERR, '&gt;', '/dev/null' or die \"Cannot redirect STDERR: $!\\n\";\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/networking-daemons/#daemon-lifecycle","title":"Daemon Lifecycle","text":"<pre><code>flowchart TD\n    A[Parent Process] --&gt;|fork| B[Child 1]\n    A --&gt;|exit| C[Parent exits]\n    B --&gt;|setsid| D[New session leader]\n    D --&gt;|fork| E[Child 2 - The Daemon]\n    D --&gt;|exit| F[Session leader exits]\n    E --&gt;|chdir /| G[Change working directory]\n    G --&gt;|close stdin/stdout/stderr| H[Redirect to /dev/null]\n    H --&gt;|write PID file| I[Running daemon]\n    I --&gt;|signal| J{Signal received}\n    J --&gt;|SIGTERM| K[Cleanup and exit]\n    J --&gt;|SIGHUP| L[Reload configuration]\n    J --&gt;|SIGUSR1| M[Log rotation]</code></pre>"},{"location":"Dev%20Zero/Perl/networking-daemons/#pid-file-management","title":"PID File Management","text":"<p>A PID file records the daemon's process ID so management scripts can send signals to it. Lock the file with <code>flock</code> to prevent duplicate instances:</p> <pre><code>use Fcntl ':flock';\n\nsub write_pidfile {\n    my ($path) = @_;\n    open my $fh, '&gt;', $path or die \"Cannot open PID file $path: $!\\n\";\n    flock($fh, LOCK_EX | LOCK_NB)\n        or die \"Another instance is running (PID file locked)\\n\";\n    print $fh $$;\n    return $fh;    # Keep open to hold the lock\n}\n</code></pre> <p>By holding the filehandle open, no other process can acquire the lock until this one exits.</p> <p>Daemonizing a Process (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/networking-daemons/#procdaemon","title":"Proc::Daemon","text":"<p>The <code>Proc::Daemon</code> CPAN module wraps the double-fork pattern into a single method call:</p> <pre><code>use Proc::Daemon;\n\nmy $daemon = Proc::Daemon-&gt;new(\n    work_dir     =&gt; '/tmp',\n    pid_file     =&gt; '/tmp/myapp.pid',\n    child_STDOUT =&gt; '/var/log/myapp.log',\n    child_STDERR =&gt; '/var/log/myapp.err',\n);\n\nmy $pid = $daemon-&gt;Init();\nexit 0 if $pid;    # Parent exits\n\n# Daemon code runs here\nwhile (1) {\n    do_work();\n    sleep 10;\n}\n</code></pre> <p><code>Proc::Daemon</code> handles forking, session creation, directory change, filehandle redirection, and PID file writing. <code>Init</code> returns the child PID to the parent and <code>0</code> to the daemon.</p>"},{"location":"Dev%20Zero/Perl/networking-daemons/#signal-handling-in-long-running-processes","title":"Signal Handling in Long-Running Processes","text":"<p>Daemons need to respond to signals - the Unix mechanism for inter-process communication. Perl exposes signal handlers through the <code>%SIG</code> hash:</p> <pre><code>my $running = 1;\n$SIG{TERM} = sub { warn \"SIGTERM received\\n\"; $running = 0 };\n$SIG{HUP}  = sub { warn \"SIGHUP received\\n\";  reload_config() };\n$SIG{INT}  = sub { $running = 0 };\n\nwhile ($running) {\n    do_work();\n    sleep 1;\n}\ncleanup();\nexit 0;\n</code></pre>"},{"location":"Dev%20Zero/Perl/networking-daemons/#common-signals","title":"Common Signals","text":"Signal Typical Daemon Use <code>SIGTERM</code> Graceful shutdown <code>SIGINT</code> Ctrl-C (interactive) <code>SIGHUP</code> Reload configuration <code>SIGUSR1</code> Rotate logs / dump state <code>SIGCHLD</code> Reap child processes <code>SIGPIPE</code> Broken socket (ignore it) <p>SIGPIPE Kills Daemons</p> <p>Writing to a closed socket sends <code>SIGPIPE</code>, which terminates the process by default. Every network daemon should ignore it: <code>$SIG{PIPE} = 'IGNORE'</code>. Then check the return value of <code>print</code> or <code>syswrite</code> instead.</p>"},{"location":"Dev%20Zero/Perl/networking-daemons/#reaping-child-processes","title":"Reaping Child Processes","text":"<p>Forking servers must reap children to prevent zombie processes. Set a <code>SIGCHLD</code> handler with non-blocking <code>waitpid</code>:</p> <pre><code>use POSIX ':sys_wait_h';\n$SIG{CHLD} = sub {\n    while ((my $pid = waitpid(-1, WNOHANG)) &gt; 0) { }\n};\n</code></pre>"},{"location":"Dev%20Zero/Perl/networking-daemons/#process-supervision-with-systemd","title":"Process Supervision with systemd","text":"<p>Production daemons should be managed by a process supervisor. systemd is the standard on modern Linux. A unit file describes how to manage your service:</p> <pre><code>[Unit]\nDescription=My Perl Application\nAfter=network.target\n\n[Service]\nType=simple\nExecStart=/usr/bin/perl /opt/myapp/server.pl\nExecReload=/bin/kill -HUP $MAINPID\nRestart=on-failure\nRestartSec=5\nUser=myapp\nWorkingDirectory=/opt/myapp\nStandardOutput=journal\nStandardError=journal\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Save this as <code>/etc/systemd/system/myapp.service</code>, then manage it with <code>systemctl</code>:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable myapp\nsudo systemctl start myapp\n</code></pre> <p>Type=simple vs. Type=forking</p> <p>With <code>Type=simple</code>, systemd expects your process to stay in the foreground - do not daemonize. systemd handles backgrounding. With <code>Type=forking</code>, systemd expects the process to fork and reads the PID file to track the child. <code>Type=simple</code> is preferred for new services.</p> <p>When using <code>Type=simple</code>, your Perl script runs as a foreground process. Set <code>$| = 1</code> to unbuffer STDOUT so log lines appear immediately in journald:</p> <pre><code>#!/usr/bin/perl\nuse strict;\nuse warnings;\n\n$| = 1;\nmy $running = 1;\n$SIG{TERM} = sub { $running = 0 };\n$SIG{HUP}  = sub { reload_config() };\n$SIG{PIPE} = 'IGNORE';\n\nwhile ($running) {\n    do_work();\n    sleep 1;\n}\nexit 0;\n</code></pre>"},{"location":"Dev%20Zero/Perl/networking-daemons/#event-driven-programming","title":"Event-Driven Programming","text":"<p>Blocking I/O handles one connection at a time. Event-driven I/O multiplexes many connections in a single process using <code>select</code>, <code>poll</code>, or <code>epoll</code> under the hood.</p>"},{"location":"Dev%20Zero/Perl/networking-daemons/#anyevent","title":"AnyEvent","text":"<p><code>AnyEvent</code> provides a unified API across multiple event loop backends (EV, Event, POE, or its own pure-Perl loop):</p> <pre><code>use AnyEvent;\nuse AnyEvent::Socket;\nuse AnyEvent::Handle;\n\nmy $cv = AnyEvent-&gt;condvar;\n\ntcp_server undef, 9000, sub {\n    my ($fh, $host, $port) = @_;\n    my $handle = AnyEvent::Handle-&gt;new(\n        fh       =&gt; $fh,\n        on_error =&gt; sub { $_[0]-&gt;destroy },\n        on_eof   =&gt; sub { $_[0]-&gt;destroy },\n    );\n    $handle-&gt;on_read(sub {\n        $handle-&gt;push_read(line =&gt; sub {\n            my (undef, $line) = @_;\n            $handle-&gt;push_write(\"Echo: $line\\n\");\n        });\n    });\n};\n\n$cv-&gt;recv;    # Enter the event loop\n</code></pre> <p>The condition variable (<code>$cv</code>) is the event loop entry point. <code>$cv-&gt;recv</code> blocks until <code>$cv-&gt;send</code> is called. Each connection gets its own <code>AnyEvent::Handle</code> for async I/O.</p>"},{"location":"Dev%20Zero/Perl/networking-daemons/#ioasync","title":"IO::Async","text":"<p><code>IO::Async</code> is another event-driven framework, structured around a central loop with notifier objects. Where AnyEvent uses bare callbacks, IO::Async wraps everything in objects. It has built-in <code>Future</code> support for composing async operations.</p> <pre><code>use IO::Async::Loop;\nuse IO::Async::Listener;\n\nmy $loop = IO::Async::Loop-&gt;new;\n\n$loop-&gt;add(IO::Async::Listener-&gt;new(\n    on_stream =&gt; sub {\n        my (undef, $stream) = @_;\n        $stream-&gt;configure(on_read =&gt; sub {\n            my ($self, $buffref, $eof) = @_;\n            while ($$buffref =~ s/^(.*)\\n//) {\n                $self-&gt;write(\"Echo: $1\\n\");\n            }\n            return 0;\n        });\n        $loop-&gt;add($stream);\n    },\n));\n\n$loop-&gt;listen(\n    addr =&gt; { family =&gt; 'inet', socktype =&gt; 'stream', port =&gt; 9000 },\n)-&gt;get;\n$loop-&gt;run;\n</code></pre> Feature AnyEvent IO::Async Style Callback-based Object notifiers Timer <code>AnyEvent-&gt;timer(...)</code> <code>$loop-&gt;delay_future(...)</code> Futures <code>AnyEvent::Future</code> Built-in <code>Future</code> Ecosystem Large (AnyEvent::*) Growing (Net::Async::*) <p>Both frameworks achieve the same goal: handling thousands of concurrent connections in a single process without threads or forks. AnyEvent is callback-centric. IO::Async uses an object hierarchy. Choose whichever fits your mental model.</p> <p>HTTP Status Checker Script (requires JavaScript)</p> <p>Chat Server with Multiple Clients (requires JavaScript)</p> <p>Command Builder (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/networking-daemons/#further-reading","title":"Further Reading","text":"<ul> <li>perlipc - Perl interprocess communication: signals, pipes, sockets</li> <li>IO::Socket::INET documentation - TCP/UDP socket interface</li> <li>HTTP::Tiny documentation - lightweight HTTP client (core module)</li> <li>LWP::UserAgent documentation - full-featured HTTP client</li> <li>Mojo::UserAgent documentation - async HTTP with promises</li> <li>JSON::MaybeXS documentation - fast, portable JSON handling</li> <li>AnyEvent documentation - event-driven programming</li> <li>IO::Async documentation - asynchronous I/O framework</li> <li>systemd.service man page - service unit configuration</li> <li>Network Programming with Perl (Stein) - comprehensive reference</li> </ul> <p>Previous: Text Processing and One-Liners | Next: Web Frameworks and APIs | Back to Index</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/","title":"Object-Oriented Perl","text":""},{"location":"Dev%20Zero/Perl/object-oriented-perl/#from-bless-to-moose-building-reusable-abstractions","title":"From bless to Moose: Building Reusable Abstractions","text":"<p>Version: 1.1 Year: 2026</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#copyright-notice","title":"Copyright Notice","text":"<p>Copyright (c) 2025-2026 Ryan Thomas Robson / Robworks Software LLC. Licensed under CC BY-NC-ND 4.0. You may share this material for non-commercial purposes with attribution, but you may not distribute modified versions.</p> <p>Perl's object system is unlike most languages. There is no <code>class</code> keyword, no built-in <code>new</code> method, and no special syntax for declaring attributes. Instead, Perl OOP is built on three concepts you already know: packages (namespaces), references (data), and subroutines (behavior). The <code>bless</code> function connects a reference to a package, and that connection is the entire foundation of Perl objects.</p> <p>This minimal design gives you full control - and full responsibility. You can build a class in five lines or layer on a sophisticated metaobject protocol with Moose. This guide covers both ends of that spectrum: the manual mechanics that every Perl programmer needs to understand, and the modern tools that make OOP productive.</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#bless-and-constructors","title":"bless and Constructors","text":"<p>An object in Perl is a reference that has been associated with a package using <code>bless</code>. That package becomes the object's class, and its subroutines become the object's methods.</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#the-bless-function","title":"The bless Function","text":"<p><code>bless</code> takes a reference and a class name (package name) and returns the reference, now tagged with that class:</p> <pre><code>package Dog;\n\nsub new {\n    my ($class, %args) = @_;\n    my $self = {\n        name  =&gt; $args{name}  // 'Unknown',\n        breed =&gt; $args{breed} // 'Mixed',\n    };\n    bless $self, $class;\n    return $self;\n}\n</code></pre> <p>Here is what happens step by step:</p> <ol> <li><code>$class</code> receives the string <code>\"Dog\"</code> (the invocant - explained below)</li> <li><code>%args</code> absorbs the remaining arguments as key-value pairs</li> <li><code>$self</code> is an anonymous hash reference holding the object's data</li> <li><code>bless $self, $class</code> tags that hash reference as a <code>Dog</code> object</li> <li>The blessed reference is returned to the caller</li> </ol>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#calling-the-constructor","title":"Calling the Constructor","text":"<pre><code>my $rex = Dog-&gt;new(name =&gt; 'Rex', breed =&gt; 'Labrador');\nprint ref($rex);  # \"Dog\"\n</code></pre> <p>The <code>ref</code> function returns the class name of a blessed reference, or the reference type (<code>HASH</code>, <code>ARRAY</code>, etc.) for unblessed references.</p> <p>Why a hash reference?</p> <p>You can bless any reference - array refs, scalar refs, even code refs. Hash references are the convention because they give you named fields (<code>$self-&gt;{name}</code>) that are readable and extensible. Array-based objects use less memory but sacrifice clarity.</p> <p>Creating a Basic Class with bless (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#methods-and-the-invocant","title":"Methods and the Invocant","text":"<p>A method is a subroutine that expects an object (or class name) as its first argument. Perl passes this automatically when you use arrow notation.</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#instance-methods","title":"Instance Methods","text":"<p>When you call <code>$obj-&gt;method()</code>, Perl translates it to <code>Package::method($obj)</code>. The object is passed as the first argument, conventionally called <code>$self</code>:</p> <pre><code>package Dog;\n\nsub speak {\n    my ($self) = @_;\n    print \"$self-&gt;{name} says: Woof!\\n\";\n}\n\nsub describe {\n    my ($self) = @_;\n    printf \"%s is a %s\\n\", $self-&gt;{name}, $self-&gt;{breed};\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#class-methods","title":"Class Methods","text":"<p>Class methods receive the class name as the first argument instead of an object. The constructor <code>new</code> is the most common class method:</p> <pre><code>package Logger;\n\nsub new {\n    my ($class, %args) = @_;\n    return bless { level =&gt; $args{level} // 'info' }, $class;\n}\n\n# Another class method\nsub default {\n    my ($class) = @_;\n    return $class-&gt;new(level =&gt; 'warn');\n}\n</code></pre> <pre><code>my $log = Logger-&gt;default;  # calls Logger::default(\"Logger\")\n</code></pre>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#shift-vs-explicit-unpacking","title":"shift vs Explicit Unpacking","text":"<p>Two common styles for extracting the invocant:</p> <pre><code># Style 1: shift (common in short methods)\nsub name {\n    my $self = shift;\n    return $self-&gt;{name};\n}\n\n# Style 2: list unpacking (common when there are other arguments)\nsub set_name {\n    my ($self, $new_name) = @_;\n    $self-&gt;{name} = $new_name;\n}\n</code></pre> <p>Both are idiomatic. Use <code>shift</code> for simple accessors and list unpacking when you have additional parameters.</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#accessor-methods","title":"Accessor Methods","text":"<p>Direct hash access (<code>$obj-&gt;{name}</code>) exposes your object's internals. Accessor methods provide a controlled interface:</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#read-only-accessors","title":"Read-Only Accessors","text":"<pre><code>sub name {\n    my ($self) = @_;\n    return $self-&gt;{name};\n}\n\nsub breed {\n    my ($self) = @_;\n    return $self-&gt;{breed};\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#read-write-accessors","title":"Read-Write Accessors","text":"<p>A combined getter/setter uses the argument count to decide behavior:</p> <pre><code>sub name {\n    my $self = shift;\n    if (@_) {\n        $self-&gt;{name} = shift;\n    }\n    return $self-&gt;{name};\n}\n</code></pre> <pre><code>print $dog-&gt;name;           # getter: returns \"Rex\"\n$dog-&gt;name(\"Buddy\");        # setter: changes name to \"Buddy\"\nprint $dog-&gt;name;           # getter: returns \"Buddy\"\n</code></pre>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#generating-accessors","title":"Generating Accessors","text":"<p>Writing the same accessor pattern for every field is tedious. You can generate them:</p> <pre><code>package Animal;\n\nsub new {\n    my ($class, %args) = @_;\n    return bless \\%args, $class;\n}\n\n# Generate accessors for each field\nfor my $field (qw(name species weight)) {\n    no strict 'refs';\n    *{\"Animal::$field\"} = sub {\n        my $self = shift;\n        $self-&gt;{$field} = shift if @_;\n        return $self-&gt;{$field};\n    };\n}\n</code></pre> <p>This loop installs a subroutine into the package's symbol table for each field name. The closure captures <code>$field</code>, so each generated method accesses the correct hash key.</p> <p>no strict 'refs'</p> <p>The <code>no strict 'refs'</code> directive is required when manipulating the symbol table with string references. Limit its scope to the smallest block possible. In production, prefer Moose or Moo (covered below) which handle accessor generation safely.</p> <p>bless and Method Dispatch (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#inheritance","title":"Inheritance","text":"<p>Inheritance in Perl is controlled by the <code>@ISA</code> array. When you call a method on an object, Perl first looks in the object's class. If the method is not found there, Perl searches through the classes listed in <code>@ISA</code>, in order.</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#the-isa-array","title":"The @ISA Array","text":"<pre><code>package Animal;\n\nsub new {\n    my ($class, %args) = @_;\n    return bless \\%args, $class;\n}\n\nsub speak {\n    my ($self) = @_;\n    print $self-&gt;{name} . \" makes a sound\\n\";\n}\n\nsub describe {\n    my ($self) = @_;\n    printf \"%s is a %s\\n\", $self-&gt;{name}, $self-&gt;{species};\n}\n\npackage Dog;\nour @ISA = ('Animal');\n\nsub speak {\n    my ($self) = @_;\n    print $self-&gt;{name} . \" says: Woof!\\n\";\n}\n\npackage Cat;\nour @ISA = ('Animal');\n\nsub speak {\n    my ($self) = @_;\n    print $self-&gt;{name} . \" says: Meow!\\n\";\n}\n</code></pre> <p><code>Dog</code> and <code>Cat</code> inherit <code>new</code> and <code>describe</code> from <code>Animal</code>, but override <code>speak</code> with their own implementations.</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#use-parent-preferred","title":"use parent (Preferred)","text":"<p>Manually setting <code>@ISA</code> works, but the <code>use parent</code> pragma is the modern way:</p> <pre><code>package Dog;\nuse parent 'Animal';\n\nsub speak {\n    my ($self) = @_;\n    print $self-&gt;{name} . \" says: Woof!\\n\";\n}\n</code></pre> <p><code>use parent</code> sets <code>@ISA</code> at compile time and loads the parent module if it has not been loaded yet. An older pragma, <code>use base</code>, does the same thing but has quirks around failed module loading. Prefer <code>use parent</code>.</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#class-hierarchy-diagram","title":"Class Hierarchy Diagram","text":"<pre><code>classDiagram\n    Animal &lt;|-- Dog\n    Animal &lt;|-- Cat\n    Animal : +name\n    Animal : +species\n    Animal : +new()\n    Animal : +speak()\n    Animal : +describe()\n    Dog : +speak()\n    Dog : +fetch()\n    Cat : +speak()\n    Cat : +purr()</code></pre> <p>Building a Class Hierarchy - Animal, Dog, and Cat (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#super-and-method-resolution","title":"SUPER:: and Method Resolution","text":"<p>When a subclass overrides a parent method but still needs the parent's behavior, use <code>SUPER::</code>:</p> <pre><code>package Dog;\nuse parent 'Animal';\n\nsub speak {\n    my ($self) = @_;\n    $self-&gt;SUPER::speak();       # call Animal::speak\n    print \"(tail wagging)\\n\";    # add Dog-specific behavior\n}\n</code></pre> <p><code>SUPER::speak()</code> calls the <code>speak</code> method from the current package's parent class. This is resolved at compile time based on the package where <code>SUPER::</code> appears, not the class of the object.</p> <p>SUPER:: is package-relative</p> <p><code>SUPER::</code> looks up the parent based on the package where the call is written, not the runtime class of <code>$self</code>. In deep inheritance chains, this distinction matters. For more flexible dispatch, see the <code>next::method</code> approach from the <code>mro</code> module.</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#method-resolution-order-mro","title":"Method Resolution Order (MRO)","text":"<p>When a class has multiple parents (multiple inheritance), Perl needs a strategy to decide which parent to search first. The default is depth-first left-to-right (DFS), but Perl 5.10 introduced the C3 linearization algorithm:</p> <pre><code>use mro 'c3';\n\npackage Dog;\nuse parent 'Animal', 'Pet';\n</code></pre> <p>The <code>mro</code> module controls method resolution order. C3 linearization prevents the \"diamond problem\" where a class could inherit the same method through two different paths, with ambiguous results.</p> <pre><code>sequenceDiagram\n    participant main\n    participant Dog\n    participant Animal\n\n    main-&gt;&gt;Dog: $dog-&gt;speak()\n    Note over Dog: Look in Dog package\n    Dog-&gt;&gt;Dog: Found Dog::speak\n    Dog-&gt;&gt;Animal: SUPER::speak()\n    Note over Animal: Look in Animal package\n    Animal-&gt;&gt;Animal: Found Animal::speak\n    Animal--&gt;&gt;Dog: Returns\n    Dog--&gt;&gt;main: Returns</code></pre>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#isa-and-can","title":"isa and can","text":"<p>Two introspection methods tell you about an object's capabilities:</p> <p><code>isa</code> checks whether an object belongs to a class (or inherits from it):</p> <pre><code>my $dog = Dog-&gt;new(name =&gt; 'Rex');\nprint $dog-&gt;isa('Dog');       # 1 (true)\nprint $dog-&gt;isa('Animal');    # 1 (true - Dog inherits from Animal)\nprint $dog-&gt;isa('Cat');       # \"\" (false)\n</code></pre> <p><code>can</code> checks whether an object has a particular method available:</p> <pre><code>print $dog-&gt;can('speak');     # returns a code reference (true)\nprint $dog-&gt;can('fly');       # undef (false)\n\n# can() returns the code ref, so you can call it\nif (my $method = $dog-&gt;can('speak')) {\n    $dog-&gt;$method();\n}\n</code></pre> <p>Both <code>isa</code> and <code>can</code> are provided by the UNIVERSAL class, which is the implicit base class of every Perl object.</p> <p>Moose vs Manual OOP (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#destroy-and-destructors","title":"DESTROY and Destructors","text":"<p>When an object goes out of scope or its last reference is removed, Perl calls the <code>DESTROY</code> method on it (if one exists). This is the destructor:</p> <pre><code>package TempFile;\n\nsub new {\n    my ($class, %args) = @_;\n    my $self = bless {\n        path =&gt; $args{path},\n    }, $class;\n    open $self-&gt;{fh}, '&gt;', $self-&gt;{path}\n        or die \"Cannot create $self-&gt;{path}: $!\\n\";\n    return $self;\n}\n\nsub DESTROY {\n    my ($self) = @_;\n    close $self-&gt;{fh} if $self-&gt;{fh};\n    unlink $self-&gt;{path} if -e $self-&gt;{path};\n    print \"Cleaned up $self-&gt;{path}\\n\";\n}\n</code></pre> <pre><code>{\n    my $tmp = TempFile-&gt;new(path =&gt; '/tmp/work.dat');\n    # ... use $tmp ...\n}\n# $tmp goes out of scope here - DESTROY is called automatically\n</code></pre>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#destroy-rules","title":"DESTROY Rules","text":"<ul> <li>Perl calls <code>DESTROY</code> automatically - never call it manually</li> <li><code>DESTROY</code> is called on each object exactly once</li> <li>The order of destruction for global objects at program exit is undefined</li> <li><code>die</code> inside <code>DESTROY</code> is silently caught (it sets <code>$@</code> but does not propagate)</li> <li>If a subclass needs cleanup, call <code>$self-&gt;SUPER::DESTROY()</code> in its destructor</li> </ul> <p>Circular references prevent DESTROY</p> <p>If object A references object B and object B references object A, neither will be destroyed until the program exits. Use <code>Scalar::Util::weaken</code> to break cycles, or use Moose's <code>weak_ref</code> attribute option.</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#moose-modern-oop","title":"Moose: Modern OOP","text":"<p>Moose is a complete metaobject protocol for Perl. It provides declarative syntax for attributes, type constraints, inheritance, roles, and method modifiers. Moose eliminates the boilerplate of manual OOP while adding features that bless-based code cannot easily replicate.</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#your-first-moose-class","title":"Your First Moose Class","text":"<pre><code>package Person;\nuse Moose;\n\nhas 'name' =&gt; (\n    is       =&gt; 'ro',\n    isa      =&gt; 'Str',\n    required =&gt; 1,\n);\n\nhas 'age' =&gt; (\n    is      =&gt; 'rw',\n    isa     =&gt; 'Int',\n    default =&gt; 0,\n);\n\nhas 'email' =&gt; (\n    is        =&gt; 'rw',\n    isa       =&gt; 'Str',\n    predicate =&gt; 'has_email',\n);\n\nno Moose;\n__PACKAGE__-&gt;meta-&gt;make_immutable;\n</code></pre> <p>That is equivalent to roughly 40 lines of manual OOP code: a constructor with validation, three accessor methods, a predicate method, and immutability optimization.</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#the-has-declaration","title":"The has Declaration","text":"<p><code>has</code> declares an attribute with options:</p> Option Purpose Example <code>is</code> Access mode <code>'ro'</code> (read-only), <code>'rw'</code> (read-write) <code>isa</code> Type constraint <code>'Str'</code>, <code>'Int'</code>, <code>'ArrayRef[Str]'</code> <code>required</code> Must be provided to constructor <code>1</code> <code>default</code> Default value (scalar or code ref) <code>0</code>, <code>sub { [] }</code> <code>builder</code> Method name that returns default <code>'_build_name'</code> <code>lazy</code> Build default on first access <code>1</code> <code>predicate</code> Generate has_attribute method <code>'has_email'</code> <code>clearer</code> Generate clear_attribute method <code>'clear_email'</code> <code>trigger</code> Callback on attribute write <code>sub { ... }</code> <code>weak_ref</code> Weaken the reference (break cycles) <code>1</code>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#type-constraints","title":"Type Constraints","text":"<p>Moose provides a hierarchy of type constraints:</p> <pre><code>has 'count'    =&gt; (is =&gt; 'rw', isa =&gt; 'Int');\nhas 'name'     =&gt; (is =&gt; 'ro', isa =&gt; 'Str');\nhas 'scores'   =&gt; (is =&gt; 'ro', isa =&gt; 'ArrayRef[Int]');\nhas 'metadata' =&gt; (is =&gt; 'rw', isa =&gt; 'HashRef[Str]');\nhas 'parent'   =&gt; (is =&gt; 'ro', isa =&gt; 'Maybe[Person]');\n</code></pre> <p>Common types: <code>Any</code>, <code>Bool</code>, <code>Int</code>, <code>Num</code>, <code>Str</code>, <code>ArrayRef</code>, <code>HashRef</code>, <code>CodeRef</code>, <code>RegexpRef</code>, <code>Object</code>, <code>ClassName</code>. Parameterized types like <code>ArrayRef[Int]</code> check every element. <code>Maybe[Type]</code> allows <code>undef</code> as well as the specified type.</p> <pre><code>my $p = Person-&gt;new(name =&gt; 'Alice', age =&gt; 'thirty');\n# Dies: Attribute (age) does not pass the type constraint\n# because: Validation failed for 'Int' with value \"thirty\"\n</code></pre>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#defaults-and-builders","title":"Defaults and Builders","text":"<p>For simple defaults, use <code>default</code>. For complex defaults, use <code>builder</code>:</p> <pre><code>has 'created_at' =&gt; (\n    is      =&gt; 'ro',\n    isa     =&gt; 'Int',\n    default =&gt; sub { time() },\n);\n\nhas 'config' =&gt; (\n    is      =&gt; 'ro',\n    isa     =&gt; 'HashRef',\n    lazy    =&gt; 1,\n    builder =&gt; '_build_config',\n);\n\nsub _build_config {\n    my ($self) = @_;\n    return { timeout =&gt; 30, retries =&gt; 3 };\n}\n</code></pre> <p>Mutable defaults</p> <p>Always use a code reference for mutable defaults: <code>default =&gt; sub { [] }</code>. Writing <code>default =&gt; []</code> would share the same array reference across all instances - modifying one object's array would modify them all.</p> <p>Moose Attribute Declarations (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#moose-inheritance","title":"Moose Inheritance","text":"<p>Moose uses <code>extends</code> instead of <code>use parent</code>:</p> <pre><code>package Animal;\nuse Moose;\n\nhas 'name' =&gt; (is =&gt; 'ro', isa =&gt; 'Str', required =&gt; 1);\nhas 'sound' =&gt; (is =&gt; 'ro', isa =&gt; 'Str', default =&gt; '...');\n\nsub speak {\n    my ($self) = @_;\n    printf \"%s says: %s\\n\", $self-&gt;name, $self-&gt;sound;\n}\n\nno Moose;\n__PACKAGE__-&gt;meta-&gt;make_immutable;\n\npackage Dog;\nuse Moose;\nextends 'Animal';\n\nhas '+sound' =&gt; (default =&gt; 'Woof!');\n\nsub fetch {\n    my ($self, $item) = @_;\n    print $self-&gt;name . \" fetches the $item\\n\";\n}\n\nno Moose;\n__PACKAGE__-&gt;meta-&gt;make_immutable;\n</code></pre> <p>The <code>has '+sound'</code> syntax modifies an inherited attribute - here overriding the default value. This is cleaner than redefining the entire attribute.</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#method-modifiers","title":"Method Modifiers","text":"<p>Moose provides method modifiers - <code>before</code>, <code>after</code>, and <code>around</code> - that wrap existing methods without overriding them:</p> <pre><code>package Dog;\nuse Moose;\nextends 'Animal';\n\nbefore 'speak' =&gt; sub {\n    my ($self) = @_;\n    print \"(tail wagging) \";\n};\n\nafter 'speak' =&gt; sub {\n    my ($self) = @_;\n    print \"(sits down)\\n\";\n};\n</code></pre> <pre><code>my $dog = Dog-&gt;new(name =&gt; 'Rex', sound =&gt; 'Woof!');\n$dog-&gt;speak;\n# Output: (tail wagging) Rex says: Woof!\n# (sits down)\n</code></pre> <p><code>around</code> gives you full control - you receive the original method as a code reference:</p> <pre><code>around 'speak' =&gt; sub {\n    my ($orig, $self, @args) = @_;\n    print \"[DEBUG] speak called\\n\";\n    $self-&gt;$orig(@args);\n    print \"[DEBUG] speak finished\\n\";\n};\n</code></pre>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#roles","title":"Roles","text":"<p>Roles are Perl's answer to mixins and interfaces. A role is a collection of methods and attributes that can be composed into any class. Unlike inheritance, roles do not create an \"is-a\" relationship - they add capabilities.</p> <pre><code>package Printable;\nuse Moose::Role;\n\nrequires 'to_string';\n\nsub print_self {\n    my ($self) = @_;\n    print $self-&gt;to_string . \"\\n\";\n}\n\npackage Serializable;\nuse Moose::Role;\n\nrequires 'to_hash';\n\nsub to_json {\n    my ($self) = @_;\n    require JSON::PP;\n    return JSON::PP::encode_json($self-&gt;to_hash);\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#consuming-roles-with-with","title":"Consuming Roles with <code>with</code>","text":"<pre><code>package Person;\nuse Moose;\nwith 'Printable', 'Serializable';\n\nhas 'name' =&gt; (is =&gt; 'ro', isa =&gt; 'Str', required =&gt; 1);\nhas 'age'  =&gt; (is =&gt; 'ro', isa =&gt; 'Int', required =&gt; 1);\n\nsub to_string {\n    my ($self) = @_;\n    return sprintf \"%s (age %d)\", $self-&gt;name, $self-&gt;age;\n}\n\nsub to_hash {\n    my ($self) = @_;\n    return { name =&gt; $self-&gt;name, age =&gt; $self-&gt;age };\n}\n\nno Moose;\n__PACKAGE__-&gt;meta-&gt;make_immutable;\n</code></pre> <p>The <code>with</code> keyword composes one or more roles into the class. If a role has a <code>requires</code> declaration, the consuming class must provide that method - Moose enforces this at compile time.</p> <pre><code>my $p = Person-&gt;new(name =&gt; 'Alice', age =&gt; 30);\n$p-&gt;print_self;                 # Alice (age 30)\nprint $p-&gt;to_json, \"\\n\";       # {\"name\":\"Alice\",\"age\":30}\nprint $p-&gt;does('Printable');    # 1\n</code></pre> <p>Roles vs Inheritance</p> <p>Use inheritance when objects share a genuine \"is-a\" relationship (a Dog is an Animal). Use roles when objects share behavior without a taxonomic relationship (a Person and a Document can both be Printable, but neither is a subclass of the other).</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#moo-lightweight-moose","title":"Moo: Lightweight Moose","text":"<p>Moo provides almost the same interface as Moose but without the metaobject protocol. It starts faster, uses less memory, and is compatible with Moose (you can upgrade to Moose later without rewriting your classes).</p> <pre><code>package Server;\nuse Moo;\n\nhas 'host' =&gt; (\n    is       =&gt; 'ro',\n    required =&gt; 1,\n);\n\nhas 'port' =&gt; (\n    is      =&gt; 'ro',\n    default =&gt; sub { 8080 },\n);\n\nhas 'connections' =&gt; (\n    is      =&gt; 'rw',\n    default =&gt; sub { 0 },\n);\n\nsub start {\n    my ($self) = @_;\n    printf \"Server listening on %s:%d\\n\", $self-&gt;host, $self-&gt;port;\n}\n\n1;\n</code></pre>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#moo-vs-moose-key-differences","title":"Moo vs Moose: Key Differences","text":"Feature Moose Moo Startup time ~500ms ~50ms <code>isa</code> type constraints Built-in type system Accepts code refs or Type::Tiny Meta-object protocol Full introspection None (until Moose is loaded) Method modifiers <code>before</code>, <code>after</code>, <code>around</code> Same Roles <code>Moose::Role</code> <code>Moo::Role</code> (compatible) <code>make_immutable</code> Required for performance Automatic Moose compatibility N/A Inflates to Moose on demand"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#type-checking-in-moo","title":"Type Checking in Moo","text":"<p>Moo uses code references or Type::Tiny for type checking:</p> <pre><code>use Types::Standard qw(Str Int ArrayRef);\n\nhas 'name' =&gt; (\n    is  =&gt; 'ro',\n    isa =&gt; Str,\n);\n\nhas 'ports' =&gt; (\n    is  =&gt; 'ro',\n    isa =&gt; ArrayRef[Int],\n);\n</code></pre> <p>Type::Tiny constraints work identically in Moo and Moose, making migration painless.</p> <p>When to choose Moo vs Moose</p> <p>Start with Moo. It handles 90% of OOP needs with a fraction of the overhead. Switch to Moose only if you need its metaobject protocol (runtime introspection, complex type coercions, or <code>MooseX::</code> extensions). The API is compatible enough that switching is straightforward.</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#when-to-use-oop-vs-procedural","title":"When to Use OOP vs Procedural","text":"<p>Not every Perl program needs objects. Choosing between OOP and procedural style depends on what you are building.</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#use-oop-when","title":"Use OOP When","text":"<ul> <li>You have entities with state - objects that hold data and have behavior tied to that data</li> <li>Multiple instances of the same type need independent state (database connections, user sessions, HTTP requests)</li> <li>Your code has a natural hierarchy (Animals, Vehicles, UI widgets)</li> <li>You need polymorphism - different objects responding to the same method in different ways</li> <li>The codebase is large and needs clear boundaries between components</li> </ul>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#use-procedural-when","title":"Use Procedural When","text":"<ul> <li>You are writing a utility script or one-liner</li> <li>The program is a pipeline - read input, transform, write output</li> <li>Functions operate on passed-in data without maintaining state</li> <li>The module is a collection of functions (like <code>File::Path</code> or <code>List::Util</code>)</li> <li>Simplicity matters more than extensibility</li> </ul>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#hybrid-approach","title":"Hybrid Approach","text":"<p>Most real Perl programs mix both styles:</p> <pre><code># Procedural: utility function\nsub normalize_phone {\n    my ($number) = @_;\n    $number =~ s/[^\\d]//g;\n    return $number;\n}\n\n# OOP: stateful object\npackage Contact;\nuse Moo;\n\nhas 'name'  =&gt; (is =&gt; 'ro', required =&gt; 1);\nhas 'phone' =&gt; (is =&gt; 'rw');\n\nsub set_phone {\n    my ($self, $raw) = @_;\n    $self-&gt;phone(normalize_phone($raw));\n}\n</code></pre> <p>Functions that do not need state live outside classes. Classes manage the entities that need state and behavior.</p> <p>OOP vs Procedural (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#exercises","title":"Exercises","text":""},{"location":"Dev%20Zero/Perl/object-oriented-perl/#beginner-bank-account-class","title":"Beginner: Bank Account Class","text":"<p>Bank Account Class (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#intermediate-task-queue-with-roles-moose","title":"Intermediate: Task Queue with Roles (Moose)","text":"<p>Task Queue with Roles (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#putting-it-all-together","title":"Putting It All Together","text":"<p>Perl's OOP model is layered. At the bottom: <code>bless</code>, <code>@ISA</code>, and subroutines give you complete control over how objects are built and dispatched. At the top: Moose and Moo provide declarative syntax that handles the boilerplate. Here is what you covered:</p> <ul> <li><code>bless</code> associates a reference with a package, turning data into an object</li> <li>Methods are subroutines that receive the invocant (<code>$self</code> or <code>$class</code>) as their first argument</li> <li>Accessors provide controlled access to object data instead of exposing hash internals</li> <li>Inheritance via <code>@ISA</code> and <code>use parent</code> lets classes share and override behavior</li> <li><code>SUPER::</code> delegates to parent methods while adding subclass-specific logic</li> <li>Method resolution order determines which parent wins in multiple inheritance (use C3)</li> <li><code>isa</code> and <code>can</code> introspect objects at runtime</li> <li><code>DESTROY</code> handles cleanup when objects are garbage collected</li> <li>Moose provides <code>has</code>, <code>extends</code>, <code>with</code>, type constraints, and method modifiers</li> <li>Moo delivers the same API with less overhead - start here for most projects</li> <li>Roles compose behavior across unrelated classes without inheritance</li> <li>OOP vs procedural is a design choice - use objects for stateful entities, functions for transformations</li> </ul> <p>The manual approach teaches you what Perl objects really are. Moose and Moo let you stop thinking about the mechanics and focus on your domain.</p>"},{"location":"Dev%20Zero/Perl/object-oriented-perl/#further-reading","title":"Further Reading","text":"<ul> <li>perlobj - Perl's official object-oriented programming reference</li> <li>perlootut - official OOP tutorial for Perl</li> <li>Moose::Manual - comprehensive Moose documentation</li> <li>Moo documentation - lightweight Moose alternative</li> <li>Moose::Manual::Roles - role composition in Moose</li> <li>Type::Tiny - type constraints for Moo and Moose</li> <li>Intermediate Perl - Chapters 8-15 cover references, objects, and testing</li> <li>Moose is Perl - community resource for Moose patterns and best practices</li> </ul> <p>Previous: Modules and CPAN | Next: Error Handling and Debugging | Back to Index</p>"},{"location":"Dev%20Zero/Perl/perl_dev0_introduction/","title":"Perl Developer Introduction","text":""},{"location":"Dev%20Zero/Perl/perl_dev0_introduction/#why-perl-and-why-unix-first","title":"Why Perl, and Why Unix First","text":"<p>Version: 1.6 Year: 2026</p>"},{"location":"Dev%20Zero/Perl/perl_dev0_introduction/#copyright-notice","title":"Copyright Notice","text":"<p>Copyright (c) 2025-2026 Ryan Thomas Robson / Robworks Software LLC. Licensed under CC BY-NC-ND 4.0. You may share this material for non-commercial purposes with attribution, but you may not distribute modified versions.</p>"},{"location":"Dev%20Zero/Perl/perl_dev0_introduction/#the-perl-story","title":"The Perl Story","text":"<p>Perl was created by Larry Wall in 1987. Wall was a linguist and Unix system administrator at Unisys who needed a tool that could handle report generation from scattered files across a network. The existing tools - <code>sed</code>, <code>awk</code>, and shell scripts - couldn't handle the job cleanly, so he built something new.</p>      Larry Wall, creator of Perl, at YAPC::NA 2007.     Photo: Randal Schwartz, CC BY-SA 2.0 <p>Wall's background in linguistics shaped Perl's design philosophy. He borrowed the idea that there should be more than one way to do it (TMTOWTDI, pronounced \"Tim Toady\") - just as natural languages offer multiple ways to express the same thought. Perl's syntax draws from C, <code>sed</code>, <code>awk</code>, and shell scripting, making it feel familiar to Unix practitioners from day one.</p>"},{"location":"Dev%20Zero/Perl/perl_dev0_introduction/#version-history","title":"Version History","text":"<p>Perl 1 (1987) was a text-processing language that replaced Wall's earlier <code>awk</code> scripts. Perl 2 (1988) added a better regex engine. Perl 3 (1989) added binary data support. Perl 4 (1991) became the \"Camel Book\" edition - the version most 1990s sysadmins learned.</p> <p>Perl 5 (1994) was a complete rewrite that introduced everything modern Perl developers rely on: lexical scoping with <code>my</code>, references for complex data structures, modules, objects (via <code>bless</code>), and the CPAN ecosystem. Perl 5 remains the production version of Perl, actively maintained and released regularly.</p> <p>Perl 6 was announced in 2000 as an ambitious redesign. After nearly two decades of development, it was renamed to Raku in 2019 to clarify that it is a separate language, not a replacement for Perl 5. The two languages share philosophy but have distinct syntax and runtimes.</p> <p>Tip</p> <p>When people say \"Perl\" today, they mean Perl 5. Raku is its own language with its own community. This course teaches Perl 5.</p>"},{"location":"Dev%20Zero/Perl/perl_dev0_introduction/#the-unix-foundation","title":"The Unix Foundation","text":"<p>Before writing a single line of Perl, you need to understand the operating system it was born on. Perl was created by Larry Wall in 1987 as a practical tool for Unix system administration - a \"Swiss Army chainsaw\" for processing text, managing files, and gluing programs together. Its design reflects Unix philosophy at every level.</p>"},{"location":"Dev%20Zero/Perl/perl_dev0_introduction/#processes-and-file-descriptors","title":"Processes and File Descriptors","text":"<p>Every running program on a Unix system is a process. The kernel assigns each process a unique PID (process ID) and manages its access to system resources. One of the most fundamental resources is the file descriptor - a per-process integer handle that refers to an open file, socket, pipe, or device.</p> <p>When a process opens a file, the kernel assigns the lowest available integer as its file descriptor. Each process maintains its own file descriptor table, so descriptor 5 in one process is completely independent from descriptor 5 in another.</p> <p>Three file descriptors are opened automatically when any process starts:</p> FD Name Purpose Default 0 stdin Standard input Terminal keyboard 1 stdout Standard output Terminal screen 2 stderr Standard error Terminal screen <p>This separation is deliberate. By keeping stdout and stderr as distinct streams, Unix lets you redirect them independently:</p> <pre><code># stdout goes to file, stderr still prints to terminal\n./my_script.pl &gt; output.txt\n\n# stderr goes to file, stdout still prints to terminal\n./my_script.pl 2&gt; errors.log\n\n# both go to separate files\n./my_script.pl &gt; output.txt 2&gt; errors.log\n</code></pre> <p>You can inspect a running process's open file descriptors through the <code>/proc</code> filesystem on Linux:</p> <pre><code>ls -la /proc/PID/fd\n# 0 -&gt; /dev/pts/0  (stdin - the terminal)\n# 1 -&gt; /dev/pts/0  (stdout - the terminal)\n# 2 -&gt; /dev/pts/0  (stderr - the terminal)\n# 3 -&gt; /path/to/some/open/file\n</code></pre> <p>This matters for Perl because Perl gives you direct control over file descriptors through <code>open</code>, <code>close</code>, <code>dup2</code>, and the <code>&lt;&gt;</code> operator. Understanding what the kernel does underneath makes Perl's I/O model intuitive rather than magical.</p> <p>In Unix, what is the relationship between a process and a file descriptor? (requires JavaScript)</p> <p>What are the standard file descriptors 0, 1, and 2? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/perl_dev0_introduction/#signals-and-process-control","title":"Signals and Process Control","text":"<p>Processes communicate with each other through signals - asynchronous notifications sent by the kernel or other processes. The most common:</p> Signal Number Default Action Typical Use <code>SIGTERM</code> 15 Terminate Polite shutdown request <code>SIGKILL</code> 9 Terminate (cannot be caught) Force kill <code>SIGINT</code> 2 Terminate Ctrl+C from terminal <code>SIGHUP</code> 1 Terminate Terminal closed / config reload <code>SIGSTOP</code> 19 Stop (cannot be caught) Freeze process <code>SIGCONT</code> 18 Continue Resume stopped process <p>Perl exposes signal handling through the <code>%SIG</code> hash, letting you write custom handlers:</p> <pre><code>$SIG{INT} = sub { print \"Caught Ctrl+C, cleaning up...\\n\"; exit 0; };\n$SIG{TERM} = sub { cleanup(); exit 0; };\n</code></pre> <p>Understanding signals is critical for writing Perl daemons, long-running scripts, and anything that manages child processes.</p> <p>Explore Unix Processes and Signals (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/perl_dev0_introduction/#why-perl-for-system-administration","title":"Why Perl for System Administration","text":"<p>Perl became the dominant sysadmin language in the 1990s and early 2000s for concrete reasons that still hold:</p> <ul> <li>First-class regular expressions: Pattern matching is built into the language syntax (<code>=~</code>, <code>s///</code>, <code>m//</code>), not bolted on as a library. Perl's regex engine remains one of the most powerful available.</li> <li>Text processing by default: The <code>&lt;&gt;</code> diamond operator, <code>chomp</code>, <code>split</code>, <code>join</code>, and autosplit mode (<code>-a</code>) make processing structured text (logs, configs, CSVs) trivial.</li> <li>Unix system calls built in: <code>fork</code>, <code>exec</code>, <code>pipe</code>, <code>open</code>, <code>kill</code>, <code>wait</code>, <code>stat</code>, <code>chmod</code> - Perl wraps nearly every POSIX system call, giving you C-level control with scripting-level convenience.</li> <li>Glue language: Perl sits naturally between shell scripts (too limited for complex logic) and C (too verbose for quick tasks). Backticks, <code>system()</code>, and <code>open(my $fh, \"-|\", ...)</code> make calling external programs and capturing their output straightforward.</li> <li>CPAN: The Comprehensive Perl Archive Network has over 200,000 modules. Need to parse JSON, connect to a database, send email, or manage an LDAP directory? Someone already wrote and tested it.</li> </ul> <pre><code># Classic Perl one-liner: find lines matching a pattern in a log file\nperl -ne 'print if /ERROR.*timeout/' /var/log/syslog\n\n# In-place edit: replace all occurrences across files\nperl -pi -e 's/old_server/new_server/g' /etc/myapp/*.conf\n\n# Parse Apache logs: count requests per IP\nperl -lane '$count{$F[0]}++; END { print \"$count{$_} $_\" for sort keys %count }' access.log\n</code></pre> <p>These one-liners show Perl's sweet spot: tasks too complex for <code>sed</code> and <code>awk</code>, but not worth writing a full program for. The <code>-n</code>, <code>-p</code>, <code>-l</code>, <code>-a</code>, and <code>-e</code> flags turn Perl into a turbocharged command-line text processor.</p> <p>What makes Perl particularly well-suited for system administration tasks? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/perl_dev0_introduction/#perl-in-the-unix-tool-ecosystem","title":"Perl in the Unix Tool Ecosystem","text":"<p>Perl occupies a specific niche between shell scripts and compiled languages. Understanding where it fits helps you choose the right tool for each job.</p> <pre><code>flowchart LR\n    subgraph Quick Tasks\n        A[Shell / Bash]\n    end\n    subgraph Text Processing\n        B[sed / awk]\n    end\n    subgraph Scripting Power\n        C[Perl]\n    end\n    subgraph Application Development\n        D[Python / C]\n    end\n\n    A --&gt;|\"Too complex\\nfor one-liners\"| B\n    B --&gt;|\"Need variables,\\nlogic, modules\"| C\n    C --&gt;|\"Need frameworks,\\nlarge teams\"| D</code></pre> <p>Shell scripts are perfect for gluing commands together. <code>sed</code> and <code>awk</code> handle line-by-line text transformation. Perl bridges the gap when you need real data structures, error handling, regular expressions that span multiple lines, or access to thousands of CPAN modules - but don't need the overhead of a compiled language or a full application framework.</p> <p>In practice, Perl one-liners replace <code>sed</code> and <code>awk</code> commands that have grown too complex, and Perl scripts replace shell scripts that have grown too long. This positioning is why Perl was called \"the duct tape of the Internet\" during the 1990s web boom.</p>"},{"location":"Dev%20Zero/Perl/perl_dev0_introduction/#your-first-perl-commands","title":"Your First Perl Commands","text":"<p>Before writing full scripts, try Perl directly from the command line. The <code>-e</code> flag lets you run Perl code without creating a file.</p> <p>First Perl Commands (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/perl_dev0_introduction/#anatomy-of-a-perl-script","title":"Anatomy of a Perl Script","text":"<p>Every well-written Perl script follows a consistent structure. Here is the skeleton you should start every script with.</p> <p>Anatomy of a Perl Script (requires JavaScript)</p> <p>Danger</p> <p>Never skip <code>use strict</code> and <code>use warnings</code>. These two lines catch more bugs than any other practice in Perl. Experienced developers consider code without them to be broken by default.</p>"},{"location":"Dev%20Zero/Perl/perl_dev0_introduction/#next-steps","title":"Next Steps","text":"<p>This introduction covers the Unix concepts that make Perl's design logical and the history behind the language. You're ready to start writing real Perl code - continue to Scalars, Strings, and Numbers for your first deep look at Perl's data types.</p>"},{"location":"Dev%20Zero/Perl/perl_dev0_introduction/#further-reading","title":"Further Reading","text":"<ul> <li>Perl Official Documentation - comprehensive Perl language reference</li> <li>CPAN - Comprehensive Perl Archive Network</li> <li>Perl.org - official Perl community site</li> <li>The Unix Programming Environment - Kernighan and Pike's classic on Unix philosophy</li> <li>Learning Perl - the \"Llama Book,\" the standard introduction to Perl</li> <li>The Perl Programming Language - Wikipedia overview of Perl's history and design</li> <li>Raku - the language formerly known as Perl 6</li> </ul> <p>Next: Scalars, Strings, and Numbers | Back to Index</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/","title":"Perl Developer Career Roadmap","text":""},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#the-road-to-perl-developer-mastery","title":"The Road to Perl Developer Mastery","text":"<p>Version: 1.6 Year: 2026</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#copyright-notice","title":"Copyright Notice","text":"<p>Copyright (c) 2025-2026 Ryan Thomas Robson / Robworks Software LLC. Licensed under CC BY-NC-ND 4.0. You may share this material for non-commercial purposes with attribution, but you may not distribute modified versions.</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction &amp; Expectations</li> <li>Phase 1: The Basics</li> <li>Phase 2: Beginning Perl</li> <li>Phase 3: Workflows, Testing &amp; Debugging</li> <li>Bibliography</li> </ol>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#introduction-expectations","title":"Introduction &amp; Expectations","text":""},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#in-it-for-the-long-haul","title":"In it for the Long Haul","text":"<p>Feynman's \"computer disease\" metaphor illustrates the irresistible drive some people have to solve problems. It's a certain itch\u2014one that refuses to be ignored\u2014and if you've ever felt compelled to keep tinkering until something finally clicks, you may already be halfway down this road. This document exists for people like you.</p> <p>The journey to mastering Perl isn't just about syntax and tools\u2014it's about cultivating a mindset of curiosity, experimentation, and resilience. Whether you're brand new to programming or transitioning from another language, this roadmap is designed to guide you not just through the how, but the why behind the skills, practices, and philosophies that define great Perl developers.</p> <p>If that resonates, you might be the right kind of person to become a Perl developer.</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#what-to-expect","title":"What to Expect","text":"<ul> <li>Work: Dedication and completion of assignments, exercises, and experiments.</li> <li>Read: Books provide structured learning not found in blogs or forums.</li> <li>Communicate: Engage with community projects and discussions.</li> </ul>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#what-not-to-expect","title":"What Not to Expect","text":"<ul> <li>A Guarantee: Success is up to your effort and the industry.</li> <li>A Complete Solution: This is a guide, not a one-stop-shop.</li> <li>A Certification: Prove your skills through demonstration, not a certificate.</li> </ul>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#phase-1-the-basics","title":"Phase 1: The Basics","text":""},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#required-learning","title":"Required Learning","text":""},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#operating-systems","title":"Operating Systems","text":"<p>Why Unix Matters for Perl</p> <p>A strong understanding of Unix concepts is essential for mastering Perl. Here's why:</p> <ul> <li>Shared Philosophy: Perl was designed with Unix in mind\u2014embracing small, composable utilities; pipelines; and powerful text processing. These principles align closely with the way Unix works.</li> <li>Direct Mappings: Key system-level features in Unix\u2014such as file descriptors, piping, and process control\u2014are directly accessible and controllable via Perl.</li> <li>Practical Impact: Knowing how Unix handles processes, permissions, input/output streams, and the file system empowers you to write code that is not only functional, but idiomatic, efficient, and maintainable.</li> </ul> <p>Mastering these concepts isn't just helpful\u2014it's fundamental to becoming an effective Perl developer in any Unix-like environment.</p> <p>Why does this roadmap start with Unix fundamentals before any Perl syntax? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#linux-concepts","title":"Linux Concepts","text":"<p>Understand the Linux Stack</p> <p>Linux is not a complete operating system by itself\u2014it's just the kernel, the core part responsible for hardware interaction, memory management, scheduling, and system calls. Everything that sits above the kernel and interacts with users is known as userland or user space.</p> <ul> <li>The Linux kernel handles low-level tasks such as managing CPU, RAM, I/O devices, and filesystems.</li> <li>Userland tools, typically provided by the GNU project, include utilities like <code>ls</code>, <code>cp</code>, <code>grep</code>, <code>bash</code>, and more. These tools allow users to interact with the system via the command line or scripts.</li> </ul> <p>Understanding the boundary and cooperation between kernel and userland tools is vital for troubleshooting, scripting, and writing system-aware applications in Perl.</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#examples-of-linux-distributions","title":"Examples of Linux Distributions","text":"<ul> <li>Ubuntu: A user-friendly, widely adopted Linux distribution. LTS (Long-Term Support) versions offer 5 years of guaranteed security and maintenance updates, making them ideal for both beginners and enterprise environments.</li> <li>Rocky Linux: A community enterprise OS designed to be a drop-in replacement for CentOS, maintaining bug-for-bug compatibility with RHEL.</li> <li>AlmaLinux: Another CentOS alternative, AlmaLinux is governed by a community foundation and backed by major cloud vendors.</li> <li>Debian: Known for its stability and commitment to free software, Debian serves as the foundation for many other distributions, including Ubuntu.</li> <li>Fedora: Sponsored by Red Hat, Fedora is a cutting-edge distribution that features the latest in open-source software and innovations.</li> <li>openSUSE: Offers two main editions\u2014Leap (stable) and Tumbleweed (rolling release). It includes YaST, a powerful configuration tool.</li> <li>Arch Linux: A minimalist, rolling-release distro aimed at advanced users who prefer full control and manual configuration. Its Arch Wiki is considered one of the most comprehensive Linux documentation sources available.</li> </ul>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#creating-a-linuxunix-learning-lab","title":"Creating a Linux/Unix Learning Lab","text":"<p>To build hands-on experience with Linux systems, set up a virtual lab environment. You can use a variety of local virtualization tools or cloud hosting services depending on your goals and available resources:</p> <p>Below are some modern tools to help manage and create these environments:</p> <ul> <li>VirtualBox: Open-source virtualization software that runs on Windows, macOS, Linux, and Solaris, popular for setting up quick dev environments.</li> <li>VMware Workstation Player: A robust and professional-grade virtualization platform for running multiple operating systems on a single machine.</li> <li>Vagrant: A powerful tool for managing virtual machine workflows, especially in development environments, built on top of tools like VirtualBox and VMware.</li> <li>Multipass: Lightweight VM manager developed by Canonical (Ubuntu creators), great for quickly launching Ubuntu instances.</li> <li>Docker Desktop: Ideal for containerized environments and lightweight application virtualization.</li> <li>Proxmox VE: A powerful open-source virtualization platform for both KVM virtual machines and LXC containers, great for lab environments.</li> <li>QEMU/KVM: A low-level virtualization tool offering excellent performance, commonly used in professional Linux environments.</li> </ul>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#command-line-proficiency","title":"Command Line Proficiency","text":"<p>Essential Online Resources for Command Line Mastery</p> <p>Here are some curated resources to help build your command-line proficiency, whether you're learning shell scripting, troubleshooting complex commands, or developing efficient workflows:</p> <ul> <li>Explainshell: Breaks down complex shell commands into human-readable components, great for learning command syntax.</li> <li>Commandlinefu: A community-driven repository of useful one-liners, tricks, and command explanations.</li> <li>Learn Shell: An interactive tutorial platform for beginners looking to master the basics of shell scripting.</li> <li>ShellCheck: A popular online shell script analyzer that highlights syntax issues and offers improvement suggestions.</li> <li>Cheat.sh: Access community-maintained command-line cheat sheets from your terminal or browser.</li> </ul>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#recommended-books","title":"Recommended Books","text":"<ul> <li>The Linux Command Line, 2nd Edition by William Shotts \u2013 A comprehensive and beginner-friendly guide to mastering the Linux terminal.</li> <li>How Linux Works, 3rd Edition by Brian Ward \u2013 Understand what's happening under the hood of a Linux system, from processes and permissions to networking and filesystems.</li> <li>Linux Pocket Guide by Daniel J. Barrett \u2013 A concise and portable reference, ideal for quick lookups and essential commands.</li> <li>Learning the Bash Shell, 3rd Edition by Cameron Newham \u2013 A solid introduction to bash scripting and command-line navigation.</li> <li>UNIX and Linux System Administration Handbook, 5th Edition by Evi Nemeth et al. \u2013 A must-have for anyone managing or working with Linux systems at scale.</li> </ul>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#finding-the-right-editor","title":"Finding the Right \\$EDITOR","text":""},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#vim","title":"Vim","text":"<p>Vim is the preferred editor for most Unix environments, celebrated for its:</p> <ul> <li>Ubiquity: Available on nearly every Unix-like system by default.</li> <li>Efficiency: Enables fast navigation and editing with minimal hand movement.</li> <li>Modal Editing: Operates in distinct modes (e.g., insert, normal, visual) for precise, powerful actions without leaving the keyboard.</li> </ul> <p>While Vim's learning curve is steep, it pays off. Once mastered, Vim becomes a tool that dramatically boosts speed, productivity, and editing comfort\u2014especially for developers who live in the terminal.</p> <p>To help with the learning process, here are some engaging and helpful resources:</p> <ul> <li>Vim Adventures \u2013 a game-based way to learn Vim</li> <li>Vimcasts \u2013 screencasts teaching practical Vim usage</li> <li>Vimbook on GitHub</li> </ul> <p>Practice regularly, and you'll soon find Vim indispensable.</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#emacs","title":"Emacs","text":"<p>Emacs is a feature-rich, highly customizable text editor with a deep history in the developer community. While it is less commonly used than Vim in Unix environments, it excels in its extensibility through Emacs Lisp, allowing users to transform it into an IDE, email client, calendar, and more. Once mastered, Emacs offers powerful workflows for programming, writing, and system administration. Notable learning resources include:</p> <ul> <li>Absolute Beginner's Guide to Emacs</li> <li>Emacs Rocks!</li> <li>Mastering Emacs</li> </ul>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#additional-learning","title":"Additional Learning","text":"<p>Process/Memory Management</p> <p>A foundational understanding of how Unix-like systems handle processes and memory is crucial for writing efficient and reliable applications. This includes learning how the system schedules processes, handles signals, manages memory allocation, and balances system resources under load. For Perl developers especially, this knowledge helps when writing scripts that interact with the OS, spawn child processes, or manage large data sets.</p> <p>Key topics to explore:</p> <ul> <li>Process lifecycle: from creation (<code>fork</code>) to execution (<code>exec</code>) and termination (<code>wait</code>, <code>kill</code>).</li> <li>Scheduling and prioritization: concepts such as nice values, real-time scheduling, and load balancing.</li> <li>Memory hierarchy: virtual memory, paging, segmentation, and memory-mapped files.</li> <li>System monitoring: tools like <code>ps</code>, <code>top</code>, <code>htop</code>, <code>vmstat</code>, and <code>/proc</code> interfaces.</li> <li>Swap, cache, and buffers: how the kernel manages memory pressure.</li> </ul> <p>This foundational layer empowers developers to write more performant code, optimize resource usage, and debug complex runtime behavior.</p> <p>File Systems and Links</p> <p>One of the core strengths of Unix-like systems lies in their powerful and flexible file system structure. Understanding how files and directories are organized, accessed, and linked is essential for anyone writing or maintaining system-level applications, particularly in Perl where working with files is a frequent task.</p> <p>Key areas to explore:</p> <ul> <li>Filesystem Hierarchy: Learn the purpose of each top-level directory in a Unix system (e.g., <code>/bin</code>, <code>/etc</code>, <code>/home</code>, <code>/var</code>).</li> <li>Permissions and Ownership: Understand how Unix enforces security through file permissions (<code>rwx</code>) and ownership by users and groups.</li> <li>Hard vs Soft Links:</li> <li>Hard links reference the same inode as the original file, making them indistinguishable from the original.</li> <li>Soft (symbolic) links are pointers to file paths, allowing more flexibility but with limitations (e.g., they break if the target is deleted).</li> <li>Mount Points and Devices: Discover how different filesystems and storage devices are mounted into a unified tree structure.</li> <li>Extended Attributes and ACLs: For advanced permission scenarios, explore access control lists and file attributes beyond the standard <code>rwx</code> model.</li> </ul> <p>Being fluent in these topics enables more effective scripting, safer automation, and a deeper understanding of how Unix-based systems operate.</p> <p>User Interaction</p> <p>Understanding how users interact with Unix systems is crucial for developing intuitive tools and scripts. Interaction typically occurs through the shell\u2014whether graphical or command-line\u2014and involves interpreting environment variables, configuring startup files (like <code>.bashrc</code> or <code>.profile</code>), and handling user input in real-time. For Perl developers, this means writing programs that gracefully accept arguments, produce meaningful output, and respect user environments.</p> <p>Key areas include:</p> <ul> <li>Customizing user shells and dotfiles</li> <li>Managing terminal I/O (STDIN/STDOUT/STDERR)</li> <li>Building interactive CLI tools with argument parsing</li> </ul> <p>Unix History</p> <p>To fully appreciate modern computing, it's important to understand where it came from. Unix, developed in the 1970s at Bell Labs, laid the foundation for nearly every major operating system in use today, including Linux and macOS. Its influence extends to software design principles, scripting languages like Perl, and the very philosophy of modular, interoperable tools.</p> <p>Recommended reading:</p> <ul> <li>The History of Unix</li> <li>A Quarter Century of Unix by Peter H. Salus</li> <li>The Unix Philosophy by Eric S. Raymond</li> </ul> <p>Build Your Phase 1 Learning Plan (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#phase-2-beginning-perl","title":"Phase 2: Beginning Perl","text":""},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#required-learning_1","title":"Required Learning","text":""},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#lamp-stack","title":"LAMP Stack","text":"<p>LAMP Stack and Core Web Services</p> <p>A strong Perl developer should have a working knowledge of the services that power modern web applications. While Perl's role has evolved\u2014often found in automation, infrastructure tooling, or legacy applications\u2014its integration with these services remains highly relevant. Mastery of web stack components ensures you're equipped to handle both greenfield and legacy projects with confidence.</p> <p>Key technologies to understand:</p> <ul> <li>Nginx: A modern, high-performance web server and reverse proxy widely adopted in today's infrastructure, favored for its scalability, load balancing, and efficiency under high traffic.</li> <li>Apache: Still commonly used in shared hosting and legacy applications. Its extensive module ecosystem and compatibility with <code>.htaccess</code> make it a valuable technology to understand.</li> <li>LiteSpeed: A commercial alternative to Apache, often used in performance-focused hosting. Supports .htaccess and Apache configuration syntax.</li> <li>PHP: Although not a Perl-centric language, understanding PHP remains important in environments where both are deployed side by side\u2014especially in shared hosting and cPanel-based systems.</li> <li>MariaDB and PostgreSQL: The modern standards in relational databases. MariaDB is a MySQL-compatible drop-in replacement, while PostgreSQL is increasingly preferred for new applications due to its compliance and features.</li> <li>SQLite: A serverless, embedded database engine ideal for development, automation tools, and standalone utilities.</li> <li>Bind: The classic authoritative DNS server, still widely used for managing zones and nameservers.</li> <li>PowerDNS: A dynamic DNS solution with API and database integration, popular in cloud-scale and provider environments.</li> <li>CoreDNS: The default DNS service in Kubernetes and cloud-native environments, vital for understanding service discovery and internal networking in containerized applications.</li> </ul> <p>Familiarity with these services will not only support your scripting and automation tasks but also help you thrive in today's hybrid infrastructure environments\u2014from legacy systems to modern DevOps ecosystems.</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#perl-skills-milestones","title":"Perl Skills &amp; Milestones","text":""},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#learning-the-ropes","title":"Learning the Ropes","text":"<p>Perl Syntax, Data Types, and Program Structure</p> <p>Perl Syntax, Data Types, and Program Structure</p> <p>Developing fluency in Perl starts with a clear understanding of its foundational elements:</p> <ul> <li>Data Types: Gain comfort with scalars (<code>$</code>), arrays (<code>@</code>), and hashes (<code>%</code>), and learn how their behavior changes based on context (scalar vs. list). Start with Scalars, Strings, and Numbers, then Arrays, Hashes, and Lists.</li> <li>Control Flow: Master core control structures like <code>if</code>, <code>unless</code>, <code>while</code>, <code>for</code>, and <code>foreach</code> to direct the logic of your programs. See Control Flow.</li> <li>Built-in Functions: Leverage Perl's rich library of built-in functions for data manipulation, text processing, and file I/O.</li> <li>Code Style: Perl's expressiveness allows multiple ways to solve problems, making it essential to develop good habits around clarity, consistency, and maintainability.</li> </ul> <p>The goal is not just to write functional code, but to write code that is efficient, idiomatic, and easy to understand\u2014hallmarks of an experienced Perl developer.</p> <p>References and Error Handling</p> <p>Perl's reference system is essential for working with complex data structures such as arrays of hashes, deeply nested configurations, or dynamically built data trees. You'll need to:</p> <ul> <li>Understand what a reference is and how to create one.</li> <li>Learn how to safely dereference scalars, arrays, and hashes.</li> <li>Use references effectively in subroutines to pass complex data structures by reference.</li> <li>Apply references when building object-oriented Perl code, where <code>$self</code> is typically a reference to a hash.</li> </ul> <p>Error Handling</p> <p>Handling errors gracefully is a hallmark of professional Perl development. While Perl allows for classic error trapping using <code>eval {}</code>, this method can lead to confusing behavior and poor readability. Instead, modern Perl favors modules like <code>Try::Tiny</code>, which offer cleaner syntax and better scoping.</p> <p>Key practices include:</p> <ul> <li>Differentiating between warnings (non-fatal) and errors (fatal).</li> <li>Using <code>warn</code> and <code>die</code> appropriately based on severity.</li> <li>Logging or reporting exceptions in ways that are actionable and user-friendly.</li> </ul> <p>Together, references and robust error handling form a critical part of writing maintainable, reusable Perl code.</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#becoming-adept","title":"Becoming Adept","text":"<p>Object-Oriented Perl</p> <p>Object-oriented programming (OOP) is a key capability of Perl and is frequently used in larger codebases or frameworks. Learning OOP in Perl involves understanding how packages work, how to bless references into objects, and how to define and use methods. While Perl doesn't enforce an OOP paradigm, it offers full support for it, giving developers the flexibility to design modular, reusable, and maintainable code.</p> <p>Key resources:</p> <ul> <li>Object-Oriented Perl by Damian Conway</li> <li>Modern Perl's Moose framework \u2013 a postmodern object system for Perl 5.</li> </ul> <p>System Interaction (fork, exec, file I/O, permissions)</p> <p>One of Perl's strengths lies in its tight integration with the underlying operating system. Proficiency in system-level programming is crucial for writing scripts that automate administrative tasks or interact directly with system components.</p> <p>Essential concepts include:</p> <ul> <li><code>fork</code>, <code>exec</code>, and <code>wait</code> for process control.</li> <li>Working with filehandles for reading and writing files.</li> <li>Interacting with file metadata (permissions, ownership, timestamps) using <code>stat</code>, <code>chmod</code>, <code>chown</code>, and <code>utime</code>.</li> <li>Redirecting output and managing subprocesses securely.</li> </ul> <p>Resources:</p> <ul> <li>perldoc -f open</li> <li>perlipc for interprocess communication and process management.</li> </ul> <p>Modulinos and Concurrency</p> <p>A modulino is a hybrid Perl script that behaves both as a standalone script and a reusable module. It enables better code reuse and testability. This pattern is common in professional Perl development where tooling needs to be scriptable yet unit-tested.</p> <p>Concurrency in Perl is typically handled using <code>fork</code> for multiprocessing, or by leveraging event-based modules. While threads exist in Perl (<code>threads.pm</code>), they are often avoided due to complexity and performance concerns.</p> <p>Tools and resources:</p> <ul> <li>Writing Modulinos with <code>caller</code></li> <li>Parallel::ForkManager for simple parallel execution.</li> <li>AnyEvent or IO::Async for asynchronous programming.</li> </ul>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#joining-the-community","title":"Joining the Community","text":"<p>Engaging with the Perl Community</p> <p>Getting involved with the Perl community is not only encouraged\u2014it's essential. Community interaction can accelerate your learning, expose you to real-world problem solving, and keep you current with modern Perl practices.</p> <ol> <li>PerlMonks: A long-established and active community focused entirely on Perl. It remains relevant in 2025 as a place to get answers, learn idioms, and participate in thoughtful programming discussions. Users post questions, tutorials, and code snippets, and receive feedback from veteran developers.</li> <li>Perl Mongers: A global network of local Perl user groups that still hold in-person and virtual meetups. While not as active as in past decades, many chapters remain valuable for community support and professional networking. Check the PM.org website or join forums like https://perl.community for modern community hubs.</li> </ol> <p>Contributing to CPAN (inspired by the CPAN PRC): The original CPAN Pull Request Challenge ended in 2018, but its legacy lives on. Today, Perl developers are encouraged to contribute to CPAN via GitHub and MetaCPAN. Identify outdated or under-maintained modules, review open issues, and submit pull requests. Tools like MetaCPAN and GitHub's \"good first issue\" label make it easier than ever to find a way to contribute.</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#diving-deep","title":"Diving Deep","text":"<p>Contribute to CPAN Modules</p> <p>Contributing to CPAN remains one of the most impactful ways to support the Perl ecosystem in 2025. While the landscape has evolved, community-driven development and open collaboration are still central. CPAN continues to power countless applications and tools, and active maintenance is critical to its health.</p> <p>Modern best practices include:</p> <ul> <li>Identifying under-maintained or outdated modules via MetaCPAN and GitHub.</li> <li>Submitting improvements as pull requests: bug fixes, documentation enhancements, modernization (e.g., switching from <code>Test::Simple</code> to <code>Test2::Suite</code>, or refactoring legacy idioms).</li> <li>Respecting semantic versioning and authoring guidelines using tools like <code>Test::CheckManifest</code>, <code>Test::Pod</code>, and <code>Perl::Critic</code> before submission.</li> </ul> <p>New contributors are encouraged to look for issues labeled good first issue or help wanted, and to follow module-specific contribution guidelines.</p> <p>Use Dist::Zilla</p> <p>Dist::Zilla continues to be a de facto standard for managing modern Perl distributions. It abstracts away much of the manual toil of packaging, releasing, and maintaining modules, making it particularly useful for complex projects or teams that rely on automation.</p> <p>Updated benefits as of 2025:</p> <ul> <li>Plugin ecosystem includes integrations for GitHub Actions, GitLab CI, and Docker tagging.</li> <li>Pre-release validation (e.g., changelog enforcement, dependency resolution) using community plugins.</li> <li>Seamless publishing to PAUSE and GitHub with <code>dzil release</code>, integrating test suites, version bumps, and tagging workflows.</li> </ul> <p>For developers looking to streamline and standardize their release cycle while avoiding boilerplate, Dist::Zilla remains an invaluable tool\u2014especially when paired with a modern CI/CD pipeline.</p> <p>What is CPAN and why is it significant for Perl development? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#staying-active","title":"Staying Active","text":"<p>Bug Reporting and Community Interaction</p> <p>Staying active in the Perl ecosystem means contributing beyond code. That includes reporting bugs, engaging in issue discussions, and collaborating respectfully with maintainers. Constructive bug reports often include reproduction steps, observed vs. expected behavior, environment details, and a proposed solution or patch. Engage through GitHub, MetaCPAN, or mailing lists like <code>perl5-porters</code> to help steer the future of modules you rely on.</p> <ul> <li>MetaCPAN Issues</li> <li>Perl Porters Mailing List</li> </ul> <p>Understanding \"perldelta\"</p> <p>The <code>perldelta</code> document is released with each new version of Perl and outlines what's changed: new features, deprecations, security fixes, and known issues. It's essential reading for any developer working with modern Perl or maintaining code across versions. Familiarity with <code>perldelta</code> helps you adopt features early, avoid deprecated behaviors, and ensure compatibility.</p> <p>Resources:</p> <ul> <li>Latest perldelta on MetaCPAN</li> <li>Perl's development GitHub repo for upcoming changes</li> </ul>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#ask-yourself","title":"Ask Yourself","text":"<p>How Does Perl Relate to Unix, Bash, PHP, etc.?</p> <p>Perl has deep historical and technical ties to Unix, Bash, and other scripting languages. Its design was heavily inspired by Unix philosophy: small, modular tools working together with powerful text processing and system integration capabilities.</p> <ul> <li> <p>Unix: Perl was born in the Unix environment, and its syntax and idioms reflect Unix traditions\u2014everything from regular expressions to filehandles and process management. Perl scripts are often used for Unix system administration, automation, and report generation.</p> </li> <li> <p>Bash: Bash and Perl frequently work together. Bash is great for quick shell automation, while Perl is better suited for complex logic, structured data handling, and modular design. Many developers use Bash to orchestrate and Perl to perform the heavy lifting.</p> </li> <li> <p>PHP: Perl and PHP have historically served similar roles in web development. While PHP dominates in modern web apps, Perl's use persists in legacy systems, backend automation, and templating. Interestingly, PHP was initially inspired by Perl and borrows from its syntax.</p> </li> <li> <p>Python &amp; Ruby (modern context): While not listed originally, it's important to note that Perl now coexists with Python and Ruby in many development environments. All three are high-level, interpreted languages used for scripting and automation, with Perl maintaining a niche in text processing and legacy infrastructure.</p> </li> </ul> <p>Understanding these relationships can help you make strategic choices about when to use Perl versus other languages, and how to interface with tools written in them.</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#important-perl-modules","title":"Important Perl Modules","text":"<p>Familiarity with modern, high-utility Perl modules is critical for writing robust, maintainable code. Below are essential categories and recommended modules to learn and use in your daily development workflow:</p> <p>Database Interaction</p> <ul> <li>DBI: The standard database interface module for Perl, providing a consistent API across various database systems.</li> <li>DBD::SQLite: Lightweight and serverless, ideal for local development or embedded use.</li> <li>Also consider: <code>DBD::Pg</code> for PostgreSQL and <code>DBD::mysql</code> for MariaDB/MySQL.</li> </ul> <p>Serialization and Configuration</p> <ul> <li>Cpanel::JSON::XS or JSON::MaybeXS: Efficient, reliable JSON serialization/deserialization.</li> <li>YAML::XS: Fast YAML support for reading configuration files.</li> <li>Config::INI: Simple parsing of INI-style config files.</li> </ul> <p>Testing Frameworks</p> <ul> <li>Test::More: A core testing module in Perl, forming the foundation of most test suites.</li> <li>Test::Spec: Enables RSpec-style testing for behavior-driven development (BDD).</li> <li>Also explore: <code>Test2::Suite</code> for a modern replacement and extensible test ecosystem.</li> </ul> <p>Code Quality Tools</p> <ul> <li>perltidy: Formats Perl code according to customizable style rules.</li> <li>Perl::Critic: Analyzes Perl code for maintainability, enforcing best practices based on Damian Conway's Perl Best Practices.</li> </ul> <p>Mastering these modules\u2014and understanding when to apply each\u2014will greatly improve your productivity, confidence, and code quality.</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#additional-learning_1","title":"Additional Learning","text":"<p>Interprocess Communication (IPC) Concepts</p> <p>IPC allows different processes to communicate and synchronize their actions. This is particularly important when writing daemon services, managing background workers, or integrating different components of a system. In Perl, IPC can be achieved using built-in functions and specialized modules.</p> <p>Key mechanisms include:</p> <ul> <li>Pipes: Used for communication between a parent and child process (<code>open</code>, <code>pipe</code>, <code>IPC::Open2</code>, <code>IPC::Open3</code>).</li> <li>Sockets: For networked or local IPC; useful in both client-server and peer-to-peer models (<code>IO::Socket::INET</code>, <code>IO::Socket::UNIX</code>).</li> <li>Shared memory and semaphores: Available via <code>IPC::SysV</code> or <code>IPC::Shareable</code> for more advanced use cases.</li> </ul> <p>Recommended Resources:</p> <ul> <li>perlipc documentation</li> <li>IPC::Run \u2013 For launching and interacting with external processes</li> </ul> <p>Daemon Creation</p> <p>Writing a daemon involves creating a background service that runs independently of user input. Common uses include job schedulers, monitoring tools, and lightweight APIs. In Perl, daemonizing a script involves steps like forking, detaching from the terminal, setting new session IDs, and handling PID files.</p> <p>Key considerations:</p> <ul> <li>Handling logging and signal management (<code>SIGHUP</code>, <code>SIGTERM</code>)</li> <li>Managing process IDs (PIDs) and preventing duplicate instances</li> <li>Monitoring via supervision tools like <code>systemd</code>, <code>supervisord</code>, or <code>daemontools</code></li> </ul> <p>Recommended Modules:</p> <ul> <li>Proc::Daemon</li> <li>Daemon::Control</li> </ul> <p>OSI Model and Perl's Networking Capabilities</p> <p>The OSI (Open Systems Interconnection) model defines how communication is structured in networks. Understanding this model helps developers write more effective networking code, troubleshoot connectivity issues, and build systems that communicate reliably.</p> <p>In Perl, you'll mostly operate at:</p> <ul> <li>Layer 4 (Transport): Working with TCP/UDP sockets</li> <li>Layer 7 (Application): Implementing or consuming protocols like HTTP, FTP, SMTP</li> </ul> <p>Perl modules for networking include:</p> <ul> <li>IO::Socket::INET: TCP/UDP communication</li> <li>LWP::UserAgent: High-level HTTP client</li> <li>HTTP::Tiny: Lightweight alternative to LWP</li> <li>Net::Server: Building robust, extensible server applications</li> </ul> <p>Mastering these areas will significantly enhance your ability to build distributed systems, background services, and web-enabled tools using Perl.</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#phase-3-workflows-testing-debugging","title":"Phase 3: Workflows, Testing &amp; Debugging","text":""},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#required-learning_2","title":"Required Learning","text":""},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#testing","title":"Testing","text":"<p>Effective testing is the backbone of sustainable Perl development. It ensures reliability, facilitates maintenance, and empowers developers to refactor confidently. While Perl has a long tradition of testing culture, the landscape in 2025 supports both time-tested modules and newer, more flexible frameworks.</p> <p>Core Practices:</p> <ul> <li>Unit Testing: The most fundamental type of test, verifying the behavior of individual components in isolation. This is typically done using:</li> <li>Test::More \u2013 the de facto standard and part of the Perl core.</li> <li>Test::Simple \u2013 a minimalistic base layer.</li> <li>Test2::Suite \u2013 a modern, extensible replacement that powers new testing ecosystems.</li> </ul> <p>Advanced Testing Techniques:</p> <ul> <li> <p>Behavior-Driven Development (BDD): Write tests in a style that mirrors expected behavior.</p> </li> <li> <p>Test::Spec and Test::BDD::Cucumber are popular options.</p> </li> <li> <p>Mocking and Isolation:</p> </li> <li> <p>Test::MockModule or Test::MockObject can simulate dependencies.</p> </li> <li> <p>Continuous Testing Integration:</p> </li> <li> <p>Pair with modern CI tools like GitHub Actions, GitLab CI, or CircleCI.</p> </li> <li>Automate coverage reporting with Devel::Cover.</li> </ul> <p>Best Practices:</p> <ul> <li>Write tests before or alongside your code.</li> <li>Use descriptive test names and group related tests into subtests.</li> <li>Ensure your tests are repeatable and deterministic.</li> </ul> <p>Testing isn't just about proving correctness\u2014it's about building trust in your code, now and in the future.</p> <p>Why does the roadmap emphasize testing (Phase 2) before advanced topics? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#application-architecture","title":"### Application Architecture","text":"<p>A modern Perl developer must understand not just the language, but how it integrates with full-stack application architecture. Perl is frequently used for building backend services, consuming APIs, and even serving as glue code between microservices or distributed systems.</p> <p>Services: Services refer to standalone applications or daemons that perform a specific task\u2014like authentication, data processing, or email delivery. These are often exposed over HTTP/HTTPS and must be built to handle concurrency, error resilience, and observability (e.g., logging and metrics).</p> <p>APIs: APIs (Application Programming Interfaces) are the primary interface by which modern services communicate. Perl can both expose APIs (e.g., via frameworks like Mojolicious or Dancer2) and consume them (via LWP::UserAgent or HTTP::Tiny). Understanding RESTful principles and HTTP status codes is key.</p> <p>REST: Representational State Transfer (REST) is the dominant architectural style for designing networked applications. REST relies on stateless communication, meaningful resource URLs, and standard HTTP methods (GET, POST, PUT, DELETE). Modern Perl APIs often follow REST patterns and return JSON responses.</p> <p>Recommended Tools &amp; Frameworks:</p> <ul> <li>Mojolicious: A feature-rich web framework for building RESTful services and full-stack web apps.</li> <li>Dancer2: A lightweight, extensible framework ideal for small- to medium-sized APIs.</li> <li>Plack/PSGI: Middleware for Perl web applications; the foundation for most modern Perl web frameworks.</li> </ul> <p>Mastering application architecture ensures that your Perl projects scale well, integrate cleanly with other systems, and follow contemporary software development practices.</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#practicing-workflows","title":"Practicing Workflows","text":"<p>Contributing to CPAN (Inspired by the CPAN PRC)</p> <p>While the original CPAN Pull Request Challenge (CPAN PRC) ended in 2018, the concept of monthly contributions to open source Perl modules lives on in spirit. Practicing workflows through regular contribution helps developers gain experience with collaborative development, version control, and best practices in CPAN module structure and testing.</p> <p>To emulate this experience today:</p> <ul> <li>Browse MetaCPAN or GitHub for active modules that need contributions.</li> <li>Filter issues labeled good first issue, help wanted, or tagged with hacktoberfest, which signal beginner-friendly opportunities.</li> <li>Use GitHub's fork-and-branch model to propose code changes. Ensure your changes are well-documented and include tests where appropriate.</li> <li>Participate in community discussions by commenting on issues, responding to feedback, and reviewing others' pull requests.</li> <li>Integrate with continuous integration tools like GitHub Actions or Travis CI to automatically run your test suite and lint your code before submission.</li> <li>Familiarize yourself with the CPAN Pull Request Guide for additional resources and etiquette.</li> </ul> <p>This modernized process helps you build fluency in CPAN tooling, Git workflows, CI/CD integration, and collaborative open-source development\u2014all essential skills for contributing to the contemporary Perl ecosystem.</p> <p>GitHub Collaboration</p> <p>GitHub remains the dominant platform for hosting and contributing to Perl projects in 2025. Mastering GitHub collaboration is essential for professional Perl development. Key practices include:</p> <ul> <li>Forking repositories and creating feature branches.</li> <li>Writing meaningful commit messages and pull request descriptions.</li> <li>Participating in code reviews and addressing feedback.</li> <li>Using CI integrations (GitHub Actions, Travis CI) to validate code.</li> <li>Keeping up with upstream changes through rebase/merge workflows.</li> </ul> <p>GitHub collaboration not only builds your technical skills\u2014it builds your reputation. Active contributors are more likely to be recognized and recruited for freelance, contract, and full-time roles.</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#using-version-control","title":"Using Version Control","text":"<p>Version control is an essential part of modern software development, enabling teams and individuals to track changes, collaborate asynchronously, and ensure project stability. For Perl developers, mastering Git is foundational.</p> <p>Core Practices:</p> <ul> <li>Git: Learn essential Git commands such as <code>clone</code>, <code>commit</code>, <code>push</code>, <code>pull</code>, <code>merge</code>, <code>rebase</code>, and <code>stash</code>. Understand branching models like Git Flow or trunk-based development.</li> <li>Commit Discipline: Write clear, concise, and purposeful commit messages. Use atomic commits to keep history readable and revert-friendly.</li> </ul> <p>Modern Learning Resources:</p> <ul> <li>GitHub Learning Lab \u2013 Offers interactive tutorials on real GitHub repositories.</li> <li>Pro Git Book \u2013 A comprehensive and freely available guide for all levels.</li> <li>Git Handbook \u2013 A high-level overview of Git and GitHub best practices.</li> </ul> <p>Tooling &amp; Extensions:</p> <ul> <li>Use GitHub CLI for scriptable GitHub workflows.</li> <li>Explore visual Git clients like Sourcetree or GitKraken if you prefer a GUI.</li> <li>Integrate version control with CI/CD systems to trigger builds, tests, and deployments automatically on code changes.</li> </ul> <p>Staying fluent in Git and GitHub workflows will make you a stronger collaborator, contributor, and maintainer\u2014skills that transcend Perl and apply across nearly every software development ecosystem.</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#debugging-methods","title":"Debugging Methods","text":"<p>Effective debugging is a skill that separates productive developers from frustrated ones. Perl provides multiple layers of debugging tools, from compile-time checks to interactive debuggers.</p> <p>Prevention first: <code>use strict</code> and <code>use warnings</code> catch the majority of bugs before your code even runs. Undeclared variables, misused data types, and common mistakes are flagged immediately. See the Introduction for why these two lines are non-negotiable.</p> <p>Print debugging: The simplest approach - add <code>print</code> or <code>warn</code> statements to trace variable values and execution flow. <code>warn</code> writes to STDERR (so it doesn't corrupt program output) and includes the file and line number. <code>Data::Dumper</code> displays complex data structures in a readable format:</p> <pre><code>use Data::Dumper;\nwarn Dumper(\\%config);  # Prints the entire hash structure to STDERR\n</code></pre> <p>The Perl debugger: Run any script with <code>perl -d script.pl</code> to start the interactive debugger. You can set breakpoints (<code>b</code>), step through code (<code>n</code> for next, <code>s</code> for step into), inspect variables (<code>x $var</code>), and evaluate expressions. The debugger is invaluable for tracing logic errors in unfamiliar code.</p> <p>Devel:: modules: Devel::NYTProf is a powerful profiler that identifies performance bottlenecks. Devel::Peek shows Perl's internal representation of variables. Devel::Cover measures test coverage so you know which code paths remain untested.</p> <p>For more on debugging strategies, see Error Handling and Debugging.</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#linux-package-managers","title":"Linux Package Managers","text":"<p>Perl development on Linux means working with the system's package manager to install libraries, development headers, and tools that Perl modules depend on.</p> <p>RPM-based systems (RHEL, Rocky Linux, AlmaLinux, Fedora) use <code>yum</code> or its successor <code>dnf</code>:</p> <pre><code># Install development tools and libraries\nsudo dnf groupinstall \"Development Tools\"\nsudo dnf install perl perl-devel perl-CPAN openssl-devel\n</code></pre> <p>Debian-based systems (Ubuntu, Debian) use <code>apt</code>:</p> <pre><code>sudo apt update\nsudo apt install build-essential perl perl-doc cpanminus libssl-dev\n</code></pre> <p>Perl module installation is handled separately from the system package manager. cpanm (cpanminus) is the recommended tool:</p> <pre><code>cpanm Mojolicious           # Install from CPAN\ncpanm --installdeps .       # Install dependencies from cpanfile\n</code></pre> <p>Use the system package manager for system libraries (like <code>libssl-dev</code>) and <code>cpanm</code> for Perl modules. Mixing the two (installing Perl modules via <code>apt</code>) can cause version conflicts.</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#application-security-awareness","title":"Application Security Awareness","text":"<p>Writing secure code is a professional responsibility. Perl applications face the same security threats as any web-facing software, plus some language-specific concerns.</p> <p>Cross-Site Scripting (XSS): Any user-supplied data rendered in HTML must be escaped. Perl web frameworks like Mojolicious auto-escape template variables by default, but raw output (using <code>&lt;%==</code> or <code>b()</code>) bypasses this protection. Always treat user input as untrusted.</p> <p>SQL Injection: Never interpolate variables directly into SQL queries. Use DBI placeholders instead:</p> <pre><code># DANGEROUS - SQL injection vulnerability\nmy $sth = $dbh-&gt;prepare(\"SELECT * FROM users WHERE name = '$name'\");\n\n# SAFE - parameterized query\nmy $sth = $dbh-&gt;prepare(\"SELECT * FROM users WHERE name = ?\");\n$sth-&gt;execute($name);\n</code></pre> <p>Taint mode: Running Perl with the <code>-T</code> flag enables taint checking, which tracks data that came from outside the program (user input, environment variables, file reads) and prevents it from being used in system calls, file operations, or database queries until it's been explicitly validated through a regex match.</p> <p>Input validation: Validate all external data at system boundaries - command-line arguments, form submissions, API payloads, file uploads. Reject invalid input early rather than trying to sanitize it later.</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#additional-architecture-concepts","title":"Additional Architecture Concepts","text":"<p>As your applications grow beyond single scripts, architectural patterns become important for maintainability and scalability.</p> <p>SOLID principles apply to Perl code just as they do to Java or Python. The most relevant for Perl development: Single Responsibility (each module does one thing), Open/Closed (extend behavior through composition, not modification), and Dependency Inversion (depend on interfaces, not implementations).</p> <p>Caching reduces repeated work. CHI provides a unified caching interface with backends for memory, file, Redis, and Memcached. Cache database query results, API responses, or expensive computations.</p> <p>Message queues decouple producers from consumers in distributed systems. Perl integrates with RabbitMQ (via <code>Net::AMQP::RabbitMQ</code>), Redis pub/sub, and job queue systems like Minion (built into Mojolicious).</p> <p>Full-text search engines like Elasticsearch or Sphinx handle search queries that SQL <code>LIKE</code> clauses cannot scale to. Perl clients exist for both, and Elasticsearch is increasingly the standard for log analysis and search features.</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#the-learning-journey","title":"The Learning Journey","text":"<p>The roadmap above covers a lot of ground. This diagram shows how the phases connect and where the dedicated guides in this course fit.</p> <pre><code>flowchart TD\n    subgraph Phase 1: Foundations\n        A[Unix &amp; Linux Basics] --&gt; B[Command Line Proficiency]\n        B --&gt; C[Editor Mastery]\n    end\n\n    subgraph Phase 2: Perl Fundamentals\n        D[Scalars, Strings, Numbers] --&gt; E[Arrays, Hashes, Lists]\n        E --&gt; F[Control Flow]\n        F --&gt; G[Regular Expressions]\n        G --&gt; H[Subroutines &amp; References]\n        H --&gt; I[File I/O &amp; System]\n    end\n\n    subgraph Phase 3: Professional Practice\n        J[Modules &amp; CPAN] --&gt; K[Object-Oriented Perl]\n        K --&gt; L[Error Handling &amp; Debugging]\n        L --&gt; M[Testing]\n    end\n\n    subgraph Phase 4: Applied Perl\n        N[Text Processing &amp; One-Liners]\n        O[Networking &amp; Daemons]\n        P[Web Frameworks &amp; APIs]\n    end\n\n    C --&gt; D\n    I --&gt; J\n    M --&gt; N\n    M --&gt; O\n    M --&gt; P</code></pre>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#the-cpan-ecosystem","title":"The CPAN Ecosystem","text":"<p>CPAN is central to productive Perl development. Understanding how its components fit together saves time when you need to find, evaluate, and install modules.</p> <pre><code>flowchart TD\n    A[Your Perl Script] --&gt;|use Module| B[Installed Modules]\n    B --&gt;|installed by| C[cpanm / cpan]\n    C --&gt;|downloads from| D[CPAN Mirror Network]\n    D --&gt;|mirrors| E[PAUSE Upload Server]\n    F[Module Author] --&gt;|uploads to| E\n    G[MetaCPAN] --&gt;|indexes| D\n    G --&gt;|shows| H[Documentation, Ratings, Dependencies]\n    C --&gt;|resolves| I[Dependency Chain]\n    I --&gt;|may require| J[System Libraries via apt/dnf]</code></pre> <p>PAUSE is where authors upload distributions. MetaCPAN is the search and documentation interface. <code>cpanm</code> handles downloading, dependency resolution, building, testing, and installing. When a CPAN module needs a C library (like OpenSSL), you install that through your system package manager first.</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#setting-up-your-perl-environment","title":"Setting Up Your Perl Environment","text":"<p>Before starting the course exercises, verify your Perl installation and set up the essential tools.</p> <p>Setting Up a Perl Dev Environment (requires JavaScript)</p> <p>Create Your Learning Tracker (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#bibliography","title":"Bibliography","text":"<ul> <li>Barrett, D. J. Linux Pocket Guide</li> <li>Conway, D. Object Oriented Perl</li> <li>Feynman, R. P. Surely You're Joking Mr. Feynman!</li> <li>Langworth, I. &amp; chromatic. Perl Testing: A Developer's Notebook</li> </ul>"},{"location":"Dev%20Zero/Perl/perl_developer_roadmap/#further-reading","title":"Further Reading","text":"<ul> <li>Perl Official Documentation - comprehensive Perl language reference</li> <li>CPAN - Comprehensive Perl Archive Network</li> <li>Perl.org - official Perl community site</li> <li>MetaCPAN - modern CPAN search and documentation</li> <li>PerlMonks - community discussion and Q&amp;A</li> </ul> <p>Previous: Web Frameworks and APIs | Back to Index</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/","title":"Regular Expressions","text":""},{"location":"Dev%20Zero/Perl/regular-expressions/#pattern-matching-and-text-transformation","title":"Pattern Matching and Text Transformation","text":"<p>Version: 1.0 Year: 2026</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#copyright-notice","title":"Copyright Notice","text":"<p>Copyright (c) 2025-2026 Ryan Thomas Robson / Robworks Software LLC. Licensed under CC BY-NC-ND 4.0. You may share this material for non-commercial purposes with attribution, but you may not distribute modified versions.</p> <p>Most languages treat regular expressions as a library feature - you import a module, create a pattern object, and call methods on it. Perl is different. Regular expressions are woven directly into the language syntax. They have their own operators (<code>=~</code>, <code>!~</code>), their own quoting constructs (<code>m//</code>, <code>s///</code>, <code>qr//</code>), and their own set of special variables (<code>$1</code>, <code>$&amp;</code>, <code>$+{name}</code>). This is not an accident - Perl was originally designed for text processing, and regex is its native tongue.</p> <p>Perl's regex engine is so influential that it became the basis for PCRE (Perl-Compatible Regular Expressions), the library used by Python, PHP, JavaScript, and dozens of other languages. When you learn Perl regex, you are learning the original that shaped every modern regex implementation.</p> <p>This guide takes you from basic matching through advanced features like lookaround assertions and compiled patterns. Every concept builds on the previous one, so read it in order if this is your first pass through Perl regex.</p> <p>See also</p> <p>Regex fundamentals are shared across tools. For how <code>grep</code>, <code>sed</code>, and <code>awk</code> use regular expressions, see Text Processing.</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#matching-with-m","title":"Matching with m//","text":"<p>The <code>=~</code> binding operator connects a string to a pattern. The basic form tests whether a string matches:</p> <pre><code>my $str = \"Hello, World!\";\n\nif ($str =~ /World/) {\n    print \"Found it!\\n\";\n}\n</code></pre> <p>The <code>/World/</code> part is a regex pattern. The <code>=~</code> operator binds <code>$str</code> to that pattern and returns true if the pattern matches anywhere in the string.</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#the-m-operator","title":"The m// Operator","text":"<p>The slashes are shorthand for <code>m//</code> (match). You can use <code>m</code> with any delimiter:</p> <pre><code># These are all equivalent\n$str =~ /World/\n$str =~ m/World/\n$str =~ m{World}\n$str =~ m|World|\n$str =~ m!World!\n</code></pre> <p>Alternate delimiters are useful when your pattern contains slashes - matching a file path like <code>/usr/local/bin</code> is cleaner as <code>m{/usr/local/bin}</code> than as <code>/\\/usr\\/local\\/bin/</code>.</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#negation-with","title":"Negation with !~","text":"<p>The <code>!~</code> operator is the opposite of <code>=~</code>. It returns true when the pattern does not match:</p> <pre><code>if ($input !~ /^\\d+$/) {\n    print \"Not a valid number\\n\";\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/regular-expressions/#default-variable-matching","title":"Default Variable Matching","text":"<p>When you omit the <code>=~</code> operator, Perl matches against <code>$_</code>, the default variable:</p> <pre><code>for ('apple', 'banana', 'cherry') {\n    print \"$_ has an 'a'\\n\" if /a/;\n}\n</code></pre> <p>This is idiomatic in loops and <code>grep</code>/<code>map</code> blocks where <code>$_</code> is set automatically.</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#match-variables","title":"Match Variables","text":"<p>After a successful match, Perl sets several special variables:</p> Variable Contains <code>$&amp;</code> The entire matched text <code>$`</code> Everything before the match <code>$'</code> Everything after the match <pre><code>my $str = \"The quick brown fox\";\nif ($str =~ /quick/) {\n    print \"Before: '$`'\\n\";    # \"The \"\n    print \"Match: '$&amp;'\\n\";     # \"quick\"\n    print \"After: '$''\\n\";     # \" brown fox\"\n}\n</code></pre> <p>Performance Cost of $&amp;</p> <p>Using <code>$&amp;</code>, <code>$`</code>, or <code>$'</code> anywhere in your program forces Perl to compute them for every regex match in the entire program - even matches that do not use these variables. In Perl 5.20+, use <code>/p</code> modifier and <code>${^MATCH}</code>, <code>${^PREMATCH}</code>, <code>${^POSTMATCH}</code> instead. Or better yet, use captures (covered below).</p> <p>Basic Pattern Matching (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#how-perl-processes-a-match","title":"How Perl Processes a Match","text":"<p>When you write <code>$str =~ /pattern/</code>, the regex engine does not simply scan through the string once. It follows a systematic process: it places a cursor at the start of the string, tries the pattern at that position, and if the match fails, advances the cursor by one character and tries again. This continues until the engine either finds a match or exhausts the entire string.</p> <pre><code>flowchart TD\n    Start[Start Match] --&gt; Pos[Position cursor at start of string]\n    Pos --&gt; Try[Try pattern at current position]\n    Try --&gt; Match{Match found?}\n    Match --&gt; |Yes| Cap[Capture groups populated&lt;br/&gt;Match variables set]\n    Cap --&gt; Done[Return successful match]\n    Match --&gt; |No| Advance[Advance cursor one position]\n    Advance --&gt; End{Past end of string?}\n    End --&gt; |No| Try\n    End --&gt; |Yes| Fail[Match fails - return false]</code></pre> <p>This is why a pattern like <code>/World/</code> can find a match in the middle of <code>\"Hello, World!\"</code> - the engine tries position 0 (\"H\"), fails, tries position 1 (\"e\"), fails, and so on until position 7 where \"World\" matches. The <code>=~</code> operator hides this retry loop, but understanding it explains why regex can be expensive on long strings with no match - the engine must attempt the pattern at every single position before giving up.</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#character-classes","title":"Character Classes","text":"<p>A character class matches one character from a defined set. Square brackets define a custom class:</p> <pre><code>/[aeiou]/       # matches any single vowel\n/[0-9]/         # matches any digit\n/[A-Za-z]/      # matches any ASCII letter\n/[a-zA-Z0-9_]/  # matches word characters\n</code></pre>"},{"location":"Dev%20Zero/Perl/regular-expressions/#ranges-and-negation","title":"Ranges and Negation","text":"<p>Hyphens inside brackets define ranges. A caret <code>^</code> at the start negates the class:</p> <pre><code>/[^aeiou]/      # matches any character that is NOT a vowel\n/[^0-9]/        # matches any non-digit\n/[a-fA-F0-9]/   # matches a hexadecimal digit\n</code></pre> <p>Literal Hyphen in Character Classes</p> <p>To include a literal hyphen, place it first, last, or escape it: <code>[-abc]</code>, <code>[abc-]</code>, or <code>[a\\-c]</code>. Placing it between characters creates a range.</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#posix-character-classes","title":"POSIX Character Classes","text":"<p>POSIX classes use a double-bracket syntax inside a character class:</p> <pre><code>/[[:alpha:]]/    # alphabetic characters\n/[[:digit:]]/    # digits (same as [0-9])\n/[[:alnum:]]/    # alphanumeric\n/[[:space:]]/    # whitespace characters\n/[[:upper:]]/    # uppercase letters\n/[[:lower:]]/    # lowercase letters\n/[[:punct:]]/    # punctuation characters\n</code></pre> <p>POSIX classes respect locale settings, which makes them more portable than hardcoded ranges for internationalized text. In practice, most Perl code uses the shorthand escapes below.</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#shorthand-character-classes","title":"Shorthand Character Classes","text":"<p>Perl provides single-character shortcuts for the most common classes:</p> Shorthand Matches Equivalent <code>\\d</code> A digit <code>[0-9]</code> (ASCII) or Unicode digit <code>\\D</code> A non-digit <code>[^0-9]</code> <code>\\w</code> A \"word\" character <code>[a-zA-Z0-9_]</code> <code>\\W</code> A non-word character <code>[^a-zA-Z0-9_]</code> <code>\\s</code> Whitespace <code>[ \\t\\n\\r\\f]</code> <code>\\S</code> Non-whitespace <code>[^ \\t\\n\\r\\f]</code> <code>.</code> Any character except newline <code>[^\\n]</code> (unless <code>/s</code> modifier) <pre><code># Match a simple date format\nif ($str =~ /\\d{4}-\\d{2}-\\d{2}/) {\n    print \"Looks like a date\\n\";\n}\n\n# Match a Perl variable name\nif ($str =~ /[\\$\\@\\%]\\w+/) {\n    print \"Looks like a variable\\n\";\n}\n</code></pre> <p>Unicode and \\d</p> <p>Under <code>use utf8</code> or the <code>/u</code> flag, <code>\\d</code> matches any Unicode digit - including Arabic-Indic, Devanagari, and other scripts. For strict ASCII digits, use <code>[0-9]</code> explicitly.</p> <p>What does the pattern /\\b\\w+\\b/ match, and why do the \\b anchors matter? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#quantifiers","title":"Quantifiers","text":"<p>Quantifiers control how many times a preceding element must appear for the pattern to match.</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#basic-quantifiers","title":"Basic Quantifiers","text":"Quantifier Meaning Example <code>*</code> 0 or more <code>/bo*/</code> matches \"b\", \"bo\", \"boo\", \"booo\" <code>+</code> 1 or more <code>/bo+/</code> matches \"bo\", \"boo\", \"booo\" (not \"b\") <code>?</code> 0 or 1 <code>/colou?r/</code> matches \"color\" and \"colour\" <code>{n}</code> Exactly n <code>/\\d{4}/</code> matches exactly 4 digits <code>{n,}</code> n or more <code>/\\d{2,}/</code> matches 2 or more digits <code>{n,m}</code> Between n and m <code>/\\d{2,4}/</code> matches 2 to 4 digits <pre><code># Phone number: 3 digits, separator, 3 digits, separator, 4 digits\nif ($phone =~ /\\d{3}[-.\\s]\\d{3}[-.\\s]\\d{4}/) {\n    print \"Valid phone format\\n\";\n}\n\n# One or more whitespace-separated words\nif ($line =~ /\\w+(\\s+\\w+)*/) {\n    print \"Contains words\\n\";\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/regular-expressions/#greedy-vs-non-greedy","title":"Greedy vs Non-Greedy","text":"<p>By default, quantifiers are greedy - they match as much text as possible while still allowing the overall pattern to succeed. Adding <code>?</code> after a quantifier makes it non-greedy (also called lazy) - it matches as little as possible.</p> <pre><code>my $html = \"&lt;b&gt;bold&lt;/b&gt; and &lt;i&gt;italic&lt;/i&gt;\";\n\n# Greedy: .* grabs as much as possible\n$html =~ /&lt;.*&gt;/;\n# $&amp; is \"&lt;b&gt;bold&lt;/b&gt; and &lt;i&gt;italic&lt;/i&gt;\"\n\n# Non-greedy: .*? grabs as little as possible\n$html =~ /&lt;.*?&gt;/;\n# $&amp; is \"&lt;b&gt;\"\n</code></pre> <p>This is one of the most common sources of regex bugs. If your pattern matches more text than expected, a greedy quantifier is usually the cause.</p> Greedy Non-greedy Meaning <code>*</code> <code>*?</code> 0 or more (prefer fewer) <code>+</code> <code>+?</code> 1 or more (prefer fewer) <code>?</code> <code>??</code> 0 or 1 (prefer 0) <code>{n,m}</code> <code>{n,m}?</code> n to m (prefer n)"},{"location":"Dev%20Zero/Perl/regular-expressions/#possessive-quantifiers","title":"Possessive Quantifiers","text":"<p>Possessive quantifiers (Perl 5.10+) add <code>+</code> after the quantifier. They behave like greedy quantifiers but never backtrack - once they consume characters, they do not give them back:</p> <pre><code># Possessive: \\d++ grabs all digits and refuses to backtrack\n\"12345abc\" =~ /\\d++\\d/;  # FAILS - \\d++ takes all digits, nothing left for \\d\n\n# Greedy: \\d+ grabs all digits, then backtracks one for the final \\d\n\"12345abc\" =~ /\\d+\\d/;   # Matches \"12345\"\n</code></pre> <p>Possessive quantifiers are a performance optimization. Use them when you know backtracking would be pointless - the engine fails faster instead of trying every possible combination.</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#how-the-engine-backtracks","title":"How the Engine Backtracks","text":"<p>The following diagram shows how Perl's regex engine processes the greedy pattern <code>a+b</code> against the string \"aaac\". The engine matches greedily, then backtracks when it cannot find <code>b</code>:</p> <pre><code>flowchart TD\n    S[Start: match a+b against 'aaac'] --&gt; A1[a+ matches 'aaa' - greedy, takes all]\n    A1 --&gt; B1{Next char is 'c' - does it match b?}\n    B1 --&gt;|No| BT1[Backtrack: a+ gives back one 'a' - now matches 'aa']\n    BT1 --&gt; B2{Next char is 'a' - does it match b?}\n    B2 --&gt;|No| BT2[Backtrack: a+ gives back another - now matches 'a']\n    BT2 --&gt; B3{Next char is 'a' - does it match b?}\n    B3 --&gt;|No| BT3[Backtrack: a+ has minimum 1 - cannot give back more]\n    BT3 --&gt; FAIL[Match fails at position 0]\n    FAIL --&gt; NEXT[Advance start position and retry]\n    NEXT --&gt; NOPE[No match found in string]</code></pre> <p>This backtracking behavior is what makes greedy quantifiers expensive on long strings with no match. Possessive quantifiers (<code>a++b</code>) would fail immediately at the first step - <code>a++</code> takes all three <code>a</code> characters and refuses to backtrack, so the engine knows instantly that <code>b</code> cannot match.</p> <p>Given the string 'bold', what does the greedy pattern /&lt;.*&gt;/ match? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#anchors","title":"Anchors","text":"<p>Anchors match a position in the string, not a character. They constrain where a pattern can match without consuming any text.</p> Anchor Position <code>^</code> Start of string (or start of line with <code>/m</code>) <code>$</code> End of string (or end of line with <code>/m</code>) <code>\\b</code> Word boundary (between <code>\\w</code> and <code>\\W</code>) <code>\\B</code> Not a word boundary <code>\\A</code> Absolute start of string (ignores <code>/m</code>) <code>\\z</code> Absolute end of string (ignores <code>/m</code>) <code>\\Z</code> End of string or before final newline <pre><code># Match only if the entire string is digits\nif ($str =~ /^\\d+$/) {\n    print \"All digits\\n\";\n}\n\n# Match 'cat' only as a whole word, not inside 'concatenate'\nif ($str =~ /\\bcat\\b/) {\n    print \"Found the word 'cat'\\n\";\n}\n\n# Validate that a string starts with a letter\nif ($str =~ /\\A[a-zA-Z]/) {\n    print \"Starts with a letter\\n\";\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/regular-expressions/#and-with-multiline-mode","title":"^ and $ with Multiline Mode","text":"<p>By default, <code>^</code> and <code>$</code> match the start and end of the entire string. With the <code>/m</code> modifier, they match the start and end of each line:</p> <pre><code>my $text = \"first line\\nsecond line\\nthird line\\n\";\n\n# Without /m - matches only at string start\nmy @starts = ($text =~ /^(\\w+)/g);\n# @starts = (\"first\")\n\n# With /m - matches at start of each line\nmy @starts = ($text =~ /^(\\w+)/gm);\n# @starts = (\"first\", \"second\", \"third\")\n</code></pre> <p>When you need to anchor to the absolute start or end regardless of <code>/m</code>, use <code>\\A</code> and <code>\\z</code>:</p> <pre><code># Always matches absolute start - even with /m\nif ($str =~ /\\A#!/) {\n    print \"Starts with shebang\\n\";\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/regular-expressions/#word-boundaries","title":"Word Boundaries","text":"<p>The <code>\\b</code> anchor matches the zero-width position between a word character (<code>\\w</code>) and a non-word character (<code>\\W</code>), or between <code>\\w</code> and the start/end of the string.</p> <pre><code>my $str = \"caterpillar has a cat in it\";\n\n$str =~ /cat/;       # matches \"cat\" in \"caterpillar\" (first occurrence)\n$str =~ /\\bcat\\b/;   # matches \"cat\" as a standalone word\n\n# Practical: highlight whole words only\n$str =~ s/\\bcat\\b/[CAT]/g;\n# \"caterpillar has a [CAT] in it\"\n</code></pre>"},{"location":"Dev%20Zero/Perl/regular-expressions/#captures-and-backreferences","title":"Captures and Backreferences","text":"<p>Parentheses in a regex do two things: they group sub-patterns and they capture the matched text into numbered variables.</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#numbered-captures","title":"Numbered Captures","text":"<p>Each pair of parentheses creates a capture group. After a successful match, <code>$1</code> holds the text matched by the first group, <code>$2</code> the second, and so on:</p> <pre><code>my $date = \"2025-01-15\";\nif ($date =~ /(\\d{4})-(\\d{2})-(\\d{2})/) {\n    print \"Year: $1\\n\";    # 2025\n    print \"Month: $2\\n\";   # 01\n    print \"Day: $3\\n\";     # 15\n}\n</code></pre> <p>Captures are numbered by the position of their opening parenthesis, counting from left to right:</p> <pre><code># Group numbering:\n#   (   1   (  2  )   (  3  )   )\n# /( \\w+ ) ( \\d+ ) ( \\w+ )/x\n</code></pre>"},{"location":"Dev%20Zero/Perl/regular-expressions/#captures-in-list-context","title":"Captures in List Context","text":"<p>In list context, a match returns all captured groups as a list:</p> <pre><code>my ($year, $month, $day) = (\"2025-01-15\" =~ /(\\d{4})-(\\d{2})-(\\d{2})/);\nprint \"$month/$day/$year\\n\";  # 01/15/2025\n</code></pre> <p>Combined with <code>/g</code>, this extracts all occurrences:</p> <pre><code>my @words = (\"hello world foo bar\" =~ /(\\w+)/g);\n# @words = (\"hello\", \"world\", \"foo\", \"bar\")\n</code></pre>"},{"location":"Dev%20Zero/Perl/regular-expressions/#named-captures","title":"Named Captures","text":"<p>Named captures (Perl 5.10+) use <code>(?&lt;name&gt;...)</code> syntax and store results in <code>$+{name}</code>:</p> <pre><code>my $line = \"192.168.1.100 - admin [15/Jan/2025:09:23:45] GET /index.html\";\n\nif ($line =~ /(?&lt;ip&gt;[\\d.]+)\\s+-\\s+(?&lt;user&gt;\\w+)\\s+\\[(?&lt;time&gt;[^\\]]+)\\]\\s+(?&lt;method&gt;\\w+)\\s+(?&lt;path&gt;\\S+)/) {\n    print \"IP: $+{ip}\\n\";        # 192.168.1.100\n    print \"User: $+{user}\\n\";    # admin\n    print \"Time: $+{time}\\n\";    # 15/Jan/2025:09:23:45\n    print \"Method: $+{method}\\n\"; # GET\n    print \"Path: $+{path}\\n\";    # /index.html\n}\n</code></pre> <p>Named captures make complex patterns self-documenting. The numbered variables (<code>$1</code>, <code>$2</code>) still work alongside named ones.</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#backreferences","title":"Backreferences","text":"<p>Backreferences refer to a previously captured group within the same pattern. Use <code>\\1</code>, <code>\\2</code>, etc. inside the pattern itself:</p> <pre><code># Match repeated words (\"the the\", \"is is\")\nif ($text =~ /\\b(\\w+)\\s+\\1\\b/i) {\n    print \"Duplicate word: $1\\n\";\n}\n\n# Match matching quotes\nif ($str =~ /([\"']).*?\\1/) {\n    print \"Found quoted string\\n\";\n}\n</code></pre> <p>Named backreferences use <code>\\k&lt;name&gt;</code>:</p> <pre><code>if ($text =~ /\\b(?&lt;word&gt;\\w+)\\s+\\k&lt;word&gt;\\b/i) {\n    print \"Duplicate: $+{word}\\n\";\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/regular-expressions/#non-capturing-groups","title":"Non-Capturing Groups","text":"<p>When you need grouping for alternation or quantifiers but do not need the captured text, use <code>(?:...)</code>:</p> <pre><code># Capturing: wastes a capture slot on something we don't need\nif ($url =~ /(https?)(:\\/\\/.+)/) { ... }\n\n# Non-capturing: groups without capturing\nif ($url =~ /(?:https?):\\/\\/(.+)/) {\n    print \"Host and path: $1\\n\";  # $1 is now the useful part\n}\n</code></pre> <p>Non-capturing groups are a habit worth building. They keep your capture numbering clean and avoid unnecessary work.</p> <p>Log Line Parser with Named Captures (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#alternation-and-grouping","title":"Alternation and Grouping","text":"<p>The <code>|</code> operator means \"or\" - it matches the pattern on the left or the pattern on the right:</p> <pre><code># Match any of these keywords\nif ($line =~ /error|warning|critical/) {\n    print \"Problem detected\\n\";\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/regular-expressions/#precedence","title":"Precedence","text":"<p>Alternation has low precedence - lower than concatenation. This means <code>/abc|def/</code> matches \"abc\" or \"def\", not \"ab(c or d)ef\". Use grouping to control scope:</p> <pre><code># Without grouping: matches \"gray\" or \"grey\"... but also wrong readings\n/gray|grey/\n\n# With grouping: clearly matches \"gray\" or \"grey\"\n/gr(?:a|e)y/\n\n# Common pattern: match file extensions\n/\\.(?:jpg|jpeg|png|gif|webp)$/i\n</code></pre>"},{"location":"Dev%20Zero/Perl/regular-expressions/#alternation-ordering","title":"Alternation Ordering","text":"<p>The regex engine tries alternatives left to right and takes the first match. This matters when alternatives overlap:</p> <pre><code># \"catfish\" matches \"cat\" (first alternative wins)\n\"catfish\" =~ /cat|catfish/;    # $&amp; is \"cat\"\n\n# Put longer alternatives first\n\"catfish\" =~ /catfish|cat/;    # $&amp; is \"catfish\"\n</code></pre> <p>Alternation vs Character Class</p> <p>For single characters, <code>[abc]</code> is more efficient than <code>a|b|c</code>. Character classes are optimized internally; alternation requires the engine to try each branch.</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#substitution-with-s","title":"Substitution with s///","text":"<p>The <code>s///</code> operator finds a pattern and replaces it:</p> <pre><code>my $str = \"Hello, World!\";\n$str =~ s/World/Perl/;\nprint $str;  # \"Hello, Perl!\"\n</code></pre> <p>The left side is a regex pattern. The right side is a replacement string (not a pattern). Captures from the left side are available in the replacement:</p> <pre><code>my $date = \"01/15/2025\";\n$date =~ s|(\\d{2})/(\\d{2})/(\\d{4})|$3-$1-$2|;\nprint $date;  # \"2025-01-15\"\n</code></pre>"},{"location":"Dev%20Zero/Perl/regular-expressions/#common-s-modifiers","title":"Common s/// Modifiers","text":"<pre><code># /g - replace ALL occurrences (not just the first)\nmy $str = \"aaa bbb aaa\";\n$str =~ s/aaa/zzz/g;\n# \"zzz bbb zzz\"\n\n# /i - case-insensitive match\n$str =~ s/hello/Hi/gi;\n\n# /r - return modified copy, leave original unchanged (5.14+)\nmy $original = \"Hello, World!\";\nmy $modified = $original =~ s/World/Perl/r;\n# $original is still \"Hello, World!\"\n# $modified is \"Hello, Perl!\"\n\n# /e - evaluate replacement as Perl code\nmy $text = \"price: 100 dollars\";\n$text =~ s/(\\d+)/$1 * 1.1/e;\n# \"price: 110 dollars\"\n</code></pre> <p>The <code>/r</code> modifier is particularly valuable in pipelines where you want to transform without mutating:</p> <pre><code>my @lowered = map { s/[A-Z]/lc($&amp;)/ger } @strings;\n</code></pre>"},{"location":"Dev%20Zero/Perl/regular-expressions/#chaining-substitutions","title":"Chaining Substitutions","text":"<p>Multiple <code>s///</code> calls can be chained, especially with <code>/r</code>:</p> <pre><code>my $clean = $input\n    =~ s/^\\s+//r       # trim leading whitespace\n    =~ s/\\s+$//r       # trim trailing whitespace\n    =~ s/\\s+/ /gr;     # collapse internal whitespace\n</code></pre> <p>Substitution Techniques (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#modifiers","title":"Modifiers","text":"<p>Pattern modifiers change how the regex engine interprets a pattern. You have already seen several - here is the complete set:</p> Modifier Effect <code>/i</code> Case-insensitive matching <code>/g</code> Global - match/replace all occurrences <code>/m</code> Multiline - <code>^</code> and <code>$</code> match line boundaries <code>/s</code> Single-line - <code>.</code> matches <code>\\n</code> <code>/x</code> Extended - ignore whitespace, allow comments <code>/e</code> Evaluate replacement as code (s/// only) <code>/r</code> Return modified copy (s/// only, 5.14+) <code>/p</code> Preserve match variables (<code>${^MATCH}</code>, etc.) <code>/a</code> ASCII - <code>\\d</code>, <code>\\w</code>, <code>\\s</code> match ASCII only"},{"location":"Dev%20Zero/Perl/regular-expressions/#the-x-modifier-for-readable-patterns","title":"The /x Modifier for Readable Patterns","text":"<p>Complex patterns become unreadable fast. The <code>/x</code> modifier lets you add whitespace and comments:</p> <pre><code># Without /x - good luck reading this\nmy $email_re = qr/^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$/;\n\n# With /x - same pattern, now readable\nmy $email_re = qr/\n    ^                       # start of string\n    [a-zA-Z0-9._%+-]+      # local part (before @)\n    @                       # literal @ sign\n    [a-zA-Z0-9.-]+         # domain name\n    \\.                      # literal dot\n    [a-zA-Z]{2,}           # TLD (2+ letters)\n    $                       # end of string\n/x;\n</code></pre> <p>With <code>/x</code>, whitespace is ignored (use <code>\\</code> or <code>\\s</code> to match a literal space) and <code>#</code> starts a comment that runs to end of line. This is essential for any pattern longer than about 30 characters.</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#combining-modifiers","title":"Combining Modifiers","text":"<p>Modifiers can be stacked:</p> <pre><code># Case-insensitive global substitution\n$str =~ s/error/WARNING/gi;\n\n# Multiline, extended - parse a multi-line log block\n$block =~ m/\n    ^                   # start of line (thanks to /m)\n    (\\d{4}-\\d{2}-\\d{2}) # date\n    \\s+\n    (\\w+)               # log level\n    \\s+\n    (.+)                # message\n    $                   # end of line\n/xm;\n\n# Single-line mode - match across newlines\n$html =~ m/&lt;div.*?&lt;\\/div&gt;/s;\n</code></pre> <p>Confusing Names: /m and /s</p> <p><code>/m</code> (multiline) changes <code>^</code> and <code>$</code> to match line boundaries. <code>/s</code> (single-line) changes <code>.</code> to match newlines. Despite their names, they are independent - you can use both at once (<code>/ms</code>) when you need <code>.</code> to cross lines AND <code>^</code>/<code>$</code> to match per-line.</p> <p>Command Builder (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#lookahead-and-lookbehind","title":"Lookahead and Lookbehind","text":"<p>Lookaround assertions check whether a pattern exists before or after the current position without including it in the match. They are zero-width - they assert a condition without consuming characters.</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#lookahead","title":"Lookahead","text":"<p>Positive lookahead <code>(?=...)</code> succeeds if the pattern ahead matches:</p> <pre><code># Match \"Perl\" only if followed by a space and a version number\n\"Perl 5.40\" =~ /Perl(?=\\s\\d)/;\n# Matches \"Perl\" (not \"Perl 5\")\n</code></pre> <p>Negative lookahead <code>(?!...)</code> succeeds if the pattern ahead does NOT match:</p> <pre><code># Match a number not followed by a percent sign\n\"50 items, 30% discount\" =~ /\\d+(?!%)/;\n# Matches \"50\" (skips \"30\" because it is followed by %)\n</code></pre>"},{"location":"Dev%20Zero/Perl/regular-expressions/#lookbehind","title":"Lookbehind","text":"<p>Positive lookbehind <code>(?&lt;=...)</code> succeeds if the pattern behind matches:</p> <pre><code># Match a number preceded by a dollar sign\n\"Price: $42.99\" =~ /(?&lt;=\\$)\\d+/;\n# Matches \"42\" (the $ is not part of the match)\n</code></pre> <p>Negative lookbehind <code>(?&lt;!...)</code> succeeds if the pattern behind does NOT match:</p> <pre><code># Match \"cat\" not preceded by \"bob\"\n\"bobcat and cat\" =~ /(?&lt;!bob)cat/;\n# Matches the second \"cat\"\n</code></pre> <p>Lookbehind Length Restriction</p> <p>In Perl, lookbehind patterns must have a fixed or bounded length. You cannot use <code>*</code> or <code>+</code> inside a lookbehind: <code>(?&lt;=\\d+)</code> is not allowed. Use <code>(?&lt;=\\d)</code> or <code>(?&lt;=\\d{1,10})</code> instead. Perl 5.30+ relaxed this for some variable-length lookbehinds, but fixed-length is safest.</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#practical-lookaround-examples","title":"Practical Lookaround Examples","text":"<pre><code># Add commas to a number: 1234567 -&gt; 1,234,567\nmy $num = \"1234567\";\n$num =~ s/(\\d)(?=(\\d{3})+(?!\\d))/$1,/g;\nprint $num;  # \"1,234,567\"\n\n# Extract values from key=value pairs, only if key is \"host\"\nmy $config = \"host=db.example.com port=5432 host=cache.local\";\nmy @hosts = ($config =~ /(?&lt;=host=)\\S+/g);\n# @hosts = (\"db.example.com\", \"cache.local\")\n\n# Password validation: at least one digit and one uppercase\nif ($pass =~ /(?=.*\\d)(?=.*[A-Z]).{8,}/) {\n    print \"Password meets requirements\\n\";\n}\n</code></pre> <p>The password example stacks two positive lookaheads at position 0. Each one scans forward independently to verify a condition, then the final <code>.{8,}</code> actually consumes the string. This \"stacked lookahead\" technique is common for validating multiple conditions simultaneously.</p> <p>What does the pattern /(?&lt;=\\$)\\d+/ match in the string 'Cost: $150 and $25'? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#split-with-regex","title":"split with Regex","text":"<p>The <code>split</code> function divides a string into a list using a regex as the delimiter:</p> <pre><code>my @fields = split /,/, \"Alice,30,Engineer\";\n# @fields = (\"Alice\", \"30\", \"Engineer\")\n\nmy @words = split /\\s+/, \"  hello   world  \";\n# @words = (\"hello\", \"world\")\n</code></pre>"},{"location":"Dev%20Zero/Perl/regular-expressions/#the-limit-parameter","title":"The Limit Parameter","text":"<p>A third argument limits how many fields are returned:</p> <pre><code>my @parts = split /:/, \"one:two:three:four\", 3;\n# @parts = (\"one\", \"two\", \"three:four\")\n# Third element contains the unsplit remainder\n</code></pre>"},{"location":"Dev%20Zero/Perl/regular-expressions/#special-cases","title":"Special Cases","text":"<pre><code># split with no arguments splits $_ on whitespace (like awk)\nfor (\"  Alice   30   Engineer  \") {\n    my @fields = split;\n    # @fields = (\"Alice\", \"30\", \"Engineer\")\n}\n\n# Single-character string splits into characters\nmy @chars = split //, \"hello\";\n# @chars = (\"h\", \"e\", \"l\", \"l\", \"o\")\n\n# split on literal string (not regex)\nmy @parts = split /\\./, \"www.example.com\";\n# @parts = (\"www\", \"example\", \"com\")\n</code></pre>"},{"location":"Dev%20Zero/Perl/regular-expressions/#capturing-separators","title":"Capturing Separators","text":"<p>When the split pattern contains captures, the captured separators are included in the output list:</p> <pre><code>my @tokens = split /(\\s+)/, \"hello   world\";\n# @tokens = (\"hello\", \"   \", \"world\")\n# The captured whitespace appears between the fields\n\n# Useful for preserving formatting\nmy @parts = split /(,\\s*)/, \"a, b,c,  d\";\n# @parts = (\"a\", \", \", \"b\", \",\", \"c\", \",  \", \"d\")\n</code></pre> <p>split vs Regex Match</p> <p>Use <code>split</code> when you know the delimiters and want the content between them. Use a regex match with captures when you know the field format and want to extract specific parts. For parsing a CSV line, <code>split /,/</code> is simpler. For extracting a timestamp from a log line, a pattern with captures is clearer.</p> <p>Apache Log Line Parser (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#compiled-patterns-with-qr","title":"Compiled Patterns with qr//","text":"<p>The <code>qr//</code> operator compiles a regex pattern into a reusable object. This is useful when you need to store patterns in variables, build them dynamically, or reuse them across multiple matches:</p> <pre><code>my $date_re = qr/\\d{4}-\\d{2}-\\d{2}/;\nmy $time_re = qr/\\d{2}:\\d{2}:\\d{2}/;\n\nif ($log_line =~ /^$date_re\\s+$time_re/) {\n    print \"Starts with a timestamp\\n\";\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/regular-expressions/#compiled-patterns-as-building-blocks","title":"Compiled Patterns as Building Blocks","text":"<p>Complex patterns become manageable when assembled from named parts:</p> <pre><code>my $ip_octet = qr/(?:25[0-5]|2[0-4]\\d|[01]?\\d\\d?)/;\nmy $ipv4     = qr/$ip_octet\\.$ip_octet\\.$ip_octet\\.$ip_octet/;\nmy $port     = qr/(?::\\d{1,5})?/;\n\nif ($addr =~ /^$ipv4$port$/) {\n    print \"Valid IPv4 address (with optional port)\\n\";\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/regular-expressions/#modifiers-on-qr","title":"Modifiers on qr//","text":"<p>Modifiers applied to <code>qr//</code> travel with the compiled pattern:</p> <pre><code>my $word = qr/hello/i;       # case-insensitive\n\"HELLO WORLD\" =~ /$word/;    # matches, /i is baked in\n\nmy $verbose_date = qr/\n    (\\d{4})     # year\n    -\n    (\\d{2})     # month\n    -\n    (\\d{2})     # day\n/x;\n</code></pre>"},{"location":"Dev%20Zero/Perl/regular-expressions/#dynamic-pattern-construction","title":"Dynamic Pattern Construction","text":"<p>Build patterns from runtime data - but be careful about metacharacters:</p> <pre><code># DANGEROUS - user input could contain regex metacharacters\nmy $search = \"file.txt\";\n$str =~ /$search/;  # The . matches ANY character\n\n# SAFE - quotemeta escapes metacharacters\n$str =~ /\\Q$search\\E/;  # \\Q...\\E treats content as literal\n\n# Building a pattern from a list of words\nmy @keywords = qw(error warning critical fatal);\nmy $pattern = join '|', map { quotemeta } @keywords;\nmy $re = qr/\\b(?:$pattern)\\b/i;\n\nif ($log =~ $re) {\n    print \"Found a problem keyword\\n\";\n}\n</code></pre> <p>The <code>\\Q...\\E</code> escape sequence (or the <code>quotemeta</code> function) is essential when interpolating user-supplied strings into patterns. Without it, a search for \"file.txt\" would match \"filextxt\" because <code>.</code> is a regex metacharacter.</p> <p>Multi-Format Date Normalizer (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#common-patterns-and-pitfalls","title":"Common Patterns and Pitfalls","text":"<p>Regex is a powerful tool, but it has well-known failure modes. Knowing where regex breaks down is as important as knowing how to write patterns.</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#email-validation-do-not-roll-your-own","title":"Email Validation - Do Not Roll Your Own","text":"<p>The \"simple\" email regex everyone writes is wrong:</p> <pre><code># Looks reasonable, misses edge cases\nmy $email_re = qr/^[\\w.+-]+@[\\w.-]+\\.\\w{2,}$/;\n</code></pre> <p>This rejects valid addresses like <code>\"quoted string\"@example.com</code> and <code>user+tag@[192.168.1.1]</code>. The RFC 5322 email spec is notoriously complex. Use Email::Valid instead:</p> <pre><code>use Email::Valid;\nif (Email::Valid-&gt;address('user@example.com')) {\n    print \"Valid\\n\";\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/regular-expressions/#ip-address-validation","title":"IP Address Validation","text":"<p>A naive IP pattern matches invalid addresses:</p> <pre><code># Matches 999.999.999.999 - not a valid IP\n/\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}/\n</code></pre> <p>A correct pattern validates octet ranges:</p> <pre><code>my $octet = qr/(?:25[0-5]|2[0-4]\\d|[01]?\\d\\d?)/;\nmy $ipv4  = qr/^$octet\\.$octet\\.$octet\\.$octet$/;\n\n# But even better - use a module\nuse Data::Validate::IP;\n</code></pre>"},{"location":"Dev%20Zero/Perl/regular-expressions/#catastrophic-backtracking","title":"Catastrophic Backtracking","text":"<p>Certain patterns cause the regex engine to try an exponential number of paths. The classic example:</p> <pre><code># DO NOT USE - exponential backtracking on non-matching input\n/(a+)+b/\n</code></pre> <p>On a string like <code>\"aaaaaaaaaaaaaaaaac\"</code>, the engine tries every way to partition the <code>a</code> characters between the inner and outer <code>+</code> quantifiers before concluding there is no <code>b</code>. Each additional <code>a</code> doubles the work.</p> <p>Regex Denial of Service</p> <p>Never use untrusted user input as a regex pattern without sanitizing it. A carefully crafted pattern can hang your program. Always use <code>\\Q...\\E</code> or <code>quotemeta()</code> when interpolating user strings into patterns.</p> <p>Signs of backtracking trouble:</p> <ul> <li>Nested quantifiers: <code>(a+)+</code>, <code>(a*)*</code>, <code>(\\w+\\s*)+</code></li> <li>Overlapping alternatives: <code>(a|a)+</code></li> <li>Long strings that almost-but-do-not-quite match</li> </ul> <p>Fixes:</p> <ul> <li>Use possessive quantifiers: <code>(a++)</code> instead of <code>(a+)+</code></li> <li>Use atomic groups: <code>(?&gt;a+)</code> prevents backtracking into the group</li> <li>Restructure the pattern to eliminate ambiguity</li> </ul>"},{"location":"Dev%20Zero/Perl/regular-expressions/#when-to-use-modules-instead-of-hand-rolled-regex","title":"When to Use Modules Instead of Hand-Rolled Regex","text":"Task Module Why Email validation <code>Email::Valid</code> RFC 5322 is too complex for a single regex URL parsing <code>URI</code> Handles schemes, encoding, relative paths HTML parsing <code>HTML::Parser</code>, <code>Mojo::DOM</code> HTML is not a regular language CSV parsing <code>Text::CSV</code> Handles quoting, escaping, edge cases JSON parsing <code>JSON::PP</code>, <code>Cpanel::JSON::XS</code> Regex cannot handle nested structures Date parsing <code>Time::Piece</code>, <code>DateTime</code> Calendar math needs more than pattern matching <p>The Right Tool</p> <p>Regex is perfect for pattern matching in strings - extracting fields, validating simple formats, search-and-replace. It is the wrong tool for parsing recursive structures (HTML, JSON, XML) or validating complex business rules. When you find yourself writing a regex longer than two lines, consider whether a module would be more maintainable.</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#putting-it-all-together","title":"Putting It All Together","text":"<p>Regular expressions in Perl are not just a feature - they are a design philosophy. The language was built around the idea that text processing should be concise and expressive. Here is what you have covered:</p> <ul> <li><code>=~</code> and <code>m//</code> bind strings to patterns and test for matches</li> <li>Character classes define sets of characters to match against</li> <li>Quantifiers control repetition - greedy, non-greedy, and possessive</li> <li>Anchors constrain where patterns match without consuming text</li> <li>Captures extract parts of the match into numbered or named variables</li> <li>Alternation provides or-logic within patterns</li> <li><code>s///</code> replaces matched text with new content</li> <li>Modifiers (<code>/i</code>, <code>/g</code>, <code>/m</code>, <code>/s</code>, <code>/x</code>) change how the engine operates</li> <li>Lookaround assertions check context without consuming characters</li> <li><code>split</code> breaks strings apart using regex delimiters</li> <li><code>qr//</code> compiles and stores patterns for reuse</li> <li>Defensive practices protect against backtracking, metacharacter injection, and hand-rolled validation</li> </ul> <p>The key to writing good regex is the same as writing good code: clarity over cleverness. Use <code>/x</code> for complex patterns. Use named captures for self-documentation. Use modules when the problem outgrows a single pattern. And always test your patterns against both matching and non-matching input.</p>"},{"location":"Dev%20Zero/Perl/regular-expressions/#further-reading","title":"Further Reading","text":"<ul> <li>perlre - Perl Regular Expressions - complete reference for Perl regex syntax and features</li> <li>perlretut - Perl Regular Expressions Tutorial - official tutorial that walks through regex fundamentals</li> <li>perlreref - Perl Regular Expressions Reference - concise quick-reference card for regex syntax</li> <li>perlop - Quote-Like Operators - documentation for <code>m//</code>, <code>s///</code>, <code>qr//</code>, and <code>tr///</code></li> <li>Mastering Regular Expressions, 3rd Edition - Jeffrey Friedl's definitive book on regex engines and optimization</li> <li>PCRE2 Specification - the Perl-compatible regex library used by most modern languages</li> </ul> <p>Previous: Control Flow | Next: Subroutines and References | Back to Index</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/","title":"Scalars, Strings, and Numbers","text":""},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#perls-building-blocks-one-thing-at-a-time","title":"Perl's Building Blocks: One Thing at a Time","text":"<p>Version: 1.1 Year: 2026</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#copyright-notice","title":"Copyright Notice","text":"<p>Copyright (c) 2025-2026 Ryan Thomas Robson / Robworks Software LLC. Licensed under CC BY-NC-ND 4.0. You may share this material for non-commercial purposes with attribution, but you may not distribute modified versions.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#variables-and-sigils","title":"Variables and Sigils","text":"<p>Every variable in Perl starts with a punctuation character called a sigil. The sigil tells you what kind of data the variable holds:</p> Sigil Type Example <code>$</code> Scalar (single value) <code>$name</code> <code>@</code> Array (ordered list) <code>@items</code> <code>%</code> Hash (key-value pairs) <code>%config</code> <p>This guide focuses entirely on the <code>$</code> sigil - the scalar. A scalar holds exactly one value: a string, a number, a reference, or the special value <code>undef</code>. Perl does not have separate types for integers, floats, and strings the way most languages do. A scalar is whatever you need it to be, and Perl converts between types automatically based on context.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#declaring-variables","title":"Declaring Variables","text":"<p>You declare a lexical variable with the <code>my</code> keyword:</p> <pre><code>my $username = \"ringo\";\nmy $port     = 8080;\nmy $pi       = 3.14159;\nmy $empty;               # declared but undefined (value is undef)\n</code></pre> <p>The <code>my</code> keyword limits the variable's visibility to the enclosing block (a pair of curly braces, a file, or an <code>eval</code>). This is called lexical scoping, and it prevents variables from leaking into unrelated code.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#use-strict-and-use-warnings","title":"use strict and use warnings","text":"<p>Every Perl script you write should start with these two lines:</p> <pre><code>use strict;\nuse warnings;\n</code></pre> <p><code>use strict</code> forces you to declare variables before using them. Without it, a typo like <code>$uesrname</code> silently creates a new global variable instead of raising an error.</p> <p><code>use warnings</code> alerts you to common mistakes: using an uninitialized variable, treating a string as a number when it does not look like one, or using a deprecated feature. These are not fatal errors - your program still runs - but ignoring them is asking for trouble.</p> <pre><code>use strict;\nuse warnings;\n\nmy $name = \"Perl\";\nprint \"Hello, $name!\\n\";\n\n# Without 'my', strict would stop this cold:\n# $oops = 42;  # Global symbol \"$oops\" requires explicit package name\n</code></pre> <p>Always use strict and warnings</p> <p>Code examples in documentation sometimes omit <code>use strict</code> and <code>use warnings</code> for brevity. Your actual scripts should always include them. The minor inconvenience of declaring variables saves hours of debugging.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#naming-conventions","title":"Naming Conventions","text":"<p>Variable names follow these rules:</p> <ul> <li>Must start with a letter or underscore after the sigil</li> <li>Can contain letters, digits, and underscores</li> <li>Are case-sensitive (<code>$Name</code> and <code>$name</code> are different variables)</li> <li>By convention, use lowercase with underscores: <code>$first_name</code>, <code>$max_retry_count</code></li> </ul> <pre><code>my $user_name   = \"admin\";     # good - clear and readable\nmy $userName    = \"admin\";     # works but not idiomatic Perl\nmy $x           = \"admin\";     # too terse - what does $x mean?\nmy $2nd_attempt = 1;           # INVALID - can't start with a digit\n</code></pre> <p>Your First Perl Variables (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#strings","title":"Strings","text":"<p>Perl has two quoting styles for strings, and the difference matters.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#single-quoted-strings","title":"Single-Quoted Strings","text":"<p>Single-quoted strings are literal. Perl takes the characters exactly as written, with two exceptions: <code>\\\\</code> produces a single backslash, and <code>\\'</code> produces a single quote.</p> <pre><code>my $path = '/usr/local/bin';\nmy $msg  = 'The variable $name is not interpolated here';\nmy $escaped = 'It\\'s a backslash: \\\\';\n\nprint $path, \"\\n\";     # /usr/local/bin\nprint $msg, \"\\n\";      # The variable $name is not interpolated here\nprint $escaped, \"\\n\";  # It's a backslash: \\\n</code></pre> <p>Use single quotes when you want the string stored exactly as typed - file paths, regex patterns, or any string that should not have variable substitution.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#double-quoted-strings","title":"Double-Quoted Strings","text":"<p>Double-quoted strings interpret escape sequences and interpolate variables:</p> <pre><code>my $user = \"admin\";\nmy $home = \"/home/$user\";       # /home/admin\nmy $msg  = \"Hello, ${user}!\\n\"; # Hello, admin! (with newline)\n</code></pre> <p>Common escape sequences in double-quoted strings:</p> Sequence Meaning <code>\\n</code> Newline <code>\\t</code> Tab <code>\\r</code> Carriage return <code>\\\\</code> Literal backslash <code>\\\"</code> Literal double quote <code>\\$</code> Literal dollar sign (prevents interpolation) <code>\\@</code> Literal at sign (prevents interpolation) <code>\\x{4F60}</code> Unicode character by hex code point <code>\\0</code> Null byte <p>The curly braces in <code>${user}</code> are optional when the variable name is unambiguous, but they help when the variable is followed by text that could be mistaken for part of the name:</p> <pre><code>my $fruit = \"apple\";\nprint \"I like ${fruit}s\\n\";  # I like apples\nprint \"I like $fruits\\n\";    # Perl looks for $fruits, not $fruit\n</code></pre>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#heredocs","title":"Heredocs","text":"<p>For multi-line strings, Perl provides heredocs (here-documents). The syntax is <code>&lt;&lt;IDENTIFIER</code>, and the string continues until a line containing only the identifier:</p> <pre><code>my $html = &lt;&lt;HTML;\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;$title&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;Welcome&lt;/h1&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nHTML\n</code></pre> <p>The identifier can be any word. By convention, it describes the content: <code>HTML</code>, <code>SQL</code>, <code>EOF</code>, <code>END</code>. The rules for quoting apply to the identifier:</p> Syntax Interpolation Example <code>&lt;&lt;EOF</code> Yes (like double quotes) <code>&lt;&lt;EOF</code> <code>&lt;&lt;\"EOF\"</code> Yes (like double quotes) <code>&lt;&lt;\"EOF\"</code> <code>&lt;&lt;'EOF'</code> No (like single quotes) <code>&lt;&lt;'EOF'</code> <p>Perl 5.26 introduced the indented heredoc with <code>&lt;&lt;~</code>, which strips leading whitespace from each line based on the indentation of the closing identifier:</p> <pre><code>sub generate_config {\n    my $config = &lt;&lt;~CONF;\n        server {\n            listen 80;\n            root /var/www;\n        }\n        CONF\n    return $config;\n}\n</code></pre> <p>Without the tilde, every line would include the leading spaces. With <code>&lt;&lt;~CONF</code>, Perl removes the common leading whitespace, producing clean output.</p> <p>What is the output of: my $x = 'Hello'; print \"$x World\\n\"; (requires JavaScript)</p> <p>String Interpolation and Heredocs (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#numbers","title":"Numbers","text":"<p>Perl does not distinguish between integers and floating-point numbers at the language level. A scalar holds a number, and Perl stores it internally as whatever format the operation requires - an integer (<code>IV</code>), an unsigned integer (<code>UV</code>), or a double-precision float (<code>NV</code>).</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#integer-literals","title":"Integer Literals","text":"<p>You can write integers in several bases:</p> <pre><code>my $decimal = 255;        # base 10\nmy $hex     = 0xFF;       # base 16 (hexadecimal)\nmy $octal   = 0377;       # base 8 (octal, leading zero)\nmy $binary  = 0b11111111; # base 2 (binary)\n</code></pre> <p>All four variables above hold the same value: 255.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#underscores-in-numbers","title":"Underscores in Numbers","text":"<p>For readability, Perl lets you use underscores as visual separators within numeric literals. The underscores are ignored by the parser:</p> <pre><code>my $population = 7_900_000_000;  # 7.9 billion\nmy $hex_color  = 0xFF_AA_00;     # easier to read hex\nmy $binary     = 0b1111_0000;    # nibble boundaries\nmy $precise    = 3.141_592_653;  # group decimal places\n</code></pre>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#floating-point-numbers","title":"Floating-Point Numbers","text":"<p>Floating-point numbers use the standard decimal notation or scientific notation:</p> <pre><code>my $pi      = 3.14159;\nmy $avogadro = 6.022e23;   # 6.022 * 10^23\nmy $tiny     = 1.6e-19;    # 1.6 * 10^-19\nmy $negative = -273.15;\n</code></pre> <p>Floating-point precision</p> <p>Like every language that uses IEEE 754 doubles, Perl has floating-point precision limits. The expression <code>0.1 + 0.2 == 0.3</code> evaluates to false because <code>0.1 + 0.2</code> is actually <code>0.30000000000000004</code>. For financial calculations or exact decimal arithmetic, use the <code>Math::BigFloat</code> module from CPAN.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#numeric-operators","title":"Numeric Operators","text":"<p>Perl provides the standard arithmetic operators:</p> Operator Meaning Example Result <code>+</code> Addition <code>7 + 3</code> <code>10</code> <code>-</code> Subtraction <code>7 - 3</code> <code>4</code> <code>*</code> Multiplication <code>7 * 3</code> <code>21</code> <code>/</code> Division <code>7 / 3</code> <code>2.33333...</code> <code>%</code> Modulus <code>7 % 3</code> <code>1</code> <code>**</code> Exponentiation <code>2 ** 10</code> <code>1024</code> <p>Perl also has shorthand assignment operators: <code>+=</code>, <code>-=</code>, <code>*=</code>, <code>/=</code>, <code>%=</code>, <code>**=</code>, and the auto-increment/decrement operators <code>++</code> and <code>--</code>.</p> <pre><code>my $count = 10;\n$count += 5;    # $count is now 15\n$count *= 2;    # $count is now 30\n$count++;       # $count is now 31\n$count--;       # $count is now 30\n</code></pre>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#numeric-comparison-operators","title":"Numeric Comparison Operators","text":"<p>Perl has separate comparison operators for numbers and strings. Using the wrong one is a common source of bugs:</p> Operation Numeric String Equal <code>==</code> <code>eq</code> Not equal <code>!=</code> <code>ne</code> Less than <code>&lt;</code> <code>lt</code> Greater than <code>&gt;</code> <code>gt</code> Less than or equal <code>&lt;=</code> <code>le</code> Greater than or equal <code>&gt;=</code> <code>ge</code> Comparison (spaceship) <code>&lt;=&gt;</code> <code>cmp</code> <pre><code># Numeric comparison - compares values as numbers\n42 == 42.0;       # true\n\"42\" == \"42.0\";   # true (both strings become the number 42)\n\n# String comparison - compares character by character\n\"42\" eq \"42.0\";   # false (\"42\" and \"42.0\" are different strings)\n\"abc\" lt \"abd\";   # true (\"c\" comes before \"d\")\n</code></pre> <p>The spaceship and cmp operators</p> <p><code>&lt;=&gt;</code> and <code>cmp</code> return -1, 0, or 1 depending on whether the left operand is less than, equal to, or greater than the right operand. These are primarily used with <code>sort</code>: <code>sort { $a &lt;=&gt; $b } @numbers</code> sorts numerically, while <code>sort { $a cmp $b } @strings</code> sorts lexically.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#context","title":"Context","text":"<p>Context is one of the most important concepts in Perl. It determines how expressions behave - the same expression can produce different results depending on what surrounds it. There are three primary contexts.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#scalar-context","title":"Scalar Context","text":"<p>When Perl expects a single value, it imposes scalar context. Assignment to a scalar variable is the most common trigger:</p> <pre><code>my @colors = (\"red\", \"green\", \"blue\");\nmy $count  = @colors;  # scalar context: @colors returns 3 (element count)\n</code></pre> <p>An array in scalar context does not return its first element or a stringified version of its contents. It returns the number of elements it contains.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#list-context","title":"List Context","text":"<p>When Perl expects multiple values, it imposes list context. Assignment to an array or a list of variables triggers it:</p> <pre><code>my @colors = (\"red\", \"green\", \"blue\");  # list context on the right side\nmy ($first, $second) = @colors;         # list context: $first=\"red\", $second=\"green\"\n</code></pre>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#boolean-context","title":"Boolean Context","text":"<p>Conditional expressions (<code>if</code>, <code>while</code>, <code>unless</code>, <code>until</code>) impose boolean context. Perl evaluates the expression and determines whether it is true or false according to simple rules covered in the Undef and Truthiness section below.</p> <pre><code>my @items = (1, 2, 3);\nif (@items) {           # boolean context: @items returns 3, which is true\n    print \"Not empty\\n\";\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#the-scalar-function","title":"The scalar() Function","text":"<p>You can force scalar context explicitly with the <code>scalar()</code> function:</p> <pre><code>my @colors = (\"red\", \"green\", \"blue\");\nprint \"Colors: \", scalar(@colors), \"\\n\";  # Colors: 3\n</code></pre> <p>Without <code>scalar()</code>, <code>print</code> imposes list context on <code>@colors</code>, and you would get the elements printed as a concatenated string. With <code>scalar()</code>, you get the count.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#how-perl-determines-context","title":"How Perl Determines Context","text":"<p>The surrounding code - the operator, function, or assignment target - dictates context. This flowchart summarizes the decision process:</p> <p>What does Perl's 'context' mean for the expression: my @arr = (1,2,3); my $x = @arr; (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#string-operations","title":"String Operations","text":"<p>Perl provides a rich set of built-in functions for working with strings. These are some of the most frequently used in day-to-day programming.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#length","title":"length","text":"<p><code>length</code> returns the number of characters in a string:</p> <pre><code>my $str = \"Hello, World!\";\nprint length($str);  # 13\n</code></pre> <p>For byte length on Unicode strings, use <code>bytes::length()</code> or <code>use bytes</code> within a scope. For character semantics (which is almost always what you want), plain <code>length</code> works correctly with <code>use utf8</code>.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#substr","title":"substr","text":"<p><code>substr</code> extracts or replaces a portion of a string:</p> <pre><code>my $str = \"Hello, World!\";\n\n# Extract: substr(STRING, OFFSET, LENGTH)\nmy $word = substr($str, 7, 5);  # \"World\"\n\n# Negative offset counts from the end\nmy $last = substr($str, -6);     # \"orld!\"\n\n# Replace: substr as an lvalue\nsubstr($str, 0, 5) = \"Goodbye\";  # $str is now \"Goodbye, World!\"\n\n# Four-argument form: substr(STRING, OFFSET, LENGTH, REPLACEMENT)\nmy $old = substr($str, 0, 7, \"Hello\");  # $old = \"Goodbye\", $str = \"Hello, World!\"\n</code></pre>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#index-and-rindex","title":"index and rindex","text":"<p><code>index</code> finds the first occurrence of a substring and returns its position (0-based). <code>rindex</code> finds the last occurrence. Both return -1 if the substring is not found:</p> <pre><code>my $path = \"/home/admin/docs/admin/notes.txt\";\nprint index($path, \"admin\");    # 6  (first occurrence)\nprint rindex($path, \"admin\");   # 18 (last occurrence)\nprint index($path, \"nobody\");   # -1 (not found)\n\n# Optional third argument: starting position\nprint index($path, \"admin\", 7); # 18 (start searching from position 7)\n</code></pre>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#chomp-and-chop","title":"chomp and chop","text":"<p><code>chomp</code> removes the trailing input record separator (usually a newline) from a string and returns the number of characters removed. <code>chop</code> unconditionally removes the last character:</p> <pre><code>my $line = \"data\\n\";\nchomp $line;          # $line is now \"data\", returns 1\nchomp $line;          # $line is still \"data\", returns 0 (no newline to remove)\n\nmy $str = \"Hello!\";\nchop $str;            # $str is now \"Hello\", returns \"!\"\n</code></pre> <p>Use <code>chomp</code> when processing input (it is safe to call even when there is no trailing newline). Reserve <code>chop</code> for when you specifically need to remove the last character regardless of what it is.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#case-conversion","title":"Case Conversion","text":"<p>Perl provides four functions for changing string case:</p> Function Effect Example <code>lc</code> Lowercase entire string <code>lc(\"HELLO\")</code> returns <code>\"hello\"</code> <code>uc</code> Uppercase entire string <code>uc(\"hello\")</code> returns <code>\"HELLO\"</code> <code>lcfirst</code> Lowercase first character <code>lcfirst(\"HELLO\")</code> returns <code>\"hELLO\"</code> <code>ucfirst</code> Uppercase first character <code>ucfirst(\"hello\")</code> returns <code>\"Hello\"</code> <pre><code>my $name = \"perl\";\nprint ucfirst($name);   # Perl\nprint uc($name);         # PERL\n</code></pre>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#concatenation-and-repetition","title":"Concatenation and Repetition","text":"<p>The dot operator (<code>.</code>) concatenates strings. The <code>x</code> operator repeats a string:</p> <pre><code>my $greeting = \"Hello\" . \", \" . \"World!\";   # \"Hello, World!\"\nmy $line     = \"-\" x 40;                     # forty dashes\nmy $padding  = \" \" x 8;                      # eight spaces\n\n# .= appends to an existing string\nmy $log = \"Step 1 done\";\n$log .= \"; Step 2 done\";  # \"Step 1 done; Step 2 done\"\n</code></pre> <p>String Manipulation in Action (requires JavaScript)</p> <p>Name and Greeting Formatter (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#undef-and-truthiness","title":"Undef and Truthiness","text":""},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#what-undef-is","title":"What undef Is","text":"<p><code>undef</code> is Perl's representation of \"no value.\" A variable that has been declared but not assigned holds <code>undef</code>:</p> <pre><code>my $x;               # $x is undef\nmy $y = undef;       # explicit, same result\n</code></pre> <p>Using <code>undef</code> in a numeric context produces <code>0</code>. In a string context, it produces the empty string <code>\"\"</code>. In both cases, if <code>use warnings</code> is active, Perl emits a \"Use of uninitialized value\" warning.</p> <pre><code>use warnings;\nmy $x;\nprint $x + 1;   # prints 1, but warns: Use of uninitialized value $x in addition\nprint $x . \"!\"; # prints !, but warns: Use of uninitialized value $x in concatenation\n</code></pre>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#perls-truth-rules","title":"Perl's Truth Rules","text":"<p>Perl does not have a dedicated boolean type. Any scalar value is either true or false. The rules are simple - exactly four values are false:</p> Value Why it is false <code>undef</code> No value <code>\"\"</code> (empty string) Empty <code>0</code> (the number zero) Zero <code>\"0\"</code> (the string containing only a zero) Stringifies to zero <p>Everything else is true. This includes <code>\"0.0\"</code>, <code>\" \"</code> (a space), <code>\"00\"</code> (two zeros), negative numbers, and strings like <code>\"false\"</code>. If this seems surprising, remember: only the four values in the table above are false.</p> <pre><code># All of these are TRUE in Perl:\nif (\"false\")   { print \"The string 'false' is true!\\n\"; }\nif (\"0.0\")     { print \"The string '0.0' is true!\\n\"; }\nif (-1)        { print \"Negative numbers are true!\\n\"; }\nif (\"00\")      { print \"The string '00' is true!\\n\"; }\nif (\" \")       { print \"A space is true!\\n\"; }\n\n# All of these are FALSE:\nif (0)     { }   # false - the number zero\nif (\"\")    { }   # false - empty string\nif (undef) { }   # false - no value\nif (\"0\")   { }   # false - the string \"0\"\n</code></pre> <p>The 'not' operator</p> <p>The <code>!</code> operator (or the <code>not</code> keyword) inverts a boolean value. <code>!0</code> is <code>1</code> (true), and <code>!1</code> is <code>\"\"</code> (false, the empty string). The <code>!!</code> double-negation trick forces any value into <code>1</code> or <code>\"\"</code>.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#the-defined-function","title":"The defined() Function","text":"<p>The <code>defined()</code> function tells you whether a scalar has a value (even if that value is false):</p> <pre><code>my $zero  = 0;\nmy $empty = \"\";\nmy $nope;         # undef\n\nprint defined($zero)  ? \"yes\" : \"no\";  # yes - 0 is defined\nprint defined($empty) ? \"yes\" : \"no\";  # yes - \"\" is defined\nprint defined($nope)  ? \"yes\" : \"no\";  # no  - undef is not defined\n</code></pre> <p>Use <code>defined()</code> when you need to distinguish between \"a value was provided\" and \"no value exists.\" A function returning <code>0</code> is different from a function returning <code>undef</code> - the first succeeded with a zero result, the second might indicate an error.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#the-defined-or-operator","title":"The Defined-Or Operator","text":"<p>Perl 5.10 introduced the defined-or operator <code>//</code>, which returns the left operand if it is defined, otherwise the right operand:</p> <pre><code>my $port = $config_port // 8080;  # use $config_port if defined, otherwise 8080\n</code></pre> <p>This is different from <code>||</code> (logical or), which tests for truth rather than definedness. If <code>$config_port</code> is <code>0</code> (a valid port? no, but a valid number), <code>||</code> would skip it because <code>0</code> is false, while <code>//</code> would keep it because <code>0</code> is defined:</p> <pre><code>my $val = 0;\nmy $a = $val || 42;   # $a = 42 (0 is false, so || uses the right side)\nmy $b = $val // 42;   # $b = 0  (0 is defined, so // uses the left side)\n</code></pre>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#numeric-functions","title":"Numeric Functions","text":"<p>Perl provides built-in functions for common mathematical operations. For anything beyond the basics, the <code>POSIX</code> module and CPAN modules like <code>Math::Trig</code> extend the repertoire.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#abs-int-sqrt","title":"abs, int, sqrt","text":"<pre><code>print abs(-42);       # 42 - absolute value\nprint int(3.999);     # 3  - truncates toward zero (does NOT round)\nprint int(-3.999);    # -3 - truncates toward zero\nprint sqrt(144);      # 12 - square root\n</code></pre> <p><code>abs</code> returns the absolute value. <code>int</code> truncates toward zero - it drops the decimal part without rounding. <code>sqrt</code> returns the square root.</p> <p>int() does not round</p> <p>A common mistake is using <code>int()</code> to round numbers. <code>int(2.9)</code> returns <code>2</code>, not <code>3</code>. To round to the nearest integer, use <code>int($n + 0.5)</code> for positive numbers, or <code>sprintf(\"%.0f\", $n)</code> for general rounding, or the <code>POSIX::round</code> function.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#rand-and-srand","title":"rand and srand","text":"<p><code>rand</code> returns a random floating-point number from 0 (inclusive) to the given value (exclusive). Without an argument, it returns a value between 0 and 1:</p> <pre><code>my $roll  = int(rand(6)) + 1;    # random integer 1-6 (simulates a die)\nmy $coin  = rand() &lt; 0.5 ? \"heads\" : \"tails\";\nmy $pct   = rand(100);           # 0 &lt;= $pct &lt; 100\n</code></pre> <p><code>srand</code> seeds the random number generator. Perl calls it automatically the first time you use <code>rand</code>, but you can call it explicitly for reproducible sequences:</p> <pre><code>srand(42);          # set seed for reproducible results\nprint rand(), \"\\n\"; # same value every time with seed 42\n</code></pre> <p>Not cryptographically secure</p> <p><code>rand()</code> uses a predictable pseudo-random number generator. For security-sensitive work (tokens, passwords, nonces), use <code>Crypt::URandom</code> or read from <code>/dev/urandom</code> directly.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#hex-and-oct","title":"hex and oct","text":"<p><code>hex</code> converts a hexadecimal string to a number. <code>oct</code> converts a string with a base prefix (<code>0x</code>, <code>0b</code>, <code>0</code>) to a number:</p> <pre><code>print hex(\"ff\");     # 255\nprint hex(\"0xFF\");   # 255\n\nprint oct(\"377\");    # 255 (octal)\nprint oct(\"0xFF\");   # 255 (hex, via prefix)\nprint oct(\"0b11111111\"); # 255 (binary, via prefix)\n</code></pre> <p>Despite its name, <code>oct</code> handles hex and binary prefixes too. It examines the prefix to determine the base.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#sprintf-for-formatting","title":"sprintf for Formatting","text":"<p><code>sprintf</code> returns a formatted string without printing it (unlike <code>printf</code>, which prints directly). The format codes follow C conventions:</p> <pre><code># Decimal places\nmy $pi = sprintf(\"%.4f\", 3.14159265);   # \"3.1416\"\n\n# Padding\nmy $padded = sprintf(\"%010d\", 42);       # \"0000000042\"\n\n# Multiple values\nmy $msg = sprintf(\"%-20s %5d %8.2f\", \"Widget\", 100, 29.95);\n# \"Widget                 100    29.95\"\n\n# Hex and octal output\nmy $hex = sprintf(\"%x\", 255);    # \"ff\"\nmy $oct = sprintf(\"%o\", 255);    # \"377\"\nmy $bin = sprintf(\"%b\", 255);    # \"11111111\"\n</code></pre> <p>Common format specifiers:</p> Code Type Example <code>%d</code> Decimal integer <code>sprintf(\"%d\", 42)</code> returns <code>\"42\"</code> <code>%f</code> Floating point <code>sprintf(\"%.2f\", 3.14)</code> returns <code>\"3.14\"</code> <code>%e</code> Scientific notation <code>sprintf(\"%e\", 12345)</code> returns <code>\"1.234500e+04\"</code> <code>%s</code> String <code>sprintf(\"%-10s\", \"hi\")</code> returns <code>\"hi        \"</code> <code>%x</code> Hexadecimal (lowercase) <code>sprintf(\"%x\", 255)</code> returns <code>\"ff\"</code> <code>%o</code> Octal <code>sprintf(\"%o\", 8)</code> returns <code>\"10\"</code> <code>%b</code> Binary <code>sprintf(\"%b\", 10)</code> returns <code>\"1010\"</code> <p>Command Builder (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#special-variables","title":"Special Variables","text":"<p>Perl has dozens of built-in special variables that control the interpreter's behavior and provide access to runtime information. These are documented in perlvar. Here are the ones you will encounter most often when working with scalars.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#_-the-default-variable","title":"$_ - The Default Variable","text":"<p><code>$_</code> is the default variable. Many Perl functions and constructs operate on <code>$_</code> when you do not specify an explicit argument:</p> <pre><code># Without $_\nfor my $item (@list) {\n    print $item, \"\\n\";\n}\n\n# With $_ (implicit)\nfor (@list) {\n    print $_, \"\\n\";     # $_ holds each element in turn\n}\n\n# Even shorter - print defaults to $_\nfor (@list) {\n    print;              # prints $_ followed by nothing (no newline!)\n}\n</code></pre> <p>Functions that default to <code>$_</code> include <code>chomp</code>, <code>chop</code>, <code>print</code>, <code>length</code>, <code>lc</code>, <code>uc</code>, <code>defined</code>, <code>ref</code>, and many others. Loop constructs like <code>for</code>, <code>foreach</code>, <code>while (&lt;FILEHANDLE&gt;)</code>, and <code>map</code>/<code>grep</code> set <code>$_</code> implicitly.</p> <pre><code># $_ in a while-read loop\nwhile (&lt;STDIN&gt;) {\n    chomp;              # chomp($_)\n    next if /^#/;       # skip comments - regex matches against $_\n    print length, \"\\n\"; # length($_)\n}\n</code></pre> <p>When to use <code>$_</code> and when not to</p> <p><code>$_</code> makes short code concise, especially one-liners and <code>map</code>/<code>grep</code> blocks. In longer code, named variables are clearer. If you find yourself wondering \"what does <code>$_</code> refer to here?\", use an explicit variable instead.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#-system-error","title":"$! - System Error","text":"<p><code>$!</code> holds the current system error message (from the C <code>errno</code> variable). Check it immediately after a system call fails:</p> <pre><code>open(my $fh, '&lt;', '/etc/shadow') or die \"Cannot open: $!\\n\";\n# If the open fails: \"Cannot open: Permission denied\"\n</code></pre> <p>In numeric context, <code>$!</code> gives the errno number. In string context, it gives the human-readable message.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#-eval-error","title":"$@ - Eval Error","text":"<p><code>$@</code> holds the error message from the most recent <code>eval</code> block or <code>die</code>:</p> <pre><code>eval {\n    die \"Something went wrong\";\n};\nif ($@) {\n    print \"Caught error: $@\\n\";\n    # \"Caught error: Something went wrong at script.pl line 2.\"\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#-input-record-separator","title":"$/ - Input Record Separator","text":"<p><code>$/</code> defines what Perl considers a \"line\" when reading from a filehandle. It defaults to <code>\"\\n\"</code>. Change it to read records delimited by something other than newlines, or set it to <code>undef</code> to read an entire file at once:</p> <pre><code># Slurp entire file into a single string\nmy $contents;\n{\n    local $/;   # temporarily undefine $/ within this block\n    open(my $fh, '&lt;', 'data.txt') or die \"Cannot open: $!\\n\";\n    $contents = &lt;$fh&gt;;\n    close $fh;\n}\n</code></pre> <p>The <code>local</code> keyword temporarily overrides <code>$/</code> for the duration of the block. When the block exits, <code>$/</code> reverts to its previous value. This is the idiomatic way to modify global special variables safely.</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#-output-record-separator","title":"$\\ - Output Record Separator","text":"<p><code>$\\</code> is appended to every <code>print</code> output. It defaults to <code>undef</code> (nothing appended). Set it to <code>\"\\n\"</code> if you want every <code>print</code> to automatically add a newline:</p> <pre><code>{\n    local $\\ = \"\\n\";\n    print \"First line\";    # prints \"First line\\n\"\n    print \"Second line\";   # prints \"Second line\\n\"\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#-line-number","title":"$. - Line Number","text":"<p><code>$.</code> tracks the current line number of the most recently read filehandle:</p> <pre><code>open(my $fh, '&lt;', 'config.txt') or die \"Cannot open: $!\\n\";\nwhile (&lt;$fh&gt;) {\n    print \"$.: $_\" if /error/i;  # prints line number and matching line\n}\nclose $fh;\n</code></pre> <p><code>$.</code> resets when you explicitly close a filehandle, but not when you open a new one without closing the old. Use <code>close</code> explicitly to keep <code>$.</code> predictable.</p> <p>Temperature Converter (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/scalars-strings-numbers/#further-reading","title":"Further Reading","text":"<ul> <li>perldata - Perl data types documentation</li> <li>perlop - Perl operators and precedence</li> <li>perlvar - Complete list of Perl special variables</li> <li>perlfunc - Perl built-in functions reference</li> <li>perlnumber - Perl number semantics</li> <li>Learning Perl, 8th Edition - Chapters 2-3 cover scalars, strings, and numbers in depth</li> <li>Programming Perl, 4th Edition - The \"Camel Book,\" comprehensive language reference</li> </ul> <p>Previous: Introduction: Why Perl, and Why Unix First | Next: Arrays, Hashes, and Lists | Back to Index</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/","title":"Subroutines and References","text":""},{"location":"Dev%20Zero/Perl/subroutines-references/#building-reusable-code-and-complex-data","title":"Building Reusable Code and Complex Data","text":"<p>Version: 1.1 Year: 2026</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#copyright-notice","title":"Copyright Notice","text":"<p>Copyright (c) 2025-2026 Ryan Thomas Robson / Robworks Software LLC. Licensed under CC BY-NC-ND 4.0. You may share this material for non-commercial purposes with attribution, but you may not distribute modified versions.</p> <p>Once your programs grow beyond a screenful of code, you need a way to organize logic into reusable pieces. Subroutines are Perl's tool for this - named blocks of code you can call from anywhere. And once you start passing data between subroutines, you run straight into Perl's flat-list problem: arrays and hashes flatten when passed as arguments. References solve this by letting a single scalar point to an entire data structure. Together, subroutines and references unlock closures, callbacks, dispatch tables, and every complex data pattern Perl supports.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#subroutines","title":"Subroutines","text":"<p>A subroutine is declared with <code>sub</code>:</p> <pre><code>sub greet {\n    print \"Hello, world!\\n\";\n}\n</code></pre> <p>Call it by name with parentheses:</p> <pre><code>greet();\n</code></pre> <p>You can also call a subroutine with the <code>&amp;</code> sigil - <code>&amp;greet()</code> - but this is rarely needed in modern Perl. The <code>&amp;</code> form has special behavior: calling <code>&amp;greet</code> without parentheses passes the current <code>@_</code> to the sub, which is almost never what you want. Stick with <code>greet()</code>.</p> <pre><code># These are equivalent\ngreet();\n&amp;greet();\n\n# This is different - passes the caller's @_ into greet\n&amp;greet;\n</code></pre> <p>Forward Declarations</p> <p>If you call a subroutine before its definition appears in the file, Perl will find it as long as it is defined somewhere in the same package. You do not need forward declarations for simple cases. However, if you use prototypes or want to call a sub without parentheses like a built-in, you need to declare or define it before the call site.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#arguments-and-_","title":"Arguments and @_","text":"<p>When you call a subroutine with arguments, Perl places them in the special array <code>@_</code>:</p> <pre><code>sub greet_user {\n    my $name = $_[0];\n    print \"Hello, $name!\\n\";\n}\n\ngreet_user(\"Alice\");   # Hello, Alice!\n</code></pre> <p>Two idiomatic patterns dominate argument handling:</p> <p>The <code>shift</code> idiom - pull arguments off one at a time:</p> <pre><code>sub greet_user {\n    my $name = shift;\n    print \"Hello, $name!\\n\";\n}\n</code></pre> <p>When called without arguments, <code>shift</code> operates on <code>@_</code> inside a subroutine. This is the most common pattern for subs with one or two arguments.</p> <p>List assignment - unpack all arguments at once:</p> <pre><code>sub add {\n    my ($x, $y) = @_;\n    return $x + $y;\n}\n\nprint add(3, 7);   # 10\n</code></pre> <p>Use list assignment when you have multiple parameters - it documents the expected arguments clearly.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#_-is-an-alias","title":"@_ Is an Alias","text":"<p>Here is the critical detail: <code>@_</code> does not contain copies of the arguments. Each element of <code>@_</code> is an alias to the original variable. Modifying <code>$_[0]</code> changes the caller's variable:</p> <pre><code>sub double_in_place {\n    $_[0] *= 2;\n}\n\nmy $val = 5;\ndouble_in_place($val);\nprint $val;   # 10 - the original changed!\n</code></pre> <p>This aliasing behavior is efficient (no copying), but dangerous if you are not expecting it. The <code>my ($x, $y) = @_</code> pattern creates local copies, which is why it is preferred for safety.</p> <p>Modifying Literal Arguments</p> <p>If you call <code>double_in_place(5)</code> with a literal, Perl will throw an error: \"Modification of a read-only value attempted.\" You can only modify <code>$_[0]</code> when the caller passed a variable.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#no-argument-checking","title":"No Argument Checking","text":"<p>Perl does not enforce argument counts. Call a sub with too few arguments and the extras are <code>undef</code>. Call with too many and the extras sit unused in <code>@_</code>:</p> <pre><code>sub needs_two {\n    my ($a, $b) = @_;\n    return $a + $b;\n}\n\nneeds_two(1);         # $b is undef - warning under 'use warnings'\nneeds_two(1, 2, 3);   # 3 is silently ignored\n</code></pre> <p>This flexibility is intentional - it enables variadic subs and optional arguments - but it means you should validate arguments yourself when correctness matters.</p> <p>Subroutine Basics (requires JavaScript)</p> <p>What happens when you modify $_[0] inside a subroutine? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#return-values","title":"Return Values","text":"<p>Subroutines return values with <code>return</code>:</p> <pre><code>sub square {\n    my $n = shift;\n    return $n * $n;\n}\n\nmy $result = square(5);   # 25\n</code></pre> <p>If you omit <code>return</code>, the sub returns the value of the last evaluated expression:</p> <pre><code>sub square {\n    my $n = shift;\n    $n * $n;    # implicitly returned\n}\n</code></pre> <p>Both styles are common. Explicit <code>return</code> is clearer for complex subs. Implicit return is idiomatic for short, simple subs.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#returning-lists","title":"Returning Lists","text":"<p>A sub can return a list:</p> <pre><code>sub min_max {\n    my @nums = sort { $a &lt;=&gt; $b } @_;\n    return ($nums[0], $nums[-1]);\n}\n\nmy ($low, $high) = min_max(42, 7, 19, 3, 88);\nprint \"Low: $low, High: $high\\n\";   # Low: 3, High: 88\n</code></pre>"},{"location":"Dev%20Zero/Perl/subroutines-references/#context-sensitive-returns-with-wantarray","title":"Context-Sensitive Returns with wantarray","text":"<p><code>wantarray</code> tells your sub how the caller is using its return value:</p> <pre><code>sub timestamp {\n    my @parts = (2025, 6, 15, 14, 30, 0);\n    if (wantarray) {\n        return @parts;                          # list context\n    } else {\n        return sprintf \"%04d-%02d-%02d %02d:%02d:%02d\", @parts;  # scalar context\n    }\n}\n\nmy @components = timestamp();    # (2025, 6, 15, 14, 30, 0)\nmy $formatted  = timestamp();    # \"2025-06-15 14:30:00\"\n</code></pre> <p><code>wantarray</code> returns true in list context, false in scalar context, and <code>undef</code> in void context (where the return value is discarded). Despite the name, it detects list context, not specifically array context.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#returning-nothing","title":"Returning Nothing","text":"<p><code>return</code> with no argument returns an empty list in list context or <code>undef</code> in scalar context. This is the correct way to signal \"no meaningful value\":</p> <pre><code>sub find_user {\n    my $name = shift;\n    return unless exists $users{$name};   # return undef/empty list\n    return $users{$name};\n}\n</code></pre> <p>return undef vs. bare return</p> <p><code>return undef</code> always returns a single <code>undef</code> value - even in list context, where it produces a one-element list <code>(undef)</code>. A bare <code>return</code> adapts to context: <code>undef</code> in scalar, empty list <code>()</code> in list. Prefer bare <code>return</code> for \"nothing found\" situations.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#named-parameters","title":"Named Parameters","text":"<p>For subs with many parameters, positional arguments become unreadable. Named parameters use a hash:</p> <pre><code>sub create_user {\n    my (%opts) = @_;\n    my $name  = $opts{name}  // 'Anonymous';\n    my $email = $opts{email} // '';\n    my $role  = $opts{role}  // 'user';\n\n    print \"Created $name ($role) - $email\\n\";\n}\n\ncreate_user(name =&gt; 'Alice', email =&gt; 'alice@example.com', role =&gt; 'admin');\ncreate_user(name =&gt; 'Bob');   # defaults for email and role\n</code></pre> <p>The <code>//=</code> operator (defined-or assignment) is perfect for defaults:</p> <pre><code>sub connect_db {\n    my (%opts) = @_;\n    $opts{host}    //= 'localhost';\n    $opts{port}    //= 5432;\n    $opts{timeout} //= 30;\n\n    print \"Connecting to $opts{host}:$opts{port} (timeout: $opts{timeout}s)\\n\";\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/subroutines-references/#hashref-parameters","title":"Hashref Parameters","text":"<p>You can also pass a hashref for slightly different syntax at the call site:</p> <pre><code>sub create_user {\n    my ($opts) = @_;\n    my $name = $opts-&gt;{name} // 'Anonymous';\n    # ...\n}\n\ncreate_user({ name =&gt; 'Alice', role =&gt; 'admin' });\n</code></pre> <p>The tradeoff: hashref syntax requires braces at the call site, but it makes the argument clearly a single unit and lets you pass it through other functions unchanged.</p> <p>Configuration Parser with Named Parameters (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#references","title":"References","text":"<p>So far, every variable you have worked with holds data directly. An array holds a list of scalars. A hash holds key-value pairs. But what if you need to store an array inside another array, or pass a hash to a subroutine without flattening it?</p> <p>A reference is a scalar value that points to another variable. Think of it as an address - the reference itself is small (one scalar), but it gives you access to the full data structure it points to.</p> <p>### Creating References with \\</p> <p>The backslash operator <code>\\</code> creates a reference to any variable:</p> <pre><code>my $name = \"Alice\";\nmy $ref  = \\$name;       # scalar reference\n\nmy @colors = ('red', 'green', 'blue');\nmy $aref   = \\@colors;   # array reference\n\nmy %config = (host =&gt; 'localhost', port =&gt; 8080);\nmy $href   = \\%config;   # hash reference\n\nsub greet { print \"Hello!\\n\" }\nmy $cref = \\&amp;greet;       # code reference\n</code></pre> <p>Each reference is a scalar - it fits in a <code>$</code> variable, an array element, or a hash value. This is the key insight: references let you nest complex structures inside scalar slots.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#reference-types","title":"Reference Types","text":"Type Created with Points to Scalar ref <code>\\$scalar</code> A scalar variable Array ref <code>\\@array</code> or <code>[...]</code> An array Hash ref <code>\\%hash</code> or <code>{...}</code> A hash Code ref <code>\\&amp;sub</code> or <code>sub {...}</code> A subroutine Regex ref <code>qr/.../</code> A compiled regex Ref ref <code>\\$ref</code> (when <code>$ref</code> is already a ref) Another reference"},{"location":"Dev%20Zero/Perl/subroutines-references/#how-references-work-in-memory","title":"How References Work in Memory","text":"<pre><code>flowchart LR\n    subgraph Variables\n        ARR[\"@arr = (1, 2, 3)\"]\n    end\n    subgraph References\n        REF[\"$ref (scalar)\"]\n    end\n    REF --&gt;|points to| ARR\n    subgraph \"Accessing via reference\"\n        D1[\"$ref-&gt;[0] \u2192 1\"]\n        D2[\"$ref-&gt;[1] \u2192 2\"]\n        D3[\"$ref-&gt;[2] \u2192 3\"]\n    end\n    ARR --&gt; D1\n    ARR --&gt; D2\n    ARR --&gt; D3</code></pre> <p>The reference <code>$ref</code> is a single scalar that holds the memory address of <code>@arr</code>. Dereferencing <code>$ref</code> gives you access to the original array's elements. Changing <code>$ref-&gt;[0]</code> changes <code>$arr[0]</code> - they are the same data.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#what-a-reference-looks-like","title":"What a Reference Looks Like","text":"<p>If you print a reference directly, you get a string like <code>ARRAY(0x55a1234)</code> - the type and memory address:</p> <pre><code>my @arr = (1, 2, 3);\nmy $ref = \\@arr;\nprint $ref;        # ARRAY(0x55a1234) - not useful for display\nprint ref $ref;    # ARRAY - the type\n</code></pre> <p>References are always true in boolean context, even if they point to an empty array or hash.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#dereferencing","title":"Dereferencing","text":"<p>Dereferencing means following the reference back to the data it points to. Perl offers two main syntaxes.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#arrow-notation","title":"Arrow Notation","text":"<p>The arrow operator <code>-&gt;</code> is the cleanest way to dereference:</p> <pre><code>my $aref = [10, 20, 30];\nprint $aref-&gt;[0];    # 10\nprint $aref-&gt;[2];    # 30\n\nmy $href = { name =&gt; 'Alice', age =&gt; 30 };\nprint $href-&gt;{name};   # Alice\nprint $href-&gt;{age};    # 30\n\nmy $cref = sub { return 42 };\nprint $cref-&gt;();       # 42\n</code></pre> <p>Arrow notation reads left to right and handles nested structures naturally:</p> <pre><code>my $data = {\n    users =&gt; [\n        { name =&gt; 'Alice', scores =&gt; [95, 87, 92] },\n        { name =&gt; 'Bob',   scores =&gt; [78, 84, 90] },\n    ],\n};\n\nprint $data-&gt;{users}[0]{name};         # Alice\nprint $data-&gt;{users}[1]{scores}[2];    # 90\n</code></pre> <p>Between adjacent brackets (<code>[]</code> or <code>{}</code>), the arrow is optional. The first arrow after a variable name is always required.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#block-syntax","title":"Block Syntax","text":"<p>For cases where you need to dereference the entire structure at once, use block syntax with sigils:</p> <pre><code>my $aref = [10, 20, 30];\n\n# Get the whole array\nmy @array = @{$aref};\n\n# Get the whole hash\nmy $href = { a =&gt; 1, b =&gt; 2 };\nmy %hash = %{$href};\n\n# Single element (less common than arrow notation)\nmy $val = ${$aref}[0];     # same as $aref-&gt;[0]\nmy $val = ${$href}{a};     # same as $href-&gt;{a}\n</code></pre> <p>Block syntax is essential when you need to use array/hash operations on a reference:</p> <pre><code>my $aref = [5, 3, 8, 1];\n\n# Sort the referenced array\nmy @sorted = sort { $a &lt;=&gt; $b } @{$aref};\n\n# Push onto a referenced array\npush @{$aref}, 99;\n\n# Get keys from a referenced hash\nmy $href = { x =&gt; 1, y =&gt; 2, z =&gt; 3 };\nmy @keys = keys %{$href};\n</code></pre> <p>When to Use Which</p> <p>Use arrow notation (<code>$ref-&gt;[0]</code>, <code>$ref-&gt;{key}</code>) for accessing individual elements - it is the most readable. Use block syntax (<code>@{$ref}</code>, <code>%{$ref}</code>) when you need the whole array or hash for operations like <code>sort</code>, <code>push</code>, <code>keys</code>, or iteration.</p> <p>References and Dereferencing (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#anonymous-data-structures","title":"Anonymous Data Structures","text":"<p>You do not always need a named variable to create a reference. Anonymous constructors create data structures and return a reference directly.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#anonymous-arrays","title":"Anonymous Arrays: [ ]","text":"<p>Square brackets create an anonymous array and return a reference to it:</p> <pre><code>my $colors = ['red', 'green', 'blue'];\nprint $colors-&gt;[0];   # red\n</code></pre> <p>This is equivalent to:</p> <pre><code>my @temp = ('red', 'green', 'blue');\nmy $colors = \\@temp;\n</code></pre> <p>But without the temporary variable.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#anonymous-hashes","title":"Anonymous Hashes: { }","text":"<p>Curly braces in a value context create an anonymous hash and return a reference:</p> <pre><code>my $user = { name =&gt; 'Alice', age =&gt; 30 };\nprint $user-&gt;{name};   # Alice\n</code></pre>"},{"location":"Dev%20Zero/Perl/subroutines-references/#anonymous-subs-sub","title":"Anonymous Subs: sub { }","text":"<p>A <code>sub</code> without a name creates an anonymous code reference:</p> <pre><code>my $double = sub { return $_[0] * 2 };\nprint $double-&gt;(5);   # 10\n</code></pre> <p>Anonymous subs are the foundation of closures and callbacks.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#the-difference-between-and","title":"The Difference Between () and []/{}","text":"<p>This distinction is fundamental:</p> <pre><code># Parentheses create a LIST (flattens into surrounding context)\nmy @arr = (1, 2, 3);\n\n# Square brackets create an ARRAYREF (a single scalar value)\nmy $ref = [1, 2, 3];\n\n# These are NOT the same\nmy @flat = (1, 2, (3, 4), 5);   # @flat = (1, 2, 3, 4, 5) - flattened!\nmy @nested = (1, 2, [3, 4], 5); # @nested has 4 elements, third is a ref\n</code></pre> <p>Parentheses are for grouping and list context. Brackets are for creating references. This is why nested data structures require references - parentheses would just flatten everything into one big list.</p> <p>What is the difference between \\@array and [@array]? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#the-ref-function","title":"The ref() Function","text":"<p>The <code>ref()</code> function tells you what type of reference a scalar holds:</p> <pre><code>my $sref = \\42;\nmy $aref = [1, 2, 3];\nmy $href = { a =&gt; 1 };\nmy $cref = sub { 1 };\nmy $rref = \\$aref;\n\nprint ref $sref;   # SCALAR\nprint ref $aref;   # ARRAY\nprint ref $href;   # HASH\nprint ref $cref;   # CODE\nprint ref $rref;   # REF\n\n# Not a reference\nmy $plain = \"hello\";\nprint ref $plain;  # '' (empty string)\n</code></pre> <p><code>ref()</code> returns an empty string for non-references, which is false in boolean context. This makes it useful as a type check:</p> <pre><code>sub process {\n    my $arg = shift;\n\n    if (!ref $arg) {\n        print \"Got a plain scalar: $arg\\n\";\n    } elsif (ref $arg eq 'ARRAY') {\n        print \"Got an array with \", scalar @{$arg}, \" elements\\n\";\n    } elsif (ref $arg eq 'HASH') {\n        print \"Got a hash with keys: \", join(', ', keys %{$arg}), \"\\n\";\n    } elsif (ref $arg eq 'CODE') {\n        print \"Got a coderef, calling it: \", $arg-&gt;(), \"\\n\";\n    }\n}\n\nprocess(\"hello\");\nprocess([1, 2, 3]);\nprocess({ x =&gt; 10 });\nprocess(sub { \"result\" });\n</code></pre> <p>For blessed objects, <code>ref()</code> returns the class name instead of the underlying type. Use <code>Scalar::Util::reftype</code> to get the underlying reference type regardless of blessing.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#closures","title":"Closures","text":"<p>A closure is a subroutine that captures variables from its enclosing lexical scope. When the enclosing scope ends, the captured variables survive as long as the closure exists:</p> <pre><code>sub make_counter {\n    my $count = 0;\n    return sub { return ++$count };\n}\n\nmy $counter = make_counter();\nprint $counter-&gt;();   # 1\nprint $counter-&gt;();   # 2\nprint $counter-&gt;();   # 3\n</code></pre> <p>Each call to <code>make_counter</code> creates a fresh <code>$count</code> variable. The returned sub holds a reference to that specific <code>$count</code>. Even after <code>make_counter</code> returns and its scope ends, the <code>$count</code> variable lives on because the closure still references it.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#independent-closures","title":"Independent Closures","text":"<p>Each call produces an independent closure with its own captured variable:</p> <pre><code>my $counter_a = make_counter();\nmy $counter_b = make_counter();\n\nprint $counter_a-&gt;();   # 1\nprint $counter_a-&gt;();   # 2\nprint $counter_b-&gt;();   # 1  - independent counter\nprint $counter_a-&gt;();   # 3\n</code></pre>"},{"location":"Dev%20Zero/Perl/subroutines-references/#closure-factory-pattern","title":"Closure Factory Pattern","text":"<p>Closures are powerful for creating families of related functions:</p> <pre><code>sub make_multiplier {\n    my $factor = shift;\n    return sub { return $_[0] * $factor };\n}\n\nmy $double = make_multiplier(2);\nmy $triple = make_multiplier(3);\n\nprint $double-&gt;(5);    # 10\nprint $triple-&gt;(5);    # 15\n</code></pre>"},{"location":"Dev%20Zero/Perl/subroutines-references/#closures-for-callbacks","title":"Closures for Callbacks","text":"<p>Closures naturally carry context into callbacks:</p> <pre><code>sub fetch_data {\n    my (%opts) = @_;\n    my $url     = $opts{url};\n    my $retries = $opts{retries} // 3;\n\n    my $on_success = $opts{on_success} // sub { print \"Data: $_[0]\\n\" };\n    my $on_error   = $opts{on_error}   // sub { warn \"Error: $_[0]\\n\" };\n\n    # ... fetch logic would go here ...\n    # Call $on_success-&gt;($data) or $on_error-&gt;($message)\n}\n\nmy @results;\nfetch_data(\n    url        =&gt; 'https://api.example.com/data',\n    on_success =&gt; sub { push @results, $_[0] },   # closure captures @results\n    on_error   =&gt; sub { warn \"Failed: $_[0]\\n\" },\n);\n</code></pre> <p>The <code>on_success</code> callback is a closure that captures <code>@results</code> from the calling scope. When the callback fires, it pushes data into the original array.</p> <p>Math Utility Library (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#passing-complex-data","title":"Passing Complex Data","text":"<p>One of the first walls you hit in Perl is the flattening problem. Arrays and hashes flatten into a single list when passed to a subroutine:</p> <pre><code>sub print_both {\n    my (@first, @second) = @_;   # BUG: @first absorbs everything!\n    print \"First: @first\\n\";\n    print \"Second: @second\\n\";   # always empty\n}\n\nmy @a = (1, 2, 3);\nmy @b = (4, 5, 6);\nprint_both(@a, @b);\n# First: 1 2 3 4 5 6   - @first consumed all six values\n# Second:               - @second got nothing\n</code></pre> <p>The caller passes <code>(1, 2, 3, 4, 5, 6)</code> - a single flat list. The sub has no way to know where <code>@a</code> ended and <code>@b</code> began.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#the-solution-pass-references","title":"The Solution: Pass References","text":"<p>References keep data structures intact through the call:</p> <pre><code>sub print_both {\n    my ($first_ref, $second_ref) = @_;\n    print \"First: @{$first_ref}\\n\";\n    print \"Second: @{$second_ref}\\n\";\n}\n\nmy @a = (1, 2, 3);\nmy @b = (4, 5, 6);\nprint_both(\\@a, \\@b);\n# First: 1 2 3\n# Second: 4 5 6\n</code></pre> <p>Each reference is a single scalar, so the list assignment works correctly. Inside the sub, you dereference to access the data.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#returning-complex-structures","title":"Returning Complex Structures","text":"<p>The same applies to return values. Return references when you need to return multiple arrays or hashes:</p> <pre><code>sub split_even_odd {\n    my @even;\n    my @odd;\n    for (@_) {\n        if ($_ % 2 == 0) { push @even, $_ }\n        else              { push @odd,  $_ }\n    }\n    return (\\@even, \\@odd);\n}\n\nmy ($even_ref, $odd_ref) = split_even_odd(1..10);\nprint \"Even: @{$even_ref}\\n\";   # Even: 2 4 6 8 10\nprint \"Odd:  @{$odd_ref}\\n\";    # Odd:  1 3 5 7 9\n</code></pre> <p>Do Not Return References to Temporaries Created with my - Except When You Can</p> <p>Actually, Perl handles this correctly. The <code>my @even</code> array in <code>split_even_odd</code> survives after the sub returns because the returned reference keeps it alive. This is the same mechanism that makes closures work. Perl uses reference counting - the data lives as long as something references it.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#callbacks-and-higher-order-functions","title":"Callbacks and Higher-Order Functions","text":"<p>A higher-order function is a subroutine that takes another subroutine as an argument or returns one. You have already used one - <code>sort</code> with a custom comparator:</p> <pre><code>my @sorted = sort { $a &lt;=&gt; $b } @numbers;\n</code></pre> <p>The <code>{ $a &lt;=&gt; $b }</code> block is an anonymous sub passed to <code>sort</code>.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#passing-coderefs-as-arguments","title":"Passing Coderefs as Arguments","text":"<pre><code>sub apply_to_list {\n    my ($func, @items) = @_;\n    return map { $func-&gt;($_) } @items;\n}\n\nmy @doubled = apply_to_list(sub { $_[0] * 2 }, 1, 2, 3, 4);\nprint \"@doubled\\n\";   # 2 4 6 8\n\nmy @lengths = apply_to_list(sub { length $_[0] }, 'hello', 'world', 'perl');\nprint \"@lengths\\n\";   # 5 5 4\n</code></pre>"},{"location":"Dev%20Zero/Perl/subroutines-references/#custom-comparators-for-sort","title":"Custom Comparators for sort","text":"<p>The <code>sort</code> built-in accepts a named sub or code block as a comparator:</p> <pre><code># Sort strings by length, then alphabetically\nmy @words = qw(perl python go javascript ruby);\nmy @sorted = sort {\n    length($a) &lt;=&gt; length($b)\n    ||\n    $a cmp $b\n} @words;\n\nprint \"@sorted\\n\";   # go perl ruby python javascript\n</code></pre> <p>You can also pass a named sub:</p> <pre><code>sub by_length { length($a) &lt;=&gt; length($b) }\nmy @sorted = sort by_length @words;\n</code></pre>"},{"location":"Dev%20Zero/Perl/subroutines-references/#dispatch-tables","title":"Dispatch Tables","text":"<p>A dispatch table is a hash that maps names to coderefs. It replaces long <code>if</code>/<code>elsif</code> chains with a clean lookup:</p> <pre><code>my %dispatch = (\n    add =&gt; sub { return $_[0] + $_[1] },\n    sub =&gt; sub { return $_[0] - $_[1] },\n    mul =&gt; sub { return $_[0] * $_[1] },\n    div =&gt; sub { return $_[1] != 0 ? $_[0] / $_[1] : 'Error: division by zero' },\n);\n\nmy $op = 'add';\nif (exists $dispatch{$op}) {\n    my $result = $dispatch{$op}-&gt;(10, 3);\n    print \"$op: $result\\n\";   # add: 13\n}\n</code></pre> <p>Dispatch tables scale better than conditionals - adding a new operation means adding one hash entry, not modifying a chain of <code>if</code> statements.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#dispatch-table-architecture","title":"Dispatch Table Architecture","text":"<pre><code>flowchart TD\n    INPUT[\"User input: 'greet'\"] --&gt; LOOKUP{\"Look up in %dispatch\"}\n    LOOKUP --&gt;|found| CALL[\"Call $dispatch{greet}-&gt;(@args)\"]\n    LOOKUP --&gt;|not found| DEFAULT[\"Handle unknown command\"]\n    CALL --&gt; RESULT[\"Output: Hello, World!\"]\n\n    subgraph \"%dispatch hash\"\n        K1[\"'help'\"] --&gt; V1[\"sub { print_help() }\"]\n        K2[\"'greet'\"] --&gt; V2[\"sub { say 'Hello!' }\"]\n        K3[\"'calc'\"] --&gt; V3[\"sub { calculate(@_) }\"]\n        K4[\"'quit'\"] --&gt; V4[\"sub { exit 0 }\"]\n    end</code></pre> <p>The dispatch table maps string keys to code references. The lookup is O(1) regardless of how many commands you support, and each handler is self-contained.</p> <p>Dispatch Table Command Router (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#practical-patterns","title":"Practical Patterns","text":"<p>The concepts covered in this guide combine into patterns you will use repeatedly in real Perl programs.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#memoization","title":"Memoization","text":"<p>Memoization caches the return value of a function based on its arguments so repeated calls with the same inputs return instantly:</p> <pre><code>sub memoize {\n    my $func = shift;\n    my %cache;\n    return sub {\n        my $key = join(\"\\0\", @_);\n        unless (exists $cache{$key}) {\n            $cache{$key} = $func-&gt;(@_);\n        }\n        return $cache{$key};\n    };\n}\n\n# Expensive calculation wrapped with memoization\nmy $fibonacci = memoize(sub {\n    my $n = shift;\n    return $n if $n &lt;= 1;\n    # Note: this recursive version needs the $fibonacci variable\n    # to be in scope, which requires a different setup for recursion.\n    # For demonstration, here is an iterative version:\n    my ($a, $b) = (0, 1);\n    for (2..$n) { ($a, $b) = ($b, $a + $b) }\n    return $b;\n});\n\nprint $fibonacci-&gt;(10), \"\\n\";   # 55\nprint $fibonacci-&gt;(10), \"\\n\";   # 55 - returned from cache\n</code></pre> <p>The <code>memoize</code> function is a higher-order function that returns a closure. The closure captures both the original <code>$func</code> and the <code>%cache</code> hash. For production use, the <code>Memoize</code> core module does this with more features.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#builder-pattern","title":"Builder Pattern","text":"<p>Chain method-like calls using closures that return themselves:</p> <pre><code>sub query_builder {\n    my %query = (table =&gt; '', conditions =&gt; [], columns =&gt; ['*'], limit =&gt; undef);\n\n    my $self;\n    $self = {\n        from =&gt; sub {\n            $query{table} = shift;\n            return $self;\n        },\n        select =&gt; sub {\n            $query{columns} = [@_];\n            return $self;\n        },\n        where =&gt; sub {\n            push @{$query{conditions}}, shift;\n            return $self;\n        },\n        limit =&gt; sub {\n            $query{limit} = shift;\n            return $self;\n        },\n        build =&gt; sub {\n            my $sql = \"SELECT \" . join(', ', @{$query{columns}});\n            $sql .= \" FROM $query{table}\";\n            if (@{$query{conditions}}) {\n                $sql .= \" WHERE \" . join(' AND ', @{$query{conditions}});\n            }\n            $sql .= \" LIMIT $query{limit}\" if defined $query{limit};\n            return $sql;\n        },\n    };\n\n    return $self;\n}\n\nmy $q = query_builder();\nmy $sql = $q-&gt;{from}-&gt;('users')\n            -&gt;{select}-&gt;('name', 'email')\n            -&gt;{where}-&gt;('active = 1')\n            -&gt;{where}-&gt;('age &gt; 18')\n            -&gt;{limit}-&gt;(10)\n            -&gt;{build}-&gt;();\n\nprint \"$sql\\n\";\n# SELECT name, email FROM users WHERE active = 1 AND age &gt; 18 LIMIT 10\n</code></pre> <p>This pattern previews object-oriented Perl - the hash of closures sharing <code>%query</code> is essentially an object with methods.</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#configuration-parser","title":"Configuration Parser","text":"<p>Here is a practical example that combines subroutines, references, named parameters, and closures:</p> <pre><code>use strict;\nuse warnings;\n\nsub create_parser {\n    my (%opts) = @_;\n    my $comment_char = $opts{comment}   // '#';\n    my $separator    = $opts{separator} // '=';\n    my $validators   = $opts{validators} // {};\n\n    return sub {\n        my ($input) = @_;\n        my %config;\n        my @errors;\n\n        my @lines = ref $input eq 'ARRAY' ? @{$input} : split /\\n/, $input;\n\n        for my $i (0..$#lines) {\n            my $line = $lines[$i];\n            $line =~ s/^\\s+|\\s+$//g;                        # trim\n            next if $line eq '' || $line =~ /^\\Q$comment_char/;  # skip blanks/comments\n\n            unless ($line =~ /^(\\w+)\\s*\\Q$separator\\E\\s*(.+)$/) {\n                push @errors, \"Line ${\\($i+1)}: malformed entry\";\n                next;\n            }\n\n            my ($key, $value) = ($1, $2);\n\n            if (exists $validators-&gt;{$key}) {\n                unless ($validators-&gt;{$key}-&gt;($value)) {\n                    push @errors, \"Line ${\\($i+1)}: invalid value for $key\";\n                    next;\n                }\n            }\n\n            $config{$key} = $value;\n        }\n\n        return (\\%config, \\@errors);\n    };\n}\n\n# Create a parser with validation rules\nmy $parse = create_parser(\n    comment    =&gt; '#',\n    separator  =&gt; '=',\n    validators =&gt; {\n        port    =&gt; sub { $_[0] =~ /^\\d+$/ &amp;&amp; $_[0] &gt; 0 &amp;&amp; $_[0] &lt; 65536 },\n        debug   =&gt; sub { $_[0] =~ /^(true|false|0|1)$/ },\n    },\n);\n\nmy @config_lines = (\n    '# Server configuration',\n    'host = localhost',\n    'port = 8080',\n    'debug = true',\n    'workers = 4',\n);\n\nmy ($config, $errors) = $parse-&gt;(\\@config_lines);\n\nprint \"Config:\\n\";\nfor my $key (sort keys %{$config}) {\n    printf \"  %-10s = %s\\n\", $key, $config-&gt;{$key};\n}\n\nif (@{$errors}) {\n    print \"\\nErrors:\\n\";\n    print \"  $_\\n\" for @{$errors};\n}\n</code></pre> <p>Output:</p> <pre><code>Config:\n  debug      = true\n  host       = localhost\n  port       = 8080\n  workers    = 4\n</code></pre> <p>This example uses nearly every concept from this guide: subroutines with named parameters, references for passing arrays, anonymous subs as validators, closures to capture configuration, and hashrefs for returning complex results.</p> <p>Command Builder (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/subroutines-references/#quick-reference","title":"Quick Reference","text":"Concept Syntax Description Declare sub <code>sub name { ... }</code> Named subroutine Call sub <code>name(@args)</code> Call with arguments Arguments <code>my ($x, $y) = @_</code> Unpack arguments (copies) shift idiom <code>my $x = shift</code> Pull first argument Return value <code>return $val</code> Explicit return Scalar ref <code>my $ref = \\$x</code> Reference to scalar Array ref <code>my $ref = \\@arr</code> Reference to array Hash ref <code>my $ref = \\%h</code> Reference to hash Code ref <code>my $ref = \\&amp;sub</code> Reference to subroutine Anon arrayref <code>[1, 2, 3]</code> Anonymous array reference Anon hashref <code>{ k =&gt; 'v' }</code> Anonymous hash reference Anon sub <code>sub { ... }</code> Anonymous code reference Deref arrow <code>$ref-&gt;[0]</code>, <code>$ref-&gt;{k}</code> Access element via reference Deref block <code>@{$ref}</code>, <code>%{$ref}</code> Dereference entire structure Ref type <code>ref $ref</code> Returns 'ARRAY', 'HASH', etc. Closure <code>sub { ... $captured }</code> Sub capturing outer variables Dispatch <code>$hash{$key}-&gt;(@args)</code> Call coderef from hash"},{"location":"Dev%20Zero/Perl/subroutines-references/#further-reading","title":"Further Reading","text":"<ul> <li>perlsub - subroutine declaration, calling conventions, prototypes, and signatures</li> <li>perlref - references, dereferencing, and reference-related syntax</li> <li>perlreftut - Mark Jason Dominus's reference tutorial - the clearest introduction</li> <li>perldsc - data structures cookbook with recipes for arrays of arrays, hashes of hashes, etc.</li> <li>perllol - manipulating arrays of arrays (lists of lists)</li> <li>perlsub - Closures - official closures documentation</li> <li>Memoize - core module for automatic memoization</li> <li>Learning Perl, Chapters 4 and 13 - subroutines and references in the \"Llama Book\"</li> <li>Intermediate Perl - the \"Alpaca Book,\" entirely focused on references, data structures, and OOP</li> <li>Higher-Order Perl - Mark Jason Dominus's free book on closures, iterators, and functional techniques</li> </ul> <p>Previous: Regular Expressions | Next: File I/O and System Interaction | Back to Index</p>"},{"location":"Dev%20Zero/Perl/testing/","title":"Testing","text":""},{"location":"Dev%20Zero/Perl/testing/#perls-testing-culture","title":"Perl's Testing Culture","text":"<p>Version: 1.1 Year: 2026</p>"},{"location":"Dev%20Zero/Perl/testing/#copyright-notice","title":"Copyright Notice","text":"<p>Copyright (c) 2025-2026 Ryan Thomas Robson / Robworks Software LLC. Licensed under CC BY-NC-ND 4.0. You may share this material for non-commercial purposes with attribution, but you may not distribute modified versions.</p> <p>Perl has one of the oldest and most deeply embedded testing cultures in programming. Every CPAN distribution ships with tests. The <code>prove</code> command and TAP protocol originated in Perl. When you install a module with <code>cpanm</code>, the test suite runs automatically before the module lands on your system.</p>"},{"location":"Dev%20Zero/Perl/testing/#testmore-basics","title":"Test::More Basics","text":"<p><code>Test::More</code> is the standard testing module that ships with Perl. It provides functions that compare actual results against expected values and produce structured output.</p>"},{"location":"Dev%20Zero/Perl/testing/#your-first-test-file","title":"Your First Test File","text":"<p>Test files live in a <code>t/</code> directory and have the <code>.t</code> extension. They are plain Perl scripts:</p> <pre><code># t/basic.t\nuse strict;\nuse warnings;\nuse Test::More tests =&gt; 4;\n\nok(1 + 1 == 2, 'addition works');\nis(lc('HELLO'), 'hello', 'lc lowercases a string');\nlike('user@example.com', qr/@/, 'email contains @');\nisnt('foo', 'bar', 'foo is not bar');\n</code></pre> <p>The <code>tests =&gt; 4</code> declaration tells the harness how many tests to expect. If the script exits early or runs extra tests, the harness flags it as a failure.</p>"},{"location":"Dev%20Zero/Perl/testing/#core-test-functions","title":"Core Test Functions","text":"Function Purpose <code>ok($test, $name)</code> Passes if <code>$test</code> is true <code>is($got, $expected, $name)</code> Passes if <code>$got eq $expected</code> <code>isnt($got, $unexpected, $name)</code> Passes if <code>$got ne $unexpected</code> <code>like($got, qr/regex/, $name)</code> Passes if <code>$got</code> matches the regex <code>unlike($got, qr/regex/, $name)</code> Passes if <code>$got</code> does not match <code>is_deeply($got, $expected, $name)</code> Deep comparison of structures <code>can_ok($module, @methods)</code> Checks a module has the methods <code>isa_ok($obj, $class)</code> Checks an object's class"},{"location":"Dev%20Zero/Perl/testing/#ok-vs-is-vs-is_deeply","title":"ok vs. is vs. is_deeply","text":"<p><code>ok</code> only reports pass/fail. <code>is</code> shows both the expected and received values on failure. <code>is_deeply</code> handles nested arrays, hashes, and mixed structures:</p> <pre><code># ok - minimal failure output: just \"not ok\"\nok($result == 42, 'answer is 42');\n\n# is - shows got vs. expected on failure\nis($result, 42, 'answer is 42');\n#   got: 41\n#   expected: 42\n\n# is_deeply - deep comparison for nested structures\nis_deeply($config, { host =&gt; 'localhost', port =&gt; 8080 }, 'config defaults');\n</code></pre> <p>Always prefer is() over ok()</p> <p>When comparing values, <code>is()</code> provides dramatically better failure messages than <code>ok()</code>. The line <code>ok($x == 5)</code> tells you only that the check failed. <code>is($x, 5)</code> tells you what <code>$x</code> actually was.</p>"},{"location":"Dev%20Zero/Perl/testing/#plan-strategies","title":"Plan Strategies","text":"<pre><code>use Test::More tests =&gt; 10;       # exact count - strictest\nuse Test::More;                    # no plan - call done_testing() at end\nuse Test::More skip_all =&gt; 'No database available' unless $ENV{TEST_DB};\n</code></pre> <p>The <code>done_testing()</code> function tells the harness you finished successfully. Use it when the number of tests varies at runtime.</p> <p>Writing and Running Your First Tests (requires JavaScript)</p> <p>A test fails with this output: 'not ok 1 - answer is 42'. No other diagnostic information is shown. Which test function was most likely used? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/testing/#the-tap-protocol","title":"The TAP Protocol","text":"<p>Test Anything Protocol (TAP) is the text-based format that Perl test scripts produce, now used across many languages.</p>"},{"location":"Dev%20Zero/Perl/testing/#tap-format","title":"TAP Format","text":"<pre><code>1..4\nok 1 - addition works\nnot ok 2 - subtraction is broken\n#          got: '7'\n#     expected: '6'\nok 3 - multiplication\nok 4 # skip no database configured\n</code></pre> Element Meaning <code>1..N</code> Plan line - expect N tests <code>ok N</code> / <code>not ok N</code> Pass / fail <code># text</code> Diagnostic comment <code># skip</code> / <code># TODO</code> Skipped / expected failure"},{"location":"Dev%20Zero/Perl/testing/#skip-and-todo","title":"SKIP and TODO","text":"<p><code>SKIP</code> marks tests that cannot run in the current environment. <code>TODO</code> marks tests that are expected to fail:</p> <pre><code>SKIP: {\n    skip 'No network available', 2 unless $ENV{TEST_NETWORK};\n    ok(ping('example.com'), 'can reach example.com');\n    ok(fetch('https://example.com'), 'can fetch page');\n}\n\nTODO: {\n    local $TODO = 'Unicode normalization not yet implemented';\n    is(normalize(\"\\x{e9}\"), \"e\\x{301}\", 'decomposes e-acute');\n}\n</code></pre> <p>Both count as passed in the harness. A TODO test that unexpectedly passes is reported as a bonus.</p>"},{"location":"Dev%20Zero/Perl/testing/#prove-the-test-runner","title":"prove: The Test Runner","text":"<p><code>prove</code> is Perl's standard test harness. It finds test files, runs them, parses TAP output, and summarizes results.</p>"},{"location":"Dev%20Zero/Perl/testing/#basic-usage","title":"Basic Usage","text":"<pre><code>prove                     # run all tests in t/\nprove t/specific.t        # run one test file\nprove -v                  # verbose - show individual test results\nprove -l                  # add lib/ to @INC\nprove -r t/               # recurse into subdirectories\nprove -j4                 # run 4 tests in parallel\n</code></pre>"},{"location":"Dev%20Zero/Perl/testing/#test-organization","title":"Test Organization","text":"<pre><code>myproject/\n  lib/\n    MyApp.pm\n    MyApp/\n      Config.pm\n      Database.pm\n  t/\n    00-load.t           # verify modules compile\n    01-config.t         # unit tests for Config\n    02-database.t       # unit tests for Database\n    03-integration.t    # integration tests\n  cpanfile\n</code></pre> <p>Prefix test files with numbers for execution order. Use <code>00-load.t</code> to verify modules compile. Group tests by module or feature, and separate unit tests from integration tests.</p> <pre><code>flowchart TD\n    A[\"t/ directory\"] --&gt; B[\"00-load.t\\nModule compilation checks\"]\n    A --&gt; C[\"01-unit-*.t\\nUnit tests per module\"]\n    A --&gt; D[\"02-integration-*.t\\nCross-module tests\"]\n    A --&gt; E[\"03-acceptance-*.t\\nEnd-to-end tests\"]\n    B --&gt; F[\"prove parses TAP\\nfrom each file\"]\n    C --&gt; F\n    D --&gt; F\n    E --&gt; F\n    F --&gt; G[\"Summary:\\nFiles, Tests, Pass/Fail\"]</code></pre>"},{"location":"Dev%20Zero/Perl/testing/#common-prove-flags","title":"Common prove Flags","text":"Flag Effect <code>-v</code> Verbose output <code>-l</code> Add <code>lib/</code> to <code>@INC</code> <code>-r</code> Recurse into subdirectories <code>-j N</code> Run N test files in parallel <code>--shuffle</code> Randomize test file order <code>--state=save</code> Re-run failures first next time <code>--timer</code> Show elapsed time per file <p>prove Options and Test Organization (requires JavaScript)</p> <p>Command Builder (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/testing/#subtests","title":"Subtests","text":"<p>Subtests group related tests under a single test point. A subtest counts as one test in the parent plan, regardless of how many assertions it contains:</p> <pre><code>use Test::More;\n\nsubtest 'addition' =&gt; sub {\n    is(add(1, 2), 3, 'positive');\n    is(add(-1, -2), -3, 'negative');\n    is(add(0, 0), 0, 'zeros');\n};\n\ndone_testing();\n</code></pre> <p>Subtests produce nested TAP and provide natural setup/teardown boundaries - variables declared inside do not leak out.</p>"},{"location":"Dev%20Zero/Perl/testing/#test2suite-the-modern-api","title":"Test2::Suite - The Modern API","text":"<p><code>Test2::Suite</code> is the next generation of Perl testing tools, providing a cleaner API and better diagnostics while remaining TAP-compatible.</p>"},{"location":"Dev%20Zero/Perl/testing/#test2v0","title":"Test2::V0","text":"<p>The <code>Test2::V0</code> bundle imports the most common functions:</p> <pre><code>use Test2::V0;\n\nis(add(2, 3), 5, 'addition');\nlike($email, qr/@/, 'has at-sign');\n\n# Deep comparison with structured validators\nis($config, hash {\n    field host =&gt; 'localhost';\n    field port =&gt; 8080;\n    end();     # no extra keys allowed\n}, 'config structure matches');\n\ndone_testing();\n</code></pre>"},{"location":"Dev%20Zero/Perl/testing/#advanced-comparisons","title":"Advanced Comparisons","text":"<p>Test2::V0 provides structured validators for arrays, hashes, and objects:</p> <pre><code>use Test2::V0;\n\nis(\\@results, array {\n    item 0 =&gt; 'first';\n    item 1 =&gt; match qr/^\\d+$/;\n    end();\n}, 'array matches pattern');\n\nis($user, object {\n    call name =&gt; 'Alice';\n    call age  =&gt; 30;\n}, 'user has expected attributes');\n</code></pre> <p>Test2::V0 vs. Test::More</p> <p>For new projects, <code>Test2::V0</code> is the recommended choice. It provides better error messages and structured validators. <code>Test::More</code> is still widely used and works fine - you do not need to rewrite existing test suites.</p> <p>Test Suite for a Configuration Module (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/testing/#exception-testing","title":"Exception Testing","text":"<p><code>Test::Exception</code> provides functions for testing that code throws or does not throw:</p> <pre><code>use Test::Exception;\n\ndies_ok  { divide(1, 0) }  'division by zero dies';\nlives_ok { divide(10, 2) } 'valid division lives';\nthrows_ok { divide(1, 0) } qr/division by zero/i, 'correct error message';\n</code></pre> Function Tests that... <code>dies_ok { }</code> Code dies <code>lives_ok { }</code> Code does not die <code>throws_ok { } qr//</code> Code dies with matching message <code>throws_ok { } 'Class'</code> Code dies with object of given class <p>With Test2::V0, use <code>dies</code> and <code>lives</code> instead:</p> <pre><code>use Test2::V0;\nmy $err = dies { divide(1, 0) };\nlike($err, qr/division by zero/i, 'correct error');\nok(lives { divide(10, 2) }, 'valid division succeeds');\n</code></pre>"},{"location":"Dev%20Zero/Perl/testing/#mocking","title":"Mocking","text":""},{"location":"Dev%20Zero/Perl/testing/#testmockmodule","title":"Test::MockModule","text":"<p><code>Test::MockModule</code> replaces subroutines within a module for the duration of a test. Originals are restored when the mock object goes out of scope:</p> <pre><code>use Test::MockModule;\nmy $mock = Test::MockModule-&gt;new('WebClient');\n$mock-&gt;mock('fetch', sub { { status =&gt; 200, body =&gt; '{\"ok\":true}' } });\n\nmy $result = WebClient-&gt;fetch('https://api.example.com/status');\nis($result-&gt;{status}, 200, 'mocked status code');\n# Original fetch restored when $mock goes out of scope\n</code></pre>"},{"location":"Dev%20Zero/Perl/testing/#testmockobject","title":"Test::MockObject","text":"<p><code>Test::MockObject</code> creates fake objects from scratch:</p> <pre><code>my $fake_db = Test::MockObject-&gt;new();\n$fake_db-&gt;mock('query', sub { [{ id =&gt; 1, name =&gt; 'Alice' }] });\nmy $users = UserService-&gt;new(db =&gt; $fake_db)-&gt;list_users();\nis(scalar @$users, 1, 'got one user from mock');\n</code></pre> <p>Do not over-mock</p> <p>Mock external boundaries (databases, network calls, filesystems) but test your own code with real objects. If you find yourself mocking internal methods, the code may need refactoring.</p>"},{"location":"Dev%20Zero/Perl/testing/#test-fixtures","title":"Test Fixtures","text":"<p>Test fixtures are known data sets that put tests in a predictable state. Store test data in <code>t/fixtures/</code> and use helper functions for setup/teardown:</p> <pre><code># File-based fixtures\nmy $input = read_text('t/fixtures/sample.csv');\nis_deeply(MyParser-&gt;parse($input), decode_json(read_text('t/fixtures/expected.json')));\n\n# Setup helper for database tests\nsub fresh_database {\n    my $db = TestDB-&gt;new(':memory:');\n    $db-&gt;deploy_schema();\n    $db-&gt;load_fixtures('t/fixtures/seed.sql');\n    return $db;   # cleaned up when $db goes out of scope\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/testing/#test-coverage-with-develcover","title":"Test Coverage with Devel::Cover","text":"<p><code>Devel::Cover</code> instruments your code and reports which lines, branches, conditions, and subroutines your tests exercise.</p> <pre><code>cover -delete                                      # clean previous data\nHARNESS_PERL_SWITCHES=-MDevel::Cover prove -l t/   # run tests under coverage\ncover -report html                                 # generate HTML report\n</code></pre> Metric What it measures Statement Was each line executed? Branch Was each side of every <code>if</code>/<code>unless</code>/<code>while</code> taken? Condition Was each sub-expression in <code>&amp;&amp;</code>/<code>||</code> tested both ways? Subroutine Was each function called at least once? <p>Target 80-90% statement coverage. Focus on public API methods, error handling paths, and boundary conditions.</p> <p>Coverage is a floor, not a ceiling</p> <p>High coverage does not mean your tests are good. Coverage tells you where tests are missing, not where they are sufficient.</p>"},{"location":"Dev%20Zero/Perl/testing/#test-driven-development","title":"Test-Driven Development","text":"<p>Test-driven development (TDD) writes a failing test before the code that makes it pass:</p> <pre><code>flowchart LR\n    A[\"RED\\nWrite a failing test\"] --&gt; B[\"GREEN\\nWrite minimal code\\nto pass the test\"]\n    B --&gt; C[\"REFACTOR\\nClean up code\\nwhile tests stay green\"]\n    C --&gt; A</code></pre>"},{"location":"Dev%20Zero/Perl/testing/#tdd-in-practice","title":"TDD in Practice","text":"<p>RED - Write the test first:</p> <pre><code>use Test::More;\nuse lib './lib';\nuse StringUtils qw(capitalize_words);\n\nis(capitalize_words('hello world'), 'Hello World', 'basic');\nis(capitalize_words('ALREADY UP'), 'Already Up', 'handles uppercase');\nis(capitalize_words(''), '', 'empty string');\ndone_testing();\n</code></pre> <p>Running <code>prove -l t/string_utils.t</code> fails because <code>capitalize_words</code> does not exist.</p> <p>GREEN - Write the minimal implementation:</p> <pre><code>sub capitalize_words {\n    my $str = shift // '';\n    return join ' ', map { ucfirst(lc($_)) } split /\\s+/, $str;\n}\n</code></pre> <p>REFACTOR - The tests pass. Review the code, clean up, move on to the next feature. This cycle guarantees test coverage for every behavior.</p> <p>What does this TAP output indicate?  1..5 ok 1 - connects ok 2 - queries not ok 3 - updates ok 4 # skip database read-only ok 5 - disconnects (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/testing/#cicd-integration","title":"CI/CD Integration","text":"<p>Automated testing in CI catches regressions before they reach production.</p>"},{"location":"Dev%20Zero/Perl/testing/#github-actions-configuration","title":"GitHub Actions Configuration","text":"<pre><code># .github/workflows/test.yml\nname: Test Suite\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        perl-version: ['5.32', '5.36', '5.38']\n    name: Perl ${{ matrix.perl-version }}\n    steps:\n      - uses: actions/checkout@v4\n      - uses: shogo82148/actions-setup-perl@v1\n        with:\n          perl-version: ${{ matrix.perl-version }}\n      - run: cpanm --installdeps --notest .\n      - run: prove -l -j4 t/\n      - name: Coverage report\n        if: matrix.perl-version == '5.38'\n        run: |\n          cpanm --notest Devel::Cover\n          cover -test\n          cover -report html\n</code></pre> <p>The matrix strategy tests against multiple Perl versions. Coverage runs on one version to avoid redundant reports.</p> <p>Never skip failing tests in CI</p> <p>If tests fail in CI, fix them. Do not add <code>|| true</code> to mask failures or mark tests as TODO to avoid investigation. Each ignored failure erodes trust in the test suite.</p>"},{"location":"Dev%20Zero/Perl/testing/#exercises","title":"Exercises","text":"<p>Write Tests for a Calculator Module (requires JavaScript)</p> <p>TDD a String Utility Module (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/testing/#quick-reference","title":"Quick Reference","text":"Task Command / Code Run all tests <code>prove -l t/</code> Verbose single test <code>prove -lv t/specific.t</code> Parallel tests <code>prove -l -j4 t/</code> Compare values <code>is($got, $expected, $name)</code> Compare structures <code>is_deeply(\\%got, \\%expected, $name)</code> Regex match <code>like($got, qr/pattern/, $name)</code> Test exception <code>eval { code() }; like($@, qr/error/)</code> Skip / TODO <code>skip 'reason', $count</code> / <code>local $TODO = 'reason'</code> Coverage <code>cover -test &amp;&amp; cover -report html</code>"},{"location":"Dev%20Zero/Perl/testing/#further-reading","title":"Further Reading","text":"<ul> <li>Test::More - standard testing module documentation</li> <li>Test2::Suite - modern testing framework</li> <li>prove - test harness command reference</li> <li>TAP specification - Test Anything Protocol standard</li> <li>Test::Exception - exception testing functions</li> <li>Test::MockModule - mock module subroutines</li> <li>Devel::Cover - code coverage analysis</li> <li>Perl Testing: A Developer's Notebook - practical guide to testing in Perl</li> </ul> <p>Previous: Error Handling and Debugging | Next: Text Processing and One-Liners | Back to Index</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/","title":"Text Processing and One-Liners","text":""},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#perl-on-the-command-line","title":"Perl on the Command Line","text":"<p>Version: 1.1 Year: 2026</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#copyright-notice","title":"Copyright Notice","text":"<p>Copyright (c) 2025-2026 Ryan Thomas Robson / Robworks Software LLC. Licensed under CC BY-NC-ND 4.0. You may share this material for non-commercial purposes with attribution, but you may not distribute modified versions.</p> <p>Perl was built for text processing. Before it was a web language, before CPAN existed, Perl was a tool for extracting information from files and generating reports. That heritage lives on in the command-line flags that turn Perl into a filter, a transformer, and a replacement for entire <code>sed</code>/<code>awk</code> pipelines - all in a single line. Every one-liner maps to a real script, and every script can be compressed into a one-liner. Understanding both directions makes you faster at solving text problems.</p> <p>See also</p> <p>Perl one-liners often replace <code>grep</code>, <code>sed</code>, and <code>awk</code>. For the Unix text processing tools that Perl builds on, see Text Processing.</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#command-line-flags","title":"Command-Line Flags","text":"<p>The <code>perl</code> interpreter accepts flags that change how it reads input and executes code. These flags are the foundation of one-liner programming.</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#-e-execute-code","title":"-e: Execute Code","text":"<p>The <code>-e</code> flag runs a string of Perl code directly from the command line:</p> <pre><code>perl -e 'print \"Hello, World!\\n\"'\n</code></pre>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#-n-implicit-read-loop","title":"-n: Implicit Read Loop","text":"<p>The <code>-n</code> flag wraps your code in a <code>while (&lt;&gt;) { ... }</code> loop. Each line from STDIN or named files is read into <code>$_</code> automatically:</p> <pre><code># Print lines containing \"error\" - equivalent to grep\nperl -ne 'print if /error/' access.log\n</code></pre> <p>The implicit loop reads every line. Your code runs once per line. If your code does not print anything, nothing is output - you are a filter that discards by default.</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#-p-read-loop-with-print","title":"-p: Read Loop with Print","text":"<p>The <code>-p</code> flag works like <code>-n</code> but automatically prints <code>$_</code> after each iteration. Your code transforms lines; the flag handles the output:</p> <pre><code># Replace \"foo\" with \"bar\" on every line - equivalent to sed\nperl -pe 's/foo/bar/g' data.txt\n</code></pre> <p>The difference between <code>-n</code> and <code>-p</code> is simple: <code>-n</code> requires you to print explicitly, <code>-p</code> prints for you.</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#-l-automatic-line-handling","title":"-l: Automatic Line Handling","text":"<p>The <code>-l</code> flag strips the trailing newline from each input line and adds a newline to each <code>print</code> output:</p> <pre><code>perl -ne 'print length($_)' file.txt    # without -l: includes \\n in count\nperl -lne 'print length($_)' file.txt   # with -l: accurate character count\n</code></pre>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#-a-and-f-autosplit-mode","title":"-a and -F: Autosplit Mode","text":"<p>The <code>-a</code> flag splits each input line on whitespace into the <code>@F</code> array (zero-indexed). The <code>-F</code> flag changes the split delimiter:</p> <pre><code>perl -lane 'print $F[1]' data.txt                  # split on whitespace\nperl -F: -lane 'print \"$F[0] -&gt; $F[5]\"' /etc/passwd  # split on colons\nperl -F, -lane 'print $F[2]' data.csv                # split on commas\n</code></pre>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#-i-in-place-editing","title":"-i: In-Place Editing","text":"<p>The <code>-i</code> flag edits files in place. With <code>-i.bak</code>, Perl saves the original as a backup before writing the modified version:</p> <pre><code>perl -i.bak -pe 's/old/new/g' config.txt   # backup to config.txt.bak\nperl -i -pe 's/old/new/g' config.txt        # no backup (dangerous)\n</code></pre> <p>Always Use a Backup Extension</p> <p>Running <code>-i</code> without a backup extension is irreversible. Always use <code>-i.bak</code> until you have verified the transformation is correct.</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#flag-pipeline","title":"Flag Pipeline","text":"<p>The following diagram shows how common flags combine to process text:</p> <pre><code>flowchart LR\n    A[Input\\nSTDIN or files] --&gt; B{-l flag?}\n    B --&gt;|Yes| C[chomp each line]\n    B --&gt;|No| D[raw line with \\\\n]\n    C --&gt; E{-a flag?}\n    D --&gt; E\n    E --&gt;|Yes| F[\"split into @F\\n(using -F pattern)\"]\n    E --&gt;|No| G[\"$_ = current line\"]\n    F --&gt; G\n    G --&gt; H[Your -e code runs]\n    H --&gt; I{-p flag?}\n    I --&gt;|Yes| J[print $_ automatically]\n    I --&gt;|No| K[print only if your\\ncode calls print]\n    J --&gt; L{-i flag?}\n    K --&gt; L\n    L --&gt;|Yes| M[Write to file in place]\n    L --&gt;|No| N[Write to STDOUT]</code></pre> <p>Command-Line Flags in Action (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#_-and-f-in-one-liners","title":"$_ and @F in One-Liners","text":"<p>Two variables dominate one-liner programming: <code>$_</code> and <code>@F</code>.</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#_-is-your-line","title":"$_ Is Your Line","text":"<p>With <code>-n</code> or <code>-p</code>, <code>$_</code> holds the current line. Perl defaults to <code>$_</code> for regex matching, <code>print</code>, <code>chomp</code>, <code>length</code>, and dozens of other functions:</p> <pre><code># These are equivalent:\nperl -ne 'print $_ if $_ =~ /error/'\nperl -ne 'print if /error/'\n</code></pre>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#f-is-your-row","title":"@F Is Your Row","text":"<p>With <code>-a</code>, <code>@F</code> holds the fields of the current line after splitting. It uses zero-based indexing: <code>$F[0]</code> is the first field, <code>$F[-1]</code> is the last:</p> <pre><code># /etc/passwd: user:pass:uid:gid:gecos:home:shell\nperl -F: -lane 'print \"$F[0] uses $F[-1]\"' /etc/passwd\n\n# Modify @F and reconstruct the line\nperl -F: -lane '$F[6] = \"/bin/bash\"; print join(\":\", @F)' /etc/passwd\n</code></pre> <p>What is the difference between perl -ne 'print' and perl -pe '' when processing a file? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#field-processing","title":"Field Processing","text":"<p>Field processing is where Perl one-liners replace <code>awk</code>. The <code>-a</code> and <code>-F</code> flags handle splitting; your code handles selection, transformation, and output.</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#selecting-and-filtering-fields","title":"Selecting and Filtering Fields","text":"<pre><code># Print columns 1 and 3 from whitespace-delimited data\nperl -lane 'print \"$F[0] $F[2]\"'\n\n# Print all fields except the first\nperl -lane 'print join(\" \", @F[1..$#F])'\n\n# Print lines where the third field exceeds 100\nperl -lane 'print if $F[2] &gt; 100'\n</code></pre>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#transforming-and-rearranging","title":"Transforming and Rearranging","text":"<pre><code># Multiply the second field by 1.1 (10% increase)\nperl -lane '$F[1] *= 1.1; print join(\"\\t\", @F)'\n\n# Uppercase the first field, leave the rest\nperl -lane '$F[0] = uc($F[0]); print join(\" \", @F)'\n\n# Swap first and second columns\nperl -lane 'print \"$F[1] $F[0] @F[2..$#F]\"'\n</code></pre>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#in-place-editing","title":"In-Place Editing","text":"<p>The <code>-i</code> flag transforms Perl from a filter into an editor. Combined with <code>-p</code>, it reads a file, applies your transformation, and writes the result back.</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#basic-in-place-edit","title":"Basic In-Place Edit","text":"<pre><code># Replace all instances of \"localhost\" with \"0.0.0.0\"\nperl -i.bak -pe 's/localhost/0.0.0.0/g' nginx.conf\n</code></pre> <p>This creates <code>nginx.conf.bak</code> (the original) and writes the modified content to <code>nginx.conf</code>.</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#multiple-files-and-safe-workflow","title":"Multiple Files and Safe Workflow","text":"<pre><code># Fix a typo across all config files\nperl -i.bak -pe 's/recieve/receive/g' *.conf\n</code></pre> <p>The safe workflow: run without <code>-i</code> first to preview, then add <code>-i.bak</code> to apply, then <code>diff</code> to verify, then delete backups:</p> <pre><code>perl -pe 's/DEBUG/INFO/g' app.conf          # preview\nperl -i.bak -pe 's/DEBUG/INFO/g' app.conf   # apply\ndiff app.conf app.conf.bak                   # verify\nrm app.conf.bak                              # clean up\n</code></pre> <p>In-Place Editing with Backup (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#begin-and-end-blocks","title":"BEGIN and END Blocks","text":"<p><code>BEGIN</code> and <code>END</code> blocks run before and after the main loop, respectively. In one-liners, they handle initialization and summary output.</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#begin-setup-before-the-loop","title":"BEGIN: Setup Before the Loop","text":"<pre><code># Print a header before processing\nperl -lane 'BEGIN { print \"Name\\tScore\" } print \"$F[0]\\t$F[1]\"' grades.txt\n</code></pre> <p><code>BEGIN</code> runs once, before the first line is read. Use it for headers, variable initialization, or loading modules.</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#end-summary-after-the-loop","title":"END: Summary After the Loop","text":"<pre><code># Sum the second column and print the total\nperl -lane '$sum += $F[1]; END { print \"Total: $sum\" }' sales.txt\n\n# Count matching lines\nperl -ne '$count++ if /ERROR/; END { print \"Errors: $count\\n\" }' app.log\n</code></pre> <p><code>END</code> runs once, after the last line has been processed. Use it for totals, averages, and summary reports.</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#combining-begin-and-end","title":"Combining BEGIN and END","text":"<pre><code># Full report: header, data, footer\nperl -lane 'BEGIN { print \"USER\\tSHELL\" } print \"$F[0]\\t$F[-1]\"; END { print \"Total: $.\" }' /etc/passwd\n</code></pre> <p>The $. Variable in END Blocks</p> <p><code>$.</code> holds the current line number. In an <code>END</code> block, it holds the line number of the last line read - effectively the total line count.</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#multi-line-processing","title":"Multi-Line Processing","text":"<p>Not every problem fits the line-by-line model. The <code>-0</code> flag changes how Perl defines a \"record\":</p> <pre><code># Paragraph mode (-00): records separated by blank lines\nperl -00 -ne 'print if /keyword/' document.txt\n\n# Slurp mode (-0777): read entire file at once\nperl -0777 -ne 'print scalar(() = /error/gi), \" errors\\n\"' app.log\n\n# Multi-line substitution: remove C-style comments\nperl -0777 -pe 's{/\\*.*?\\*/}{}gs' source.c\n\n# Custom record separator\nperl -ne 'BEGIN { $/ = \"---\\n\" } chomp; print \"RECORD: $_\\n\" if length' data.txt\n</code></pre> <p>The <code>/s</code> modifier makes <code>.</code> match newlines, so <code>.*?</code> can span line boundaries.</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#log-parsing-patterns","title":"Log Parsing Patterns","text":"<p>Log files are where Perl one-liners earn their keep. The combination of regex, field splitting, and aggregation handles most log analysis tasks without a dedicated tool.</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#counting-and-ranking","title":"Counting and Ranking","text":"<pre><code># Count HTTP status codes\nperl -lane '$c{$F[8]}++; END { print \"$_: $c{$_}\" for sort keys %c }' access.log\n\n# Top 10 IPs by request count\nperl -lane '$ip{$F[0]}++; END { print \"$ip{$_} $_\" for (sort { $ip{$b} &lt;=&gt; $ip{$a} } keys %ip)[0..9] }' access.log\n</code></pre>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#timestamps-and-aggregation","title":"Timestamps and Aggregation","text":"<pre><code># Print the first and last timestamp from a log\nperl -ne '\n  if (/\\[(\\d{2}\\/\\w+\\/\\d{4}:\\d{2}:\\d{2}:\\d{2})/) { $first //= $1; $last = $1 }\n  END { print \"First: $first\\nLast: $last\\n\" }\n' access.log\n\n# Filter log lines between 09:00 and 10:00\nperl -ne 'print if m{\\[.*/\\d{4}:09:\\d{2}:\\d{2}}' access.log\n\n# Average response time (last field, in microseconds)\nperl -lane '$sum += $F[-1]; $n++; END { printf \"Avg: %.2f ms (%d reqs)\\n\", $sum/$n/1000, $n }' access.log\n</code></pre> <p>Apache Log Analyzer One-Liner Chain (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#csv-and-tsv-manipulation","title":"CSV and TSV Manipulation","text":"<p>For simple CSV (no embedded commas or quotes), Perl one-liners are fast and effective. For complex CSV with quoting, use the <code>Text::CSV</code> module instead.</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#basic-csv-operations","title":"Basic CSV Operations","text":"<pre><code># Extract the third column from a CSV\nperl -F, -lane 'print $F[2]' data.csv\n\n# Convert CSV to TSV\nperl -F, -lane 'print join(\"\\t\", @F)' data.csv\n\n# Convert TSV to CSV\nperl -F'\\t' -lane 'print join(\",\", @F)' data.tsv\n\n# Skip the header line\nperl -F, -lane 'print $F[2] if $. &gt; 1' data.csv\n</code></pre>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#modifying-columns-and-aggregating","title":"Modifying Columns and Aggregating","text":"<pre><code># Add a row number as the first column\nperl -F, -lane 'print join(\",\", $., @F)' data.csv\n\n# Remove the second column (index 1)\nperl -F, -lane 'splice(@F, 1, 1); print join(\",\", @F)' data.csv\n\n# Print rows where column 3 exceeds a threshold\nperl -F, -lane 'print if $F[2] &gt; 1000' sales.csv\n\n# Group by column 1, sum column 2\nperl -F, -lane '$t{$F[0]} += $F[1]; END { print \"$_: $t{$_}\" for sort keys %t }' expenses.csv\n</code></pre> <p>CSV Edge Cases</p> <p>Simple comma splitting fails when fields contain commas, newlines, or quotes. A field like <code>\"Smith, Jr.\"</code> splits incorrectly with <code>-F,</code>. For production CSV parsing, use <code>perl -MText::CSV -e '...'</code> or write a proper script with <code>Text::CSV</code>.</p> <p>In perl -F: -lane 'print $F[2]' /etc/passwd, what does $F[2] contain for a line like root:x:0:0:root:/root:/bin/bash? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#building-one-liners-incrementally","title":"Building One-Liners Incrementally","text":"<p>Complex one-liners grow through small, testable steps: inspect the data, split fields, add the filter, aggregate results, format output.</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#example-finding-the-busiest-hour","title":"Example: Finding the Busiest Hour","text":"<pre><code># Step 1: See the data\nhead -1 access.log\n# 192.168.1.50 - - [15/Jan/2025:09:23:45 +0000] \"GET /index.html HTTP/1.1\" 200 2326\n\n# Step 2: Extract the hour\nperl -ne 'print \"$1\\n\" if /\\[.*?:(\\d{2}):\\d{2}:\\d{2}/' access.log | head\n\n# Step 3: Aggregate by hour\nperl -ne '$h{$1}++ if /\\[.*?:(\\d{2}):\\d{2}:\\d{2}/; END { print \"$_: $h{$_}\\n\" for sort keys %h }' access.log\n\n# Step 4: Sort by count (most requests first)\nperl -ne '$h{$1}++ if /\\[.*?:(\\d{2}):\\d{2}:\\d{2}/; END { printf \"%s:00 - %d requests\\n\", $_, $h{$_} for sort { $h{$b} &lt;=&gt; $h{$a} } keys %h }' access.log\n</code></pre> <p>Each step is independently testable. If step 3 gives wrong numbers, you debug step 2's regex.</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#converting-between-one-liners-and-scripts","title":"Converting Between One-Liners and Scripts","text":""},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#one-liner-to-script","title":"One-Liner to Script","text":"<p>This one-liner counts words per line:</p> <pre><code>perl -lne '$w += scalar(split); END { print $w }'\n</code></pre> <p>The equivalent script:</p> <pre><code>#!/usr/bin/env perl\nuse strict;\nuse warnings;\n\nmy $w = 0;\nwhile (&lt;&gt;) {\n    chomp;                        # -l flag\n    $w += scalar(split);          # -a not used, but split works on $_\n}\nprint \"$w\\n\";                     # END block\n</code></pre> <p>The mapping:</p> Flag Script equivalent <code>-n</code> <code>while (&lt;&gt;) { ... }</code> <code>-p</code> <code>while (&lt;&gt;) { ... } continue { print }</code> <code>-l</code> <code>chomp</code> on input, <code>\"\\n\"</code> on output <code>-a</code> <code>@F = split</code> at the start of the loop <code>-F:</code> <code>@F = split /:/, $_</code> <code>-i.bak</code> Open input, rename, open output <code>BEGIN {}</code> Code before the loop <code>END {}</code> Code after the loop"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#script-to-one-liner","title":"Script to One-Liner","text":"<p>Start with the loop body. Strip variable declarations. Use <code>$_</code> defaults. Collapse to one line:</p> <pre><code># Script version: while (&lt;&gt;) { chomp; my @f = split /,/; print join(\",\", @f), \"\\n\" if $f[2] &gt; 100 }\n# One-liner:\nperl -F, -lane 'print join(\",\", @F) if $F[2] &gt; 100'\n</code></pre> <p>When to Convert</p> <p>Keep it as a one-liner if it fits in a single readable line (roughly 80-120 characters). Convert to a script when it needs error handling, multiple data sources, or will be run by others.</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#comparison-with-sed-and-awk","title":"Comparison with sed and awk","text":""},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#sed-vs-perl","title":"sed vs Perl","text":"<p><code>sed</code> excels at simple substitutions. Perl handles everything <code>sed</code> does, plus complex logic:</p> Task sed Perl Substitution <code>sed 's/old/new/g'</code> <code>perl -pe 's/old/new/g'</code> Delete lines <code>sed '/pattern/d'</code> <code>perl -ne 'print unless /pattern/'</code> Line range <code>sed -n '5,10p'</code> <code>perl -ne 'print if 5..10'</code> Conditional sub Awkward <code>perl -pe 's/old/new/ if /context/'</code> Math on captures Not possible <code>perl -pe 's/(\\d+)/$1*2/ge'</code> <p>The <code>/e</code> modifier - evaluating the replacement as code - is where Perl surpasses <code>sed</code> entirely.</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#awk-vs-perl","title":"awk vs Perl","text":"<p><code>awk</code> is a field-oriented processor. Perl's <code>-a</code> flag provides the same model with full language access:</p> Task awk Perl Print field 2 <code>awk '{print $2}'</code> <code>perl -lane 'print $F[1]'</code> Sum a column <code>awk '{s+=$3} END{print s}'</code> <code>perl -lane '$s+=$F[2]; END{print $s}'</code> Field separator <code>awk -F:</code> <code>perl -F:</code> <p>The key difference: <code>awk</code> fields start at 1 (<code>$1</code> is the first field, <code>$0</code> is the entire line), while Perl's <code>@F</code> starts at 0 (<code>$F[0]</code> is the first field, <code>$_</code> is the entire line).</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#when-to-use-which","title":"When to Use Which","text":"<p>Use <code>sed</code> for simple substitutions on one file. Use <code>awk</code> for quick column extraction. Use Perl when you need conditional logic, computed replacements, multiple operations in one pass, or code that will grow into a script.</p> <p>Extract and Count Patterns from a Log (requires JavaScript)</p> <p>Command Builder (requires JavaScript)</p> <p>CSV-to-JSON Converter One-Liner (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/text-processing-oneliners/#further-reading","title":"Further Reading","text":"<ul> <li>perlrun - complete reference for Perl command-line flags and environment variables</li> <li>perlvar - special variables including <code>$_</code>, <code>@F</code>, <code>$.</code>, and <code>$/</code></li> <li>Perl One-Liners Cookbook - Peteris Krumins' collection of practical one-liners with explanations</li> <li>Minimal Perl - Tim Maher's guide to Perl as a Unix command-line tool</li> <li>perlre - regex reference for the patterns used throughout one-liners</li> <li>Text::CSV documentation - robust CSV parsing for when simple splitting is not enough</li> </ul> <p>Previous: Testing | Next: Networking and Daemons | Back to Index</p>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/","title":"Web Frameworks and APIs","text":""},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#modern-perl-web-development","title":"Modern Perl Web Development","text":"<p>Version: 1.1 Year: 2026</p>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#copyright-notice","title":"Copyright Notice","text":"<p>Copyright (c) 2025-2026 Ryan Thomas Robson / Robworks Software LLC. Licensed under CC BY-NC-ND 4.0. You may share this material for non-commercial purposes with attribution, but you may not distribute modified versions.</p> <p>Perl has a mature web ecosystem that runs everything from small internal APIs to high-traffic production services. Two frameworks dominate modern Perl web development: Mojolicious for full-featured applications and Dancer2 for lightweight microservices. Both sit on top of PSGI, a specification that decouples your application code from the web server, much like Python's WSGI or Ruby's Rack.</p> <p>This guide covers the foundation layer (PSGI/Plack), the two major frameworks, RESTful API design patterns, and everything you need to go from a hello-world route to a deployed, production-ready web application.</p>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#psgi-and-plack","title":"PSGI and Plack","text":"<p>Before PSGI, Perl web applications were tied to specific deployment mechanisms - CGI, mod_perl, FastCGI - and switching between them meant rewriting glue code. PSGI (Perl Web Server Gateway Interface) defines a standard interface between web servers and Perl applications. Your application is a code reference that receives a request environment hash and returns a three-element array reference:</p> <pre><code>my $app = sub {\n    my ($env) = @_;\n    return [\n        200,                                    # HTTP status\n        ['Content-Type' =&gt; 'text/plain'],       # headers\n        ['Hello, PSGI!'],                       # body\n    ];\n};\n</code></pre> <p>Plack is the reference implementation of PSGI. It provides <code>plackup</code> (a development server), middleware components, and adapters for production servers like Starman and Twiggy.</p> <pre><code>plackup app.psgi                                       # development\nplackup -s Starman --workers 4 --port 5000 app.psgi    # production\n</code></pre>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#the-psgi-stack","title":"The PSGI Stack","text":"<pre><code>flowchart TD\n    A[HTTP Client] --&gt; B[Web Server / Reverse Proxy]\n    B --&gt; C[PSGI Server\\nStarman / Hypnotoad / Twiggy]\n    C --&gt; D[Middleware Stack\\nPlack::Middleware::*]\n    D --&gt; E[PSGI Application\\nMojolicious / Dancer2 / Raw PSGI]\n    E --&gt; F[Response]\n    F --&gt; D\n    D --&gt; C\n    C --&gt; B\n    B --&gt; A</code></pre> <p>The PSGI environment hash (<code>$env</code>) contains request data:</p> Key Description Example <code>REQUEST_METHOD</code> HTTP method <code>GET</code>, <code>POST</code> <code>PATH_INFO</code> Request path <code>/api/users/42</code> <code>QUERY_STRING</code> URL parameters <code>page=2&amp;limit=10</code> <code>HTTP_HOST</code> Host header <code>example.com</code> <code>CONTENT_TYPE</code> Request content type <code>application/json</code> <code>psgi.input</code> Request body stream IO handle <p>What does a PSGI application return? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#mojolicious","title":"Mojolicious","text":"<p>Mojolicious is a real-time web framework with zero non-core dependencies. It includes an HTTP client, WebSocket support, a template engine, a built-in development server, and a hot-reloading production server called Hypnotoad.</p>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#installation","title":"Installation","text":"<pre><code>cpanm Mojolicious\n</code></pre>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#mojoliciouslite","title":"Mojolicious::Lite","text":"<p>For small applications and prototyping, Mojolicious::Lite provides a single-file DSL:</p> <pre><code>use Mojolicious::Lite -signatures;\n\nget '/' =&gt; sub ($c) {\n    $c-&gt;render(text =&gt; 'Hello, Mojolicious!');\n};\n\nget '/greet/:name' =&gt; sub ($c) {\n    my $name = $c-&gt;param('name');\n    $c-&gt;render(text =&gt; \"Hello, $name!\");\n};\n\napp-&gt;start;\n</code></pre> <p>Mojolicious Hello World and Routes (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#routes-and-controllers","title":"Routes and Controllers","text":"<p>Routes map HTTP methods and paths to handler code. Beyond simple placeholders, Mojolicious supports route constraints, optional parameters, and nested routes:</p> <pre><code>get '/user/:id' =&gt; [id =&gt; qr/\\d+/] =&gt; sub ($c) {\n    $c-&gt;render(json =&gt; {user_id =&gt; $c-&gt;param('id')});\n};\n\nget '/page/:num' =&gt; {num =&gt; 1} =&gt; sub ($c) {\n    $c-&gt;render(text =&gt; \"Page \" . $c-&gt;param('num'));\n};\n</code></pre> <p>For larger applications, move handlers into controller classes:</p> <pre><code># lib/MyApp/Controller/Users.pm\npackage MyApp::Controller::Users;\nuse Mojo::Base 'Mojolicious::Controller', -signatures;\n\nsub show ($self) {\n    my $user = $self-&gt;app-&gt;model-&gt;get_user($self-&gt;param('id'));\n    return $self-&gt;reply-&gt;not_found unless $user;\n    $self-&gt;render(json =&gt; $user);\n}\n1;\n</code></pre>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#templates","title":"Templates","text":"<p>Mojolicious includes Embedded Perl (EP) templates:</p> <pre><code>get '/dashboard' =&gt; sub ($c) {\n    $c-&gt;stash(title =&gt; 'Dashboard', items =&gt; ['Tasks', 'Messages']);\n    $c-&gt;render(template =&gt; 'dashboard');\n};\n\n__DATA__\n@@ dashboard.html.ep\n&lt;h1&gt;&lt;%= $title %&gt;&lt;/h1&gt;\n&lt;ul&gt;\n% for my $item (@$items) {\n  &lt;li&gt;&lt;%= $item %&gt;&lt;/li&gt;\n% }\n&lt;/ul&gt;\n</code></pre> Tag Purpose <code>&lt;%= expr %&gt;</code> Output expression (HTML-escaped) <code>&lt;%== expr %&gt;</code> Output raw (no escaping) <code>% code</code> Perl code line"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#websockets","title":"WebSockets","text":"<p>Mojolicious has first-class WebSocket support:</p> <pre><code>websocket '/ws' =&gt; sub ($c) {\n    $c-&gt;on(message =&gt; sub ($c, $msg) {\n        $c-&gt;send(\"Echo: $msg\");\n    });\n};\n</code></pre>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#dancer2","title":"Dancer2","text":"<p>Dancer2 is a lightweight framework inspired by Ruby's Sinatra. It emphasizes simplicity and convention over configuration.</p> <pre><code>cpanm Dancer2\ndancer2 gen -a MyApp\n</code></pre> <p>Dancer2 App Setup (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#routes-and-plugins","title":"Routes and Plugins","text":"<p>Dancer2 routes use a concise DSL:</p> <pre><code>package MyApp;\nuse Dancer2;\n\nget '/hello/:name' =&gt; sub {\n    my $name = route_parameters-&gt;get('name');\n    return \"Hello, $name!\";\n};\n\npost '/data' =&gt; sub {\n    my $payload = body_parameters-&gt;get('key');\n    return \"Received: $payload\";\n};\n</code></pre> <p>The plugin ecosystem provides extensions for databases, authentication, sessions, and templates. Template Toolkit is the most common template engine:</p> <pre><code># config.yml: template: \"template_toolkit\"\n\nget '/user/:id' =&gt; sub {\n    template 'user' =&gt; { user =&gt; get_user(route_parameters-&gt;get('id')) };\n};\n</code></pre> <p>When would you choose Dancer2 over Mojolicious? (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#restful-api-design","title":"RESTful API Design","text":""},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#resource-oriented-routes","title":"Resource-Oriented Routes","text":"<p>Map HTTP methods to CRUD operations on resources:</p> Method Path Action <code>GET</code> <code>/api/tasks</code> List all tasks <code>POST</code> <code>/api/tasks</code> Create a task <code>GET</code> <code>/api/tasks/:id</code> Get one task <code>PUT</code> <code>/api/tasks/:id</code> Replace a task <code>DELETE</code> <code>/api/tasks/:id</code> Remove a task"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#json-handling","title":"JSON Handling","text":"<p>Mojolicious has built-in JSON support through Mojo::JSON:</p> <pre><code>post '/api/tasks' =&gt; sub ($c) {\n    my $data = $c-&gt;req-&gt;json;\n    return $c-&gt;render(json =&gt; {error =&gt; 'Invalid JSON'}, status =&gt; 400)\n        unless $data;\n    $c-&gt;render(json =&gt; create_task($data), status =&gt; 201);\n};\n</code></pre> <p>Dancer2 auto-serializes with Dancer2::Serializer::JSON:</p> <pre><code># config.yml: serializer: JSON\npost '/api/tasks' =&gt; sub {\n    my $data = request-&gt;data;    # auto-deserialized\n    status 201;\n    return create_task($data);   # auto-serialized to JSON\n};\n</code></pre>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#request-lifecycle","title":"Request Lifecycle","text":"<pre><code>sequenceDiagram\n    participant Client\n    participant Server\n    participant Middleware\n    participant Router\n    participant Controller\n    participant Model\n\n    Client-&gt;&gt;Server: HTTP Request\n    Server-&gt;&gt;Middleware: PSGI $env\n    Middleware-&gt;&gt;Middleware: Auth / Logging / CORS\n    Middleware-&gt;&gt;Router: Processed $env\n    Router-&gt;&gt;Controller: Dispatch to handler\n    Controller-&gt;&gt;Model: Business logic / DB query\n    Model--&gt;&gt;Controller: Data\n    Controller--&gt;&gt;Middleware: PSGI response\n    Middleware--&gt;&gt;Server: Final response\n    Server--&gt;&gt;Client: HTTP Response</code></pre> <p>RESTful Task API with Mojolicious (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#middleware","title":"Middleware","text":"<p>Middleware wraps your application to add cross-cutting concerns - logging, authentication, CORS headers, compression - without cluttering route handlers.</p>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#plack-middleware","title":"Plack Middleware","text":"<p>Plack::Middleware provides a library of reusable components:</p> <pre><code>use Plack::Builder;\nbuilder {\n    enable 'AccessLog';\n    enable 'ContentLength';\n    enable 'Deflater';\n    enable 'Static', path =&gt; qr{^/static/}, root =&gt; './public/';\n    MyApp-&gt;to_app;\n};\n</code></pre>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#framework-hooks","title":"Framework Hooks","text":"<p>Both frameworks provide lifecycle hooks that serve as framework-native middleware:</p> <pre><code># Mojolicious\napp-&gt;hook(before_dispatch =&gt; sub ($c) {\n    $c-&gt;app-&gt;log-&gt;info($c-&gt;req-&gt;method . ' ' . $c-&gt;req-&gt;url);\n});\n\n# Dancer2\nhook before =&gt; sub {\n    if (request-&gt;path =~ m{^/api/} &amp;&amp; !is_authenticated()) {\n        send_error('Unauthorized', 401);\n    }\n};\n</code></pre>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#session-management-and-authentication","title":"Session Management and Authentication","text":""},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#sessions","title":"Sessions","text":"<p>Mojolicious sessions are cookie-based by default, signed using the application secret:</p> <pre><code>app-&gt;secrets(['change-this-to-a-random-string']);\n\npost '/login' =&gt; sub ($c) {\n    my $user = authenticate($c-&gt;req-&gt;json);\n    return $c-&gt;render(json =&gt; {error =&gt; 'Invalid credentials'}, status =&gt; 401)\n        unless $user;\n    $c-&gt;session(user_id =&gt; $user-&gt;{id});\n    $c-&gt;session(expires =&gt; time + 3600);\n    $c-&gt;render(json =&gt; {message =&gt; 'Logged in'});\n};\n</code></pre> <p>Dancer2 sessions are configured through <code>config.yml</code>:</p> <pre><code>session: YAML    # or Cookie, Memcached, Redis\n</code></pre> <pre><code>post '/login' =&gt; sub {\n    session user_id =&gt; authenticate(request-&gt;data)-&gt;{id};\n    return {message =&gt; 'Logged in'};\n};\n</code></pre>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#token-based-authentication-jwt","title":"Token-Based Authentication (JWT)","text":"<p>For APIs, JSON Web Tokens avoid server-side session storage. The Mojo::JWT module handles encoding and decoding:</p> <pre><code>use Mojo::JWT;\nmy $SECRET = $ENV{JWT_SECRET} || die \"JWT_SECRET required\\n\";\n\nsub generate_token {\n    my ($user_id) = @_;\n    return Mojo::JWT-&gt;new(\n        secret  =&gt; $SECRET,\n        claims  =&gt; {sub =&gt; $user_id, iat =&gt; time},\n        expires =&gt; time + 3600,\n    )-&gt;encode;\n}\n</code></pre>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#authentication-middleware-with-under","title":"Authentication Middleware with under","text":"<p>Protect API routes with Mojolicious's <code>under</code>, which acts as a route prefix guard:</p> <pre><code>under '/api' =&gt; sub ($c) {\n    my $auth = $c-&gt;req-&gt;headers-&gt;authorization // '';\n    my ($token) = $auth =~ /^Bearer\\s+(.+)$/;\n    my $claims = eval { Mojo::JWT-&gt;new(secret =&gt; $SECRET)-&gt;decode($token // '') };\n\n    unless ($claims) {\n        $c-&gt;render(json =&gt; {error =&gt; 'Unauthorized'}, status =&gt; 401);\n        return undef;\n    }\n    $c-&gt;stash(user_id =&gt; $claims-&gt;{sub});\n    return 1;\n};\n</code></pre> <p>Mojolicious 'under' Routes</p> <p><code>under</code> creates a route prefix that acts as middleware. If the handler returns a false value, the request chain stops. This is the idiomatic way to add authentication guards in Mojolicious.</p>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#database-integration","title":"Database Integration","text":""},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#dbi-direct","title":"DBI Direct","text":"<p>DBI is the standard database interface for Perl:</p> <pre><code>use DBI;\nmy $dbh = DBI-&gt;connect('dbi:Pg:dbname=myapp', 'appuser', 'secret',\n    {RaiseError =&gt; 1, AutoCommit =&gt; 1});\n\nget '/api/users' =&gt; sub ($c) {\n    my $users = $dbh-&gt;selectall_arrayref(\n        'SELECT id, name, email FROM users', {Slice =&gt; {}});\n    $c-&gt;render(json =&gt; $users);\n};\n</code></pre>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#mojopg","title":"Mojo::Pg","text":"<p>Mojo::Pg integrates PostgreSQL with Mojolicious, providing connection pooling, migrations, and non-blocking queries:</p> <pre><code>use Mojo::Pg;\nhelper pg =&gt; sub { state $pg = Mojo::Pg-&gt;new($ENV{DATABASE_URL}) };\napp-&gt;pg-&gt;auto_migrate(1)-&gt;migrations-&gt;from_data;\n\npost '/api/tasks' =&gt; sub ($c) {\n    my $task = $c-&gt;pg-&gt;db-&gt;insert('tasks',\n        {title =&gt; $c-&gt;req-&gt;json-&gt;{title}},\n        {returning =&gt; '*'})-&gt;hash;\n    $c-&gt;render(json =&gt; $task, status =&gt; 201);\n};\n</code></pre>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#dbixclass","title":"DBIx::Class","text":"<p>DBIx::Class is Perl's ORM with relationship mapping and a Perl-level query interface:</p> <pre><code>package MyApp::Schema::Result::Task;\nuse base 'DBIx::Class::Core';\n__PACKAGE__-&gt;table('tasks');\n__PACKAGE__-&gt;add_columns(\n    id    =&gt; {data_type =&gt; 'integer', is_auto_increment =&gt; 1},\n    title =&gt; {data_type =&gt; 'text', is_nullable =&gt; 0},\n);\n__PACKAGE__-&gt;set_primary_key('id');\n1;\n</code></pre> Approach Best For Trade-off DBI direct Simple queries, full SQL control Manual SQL, no abstraction Mojo::Pg Mojolicious apps, PostgreSQL Postgres-specific DBIx::Class Complex schemas, relationships Learning curve, startup overhead"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#deployment","title":"Deployment","text":""},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#development-servers","title":"Development Servers","text":"<pre><code>morbo myapp.pl                        # Mojolicious - auto-reloads on changes\nplackup -R lib/ bin/app.psgi          # Dancer2 / any PSGI app\n</code></pre>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#production-servers","title":"Production Servers","text":"<p>Hypnotoad is Mojolicious's preforking server with zero-downtime restarts:</p> <pre><code>hypnotoad myapp.pl             # start\nhypnotoad myapp.pl             # run again for zero-downtime restart\nhypnotoad -s myapp.pl          # stop\n</code></pre> <pre><code>app-&gt;config(hypnotoad =&gt; {\n    listen  =&gt; ['http://*:8080'],\n    workers =&gt; 4,\n    proxy   =&gt; 1,\n});\n</code></pre> <p>Starman is a high-performance preforking PSGI server for any framework:</p> <pre><code>starman --workers 4 --port 5000 bin/app.psgi\n</code></pre>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#reverse-proxy","title":"Reverse Proxy","text":"<p>Place Nginx in front for TLS termination, static files, and load balancing:</p> <pre><code>upstream myapp { server 127.0.0.1:8080; }\nserver {\n    listen 443 ssl;\n    server_name example.com;\n    location /static/ { alias /var/www/myapp/public/; expires 30d; }\n    location / {\n        proxy_pass http://myapp;\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n</code></pre> <p>Enable Proxy Mode</p> <p>When behind a reverse proxy, enable proxy mode so your app trusts <code>X-Forwarded-For</code> and <code>X-Forwarded-Proto</code> headers. In Mojolicious: <code>proxy =&gt; 1</code> in the hypnotoad config. In Dancer2: <code>behind_proxy: 1</code> in <code>config.yml</code>.</p>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#docker","title":"Docker","text":"<pre><code>FROM perl:5.38-slim\nRUN cpanm --notest Mojolicious Mojo::Pg\nWORKDIR /app\nCOPY . .\nEXPOSE 8080\nCMD [\"hypnotoad\", \"-f\", \"myapp.pl\"]\n</code></pre> <p>The <code>-f</code> flag keeps Hypnotoad in the foreground, which Docker requires. Pair with <code>docker-compose.yml</code> for database services:</p> <pre><code>services:\n  web:\n    build: .\n    ports: [\"8080:8080\"]\n    environment:\n      DATABASE_URL: postgresql://user:pass@db:5432/myapp\n    depends_on: [db]\n  db:\n    image: postgres:16\n    environment: { POSTGRES_USER: user, POSTGRES_PASSWORD: pass, POSTGRES_DB: myapp }\n</code></pre>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#exercises","title":"Exercises","text":"<p>Build a URL Shortener API (requires JavaScript)</p> <p>Task Management API with Authentication (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#scaffolding-with-mojo-generate","title":"Scaffolding with mojo generate","text":"<p>The <code>mojo generate</code> command creates boilerplate for Mojolicious projects:</p> <p>Command Builder (requires JavaScript)</p>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#putting-it-all-together","title":"Putting It All Together","text":"<p>Perl's web ecosystem gives you a clear stack from the protocol layer to the application layer:</p> <ul> <li>PSGI/Plack provides the universal interface between servers and applications</li> <li>Mojolicious is a batteries-included framework with built-in HTTP client, WebSockets, templates, and production server</li> <li>Dancer2 is a micro-framework that stays out of your way and lets you choose your components</li> <li>RESTful patterns map HTTP methods to CRUD operations on resources with proper status codes</li> <li>Middleware adds cross-cutting concerns without coupling them to your route logic</li> <li>Sessions and authentication can use cookies, tokens (JWT), or server-side storage</li> <li>Database integration ranges from raw DBI to Mojo::Pg to DBIx::Class for full ORM</li> <li>Deployment means a reverse proxy (Nginx) in front of a preforking server (Hypnotoad or Starman), optionally containerized with Docker</li> </ul> <p>Start with Mojolicious::Lite for prototyping, graduate to full Mojolicious when you need controllers and models, and reach for Dancer2 when you want a Sinatra-style micro-framework. The PSGI layer means you can mix and match servers without changing application code.</p>"},{"location":"Dev%20Zero/Perl/web-frameworks-apis/#further-reading","title":"Further Reading","text":"<ul> <li>Mojolicious documentation - official guides, cookbook, and API reference</li> <li>Mojolicious::Lite tutorial - step-by-step introduction</li> <li>Dancer2 documentation - framework reference and plugin list</li> <li>Dancer2 manual - comprehensive usage guide</li> <li>PSGI/Plack specification - the interface standard</li> <li>Mojo::Pg documentation - PostgreSQL integration for Mojolicious</li> <li>DBIx::Class manual - ORM documentation</li> </ul> <p>Previous: Networking and Daemons | Next: Developer Roadmap | Back to Index</p>"},{"location":"Git/","title":"Git","text":"<p>A comprehensive course on Git - from your first commit through the internals of the object model, collaboration workflows across platforms, and advanced operations at scale. These guides take you from \"I know I should use version control\" to understanding Git deeply enough to debug, optimize, and architect workflows around it.</p> <p>Each guide is self-contained, but the order below follows a natural learning path.</p>"},{"location":"Git/#foundations","title":"Foundations","text":""},{"location":"Git/#introduction-why-git-and-why-version-control","title":"Introduction: Why Git, and Why Version Control","text":"<p>The history of version control from RCS through SVN to distributed systems, and why Linus Torvalds built Git for the Linux kernel. Covers installing Git on every platform, first-time configuration, and a mental model preview of Git as a content-addressable filesystem.</p>"},{"location":"Git/#the-three-trees-working-directory-index-and-repository","title":"The Three Trees: Working Directory, Index, and Repository","text":"<p>Git's core mental model - the working tree, staging area, and committed history. Covers the complete file lifecycle from untracked through committed, the <code>.gitignore</code> pattern syntax, and a first look inside the <code>.git</code> directory.</p>"},{"location":"Git/#commits-and-history","title":"Commits and History","text":"<p>What a commit actually stores and how to explore history. Covers commit object anatomy, writing effective messages, <code>git log</code> formatting and filtering in depth, <code>git diff</code> across the three trees, and amending commits.</p>"},{"location":"Git/#branches-and-merging","title":"Branches and Merging","text":"<p>Branches as movable pointers, not copies. Covers creating, switching, and managing branches, fast-forward vs three-way merges, conflict resolution strategies, and the difference between <code>git switch</code> and <code>git checkout</code>.</p>"},{"location":"Git/#core-workflows","title":"Core Workflows","text":""},{"location":"Git/#remote-repositories","title":"Remote Repositories","text":"<p>Working with code that lives somewhere else. Covers <code>clone</code>, <code>fetch</code> vs <code>pull</code>, <code>push</code> and rejection handling, tracking branches, multiple remotes for fork workflows, and SSH vs HTTPS authentication setup.</p>"},{"location":"Git/#rewriting-history","title":"Rewriting History","text":"<p>When and how to clean up your commit history. Covers <code>commit --amend</code>, interactive rebase (squash, fixup, reorder, edit), <code>cherry-pick</code>, <code>revert</code> vs <code>reset</code> with all three modes, and the reflog as your safety net.</p>"},{"location":"Git/#stashing-and-the-worktree","title":"Stashing and the Worktree","text":"<p>Interrupting work without losing it. Covers <code>git stash</code> in depth (push, pop, apply, branch), stashing untracked and ignored files, multiple working trees with <code>git worktree</code>, and <code>git clean</code> for resetting your workspace.</p>"},{"location":"Git/#configuring-git","title":"Configuring Git","text":"<p>Making Git work the way you want. Covers configuration levels and precedence, aliases, custom diff/merge tools, conditional includes for work vs personal, <code>.gitattributes</code> for line endings and diff drivers, and environment variables.</p>"},{"location":"Git/#git-internals","title":"Git Internals","text":""},{"location":"Git/#the-object-model","title":"The Object Model","text":"<p>Git as a content-addressable filesystem. Covers the four object types (blobs, trees, commits, annotated tags), SHA-1 hashing, the <code>.git/objects</code> directory, and building a commit entirely with plumbing commands.</p>"},{"location":"Git/#refs-the-reflog-and-the-dag","title":"Refs, the Reflog, and the DAG","text":"<p>How Git names things and connects them. Covers references, symbolic refs like HEAD, the directed acyclic graph, garbage collection, the reflog's per-ref change history, and how packfiles compress objects with delta encoding.</p>"},{"location":"Git/#transfer-protocols-and-plumbing","title":"Transfer Protocols and Plumbing","text":"<p>How <code>fetch</code> and <code>push</code> work at the protocol level. Covers smart HTTP, SSH, and native transports, pack negotiation, shallow and partial clones, sparse checkout, <code>git bundle</code> for offline transfer, and protocol v2 improvements.</p>"},{"location":"Git/#platform-collaboration","title":"Platform Collaboration","text":""},{"location":"Git/#collaboration-workflows","title":"Collaboration Workflows","text":"<p>Choosing how your team works with Git. Covers centralized, feature branch, Gitflow, trunk-based, and forking workflows, plus pull request best practices, branch protection, and release management with semantic versioning.</p>"},{"location":"Git/#github-gitlab-and-bitbucket","title":"GitHub, GitLab, and Bitbucket","text":"<p>Platform-specific features and tooling. Covers PRs vs MRs, CI/CD configuration (Actions, <code>.gitlab-ci.yml</code>, Pipelines), <code>gh</code> and <code>glab</code> CLIs, code owners, and migrating between platforms.</p>"},{"location":"Git/#advanced-operations","title":"Advanced Operations","text":""},{"location":"Git/#git-hooks-and-automation","title":"Git Hooks and Automation","text":"<p>Automating quality checks and workflows. Covers every client-side and server-side hook, hook frameworks (Husky, pre-commit, Lefthook), <code>git bisect</code> for binary search debugging, <code>git blame</code>, and smudge/clean filters.</p>"},{"location":"Git/#git-security","title":"Git Security","text":"<p>Signing, credentials, and secret management. Covers GPG and SSH commit signing, credential helpers, secret scanning tools, removing secrets from history with <code>git filter-repo</code>, and verified commits on platforms.</p>"},{"location":"Git/#monorepos-and-scaling-git","title":"Monorepos and Scaling Git","text":"<p>Git at enterprise scale. Covers sparse checkout, partial clones, the commit graph file, <code>git maintenance</code>, Scalar, filesystem monitors, submodules vs subtrees, and build system integration.</p>"},{"location":"Git/#troubleshooting-and-recovery","title":"Troubleshooting and Recovery","text":"<p>The \"oh no\" recovery guide. Covers recovering lost commits from the reflog, undoing every common mistake, Git LFS for large files, history rewriting at scale, performance diagnosis, and repository corruption repair.</p>"},{"location":"Git/branches-and-merging/","title":"Branches and Merging","text":"<p>Branching is where Git's design pays off. In older systems like SVN, creating a branch meant copying the entire directory tree - it was slow, expensive, and merging was painful enough that teams avoided branches. In Git, a branch is a 41-byte file containing a commit hash. Creating one is instant. Merging is a first-class operation. This changes how you work: branches become disposable tools for isolating work, not heavyweight decisions.</p>"},{"location":"Git/branches-and-merging/#what-is-a-branch","title":"What Is a Branch?","text":"<p>A branch in Git is a movable pointer to a commit. That's it. The file <code>.git/refs/heads/main</code> contains the 40-character SHA-1 hash of the commit that <code>main</code> currently points to. When you make a new commit on <code>main</code>, Git updates that pointer to the new commit.</p> <pre><code>main \u2500\u2500\u2192 e4f5a6b \u2500\u2500\u2192 c3d4e5f \u2500\u2500\u2192 a1b2c3d\n</code></pre> <p>When you create a new branch, Git creates a new pointer at the current commit. Both branches now point to the same commit - no files are copied:</p> <pre><code>main \u2500\u2500\u2500\u2500\u2500\u2500\u2192 e4f5a6b\nfeature/auth \u2500\u2192 e4f5a6b\n</code></pre> <p>As you make commits on each branch, their pointers diverge:</p> <pre><code>main \u2500\u2500\u2500\u2500\u2192 f7g8h9i \u2500\u2500\u2192 e4f5a6b\n                         \u2191\nfeature/auth \u2500\u2500\u2192 x1y2z3a \u2500\u2500\u2192 e4f5a6b\n</code></pre> <p><code>HEAD</code> points to whichever branch you currently have checked out. It's how Git knows which branch to advance when you commit.</p> <p>What is a branch in Git? (requires JavaScript)</p> <p>Branches as Files in .git/refs/heads/ (requires JavaScript)</p>"},{"location":"Git/branches-and-merging/#creating-branches","title":"Creating Branches","text":"<pre><code># Create a new branch at the current commit\ngit branch feature/search\n\n# Create and switch to it in one step\ngit switch -c feature/search\n\n# Create a branch at a specific commit\ngit branch hotfix/login a1b2c3d\n\n# Create a branch from another branch\ngit branch feature/search-v2 feature/search\n</code></pre>"},{"location":"Git/branches-and-merging/#listing-branches","title":"Listing Branches","text":"<pre><code># List local branches (* marks current)\ngit branch\n\n# List all branches (including remote-tracking)\ngit branch -a\n\n# List branches with last commit info\ngit branch -v\n\n# List branches merged into current branch\ngit branch --merged\n\n# List branches NOT merged into current branch\ngit branch --no-merged\n</code></pre>"},{"location":"Git/branches-and-merging/#switching-branches-git-switch-vs-git-checkout","title":"Switching Branches: <code>git switch</code> vs <code>git checkout</code>","text":"<p>Git 2.23 introduced <code>git switch</code> as a clearer alternative to <code>git checkout</code> for changing branches:</p> <pre><code># Modern (Git 2.23+)\ngit switch main\ngit switch feature/auth\ngit switch -c new-branch          # Create and switch\n\n# Legacy (still works)\ngit checkout main\ngit checkout feature/auth\ngit checkout -b new-branch        # Create and switch\n</code></pre> <p><code>git checkout</code> is overloaded - it switches branches, restores files, creates branches, and detaches HEAD. <code>git switch</code> does one thing: switch branches. Use <code>git switch</code> for branch operations and <code>git restore</code> for file operations.</p> <p>Uncommitted changes and switching</p> <p>If you have uncommitted changes that would conflict with the branch you're switching to, Git refuses the switch to prevent data loss. You have three options: commit the changes, stash them (<code>git stash</code>), or discard them (<code>git restore .</code>).</p>"},{"location":"Git/branches-and-merging/#renaming-and-deleting-branches","title":"Renaming and Deleting Branches","text":"<pre><code># Rename current branch\ngit branch -m new-name\n\n# Rename a specific branch\ngit branch -m old-name new-name\n\n# Delete a branch (only if fully merged)\ngit branch -d feature/old\n\n# Force delete (even if not merged - you'll lose unmerged commits)\ngit branch -D feature/abandoned\n</code></pre> <p>Recovering a deleted branch</p> <p>Deleted a branch by accident? The commits still exist in the object database. Use <code>git reflog</code> to find the commit hash, then recreate the branch: <code>git branch recovered-branch a1b2c3d</code>. The Rewriting History guide covers the reflog in depth.</p>"},{"location":"Git/branches-and-merging/#merging","title":"Merging","text":"<p>Merging combines the work from two branches. When you run <code>git merge</code>, Git takes the commits from one branch and integrates them into the current branch.</p>"},{"location":"Git/branches-and-merging/#fast-forward-merge","title":"Fast-Forward Merge","text":"<p>A fast-forward merge happens when the target branch has no new commits since the source branch diverged. Git simply moves the branch pointer forward - no new commit is created:</p> <p>Before: <pre><code>main:    A \u2500\u2500 B\n                \\\nfeature:         C \u2500\u2500 D\n</code></pre></p> <pre><code>git switch main\ngit merge feature\n</code></pre> <p>After: <pre><code>main:    A \u2500\u2500 B \u2500\u2500 C \u2500\u2500 D\n</code></pre></p> <p>Git moved <code>main</code> forward to point at <code>D</code>. The history is linear - no merge commit.</p>"},{"location":"Git/branches-and-merging/#three-way-merge","title":"Three-Way Merge","text":"<p>A three-way merge happens when both branches have new commits since they diverged. Git finds their common ancestor, compares both branches against it, and creates a new merge commit with two parents:</p> <p>Before: <pre><code>main:    A \u2500\u2500 B \u2500\u2500 E\n                \\\nfeature:         C \u2500\u2500 D\n</code></pre></p> <pre><code>git switch main\ngit merge feature\n</code></pre> <p>After: <pre><code>main:    A \u2500\u2500 B \u2500\u2500 E \u2500\u2500 M\n                \\      /\nfeature:         C \u2500\u2500 D\n</code></pre></p> <p><code>M</code> is the merge commit. It has two parents: <code>E</code> (from main) and <code>D</code> (from feature). The merge commit records that these two lines of development were combined.</p> <pre><code>gitGraph\n   commit id: \"A\"\n   commit id: \"B\"\n   branch feature\n   commit id: \"C\"\n   commit id: \"D\"\n   checkout main\n   commit id: \"E\"\n   merge feature id: \"M\"</code></pre>"},{"location":"Git/branches-and-merging/#forcing-a-merge-commit","title":"Forcing a Merge Commit","text":"<p>Sometimes you want a merge commit even when a fast-forward is possible, to preserve the record that a branch existed:</p> <pre><code>git merge --no-ff feature\n</code></pre> <p>This is common in workflows where you want the history to show that features were developed on separate branches, even if the history could be linear.</p>"},{"location":"Git/branches-and-merging/#merge-conflicts","title":"Merge Conflicts","text":"<p>A merge conflict happens when both branches modified the same part of the same file. Git can automatically merge changes to different files or different parts of the same file, but when two branches change the same lines, Git can't decide which version to keep.</p>"},{"location":"Git/branches-and-merging/#what-causes-conflicts","title":"What Causes Conflicts","text":"<ul> <li>Both branches edited the same line(s) of the same file</li> <li>One branch deleted a file that the other modified</li> <li>Both branches added a file with the same name but different content</li> </ul>"},{"location":"Git/branches-and-merging/#reading-conflict-markers","title":"Reading Conflict Markers","text":"<p>When a conflict occurs, Git marks the conflicting sections in the file:</p> <pre><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nconst timeout = 3000;\n=======\nconst timeout = 5000;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature/timeout-update\n</code></pre> <ul> <li><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</code> to <code>=======</code>: your current branch's version</li> <li><code>=======</code> to <code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;</code>: the incoming branch's version</li> </ul> <p>With <code>merge.conflictstyle = diff3</code> configured, Git also shows the original (base) version:</p> <pre><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nconst timeout = 3000;\n||||||| merged common ancestors\nconst timeout = 1000;\n=======\nconst timeout = 5000;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature/timeout-update\n</code></pre> <p>This third section (the base) is invaluable - it shows what the code looked like before either change, making it much easier to understand the intent of both modifications.</p>"},{"location":"Git/branches-and-merging/#resolution-process","title":"Resolution Process","text":"<ol> <li>Run <code>git merge</code> and see the conflict</li> <li>Open the conflicted file(s)</li> <li>Edit to resolve - keep one version, combine both, or write something new</li> <li>Remove all conflict markers (<code>&lt;&lt;&lt;&lt;&lt;&lt;&lt;</code>, <code>=======</code>, <code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;</code>)</li> <li>Stage the resolved file(s) with <code>git add</code></li> <li>Complete the merge with <code>git commit</code></li> </ol> <pre><code>git merge feature/timeout-update\n# CONFLICT: Merge conflict in config.js\n\n# Edit config.js to resolve the conflict\n# ... make your edits ...\n\ngit add config.js\ngit commit    # Opens editor with pre-populated merge commit message\n</code></pre> <p>To abort a merge in progress and return to the pre-merge state:</p> <pre><code>git merge --abort\n</code></pre> <p>Triggering and Resolving a Merge Conflict (requires JavaScript)</p> <p>What is the key difference between git merge and git rebase? (requires JavaScript)</p>"},{"location":"Git/branches-and-merging/#branch-management-practices","title":"Branch Management Practices","text":""},{"location":"Git/branches-and-merging/#naming-conventions","title":"Naming Conventions","text":"<p>Most teams use prefixed branch names to categorize work:</p> Prefix Purpose Example <code>feature/</code> New functionality <code>feature/user-search</code> <code>bugfix/</code> or <code>fix/</code> Bug fixes <code>bugfix/login-timeout</code> <code>hotfix/</code> Urgent production fixes <code>hotfix/security-patch</code> <code>release/</code> Release preparation <code>release/2.1.0</code> <code>chore/</code> Maintenance tasks <code>chore/upgrade-deps</code> <p>Keep branch names lowercase, use hyphens for spaces, and include a ticket number if your team uses issue trackers: <code>feature/PROJ-123-user-search</code>.</p>"},{"location":"Git/branches-and-merging/#short-lived-branches","title":"Short-Lived Branches","text":"<p>Branches work best when they're short-lived. A branch that lives for months accumulates merge conflicts and diverges from the mainline. Aim for branches that last days, not weeks. If a feature is large, break it into smaller branches that merge incrementally.</p>"},{"location":"Git/branches-and-merging/#exercises","title":"Exercises","text":"<p>Merge Conflict Resolution (requires JavaScript)</p> <p>Branch Management (requires JavaScript)</p>"},{"location":"Git/branches-and-merging/#further-reading","title":"Further Reading","text":"<ul> <li>Pro Git - Chapter 3: Git Branching - comprehensive coverage of branching and merging</li> <li>Official git-merge documentation - complete reference for merge strategies and options</li> <li>Official git-switch documentation - the modern branch-switching command</li> <li>Official git-branch documentation - creating, listing, renaming, and deleting branches</li> </ul> <p>Previous: Commits and History | Next: Remote Repositories | Back to Index</p>"},{"location":"Git/collaboration-workflows/","title":"Collaboration Workflows","text":"<p>Git is flexible enough to support many collaboration models. The right workflow depends on your team size, release cadence, and how much process you want. This guide covers the major workflow patterns, how pull requests fit in, and how to choose the right model for your situation.</p>"},{"location":"Git/collaboration-workflows/#centralized-workflow","title":"Centralized Workflow","text":"<p>The simplest model: everyone pushes to <code>main</code>. No feature branches, no pull requests. Each developer pulls, makes changes, and pushes directly.</p> <pre><code>git pull origin main\n# ... make changes ...\ngit add . &amp;&amp; git commit -m \"Add new feature\"\ngit push origin main\n</code></pre> <p>When it works: Small teams (2-3 people) working on different parts of the codebase with minimal overlap. Quick prototypes and internal tools.</p> <p>When it breaks: As the team grows, push conflicts increase. There's no code review before changes hit main. One bad push breaks everyone.</p>"},{"location":"Git/collaboration-workflows/#feature-branch-workflow","title":"Feature Branch Workflow","text":"<p>The most common workflow in modern development. Each change gets its own branch, work happens in isolation, and code merges into main through a review process.</p> <pre><code>git switch -c feature/user-search\n# ... develop the feature ...\ngit push -u origin feature/user-search\n# Open a pull request / merge request\n# Team reviews, approves, merges\n</code></pre> <p>Key principles:</p> <ul> <li><code>main</code> is always deployable</li> <li>All work happens on branches</li> <li>Branches are short-lived (days, not months)</li> <li>Changes merge through reviewed pull requests</li> <li>Branches are deleted after merging</li> </ul> <p>Feature Branch Workflow End-to-End (requires JavaScript)</p>"},{"location":"Git/collaboration-workflows/#gitflow","title":"Gitflow","text":"<p>Gitflow is a structured branching model designed for projects with scheduled releases. It defines specific branch roles:</p> Branch Purpose Lifetime <code>main</code> Production-ready code, every commit is a release Permanent <code>develop</code> Integration branch for features Permanent <code>feature/*</code> Individual features Short (merge into develop) <code>release/*</code> Release preparation, bug fixes, versioning Short (merge into main and develop) <code>hotfix/*</code> Urgent production fixes Short (merge into main and develop) <pre><code>gitGraph\n   commit id: \"v1.0\" tag: \"v1.0\"\n   branch develop\n   commit id: \"dev-1\"\n   branch feature/search\n   commit id: \"search-1\"\n   commit id: \"search-2\"\n   checkout develop\n   merge feature/search id: \"merge-search\"\n   branch release/1.1\n   commit id: \"bump-version\"\n   commit id: \"fix-typo\"\n   checkout main\n   merge release/1.1 id: \"v1.1\" tag: \"v1.1\"\n   checkout develop\n   merge release/1.1 id: \"sync-develop\"</code></pre> <p>When it works: Projects with formal release schedules, multiple environments (staging, production), and regulatory requirements for release tracking.</p> <p>When it breaks: Teams practicing continuous deployment. The overhead of maintaining <code>develop</code>, creating release branches, and double-merging is unnecessary when you ship from main directly. The original author of Gitflow has acknowledged this - for web applications with continuous delivery, simpler models are better.</p> <p>What problem does Gitflow solve that the feature-branch workflow doesn't? (requires JavaScript)</p>"},{"location":"Git/collaboration-workflows/#trunk-based-development","title":"Trunk-Based Development","text":"<p>The opposite philosophy from Gitflow. All developers commit to a single branch (<code>main</code> / <code>trunk</code>), branches are extremely short-lived (hours, not days), and feature flags control what's visible to users.</p> <pre><code>gitGraph\n   commit id: \"A\"\n   commit id: \"B\"\n   branch short-lived\n   commit id: \"C\"\n   checkout main\n   merge short-lived id: \"D\"\n   commit id: \"E\"\n   commit id: \"F\"\n   branch another\n   commit id: \"G\"\n   checkout main\n   merge another id: \"H\"</code></pre> <p>Key principles:</p> <ul> <li>Branches last hours, not days</li> <li>Everyone merges to main multiple times per day</li> <li>Feature flags gate incomplete features</li> <li>Automated tests run on every commit to main</li> <li>Continuous integration catches conflicts immediately</li> </ul> <p>When it works: Teams with strong CI/CD, comprehensive automated tests, and the discipline to keep branches tiny. Google, Facebook, and many high-velocity teams use this.</p> <p>When it breaks: Teams without good test coverage or CI. Without automated quality gates, rapid main-line commits can break production frequently.</p> <p>Which workflow fits a small team shipping continuously? (requires JavaScript)</p>"},{"location":"Git/collaboration-workflows/#forking-workflow","title":"Forking Workflow","text":"<p>The standard model for open-source contributions. Contributors don't have push access to the original repository. Instead:</p> <ol> <li>Fork the repository (creates a copy under your account)</li> <li>Clone your fork</li> <li>Add the original repo as <code>upstream</code></li> <li>Create feature branches on your fork</li> <li>Push to your fork</li> <li>Open a pull request from your fork to the original</li> </ol> <pre><code>flowchart LR\n    subgraph Upstream[\"Original Repository\"]\n        UM[\"main\"]\n    end\n\n    subgraph Origin[\"Your Fork\"]\n        OM[\"main\"]\n        FB[\"feature/fix\"]\n    end\n\n    subgraph Local[\"Your Machine\"]\n        LM[\"main\"]\n        LF[\"feature/fix\"]\n    end\n\n    UM --&gt;|\"fork\"| OM\n    OM --&gt;|\"git clone\"| LM\n    LF --&gt;|\"git push origin\"| FB\n    FB --&gt;|\"Pull Request\"| UM\n    UM --&gt;|\"git fetch upstream\"| LM</code></pre> <p>This workflow protects the original repository. The maintainer reviews every contribution before it enters the codebase. The contributor doesn't need any special permissions.</p> <p>A 15-person e-commerce team deploys to production several times per day. They have extensive automated test suites and use feature flags to control rollouts. Which workflow is the best fit? (requires JavaScript)</p>"},{"location":"Git/collaboration-workflows/#pull-requests-merge-requests","title":"Pull Requests / Merge Requests","text":"<p>A pull request (GitHub, Bitbucket) or merge request (GitLab) is a platform feature - not a Git concept - that wraps the merge process with review, discussion, CI checks, and approval.</p>"},{"location":"Git/collaboration-workflows/#what-a-pr-contains","title":"What a PR Contains","text":"<ul> <li>The source branch and target branch</li> <li>A title and description</li> <li>The diff of all changes</li> <li>A timeline of commits, comments, and reviews</li> <li>CI/CD check results</li> <li>Approval status</li> </ul>"},{"location":"Git/collaboration-workflows/#code-review-best-practices","title":"Code Review Best Practices","text":"<p>For authors:</p> <ul> <li>Keep PRs small and focused (under 400 lines of diff is ideal)</li> <li>Write a clear description explaining what and why</li> <li>Reference related issues</li> <li>Self-review before requesting review</li> <li>Respond to feedback constructively</li> </ul> <p>For reviewers:</p> <ul> <li>Review promptly (within one business day)</li> <li>Focus on correctness, clarity, and maintainability</li> <li>Distinguish between blocking issues and suggestions</li> <li>Ask questions rather than making demands</li> <li>Approve when \"good enough\" - don't block on style preferences</li> </ul>"},{"location":"Git/collaboration-workflows/#branch-protection","title":"Branch Protection","text":"<p>Most platforms support branch protection rules for shared branches:</p> <ul> <li>Require pull request reviews before merging (minimum number of approvals)</li> <li>Require status checks (CI must pass)</li> <li>Require up-to-date branches (must be rebased/merged with main)</li> <li>Restrict who can push (prevent direct pushes to main)</li> <li>Require signed commits</li> <li>Prohibit force pushes</li> </ul>"},{"location":"Git/collaboration-workflows/#release-management","title":"Release Management","text":""},{"location":"Git/collaboration-workflows/#semantic-versioning","title":"Semantic Versioning","text":"<p>Semantic versioning (SemVer) uses three numbers: <code>MAJOR.MINOR.PATCH</code>:</p> <ul> <li>MAJOR: Breaking changes (incompatible API changes)</li> <li>MINOR: New features (backward-compatible)</li> <li>PATCH: Bug fixes (backward-compatible)</li> </ul>"},{"location":"Git/collaboration-workflows/#tagging-releases","title":"Tagging Releases","text":"<pre><code># Create an annotated tag for a release\ngit tag -a v2.1.0 -m \"Release 2.1.0: Add user search, fix login timeout\"\n\n# Push tags to remote\ngit push origin v2.1.0\n# or push all tags\ngit push origin --tags\n</code></pre>"},{"location":"Git/collaboration-workflows/#release-branches","title":"Release Branches","text":"<p>When you need to stabilize a release while development continues:</p> <pre><code># Create release branch\ngit switch -c release/2.1 main\n\n# Bug fixes go on the release branch\ngit commit -m \"Fix edge case in search pagination\"\n\n# When ready, merge into main and tag\ngit switch main\ngit merge release/2.1\ngit tag -a v2.1.0 -m \"Release 2.1.0\"\n\n# Merge back into develop (if using Gitflow)\ngit switch develop\ngit merge release/2.1\n</code></pre>"},{"location":"Git/collaboration-workflows/#monorepos-vs-polyrepos","title":"Monorepos vs Polyrepos","text":"Monorepo Polyrepo Structure All projects in one repository Each project in its own repository Dependencies Shared code is directly importable Shared code is versioned and published Atomic changes One commit can change multiple projects Cross-project changes require coordinated releases CI/CD Must scope builds to affected code Each repo has its own pipeline Scale Needs Git optimization (sparse checkout, etc.) Each repo stays small Examples Google, Meta, Microsoft (Windows) Most open-source projects, microservices <p>The Monorepos and Scaling Git guide covers the Git-specific challenges and solutions for monorepos.</p>"},{"location":"Git/collaboration-workflows/#choosing-a-workflow","title":"Choosing a Workflow","text":"Team Size Release Model Recommended Workflow 1-3 Continuous Feature branch (simple) or trunk-based 4-10 Continuous Feature branch with PR reviews 4-10 Scheduled releases Gitflow or modified feature branch 10+ Continuous Trunk-based with feature flags Open source Maintainer-gated Forking workflow <p>There's no universally correct workflow. Start simple (feature branches with PRs), and add structure (release branches, hotfix process) only when the team's needs demand it.</p> <p>Design a Branching Strategy for a Team (requires JavaScript)</p>"},{"location":"Git/collaboration-workflows/#exercise","title":"Exercise","text":"<p>Feature Branch Workflow End-to-End (requires JavaScript)</p>"},{"location":"Git/collaboration-workflows/#further-reading","title":"Further Reading","text":"<ul> <li>Pro Git - Chapter 5: Distributed Git - workflow models and contribution patterns</li> <li>Atlassian Git Workflow Comparison - visual comparisons of workflow models</li> <li>Martin Fowler: Patterns for Managing Source Code Branches - comprehensive branching pattern analysis</li> <li>Trunk-Based Development - methodology reference for trunk-based workflows</li> <li>Semantic Versioning - version numbering specification</li> <li>A Successful Git Branching Model (Gitflow) - the original Gitflow post</li> </ul> <p>Previous: Transfer Protocols and Plumbing | Next: GitHub, GitLab, and Bitbucket | Back to Index</p>"},{"location":"Git/commits-and-history/","title":"Commits and History","text":"<p>A commit is Git's fundamental unit of work - a permanent snapshot of your entire project at a specific point in time. Understanding what commits contain, how to write good commit messages, and how to navigate history with <code>git log</code> and <code>git diff</code> are essential skills you'll use every day.</p>"},{"location":"Git/commits-and-history/#anatomy-of-a-commit","title":"Anatomy of a Commit","text":"<p>Every commit in Git stores four pieces of information:</p> <ol> <li>Tree - a reference to a tree object that captures the state of every file and directory at the moment of the commit</li> <li>Parent(s) - a reference to the commit(s) that came immediately before. A root commit has no parent. A merge commit has two (or more) parents.</li> <li>Author - who originally wrote the change (name, email, timestamp)</li> <li>Committer - who applied the change to the repository (name, email, timestamp)</li> </ol> <p>The author and committer are usually the same person. They differ when someone applies a patch written by another developer, or during cherry-picks and rebases.</p> <p>A commit also has a message - the human-readable description of what changed and why.</p> <p>Each commit is identified by a SHA-1 hash - a 40-character hexadecimal string computed from all of the above. Change any part of a commit (the files, the message, the parent, the author) and the hash changes. This makes Git's history tamper-evident: you cannot alter past commits without changing every subsequent hash in the chain.</p> <pre><code>git cat-file -p HEAD\n</code></pre> <pre><code>tree 4b825dc642cb6eb9a060e54bf899d15f7e8c9c2f\nparent a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0\nauthor Jane Developer &lt;jane@example.com&gt; 1700000000 -0500\ncommitter Jane Developer &lt;jane@example.com&gt; 1700000000 -0500\n\nAdd user authentication module\n</code></pre> <p>What information is stored in a Git commit object? (requires JavaScript)</p> <p>Reading a Raw Commit Object (requires JavaScript)</p>"},{"location":"Git/commits-and-history/#writing-good-commit-messages","title":"Writing Good Commit Messages","text":"<p>Commit messages are documentation. Six months from now, when someone (including you) runs <code>git log</code> to understand why a change was made, the commit message is the primary source of context.</p>"},{"location":"Git/commits-and-history/#the-format","title":"The Format","text":"<pre><code>Short summary of the change (50 chars or less)\n\nMore detailed explanation if needed. Wrap at 72 characters.\nExplain what changed and why, not how (the diff shows how).\n\nReference issue tracker IDs if applicable.\n</code></pre>"},{"location":"Git/commits-and-history/#the-rules","title":"The Rules","text":"<p>Subject line:</p> <ul> <li>Keep it under 50 characters (hard limit: 72)</li> <li>Use imperative mood: \"Add feature\" not \"Added feature\" or \"Adds feature\"</li> <li>Capitalize the first word</li> <li>No period at the end</li> <li>Make it specific: \"Fix null pointer in user lookup\" not \"Fix bug\"</li> </ul> <p>Body (optional but recommended for non-trivial changes):</p> <ul> <li>Separate from subject with a blank line</li> <li>Wrap at 72 characters</li> <li>Explain what changed and why, not how (the diff shows how)</li> <li>Reference issue numbers, related commits, or design decisions</li> </ul>"},{"location":"Git/commits-and-history/#conventional-commits","title":"Conventional Commits","text":"<p>Many teams use the Conventional Commits format, which adds structured prefixes:</p> <pre><code>feat: add email notification for failed builds\nfix: correct timezone handling in scheduler\ndocs: update API authentication guide\nrefactor: extract validation into shared module\ntest: add integration tests for payment flow\nchore: upgrade dependencies to latest patch versions\n</code></pre> <p>The prefix tells you at a glance what kind of change this is. Some tools use these prefixes to automatically generate changelogs and determine version bumps.</p> <p>Which commit message follows best practices? (requires JavaScript)</p>"},{"location":"Git/commits-and-history/#viewing-history-git-log","title":"Viewing History: <code>git log</code>","text":"<p><code>git log</code> is your primary tool for exploring commit history. It's deeply configurable - you can filter, format, search, and graph the output in dozens of ways.</p>"},{"location":"Git/commits-and-history/#basic-usage","title":"Basic Usage","text":"<pre><code># Full log (press q to exit, space to page)\ngit log\n\n# Compact one-line format\ngit log --oneline\n\n# Show the last 5 commits\ngit log -5\n\n# Show graph with branch structure\ngit log --oneline --graph --all\n\n# Show graph with decorations (branch/tag names)\ngit log --oneline --graph --all --decorate\n</code></pre>"},{"location":"Git/commits-and-history/#filtering-by-date","title":"Filtering by Date","text":"<pre><code># Commits from the last week\ngit log --since=\"1 week ago\"\n\n# Commits between two dates\ngit log --after=\"2024-01-01\" --before=\"2024-02-01\"\n\n# Relative date display\ngit log --oneline --date=relative\n</code></pre>"},{"location":"Git/commits-and-history/#filtering-by-author","title":"Filtering by Author","text":"<pre><code># Commits by a specific author (partial match)\ngit log --author=\"Jane\"\n\n# Commits by multiple authors\ngit log --author=\"Jane\\|Bob\"\n</code></pre>"},{"location":"Git/commits-and-history/#searching-commit-messages","title":"Searching Commit Messages","text":"<pre><code># Search commit messages for a string\ngit log --grep=\"authentication\"\n\n# Case-insensitive search\ngit log --grep=\"auth\" -i\n\n# Commits matching ALL grep patterns (not just any)\ngit log --grep=\"fix\" --grep=\"login\" --all-match\n</code></pre>"},{"location":"Git/commits-and-history/#searching-code-changes","title":"Searching Code Changes","text":"<pre><code># Find commits that added or removed the string \"TODO\"  (pickaxe)\ngit log -S \"TODO\"\n\n# Find commits where the number of occurrences of \"TODO\" changed\ngit log -S \"TODO\" --diff-filter=M\n\n# Search with regex in code changes\ngit log -G \"function\\s+authenticate\"\n</code></pre> <p><code>-S</code> (the pickaxe) finds commits where the number of occurrences of a string changed. <code>-G</code> finds commits where a line matching a regex was added or removed. The pickaxe is faster for exact strings; <code>-G</code> handles patterns.</p>"},{"location":"Git/commits-and-history/#formatting-output","title":"Formatting Output","text":"<pre><code># Custom format\ngit log --format=\"%h %an %ar %s\"\n\n# Format with colors\ngit log --format=\"%C(yellow)%h%C(reset) %C(blue)%an%C(reset) %C(green)%ar%C(reset) %s\"\n</code></pre> <p>Common format placeholders:</p> Placeholder Output <code>%H</code> Full commit hash <code>%h</code> Abbreviated hash <code>%an</code> Author name <code>%ae</code> Author email <code>%ar</code> Author date, relative <code>%ai</code> Author date, ISO format <code>%s</code> Subject line <code>%b</code> Body <code>%d</code> Ref names (branches, tags)"},{"location":"Git/commits-and-history/#following-file-history","title":"Following File History","text":"<pre><code># History of a specific file\ngit log -- path/to/file.py\n\n# Follow renames (track file across renames)\ngit log --follow -- path/to/file.py\n\n# Show the patch (actual diff) for each commit\ngit log -p -- path/to/file.py\n\n# Show stats (files changed, insertions, deletions)\ngit log --stat\n</code></pre> <p>Exploring History with git log (requires JavaScript)</p> <p>git log Command Builder (requires JavaScript)</p>"},{"location":"Git/commits-and-history/#comparing-changes-git-diff","title":"Comparing Changes: <code>git diff</code>","text":"<p>While <code>git log</code> shows you what happened, <code>git diff</code> shows you exactly what changed. It compares content between any two of the three trees, or between any two commits.</p>"},{"location":"Git/commits-and-history/#the-three-comparisons","title":"The Three Comparisons","text":"<pre><code># Working directory vs staging area (unstaged changes)\ngit diff\n\n# Staging area vs last commit (what will be committed)\ngit diff --staged    # or --cached (identical)\n\n# Working directory vs last commit (all uncommitted changes)\ngit diff HEAD\n</code></pre>"},{"location":"Git/commits-and-history/#comparing-commits","title":"Comparing Commits","text":"<pre><code># Difference between two commits\ngit diff a1b2c3d e4f5a6b\n\n# Difference between current commit and two commits ago\ngit diff HEAD~2 HEAD\n\n# Difference between two branches\ngit diff main feature/auth\n\n# Only show which files changed (not the content)\ngit diff --name-only main feature/auth\n\n# Show stats (like git log --stat)\ngit diff --stat HEAD~3 HEAD\n</code></pre>"},{"location":"Git/commits-and-history/#scoping-diffs-to-files","title":"Scoping Diffs to Files","text":"<pre><code># Diff for a specific file\ngit diff -- src/auth.py\n\n# Diff for a directory\ngit diff -- src/\n\n# Staged changes for a specific file\ngit diff --staged -- src/auth.py\n</code></pre>"},{"location":"Git/commits-and-history/#reading-diff-output","title":"Reading Diff Output","text":"<pre><code>diff --git a/src/auth.py b/src/auth.py\nindex 4a39281..e4f5a6b 100644\n--- a/src/auth.py\n+++ b/src/auth.py\n@@ -12,7 +12,9 @@ def authenticate(username, password):\n     user = db.find_user(username)\n     if not user:\n         return None\n-    if user.check_password(password):\n+    if not user.is_active:\n+        raise AccountDisabledError(username)\n+    if user.check_password(password) and user.is_active:\n         return create_session(user)\n     return None\n</code></pre> <p>The header shows which file changed. Lines starting with <code>-</code> were removed, lines starting with <code>+</code> were added. The <code>@@</code> line shows the location in the file (starting at line 12, showing 7 lines of context in the old file, 9 in the new).</p> <p>Using git diff Across the Three Trees (requires JavaScript)</p>"},{"location":"Git/commits-and-history/#inspecting-a-single-commit-git-show","title":"Inspecting a Single Commit: <code>git show</code>","text":"<p><code>git show</code> displays the details of a specific commit - the message, author, date, and the full diff:</p> <pre><code># Show the most recent commit\ngit show\n\n# Show a specific commit\ngit show a1b2c3d\n\n# Show only the stat (no diff)\ngit show --stat a1b2c3d\n\n# Show a specific file as it was in a commit\ngit show a1b2c3d:src/auth.py\n</code></pre> <p>The <code>commit:path</code> syntax is useful for viewing a file at any point in history without checking it out.</p>"},{"location":"Git/commits-and-history/#amending-the-most-recent-commit","title":"Amending the Most Recent Commit","text":"<p>Made a typo in your commit message? Forgot to add a file? <code>git commit --amend</code> lets you modify the most recent commit:</p> <pre><code># Fix the commit message\ngit commit --amend -m \"Corrected commit message\"\n\n# Add a forgotten file to the last commit\ngit add forgotten-file.py\ngit commit --amend --no-edit    # Keep the same message\n</code></pre> <p><code>--amend</code> doesn't actually modify the old commit. It creates a new commit with a new hash and moves the branch pointer to it. The old commit becomes unreachable (but can still be found via the reflog for a while).</p> <p>Only amend unpushed commits</p> <p>If you've already pushed a commit to a shared branch, amending it rewrites history. Other developers who pulled the original commit will have a diverged history. Amend freely on local branches; use <code>git revert</code> on shared branches instead. The Rewriting History guide covers this in depth.</p>"},{"location":"Git/commits-and-history/#commit-references","title":"Commit References","text":"<p>Git provides several ways to reference commits without typing full SHA-1 hashes:</p> Reference Meaning <code>HEAD</code> The current commit <code>HEAD~1</code> or <code>HEAD~</code> One commit before HEAD (first parent) <code>HEAD~3</code> Three commits before HEAD <code>HEAD^</code> First parent of HEAD (same as <code>HEAD~1</code> for non-merge commits) <code>HEAD^2</code> Second parent of HEAD (only meaningful for merge commits) <code>main</code> The commit that the <code>main</code> branch points to <code>v1.0</code> The commit that the <code>v1.0</code> tag points to <code>@{2}</code> Where HEAD was two moves ago (from the reflog) <p>The <code>~</code> operator follows first parents (the \"main line\"). The <code>^</code> operator selects among multiple parents (relevant for merge commits). For linear history, <code>HEAD~1</code> and <code>HEAD^</code> are identical.</p>"},{"location":"Git/commits-and-history/#practical-exercise","title":"Practical Exercise","text":"<p>Exploring History with log and diff (requires JavaScript)</p>"},{"location":"Git/commits-and-history/#further-reading","title":"Further Reading","text":"<ul> <li>Pro Git - Chapter 2.3: Viewing the Commit History - comprehensive coverage of <code>git log</code> options</li> <li>Official git-log documentation - complete reference for all flags and format placeholders</li> <li>Official git-diff documentation - complete reference for diff modes</li> <li>Conventional Commits - structured commit message specification</li> <li>A Note About Git Commit Messages (Tim Pope) - the classic post on commit message formatting</li> </ul> <p>Previous: The Three Trees | Next: Branches and Merging | Back to Index</p>"},{"location":"Git/configuring-git/","title":"Configuring Git","text":"<p>Git is deeply configurable. Every aspect of its behavior - from how it displays output to how it handles line endings - can be tuned through configuration. Understanding the configuration system lets you build a Git setup that matches your workflow, your editor, and your team's conventions.</p>"},{"location":"Git/configuring-git/#configuration-levels","title":"Configuration Levels","text":"<p>Git reads configuration from four levels, each overriding the previous:</p> Level Flag Location Scope System <code>--system</code> <code>/etc/gitconfig</code> Every user on the machine Global <code>--global</code> <code>~/.gitconfig</code> or <code>~/.config/git/config</code> Your user account Local <code>--local</code> <code>.git/config</code> in the repo One repository Worktree <code>--worktree</code> <code>.git/config.worktree</code> One worktree (Git 2.20+) <p>A setting at a more specific level overrides the same setting at a broader level. Local overrides global, global overrides system.</p> <pre><code># See all settings and where they come from\ngit config --list --show-origin\n\n# See just global settings\ngit config --global --list\n\n# See just local (repo) settings\ngit config --local --list\n</code></pre> <p>If user.email is set to 'personal@example.com' in --global and 'work@company.com' in --local, which email is used for commits in that repository? (requires JavaScript)</p>"},{"location":"Git/configuring-git/#reading-and-writing-configuration","title":"Reading and Writing Configuration","text":""},{"location":"Git/configuring-git/#setting-values","title":"Setting Values","text":"<pre><code># Set a value\ngit config --global user.name \"Jane Developer\"\n\n# Set with a specific scope\ngit config --local core.autocrlf input\n\n# Set a boolean\ngit config --global color.ui true\n</code></pre>"},{"location":"Git/configuring-git/#reading-values","title":"Reading Values","text":"<pre><code># Get a specific value\ngit config user.name\n\n# Get with scope\ngit config --global user.email\n\n# Get showing the origin\ngit config --show-origin user.name\n</code></pre>"},{"location":"Git/configuring-git/#removing-and-editing","title":"Removing and Editing","text":"<pre><code># Remove a specific key\ngit config --global --unset core.editor\n\n# Remove a section\ngit config --global --remove-section alias\n\n# Open config in editor\ngit config --global --edit\n</code></pre> <p>git config Command Builder (requires JavaScript)</p>"},{"location":"Git/configuring-git/#aliases","title":"Aliases","text":"<p>Aliases let you create shortcuts for Git commands you use frequently. They range from simple abbreviations to complex shell commands.</p>"},{"location":"Git/configuring-git/#simple-aliases","title":"Simple Aliases","text":"<pre><code>git config --global alias.st status\ngit config --global alias.co checkout\ngit config --global alias.br branch\ngit config --global alias.ci commit\ngit config --global alias.sw switch\n</code></pre> <p>Now <code>git st</code> runs <code>git status</code>, <code>git co main</code> runs <code>git checkout main</code>, and so on.</p>"},{"location":"Git/configuring-git/#compound-aliases","title":"Compound Aliases","text":"<pre><code># Compact log with graph\ngit config --global alias.lg \"log --oneline --graph --all --decorate\"\n\n# Unstage files\ngit config --global alias.unstage \"reset HEAD --\"\n\n# Show last commit\ngit config --global alias.last \"log -1 HEAD --stat\"\n\n# Diff of staged changes\ngit config --global alias.staged \"diff --staged\"\n\n# List branches sorted by last commit date\ngit config --global alias.recent \"branch --sort=-committerdate --format='%(committerdate:relative)%09%(refname:short)'\"\n</code></pre>"},{"location":"Git/configuring-git/#shell-command-aliases","title":"Shell Command Aliases","text":"<p>Prefix with <code>!</code> to run arbitrary shell commands:</p> <pre><code># Delete all merged branches except main/master\ngit config --global alias.cleanup '!git branch --merged | grep -v \"main\\|master\\|\\*\" | xargs -r git branch -d'\n\n# Show all aliases\ngit config --global alias.aliases '!git config --get-regexp ^alias\\. | sed s/alias\\.//'\n\n# Open the repo in the browser (GitHub)\ngit config --global alias.browse '!open $(git remote get-url origin | sed \"s/git@/https:\\/\\//\" | sed \"s/\\.git$//\" | sed \"s/:/\\//\")'\n</code></pre> <p>Setting Up Git Aliases (requires JavaScript)</p>"},{"location":"Git/configuring-git/#custom-diff-and-merge-tools","title":"Custom Diff and Merge Tools","text":"<p>Git's built-in diff and merge output works in the terminal, but graphical tools can be easier for complex diffs and multi-file merge conflicts.</p>"},{"location":"Git/configuring-git/#configuring-a-diff-tool","title":"Configuring a Diff Tool","text":"<pre><code># Use vimdiff\ngit config --global diff.tool vimdiff\n\n# Use VS Code\ngit config --global diff.tool vscode\ngit config --global difftool.vscode.cmd 'code --wait --diff $LOCAL $REMOTE'\n\n# Use meld (Linux)\ngit config --global diff.tool meld\n</code></pre> <p>Run the visual diff:</p> <pre><code>git difftool                    # Working dir vs staging\ngit difftool --staged           # Staging vs HEAD\ngit difftool main feature/auth  # Between branches\n</code></pre>"},{"location":"Git/configuring-git/#configuring-a-merge-tool","title":"Configuring a Merge Tool","text":"<pre><code># Use vimdiff\ngit config --global merge.tool vimdiff\n\n# Use VS Code\ngit config --global merge.tool vscode\ngit config --global mergetool.vscode.cmd 'code --wait --merge $REMOTE $LOCAL $BASE $MERGED'\n\n# Show the base version in conflicts (highly recommended)\ngit config --global merge.conflictstyle diff3\n</code></pre> <p>Run the merge tool during a conflict:</p> <pre><code>git merge feature/branch    # Conflict occurs\ngit mergetool               # Opens each conflicted file in the configured tool\n</code></pre> <p>diff3 conflict style</p> <p><code>merge.conflictstyle = diff3</code> adds a third section to conflict markers showing the base version (what the code looked like before either change). This makes it much easier to understand the intent of both changes. Set it globally - you'll wonder how you ever resolved conflicts without it.</p> <p>Configuring a Custom Diff and Merge Tool (requires JavaScript)</p>"},{"location":"Git/configuring-git/#conditional-includes","title":"Conditional Includes","text":"<p>If you use Git for both work and personal projects, you need different identities (name, email, signing key) for each. Conditional includes (Git 2.13+) let you load different configuration files based on the repository's location:</p> <pre><code># ~/.gitconfig\n\n[user]\n    name = Jane Developer\n    email = jane@personal.com\n\n[includeIf \"gitdir:~/work/\"]\n    path = ~/.gitconfig-work\n\n[includeIf \"gitdir:~/opensource/\"]\n    path = ~/.gitconfig-opensource\n</code></pre> <pre><code># ~/.gitconfig-work\n\n[user]\n    email = jane.developer@company.com\n    signingkey = ~/.ssh/work_ed25519.pub\n\n[commit]\n    gpgsign = true\n</code></pre> <p>Any repository under <code>~/work/</code> automatically uses the work email and signing key. Repositories elsewhere use the personal defaults.</p> <p>The <code>gitdir:</code> condition matches the <code>.git</code> directory location. Other conditions:</p> Condition Matches <code>gitdir:~/work/</code> Repos under <code>~/work/</code> <code>gitdir/i:~/Work/</code> Case-insensitive match (useful on macOS) <code>onbranch:main</code> When the <code>main</code> branch is checked out <code>hasconfig:remote.*.url:*github.com*</code> Repos with a GitHub remote (Git 2.36+) <p>Set Up Conditional Includes (requires JavaScript)</p>"},{"location":"Git/configuring-git/#core-settings","title":"Core Settings","text":"<p>The <code>core.*</code> namespace contains Git's most fundamental behavior settings.</p>"},{"location":"Git/configuring-git/#line-endings","title":"Line Endings","text":"<p>Line ending handling is critical for cross-platform teams. Windows uses CRLF (<code>\\r\\n</code>), Linux and macOS use LF (<code>\\n</code>).</p> <pre><code># On Linux/macOS: convert CRLF to LF on commit, no conversion on checkout\ngit config --global core.autocrlf input\n\n# On Windows: convert LF to CRLF on checkout, CRLF to LF on commit\ngit config --global core.autocrlf true\n\n# No conversion (if your team standardizes on LF and uses .gitattributes)\ngit config --global core.autocrlf false\n</code></pre> <p>Use .gitattributes for line endings</p> <p><code>core.autocrlf</code> is a per-user setting - it only works if everyone configures it. <code>.gitattributes</code> is committed to the repository and enforces line ending rules for the whole team. See the <code>.gitattributes</code> section below.</p>"},{"location":"Git/configuring-git/#other-core-settings","title":"Other Core Settings","text":"<pre><code># Set your editor (for commit messages, rebase, etc.)\ngit config --global core.editor \"vim\"\n# git config --global core.editor \"code --wait\"\n# git config --global core.editor \"nano\"\n\n# Set the pager (for log, diff, etc.)\ngit config --global core.pager \"less -FRX\"\n\n# Detect whitespace problems\ngit config --global core.whitespace trailing-space,space-before-tab\n\n# Improve performance on large repos with filesystem monitor\ngit config --global core.fsmonitor true\n</code></pre>"},{"location":"Git/configuring-git/#gitattributes","title":"<code>.gitattributes</code>","text":"<p>While <code>.gitconfig</code> controls your personal Git behavior, <code>.gitattributes</code> is committed to the repository and controls per-file behavior for the whole team: line endings, diff drivers, merge strategies, and LFS tracking.</p>"},{"location":"Git/configuring-git/#line-endings_1","title":"Line Endings","text":"<pre><code># .gitattributes - normalize line endings\n\n# Default: auto-detect\n* text=auto\n\n# Force LF for source files\n*.py text eol=lf\n*.js text eol=lf\n*.css text eol=lf\n*.html text eol=lf\n*.md text eol=lf\n*.yml text eol=lf\n*.json text eol=lf\n\n# Force CRLF for Windows-specific files\n*.bat text eol=crlf\n*.cmd text eol=crlf\n*.ps1 text eol=crlf\n\n# Binary files (no conversion, no diff)\n*.png binary\n*.jpg binary\n*.gif binary\n*.ico binary\n*.zip binary\n*.pdf binary\n</code></pre>"},{"location":"Git/configuring-git/#custom-diff-drivers","title":"Custom Diff Drivers","text":"<pre><code># Show meaningful diffs for specific file types\n*.md diff=markdown\n*.py diff=python\n*.rb diff=ruby\n</code></pre> <p>These tell Git to use language-aware diff heuristics - better function/class detection in diff headers.</p>"},{"location":"Git/configuring-git/#linguist-overrides","title":"Linguist Overrides","text":"<pre><code># Exclude from GitHub language statistics\ndocs/* linguist-documentation\nvendor/* linguist-vendored\n*.min.js linguist-generated\n</code></pre> <p>A Well-Organized .gitconfig (requires JavaScript)</p> <p>.gitattributes for a Polyglot Repository (requires JavaScript)</p>"},{"location":"Git/configuring-git/#environment-variables","title":"Environment Variables","text":"<p>Git reads several environment variables that override configuration:</p> Variable Overrides Use case <code>GIT_AUTHOR_NAME</code> <code>user.name</code> Set author for the current command <code>GIT_AUTHOR_EMAIL</code> <code>user.email</code> Set author email for the current command <code>GIT_COMMITTER_NAME</code> <code>user.name</code> Set committer identity <code>GIT_COMMITTER_EMAIL</code> <code>user.email</code> Set committer email <code>GIT_DIR</code> default <code>.git</code> Path to the <code>.git</code> directory <code>GIT_WORK_TREE</code> default parent of <code>.git</code> Path to the working tree <code>GIT_EDITOR</code> <code>core.editor</code> Editor for commit messages <code>GIT_PAGER</code> <code>core.pager</code> Pager for output <code>GIT_SSH_COMMAND</code> <code>core.sshCommand</code> Custom SSH command <pre><code># One-off commit with a different author\nGIT_AUTHOR_NAME=\"Pair Partner\" GIT_AUTHOR_EMAIL=\"pair@example.com\" git commit -m \"Paired on auth fix\"\n\n# Use a different SSH key for one command\nGIT_SSH_COMMAND=\"ssh -i ~/.ssh/deploy_key\" git push\n</code></pre>"},{"location":"Git/configuring-git/#mailmap-author-normalization","title":"<code>.mailmap</code>: Author Normalization","text":"<p>Over time, contributors may use different names or emails across commits. <code>.mailmap</code> normalizes these for <code>git log</code> and <code>git shortlog</code>:</p> <pre><code># .mailmap\nJane Developer &lt;jane@current.com&gt; &lt;jane.dev@oldcompany.com&gt;\nJane Developer &lt;jane@current.com&gt; &lt;jdev@personal.com&gt;\nBob Smith &lt;bob@company.com&gt; Bobby &lt;bobby@typo.com&gt;\n</code></pre> <p>Now <code>git shortlog -sne</code> shows unified author counts instead of splitting the same person across multiple entries.</p>"},{"location":"Git/configuring-git/#further-reading","title":"Further Reading","text":"<ul> <li>Pro Git - Chapter 8.1: Customizing Git Configuration - comprehensive coverage of configuration options</li> <li>Official git-config documentation - complete reference for all settings</li> <li>Official gitattributes documentation - line endings, diff drivers, merge strategies, LFS</li> <li>Official gitignore documentation - pattern syntax and precedence</li> </ul> <p>Previous: Stashing and the Worktree | Next: The Object Model | Back to Index</p>"},{"location":"Git/hooks-and-automation/","title":"Git Hooks and Automation","text":"<p>Git hooks are scripts that run automatically at specific points in the Git workflow - before a commit, after a merge, before a push. They let you enforce coding standards, run tests, validate commit messages, and automate repetitive tasks. This guide covers every hook, how to write them, hook management frameworks, and Git's built-in tools for debugging and forensic investigation.</p>"},{"location":"Git/hooks-and-automation/#how-hooks-work","title":"How Hooks Work","text":"<p>Hooks are executable scripts stored in <code>.git/hooks/</code>. When Git reaches a trigger point (like committing), it checks for a hook with the corresponding name and runs it if found.</p> <ul> <li>Hooks must be executable (<code>chmod +x .git/hooks/pre-commit</code>)</li> <li>They can be written in any language (bash, Python, Ruby, Node.js - as long as the shebang line is correct)</li> <li>Client-side hooks run on your machine. They're not pushed or shared through the repository (<code>.git/hooks/</code> is local).</li> <li>Server-side hooks run on the remote when receiving pushes.</li> <li>Hooks that exit with non-zero status abort the operation they guard.</li> </ul> <p>Hooks are local</p> <p>Client-side hooks live in <code>.git/hooks/</code>, which isn't tracked by Git. You can't enforce them through the repository alone. That's why hook frameworks (covered below) exist - they let you commit hook definitions that teammates install locally.</p>"},{"location":"Git/hooks-and-automation/#client-side-hooks","title":"Client-Side Hooks","text":""},{"location":"Git/hooks-and-automation/#pre-commit-hooks","title":"Pre-Commit Hooks","text":"Hook Runs Purpose <code>pre-commit</code> Before commit message editor opens Validate the code being committed <code>prepare-commit-msg</code> After default message is created, before editor opens Modify the commit message template <code>commit-msg</code> After you write the message, before commit is created Validate the commit message format <code>post-commit</code> After commit is created Notifications, logging"},{"location":"Git/hooks-and-automation/#pre-commit","title":"<code>pre-commit</code>","text":"<p>The most commonly used hook. It runs before the commit message editor opens. If it exits non-zero, the commit is aborted. Use it for linting, formatting checks, and preventing debug code from being committed.</p> <pre><code>#!/bin/bash\n# .git/hooks/pre-commit - Check for debug statements\n\n# Check staged Python files for debug prints\nFILES=$(git diff --cached --name-only --diff-filter=ACM | grep '\\.py$')\n\nif [ -n \"$FILES\" ]; then\n    if grep -n 'import pdb\\|breakpoint()\\|print(' $FILES; then\n        echo \"ERROR: Debug statements found in staged files.\"\n        echo \"Remove them before committing.\"\n        exit 1\n    fi\nfi\n\nexit 0\n</code></pre> <p>See also</p> <p>Git hooks are shell scripts at their core. For Bash scripting fundamentals like conditionals, loops, and exit codes, see Scripting Fundamentals.</p>"},{"location":"Git/hooks-and-automation/#commit-msg","title":"<code>commit-msg</code>","text":"<p>Receives the commit message file path as its argument. Use it to enforce message conventions:</p> <pre><code>#!/bin/bash\n# .git/hooks/commit-msg - Enforce Conventional Commits format\n\nMSG_FILE=$1\nMSG=$(cat \"$MSG_FILE\")\n\n# Check for conventional commit prefix\nif ! echo \"$MSG\" | grep -qE '^(feat|fix|docs|refactor|test|chore|style|perf|ci|build|revert)(\\(.+\\))?: .+'; then\n    echo \"ERROR: Commit message must follow Conventional Commits format:\"\n    echo \"  feat: add new feature\"\n    echo \"  fix(auth): resolve login timeout\"\n    echo \"  docs: update API guide\"\n    echo \"\"\n    echo \"Your message: $MSG\"\n    exit 1\nfi\n\nexit 0\n</code></pre> <p>Which hook runs before the commit message editor opens? (requires JavaScript)</p>"},{"location":"Git/hooks-and-automation/#other-client-side-hooks","title":"Other Client-Side Hooks","text":"Hook Runs Purpose <code>pre-rebase</code> Before rebase starts Prevent rebasing certain branches <code>pre-push</code> Before push transmits data Run tests before pushing <code>post-checkout</code> After <code>git checkout</code>/<code>git switch</code> Set up environment, update dependencies <code>post-merge</code> After a successful merge Install dependencies, rebuild <code>pre-auto-gc</code> Before automatic garbage collection Notify or prevent GC <p>The <code>pre-push</code> hook is particularly useful for running a quick test suite before pushing:</p> <pre><code>#!/bin/bash\n# .git/hooks/pre-push - Run tests before push\n\necho \"Running tests before push...\"\nnpm test\n\nif [ $? -ne 0 ]; then\n    echo \"Tests failed. Push aborted.\"\n    exit 1\nfi\n\nexit 0\n</code></pre>"},{"location":"Git/hooks-and-automation/#server-side-hooks","title":"Server-Side Hooks","text":"<p>Server-side hooks run on the remote repository when receiving pushes. They're managed by the server administrator, not individual developers.</p> Hook Runs Purpose <code>pre-receive</code> Before any refs are updated Global policy enforcement <code>update</code> Once per branch being updated Per-branch policy enforcement <code>post-receive</code> After all refs are updated Notifications, CI triggers, deploys <p><code>pre-receive</code> is the enforcement point for server-side rules. If it exits non-zero, the entire push is rejected.</p> <pre><code>sequenceDiagram\n    participant D as Developer\n    participant L as Local Git\n    participant R as Remote Git\n    participant H as Server Hooks\n\n    D-&gt;&gt;L: git push\n    L-&gt;&gt;R: Connect and send pack\n    R-&gt;&gt;H: pre-receive (all refs)\n    alt Hook rejects\n        H--&gt;&gt;R: Exit non-zero\n        R--&gt;&gt;L: Push rejected\n        L--&gt;&gt;D: Error (entire push fails)\n    else Hook accepts\n        H--&gt;&gt;R: Exit 0\n    end\n    loop Each ref being updated\n        R-&gt;&gt;H: update (old, new, refname)\n        alt Hook rejects\n            H--&gt;&gt;R: Exit non-zero (ref skipped)\n        else Hook accepts\n            H--&gt;&gt;R: Exit 0\n            R-&gt;&gt;R: Update ref\n        end\n    end\n    R-&gt;&gt;H: post-receive (all updated refs)\n    Note over H: Notifications, CI triggers, deploys (cannot abort)\n    H--&gt;&gt;R: Done\n    R--&gt;&gt;L: Push result\n    L--&gt;&gt;D: Success</code></pre> <p>Where do client-side hooks run vs server-side hooks? (requires JavaScript)</p> <p>A commit-msg Hook Enforcing Conventional Commits (requires JavaScript)</p> <p>A pre-receive Hook Rejecting Force Pushes to Main (requires JavaScript)</p>"},{"location":"Git/hooks-and-automation/#hook-frameworks","title":"Hook Frameworks","text":"<p>Since hooks aren't committed to the repository, teams need a way to share and enforce them. Hook frameworks solve this.</p>"},{"location":"Git/hooks-and-automation/#pre-commit-python","title":"pre-commit (Python)","text":"<p>pre-commit is a framework that manages hook installation from a shared config file:</p> <pre><code># Install\npip install pre-commit\n\n# Create .pre-commit-config.yaml in your repo\n# Install hooks based on the config\npre-commit install\n</code></pre> <pre><code># .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.5.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-added-large-files\n        args: ['--maxkb=500']\n\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.3.0\n    hooks:\n      - id: ruff\n        args: ['--fix']\n      - id: ruff-format\n</code></pre>"},{"location":"Git/hooks-and-automation/#husky-nodejs","title":"Husky (Node.js)","text":"<p>Husky integrates with npm/yarn projects:</p> <pre><code>npx husky init\necho \"npm test\" &gt; .husky/pre-commit\necho \"npx commitlint --edit \\$1\" &gt; .husky/commit-msg\n</code></pre>"},{"location":"Git/hooks-and-automation/#lefthook","title":"Lefthook","text":"<p>Lefthook is a fast, polyglot hook manager:</p> <pre><code># lefthook.yml\npre-commit:\n  parallel: true\n  commands:\n    lint:\n      glob: \"*.{js,ts}\"\n      run: npx eslint {staged_files}\n    format:\n      glob: \"*.py\"\n      run: ruff format --check {staged_files}\n</code></pre>"},{"location":"Git/hooks-and-automation/#git-bisect-binary-search-for-bugs","title":"Git Bisect: Binary Search for Bugs","text":"<p><code>git bisect</code> performs a binary search through commit history to find which commit introduced a bug. Instead of checking every commit, it cuts the search space in half each step.</p>"},{"location":"Git/hooks-and-automation/#manual-bisect","title":"Manual Bisect","text":"<pre><code># Start bisecting\ngit bisect start\n\n# Mark the current commit as bad (has the bug)\ngit bisect bad\n\n# Mark an older commit as good (doesn't have the bug)\ngit bisect good v1.0\n\n# Git checks out a commit halfway between good and bad\n# Test it, then mark:\ngit bisect good    # if this commit doesn't have the bug\ngit bisect bad     # if this commit has the bug\n\n# Git narrows the range and checks out the next midpoint\n# Repeat until the first bad commit is found\n\n# When done\ngit bisect reset\n</code></pre>"},{"location":"Git/hooks-and-automation/#automated-bisect","title":"Automated Bisect","text":"<p>If you have a script that returns 0 for good and non-zero for bad:</p> <pre><code>git bisect start HEAD v1.0\ngit bisect run npm test\n# or\ngit bisect run python -m pytest tests/test_auth.py\n</code></pre> <p>Git runs the script at each step automatically and reports the first bad commit.</p> <p>Using git bisect to Find a Bug (requires JavaScript)</p>"},{"location":"Git/hooks-and-automation/#git-blame-and-forensic-investigation","title":"Git Blame and Forensic Investigation","text":""},{"location":"Git/hooks-and-automation/#git-blame","title":"<code>git blame</code>","text":"<p><code>git blame</code> annotates each line of a file with the commit that last modified it:</p> <pre><code># Blame a file\ngit blame src/auth.py\n\n# Ignore whitespace changes\ngit blame -w src/auth.py\n\n# Show original author even after move/copy\ngit blame -C src/auth.py\n\n# Blame a specific range of lines\ngit blame -L 10,20 src/auth.py\n\n# Show blame at a specific commit (before a refactor)\ngit blame v1.0 -- src/auth.py\n</code></pre>"},{"location":"Git/hooks-and-automation/#code-search-with-git-log","title":"Code Search with <code>git log</code>","text":"<pre><code># Find commits where a string was added/removed (pickaxe)\ngit log -S \"authenticate\" --oneline\n\n# Find commits where a regex was added/removed in the diff\ngit log -G \"def authenticate\\(\" --oneline\n\n# Search with patch output to see the actual changes\ngit log -S \"authenticate\" -p\n\n# Combine with file path\ngit log -S \"authenticate\" -- src/auth.py\n</code></pre>"},{"location":"Git/hooks-and-automation/#exercises","title":"Exercises","text":"<p>Set Up a Pre-Commit Framework (requires JavaScript)</p> <p>Use git bisect to Find a Regression (requires JavaScript)</p>"},{"location":"Git/hooks-and-automation/#further-reading","title":"Further Reading","text":"<ul> <li>Pro Git - Chapter 8.3: Git Hooks - comprehensive hook documentation</li> <li>Official githooks documentation - complete reference for all hook types</li> <li>pre-commit.com - Python-based hook framework</li> <li>Husky Documentation - Node.js hook management</li> <li>Lefthook Documentation - polyglot hook manager</li> <li>Official git-bisect documentation - binary search debugging</li> <li>Official git-blame documentation - line-level authorship annotation</li> </ul> <p>Previous: GitHub, GitLab, and Bitbucket | Next: Git Security | Back to Index</p>"},{"location":"Git/introduction/","title":"Introduction: Why Git, and Why Version Control","text":"<p>Before you type your first <code>git</code> command, it helps to understand the problem Git solves and the decades of tools that failed to solve it as well. Version control is one of those ideas that sounds obvious in hindsight - of course you should track changes to your code - but the path from \"save a backup copy\" to a distributed system that manages the Linux kernel involved some hard lessons and a few legendary arguments.</p>"},{"location":"Git/introduction/#the-problem-tracking-changes","title":"The Problem: Tracking Changes","text":"<p>Every programmer eventually learns the hard way that code changes need tracking. Without version control, you end up with directories full of files named <code>project-final.zip</code>, <code>project-final-REALLY-final.zip</code>, and <code>project-final-v2-fixed-bug-DONT-DELETE.zip</code>. You lose track of what changed, when, and why. Collaboration becomes a nightmare of emailing files back and forth and manually merging edits.</p> <p>Version control systems (VCS) solve this by recording every change to every file over time, letting you recall specific versions, compare changes, and work in parallel with other developers without stepping on each other.</p>"},{"location":"Git/introduction/#a-brief-history-of-version-control","title":"A Brief History of Version Control","text":""},{"location":"Git/introduction/#local-version-control-rcs","title":"Local Version Control: RCS","text":"<p>The earliest approach was simple: keep a local database of changes to files. RCS (Revision Control System), released in 1982, stored patch sets (differences between file versions) in a special format on disk. You could roll any file back to any previous state by applying patches in sequence.</p> <p>RCS worked for a single developer on a single machine. It tracked individual files, not projects. If you needed to coordinate with other people, you were out of luck.</p>"},{"location":"Git/introduction/#centralized-version-control-cvs-and-svn","title":"Centralized Version Control: CVS and SVN","text":"<p>The next generation moved the version database to a central server. CVS (Concurrent Versions System, 1990) and its successor SVN (Subversion, 2000) let multiple developers check out files from one server, make changes, and commit them back.</p> <p>This was a huge improvement. Teams could collaborate. You could see who changed what. But centralized systems had a critical weakness: the server was a single point of failure. If the server went down, nobody could commit. If the server's disk failed and backups were stale, you could lose the entire project history. Every operation - viewing logs, comparing versions, committing - required a network connection to the server.</p> <p>SVN also modeled versions as directory snapshots with linear revision numbers. Branching existed but was implemented as a cheap copy of the directory tree, which made merging painful. Developers avoided branches because merging them back was error-prone and tedious.</p>"},{"location":"Git/introduction/#distributed-version-control-bitkeeper-mercurial-and-git","title":"Distributed Version Control: BitKeeper, Mercurial, and Git","text":"<p>Distributed version control systems (DVCS) changed the model entirely. Instead of checking out a working copy from a central server, every developer clones the entire repository - every file, every version, the complete history. Your local copy is a full repository. You can commit, branch, view history, and diff entirely offline. Synchronization happens when you choose to push or pull changes.</p> <p>BitKeeper was the first DVCS to gain major traction. The Linux kernel team used it from 2002 to 2005 under a free (as in beer) license for open-source projects. When that license was revoked after a developer reverse-engineered the protocol, Linus Torvalds had a problem - and a very specific idea of what the replacement should look like.</p> <p>What is the fundamental difference between centralized and distributed version control? (requires JavaScript)</p> <p>In a centralized VCS like SVN, what happens if the central server goes down? (requires JavaScript)</p>"},{"location":"Git/introduction/#the-birth-of-git","title":"The Birth of Git","text":"Linus Torvalds, creator of Git and the Linux kernel. Photo: Wikimedia Commons, CC BY-SA 3.0 <p>In April 2005, Linus Torvalds started writing Git. The Linux kernel had 6.7 million lines of code and thousands of contributors. Linus had specific requirements based on his experience with BitKeeper and the kernel's scale:</p> <ul> <li>Speed. The kernel generates massive diffs and has thousands of files. Operations had to be fast.</li> <li>Strong integrity. Every object is checksummed with SHA-1. Corruption is detectable. You cannot change file contents, commit messages, or any part of history without changing the hash of everything that depends on it.</li> <li>Support for non-linear development. The kernel uses a workflow with thousands of parallel branches. Branching and merging had to be cheap and fast.</li> <li>Fully distributed. No central point of failure. Every developer has the complete history.</li> <li>Scalable to massive projects. The Linux kernel was (and remains) one of the largest open-source projects in existence.</li> </ul> <p>Git's first commit was on April 7, 2005. By April 29, Git could track itself. By June 16, Linux kernel 2.6.12 was released using Git. The entire development took about two months.</p> <p>Why the name?</p> <p>Linus described the name choice: \"I'm an egotistical bastard, and I name all my projects after myself. First 'Linux', now 'git'.\" The word \"git\" is British slang for an unpleasant person. The README in the original source code also offers: \"Global Information Tracker\" when you're in a good mood, or \"Conditions of Conditions of Bad Conditions\" (a rough translation from the slang) when it breaks.</p>"},{"location":"Git/introduction/#what-made-git-different","title":"What Made Git Different","text":"<p>Git's design broke from every previous VCS in a fundamental way: it treats your project as a content-addressable filesystem that happens to track file history. Most VCS tools store a list of file changes (deltas). Git stores snapshots of your entire project at each commit and uses SHA-1 hashes to identify everything - files, directories, commits. If two files have identical content, Git stores one copy and references it from both locations.</p> <p>This snapshot model, combined with the content-addressable storage, makes branching and merging nearly instantaneous. Creating a branch is writing 41 bytes to a file (a 40-character SHA-1 hash plus a newline). Merging compares tree structures rather than replaying individual file patches.</p> <p>You don't need to understand the object model to use Git (that comes in a later guide), but knowing that Git thinks in snapshots rather than diffs helps explain why some operations that were painful in SVN are trivial in Git.</p> <p>How does Git store project history differently from most earlier version control systems? (requires JavaScript)</p>"},{"location":"Git/introduction/#installing-git","title":"Installing Git","text":"<p>Git runs on Linux, macOS, Windows, and most Unix-like systems. Here's how to get it installed.</p>"},{"location":"Git/introduction/#linux","title":"Linux","text":"<p>Most distributions include Git in their package manager:</p> <pre><code># Debian/Ubuntu\nsudo apt update &amp;&amp; sudo apt install git\n\n# Fedora\nsudo dnf install git\n\n# Arch Linux\nsudo pacman -S git\n\n# RHEL/CentOS (EPEL may be needed for newer versions)\nsudo yum install git\n</code></pre> <p>For the latest version on Debian/Ubuntu, the Git maintainers provide a PPA:</p> <pre><code>sudo add-apt-repository ppa:git-core/ppa\nsudo apt update &amp;&amp; sudo apt install git\n</code></pre>"},{"location":"Git/introduction/#macos","title":"macOS","text":"<p>macOS ships with a version of Git as part of the Xcode Command Line Tools. Open a terminal and run:</p> <pre><code>git --version\n</code></pre> <p>If Git isn't installed, macOS prompts you to install the Command Line Tools. For a newer version, use Homebrew:</p> <pre><code>brew install git\n</code></pre>"},{"location":"Git/introduction/#windows","title":"Windows","text":"<p>Download the installer from git-scm.com. The installer includes Git Bash (a MSYS2-based terminal that provides a Unix-like shell) and optionally integrates with the Windows command prompt and PowerShell.</p> <p>Alternatively, if you use winget:</p> <pre><code>winget install Git.Git\n</code></pre> <p>Or with Chocolatey:</p> <pre><code>choco install git\n</code></pre>"},{"location":"Git/introduction/#verifying-your-installation","title":"Verifying Your Installation","text":"<p>After installing, verify it works:</p> <pre><code>git --version\n</code></pre> <p>You should see something like <code>git version 2.47.1</code>. The exact version depends on your platform and package manager.</p> <p>Installing and Verifying Git (requires JavaScript)</p>"},{"location":"Git/introduction/#first-time-configuration","title":"First-Time Configuration","text":"<p>Git stores configuration at three levels, each overriding the previous:</p> Level Flag File Scope System <code>--system</code> <code>/etc/gitconfig</code> Every user on the machine Global <code>--global</code> <code>~/.gitconfig</code> or <code>~/.config/git/config</code> Your user account Local <code>--local</code> <code>.git/config</code> in each repo One specific repository <p>For your initial setup, you need to set your identity. Every Git commit records an author name and email:</p> <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n</code></pre> <p>Choose your text editor for commit messages (defaults to <code>vi</code> on most systems):</p> <pre><code>git config --global core.editor \"nano\"        # or vim, code --wait, etc.\n</code></pre> <p>Set the default branch name for new repositories (Git defaults to <code>master</code>, but many teams and platforms now use <code>main</code>):</p> <pre><code>git config --global init.defaultBranch main\n</code></pre> <p>Enable color output (usually on by default, but worth setting explicitly):</p> <pre><code>git config --global color.ui auto\n</code></pre> <p>Verify your configuration:</p> <pre><code>git config --list --show-origin\n</code></pre> <p>This shows every setting and which file it comes from, so you can see exactly what's set where.</p> <p>Your identity matters</p> <p>The name and email you configure are baked into every commit you make. If you contribute to open-source projects, the email you use here will be public. GitHub lets you use a no-reply email address (<code>username@users.noreply.github.com</code>) if you prefer to keep your real email private.</p> <p>Anatomy of a .gitconfig File (requires JavaScript)</p>"},{"location":"Git/introduction/#gits-mental-model-a-preview","title":"Git's Mental Model: A Preview","text":"<p>Most guides jump straight into commands. Before you do that, it helps to have a high-level picture of what Git is actually doing.</p> <p>Git manages your project through three areas (often called the \"three trees\"):</p> <ol> <li>Working directory - the actual files on your filesystem that you edit</li> <li>Staging area (also called the index) - a holding area where you prepare the next commit</li> <li>Repository (the <code>.git</code> directory) - the complete history of committed snapshots</li> </ol> <p>When you work with Git, you're moving file changes between these three areas:</p> <pre><code>Edit files \u2500\u2500\u2192 Stage changes \u2500\u2500\u2192 Commit snapshot\n(working dir)    (index)          (repository)\n</code></pre> <p>This three-stage workflow is deliberate. The staging area lets you control exactly which changes go into each commit, even if you've modified ten files. You can stage three of them, commit those with a focused message, then stage and commit the rest separately. This produces a clean, meaningful history rather than giant \"changed a bunch of stuff\" commits.</p> <p>Beyond these three areas, Git's entire storage model is built on four types of objects - blobs, trees, commits, and tags - all identified by SHA-1 hashes. This content-addressable design means Git can instantly tell if two files are identical, deduplicates storage automatically, and guarantees that any corruption is immediately detectable. The Object Model guide covers this in depth.</p> <p>For now, the key insight is: Git doesn't track files. It tracks content. A file's name and location are stored in tree objects, while the file's content is stored in blob objects. Rename a file and Git knows it's the same content with a new name.</p> <p>The following walkthrough puts this three-area workflow into practice - creating a repository from scratch, staging files, and committing a snapshot.</p> <p>Your First Repository (requires JavaScript)</p>"},{"location":"Git/introduction/#whats-ahead","title":"What's Ahead","text":"<p>This course is organized in five phases:</p> <p>Foundations (guides 1-4) cover the three trees, commits, branches, and merging - everything you need for solo work.</p> <p>Core Workflows (guides 5-8) add remotes, history rewriting, stashing, and configuration - everything for working with others.</p> <p>Git Internals (guides 9-11) open up the object model, references, the DAG, and transfer protocols - how Git actually works under the hood.</p> <p>Platform Collaboration (guides 12-13) cover workflow models and platform-specific features across GitHub, GitLab, and Bitbucket.</p> <p>Advanced Operations (guides 14-17) cover hooks, security, scaling to monorepos, and troubleshooting when things go wrong.</p> <p>Each guide includes interactive quizzes, terminal simulations, hands-on exercises, and code walkthroughs. The quizzes test understanding, the terminals demonstrate commands in action, and the exercises give you practice in a real Git environment.</p>"},{"location":"Git/introduction/#further-reading","title":"Further Reading","text":"<ul> <li>Pro Git (2nd ed.) - Chapter 1: Getting Started - comprehensive coverage of VCS history and Git installation</li> <li>Official Git Documentation - reference manual, videos, and external links</li> <li>A Short History of Git - the BitKeeper story and Git's design goals</li> <li>Git's First Commit (Linus Torvalds, April 2005) - the initial commit of Git itself</li> </ul> <p>Next: The Three Trees: Working Directory, Index, and Repository | Back to Index</p>"},{"location":"Git/monorepos-and-scaling/","title":"Monorepos and Scaling Git","text":"<p>Git was designed for the Linux kernel - a large project, but one with a relatively straightforward directory structure. When organizations put hundreds of projects, millions of files, and decades of history into a single repository, Git's default behavior starts to struggle. Clone times balloon, <code>git status</code> takes seconds, and CI builds trigger unnecessarily. This guide covers the tools and strategies for making Git perform at scale.</p>"},{"location":"Git/monorepos-and-scaling/#when-monorepos-make-sense","title":"When Monorepos Make Sense","text":"<p>A monorepo stores multiple projects, services, or packages in a single Git repository. Major organizations use them:</p> <ul> <li>Google - billions of lines of code in a single repository (custom VCS, not Git)</li> <li>Meta - millions of files, custom Mercurial extensions (migrating to a custom Git-like system)</li> <li>Microsoft - Windows codebase moved to Git using VFS for Git and later Scalar</li> </ul> Advantage Trade-off Atomic cross-project changes Clone and checkout take longer Shared code without versioning overhead <code>git status</code> is slower with many files Unified CI/CD and tooling CI must scope to affected code Single source of truth Access control is repository-wide Easier refactoring across boundaries Git wasn't designed for millions of files <p>The challenges are real but solvable with the right configuration.</p>"},{"location":"Git/monorepos-and-scaling/#sparse-checkout","title":"Sparse Checkout","text":"<p>Sparse checkout tells Git to only materialize a subset of files in your working directory. The full history is available, but you only see the directories you need.</p>"},{"location":"Git/monorepos-and-scaling/#setting-up-sparse-checkout","title":"Setting Up Sparse Checkout","text":"<pre><code># Clone with sparse checkout enabled\ngit clone --sparse https://github.com/org/monorepo.git\ncd monorepo\n\n# Check out specific directories\ngit sparse-checkout set services/auth services/api shared/utils\n\n# Add more directories later\ngit sparse-checkout add services/web\n\n# List current sparse checkout patterns\ngit sparse-checkout list\n\n# Disable sparse checkout (check out everything)\ngit sparse-checkout disable\n</code></pre>"},{"location":"Git/monorepos-and-scaling/#cone-mode-vs-non-cone-mode","title":"Cone Mode vs Non-Cone Mode","text":"<p>Sparse checkout has two modes:</p> <ul> <li>Cone mode (default, recommended) - specifies directories. Fast because Git can skip entire subtrees without pattern matching.</li> <li>Non-cone mode - specifies file glob patterns. More flexible but slower.</li> </ul> <pre><code># Cone mode (default) - specify directories\ngit sparse-checkout set services/auth tests/auth\n\n# Non-cone mode - specify patterns\ngit sparse-checkout set --no-cone '!/*' '/README.md' '/services/auth/**'\n</code></pre> <p>Stick with cone mode unless you need file-level granularity.</p> <p>Setting Up Sparse Checkout (requires JavaScript)</p>"},{"location":"Git/monorepos-and-scaling/#partial-clone","title":"Partial Clone","text":"<p>Partial clone (Git 2.22+) lets you clone a repository without downloading all objects. Git fetches missing objects on demand when you need them.</p>"},{"location":"Git/monorepos-and-scaling/#filter-options","title":"Filter Options","text":"<pre><code># Blobless clone - skip file content, download on checkout\ngit clone --filter=blob:none https://github.com/org/monorepo.git\n\n# Treeless clone - skip trees too (minimal download)\ngit clone --filter=tree:0 https://github.com/org/monorepo.git\n\n# Size-limited - skip blobs larger than a threshold\ngit clone --filter=blob:limit=1m https://github.com/org/monorepo.git\n</code></pre>"},{"location":"Git/monorepos-and-scaling/#combining-partial-clone-and-sparse-checkout","title":"Combining Partial Clone and Sparse Checkout","text":"<p>The power combo for monorepos:</p> <pre><code># Clone with partial objects and sparse checkout\ngit clone --filter=blob:none --sparse https://github.com/org/monorepo.git\ncd monorepo\n\n# Check out only what you need\ngit sparse-checkout set services/my-service tests/my-service\n\n# Git fetches blobs only for files in your sparse checkout\n</code></pre> <p>This gives you:</p> <ul> <li>Full commit history (for <code>git log</code>, <code>git blame</code>)</li> <li>Minimal disk usage (only blobs for your directories)</li> <li>Fast initial clone</li> </ul> <p>What does a partial clone with --filter=blob:none exclude? (requires JavaScript)</p>"},{"location":"Git/monorepos-and-scaling/#the-commit-graph-file","title":"The Commit Graph File","text":"<p>The commit graph file (<code>.git/objects/info/commit-graph</code>) is a pre-computed index of the commit DAG. It stores commit hashes, parent hashes, root tree hashes, commit dates, and generation numbers in a compact binary format.</p> <p>Without the commit graph file, Git must decompress and parse individual commit objects to traverse history. With it, operations like <code>git log --graph</code>, <code>git merge-base</code>, and reachability queries are significantly faster.</p> <pre><code># Generate/update the commit graph\ngit commit-graph write\n\n# Verify the commit graph\ngit commit-graph verify\n\n# It's also maintained by git maintenance\ngit maintenance run --task=commit-graph\n</code></pre> <p>What does the commit graph file speed up? (requires JavaScript)</p>"},{"location":"Git/monorepos-and-scaling/#git-maintenance-background-optimization","title":"<code>git maintenance</code> - Background Optimization","text":"<p>Git 2.29+ includes <code>git maintenance</code> for scheduling automatic optimization:</p> <pre><code># Register this repo for background maintenance\ngit maintenance register\n\n# Run all tasks now\ngit maintenance run\n\n# Start the scheduler (uses system scheduler: launchd/cron/systemd)\ngit maintenance start\n\n# Stop the scheduler\ngit maintenance stop\n</code></pre>"},{"location":"Git/monorepos-and-scaling/#maintenance-tasks","title":"Maintenance Tasks","text":"Task Frequency What it does <code>commit-graph</code> Hourly Updates the commit graph file <code>prefetch</code> Hourly Background fetch from remotes (no merge) <code>loose-objects</code> Daily Packs loose objects <code>incremental-repack</code> Daily Consolidates packfiles <code>gc</code> Weekly Full garbage collection <p>Configuring git maintenance (requires JavaScript)</p>"},{"location":"Git/monorepos-and-scaling/#scalar","title":"Scalar","text":"<p>Scalar is Microsoft's tool for optimizing large Git repositories. Since Git 2.38, a subset of Scalar is built into Git itself:</p> <pre><code># Initialize a repo with Scalar optimizations\nscalar clone https://github.com/org/large-repo.git\n\n# Register an existing repo for Scalar management\nscalar register\n\n# Scalar automatically configures:\n# - Sparse checkout\n# - Partial clone (blob:none)\n# - Commit graph\n# - Multi-pack index\n# - File system monitor (fsmonitor)\n# - Background maintenance\n</code></pre> <p>Scalar is essentially a convenience wrapper that enables all the individual optimizations covered in this guide in one command.</p>"},{"location":"Git/monorepos-and-scaling/#file-system-monitor","title":"File System Monitor","text":"<p><code>git status</code> needs to check every tracked file for changes. On large repositories, this filesystem scan is the bottleneck. The file system monitor (<code>fsmonitor</code>) uses OS-level file change notifications to skip files that haven't changed.</p> <pre><code># Enable the built-in fsmonitor daemon (Git 2.37+)\ngit config core.fsmonitor true\n\n# Or use Watchman (Facebook's file watcher)\ngit config core.fsmonitor \"$(which watchman)\"\n</code></pre> <p>The built-in <code>fsmonitor--daemon</code> (Git 2.37+) watches for filesystem events and tells Git which files have changed since the last query. This can make <code>git status</code> near-instantaneous on repos with hundreds of thousands of files.</p>"},{"location":"Git/monorepos-and-scaling/#submodules-vs-subtrees","title":"Submodules vs Subtrees","text":"<p>For projects that need to include code from other repositories, Git offers two mechanisms:</p>"},{"location":"Git/monorepos-and-scaling/#submodules","title":"Submodules","text":"<p>A submodule is a pointer to a specific commit in another repository:</p> <pre><code># Add a submodule\ngit submodule add https://github.com/lib/awesome-lib.git vendor/awesome-lib\n\n# Clone a repo with submodules\ngit clone --recurse-submodules https://github.com/org/project.git\n\n# Update submodules to their tracked commits\ngit submodule update --init --recursive\n\n# Update submodules to the latest remote commit\ngit submodule update --remote\n</code></pre> <p>Submodules are simple in concept but have notorious UX issues: detached HEAD state inside the submodule, forgetting to init/update after clone, and confusing merge conflicts.</p>"},{"location":"Git/monorepos-and-scaling/#subtrees","title":"Subtrees","text":"<p>A subtree merges another repository's content directly into a subdirectory:</p> <pre><code># Add a subtree\ngit subtree add --prefix=vendor/awesome-lib https://github.com/lib/awesome-lib.git main --squash\n\n# Pull updates\ngit subtree pull --prefix=vendor/awesome-lib https://github.com/lib/awesome-lib.git main --squash\n\n# Push changes back upstream\ngit subtree push --prefix=vendor/awesome-lib https://github.com/lib/awesome-lib.git main\n</code></pre> <p>Subtrees are simpler for consumers (no special init commands), but the history can be messy and push-back workflows are less intuitive.</p> Submodules Subtrees Storage Pointer to external commit Content merged into repo Clone experience Requires <code>--recurse-submodules</code> Works with normal clone Updating <code>submodule update --remote</code> <code>subtree pull</code> History Separate (external repo) Mixed into your repo Push changes back cd into submodule, commit, push <code>subtree push</code>"},{"location":"Git/monorepos-and-scaling/#build-system-integration","title":"Build System Integration","text":"<p>Large monorepos need build systems that understand which projects are affected by a change:</p> <ul> <li>Bazel - Google's build system, tracks dependencies explicitly</li> <li>Nx - smart monorepo build system for JavaScript/TypeScript</li> <li>Turborepo - incremental build system for JS monorepos</li> <li>Pants - scalable build system for Python, Go, Java</li> </ul> <p>These tools use the Git commit graph to determine which packages changed and only build/test those, making CI feasible for large monorepos.</p> <p>Clone Strategies for Large Repos (requires JavaScript)</p>"},{"location":"Git/monorepos-and-scaling/#exercise","title":"Exercise","text":"<p>Configure Sparse Checkout and Partial Clone (requires JavaScript)</p>"},{"location":"Git/monorepos-and-scaling/#further-reading","title":"Further Reading","text":"<ul> <li>Pro Git - Chapter 7.11: Submodules - submodule setup and workflow</li> <li>Official git-sparse-checkout documentation - sparse checkout modes and patterns</li> <li>Official git-maintenance documentation - background optimization tasks</li> <li>Scalar Documentation - Microsoft's monorepo optimization tool</li> <li>Git at Scale (Microsoft DevOps Blog) - Scalar and VFS for Git</li> <li>Partial Clone Documentation - filtering objects during clone</li> </ul> <p>Previous: Git Security | Next: Troubleshooting and Recovery | Back to Index</p>"},{"location":"Git/object-model/","title":"The Object Model","text":"<p>Everything you've learned so far - commits, branches, staging, merging - is built on top of a surprisingly simple storage system. Git is, at its core, a content-addressable filesystem: a key-value store where the key is a SHA-1 hash of the content and the value is the content itself. Understanding this layer explains why Git behaves the way it does and gives you the tools to inspect and repair repositories at the lowest level.</p>"},{"location":"Git/object-model/#content-addressable-storage","title":"Content-Addressable Storage","text":"<p>The term content-addressable means that the address (name) of every piece of data is derived from the data itself. Git computes a SHA-1 hash of each object's content, and that hash becomes the object's identity. Two files with identical content produce the same hash and are stored once. Change a single byte and the hash - and therefore the identity - changes completely.</p> <p>This has profound implications:</p> <ul> <li>Deduplication is automatic. If the same file appears in 1,000 commits, Git stores one copy.</li> <li>Integrity is guaranteed. If any bit of a stored object changes (disk corruption, tampering), the hash no longer matches and Git detects it immediately.</li> <li>History is tamper-evident. Since each commit's hash includes its parent hash, changing any commit changes every subsequent hash in the chain. You can't alter history silently.</li> </ul> <p>SHA-1 and SHA-256</p> <p>Git has historically used SHA-1 (160-bit, 40 hex characters). While SHA-1 has known collision vulnerabilities in theory, Git includes additional hardening against known attacks. Git is transitioning to SHA-256 (256-bit, 64 hex characters) with a compatibility layer. New repository formats can opt into SHA-256, but most repositories still use SHA-1.</p>"},{"location":"Git/object-model/#the-four-object-types","title":"The Four Object Types","text":"<p>Every object in Git's database is one of four types:</p> <pre><code>flowchart TD\n    C[\"commit&lt;br/&gt;snapshot + metadata\"] --&gt; T[\"tree&lt;br/&gt;directory listing\"]\n    T --&gt; B1[\"blob&lt;br/&gt;file content\"]\n    T --&gt; B2[\"blob&lt;br/&gt;file content\"]\n    T --&gt; T2[\"tree&lt;br/&gt;subdirectory\"]\n    T2 --&gt; B3[\"blob&lt;br/&gt;file content\"]\n    TAG[\"tag&lt;br/&gt;named reference + message\"] --&gt; C</code></pre>"},{"location":"Git/object-model/#blob-binary-large-object","title":"Blob (Binary Large Object)","text":"<p>A blob stores the content of a single file - nothing else. No filename, no permissions, no metadata. Just the raw bytes. Two files with identical content, regardless of their names or locations, produce the same blob.</p> <pre><code># Hash a string as a blob\necho \"Hello, Git\" | git hash-object --stdin\n# 41e40e5a20c7e8657a8a92e2ce0bfa39a9e0d40c\n\n# Hash a file\ngit hash-object README.md\n</code></pre>"},{"location":"Git/object-model/#tree","title":"Tree","text":"<p>A tree represents a directory. It contains entries, each pointing to a blob (file) or another tree (subdirectory), along with the file's name and permission mode:</p> <pre><code>100644 blob a1b2c3d4...  README.md\n100644 blob e5f6a7b8...  app.py\n040000 tree c9d0e1f2...  src\n</code></pre> <p>Permission modes:</p> Mode Meaning <code>100644</code> Regular file <code>100755</code> Executable file <code>120000</code> Symbolic link <code>040000</code> Subdirectory (tree)"},{"location":"Git/object-model/#commit","title":"Commit","text":"<p>A commit ties everything together. It references:</p> <ul> <li>A tree (the root directory snapshot)</li> <li>Zero or more parent commits</li> <li>Author and committer identity with timestamps</li> <li>A message</li> </ul> <p>The first commit in a repository has no parent. A merge commit has two or more parents. Every other commit has exactly one parent.</p>"},{"location":"Git/object-model/#annotated-tag","title":"Annotated Tag","text":"<p>An annotated tag is a named reference to a commit (or any object) with additional metadata: a tagger identity, timestamp, and message. Unlike lightweight tags (which are just refs), annotated tags are full objects stored in the database.</p> <pre><code># Create an annotated tag\ngit tag -a v1.0 -m \"First stable release\"\n\n# Show the tag object\ngit cat-file -p v1.0\n</code></pre> <p>What type of Git object stores file content? (requires JavaScript)</p> <p>How does Git know that two files with identical content are the same? (requires JavaScript)</p>"},{"location":"Git/object-model/#the-gitobjects-directory","title":"The <code>.git/objects</code> Directory","text":"<p>All objects are stored in <code>.git/objects/</code>. Git uses the first two characters of the hash as a directory name and the remaining 38 as the filename:</p> <pre><code>.git/objects/\n\u251c\u2500\u2500 41/\n\u2502   \u2514\u2500\u2500 e40e5a20c7e8657a8a92e2ce0bfa39a9e0d40c  (a blob)\n\u251c\u2500\u2500 8f/\n\u2502   \u2514\u2500\u2500 a3c9b1d2e4f5a6b7c8d9e0f1a2b3c4d5e6f7a8  (a tree)\n\u251c\u2500\u2500 e4/\n\u2502   \u2514\u2500\u2500 f5a6b7c8d9e0f1a2b3c4d5e6f7a8b9c0d1e2f3  (a commit)\n\u251c\u2500\u2500 info/\n\u2514\u2500\u2500 pack/\n    \u251c\u2500\u2500 pack-abc123.idx\n    \u2514\u2500\u2500 pack-abc123.pack\n</code></pre> <p>Individual objects are called loose objects. As a repository grows, Git periodically packs loose objects into packfiles (<code>.pack</code> with an <code>.idx</code> index) for efficiency. Packfiles use delta compression - storing only the differences between similar objects. The Refs, the Reflog, and the DAG guide covers packfiles in depth.</p> <p>Each loose object is stored as: <code>type size\\0content</code>, compressed with zlib.</p>"},{"location":"Git/object-model/#plumbing-commands","title":"Plumbing Commands","text":"<p>Git has two categories of commands: porcelain (user-facing: <code>commit</code>, <code>merge</code>, <code>push</code>) and plumbing (low-level: <code>hash-object</code>, <code>cat-file</code>, <code>write-tree</code>). Plumbing commands let you interact directly with the object database.</p> <p>See also</p> <p>Git's plumbing commands are designed for piping. For a deep dive into Unix pipes, redirection, and stream processing, see Streams and Redirection.</p>"},{"location":"Git/object-model/#git-hash-object-store-content","title":"<code>git hash-object</code> - Store Content","text":"<pre><code># Hash content from stdin (just compute the hash, don't store)\necho \"Hello\" | git hash-object --stdin\n# ce013625030ba8dba906f756967f9e9ca394464a\n\n# Hash and store into the object database\necho \"Hello\" | git hash-object --stdin -w\n\n# Hash a file\ngit hash-object README.md\n</code></pre>"},{"location":"Git/object-model/#git-cat-file-read-objects","title":"<code>git cat-file</code> - Read Objects","text":"<pre><code># Show object type\ngit cat-file -t a1b2c3d\n# blob, tree, commit, or tag\n\n# Show object size\ngit cat-file -s a1b2c3d\n# 42\n\n# Pretty-print object content\ngit cat-file -p a1b2c3d\n</code></pre> <p>Command Builder (requires JavaScript)</p>"},{"location":"Git/object-model/#git-ls-tree-list-tree-contents","title":"<code>git ls-tree</code> - List Tree Contents","text":"<pre><code># List the root tree of HEAD\ngit ls-tree HEAD\n\n# List recursively (all files in all subdirectories)\ngit ls-tree -r HEAD\n\n# List a specific directory\ngit ls-tree HEAD src/\n</code></pre>"},{"location":"Git/object-model/#git-write-tree-create-a-tree-from-the-index","title":"<code>git write-tree</code> - Create a Tree from the Index","text":"<pre><code># Write the current index as a tree object\ngit write-tree\n# Returns the hash of the new tree\n</code></pre>"},{"location":"Git/object-model/#git-commit-tree-create-a-commit-object","title":"<code>git commit-tree</code> - Create a Commit Object","text":"<pre><code># Create a commit from a tree, with a parent and message\necho \"My commit message\" | git commit-tree &lt;tree-hash&gt; -p &lt;parent-hash&gt;\n# Returns the hash of the new commit\n</code></pre>"},{"location":"Git/object-model/#building-a-commit-with-plumbing-commands","title":"Building a Commit with Plumbing Commands","text":"<p>This is the most illuminating exercise in the entire course. Instead of using <code>git add</code> and <code>git commit</code>, you'll create a commit entirely with low-level plumbing commands - the same operations Git performs internally.</p> <p>Creating a Commit Using Only Plumbing Commands (requires JavaScript)</p> <p>Inspect Git Objects After Creating Files (requires JavaScript)</p>"},{"location":"Git/object-model/#tracing-the-object-graph","title":"Tracing the Object Graph","text":"<p>Every commit points to a tree, every tree points to blobs and subtrees, and everything is connected by SHA-1 hashes. You can trace the entire object graph starting from any commit:</p> <p>Trace from Commit to Blob (requires JavaScript)</p>"},{"location":"Git/object-model/#object-graph-visualization","title":"Object Graph Visualization","text":"<p>The relationships between objects form a directed acyclic graph (DAG). Here's what a small repository's object graph looks like:</p> <pre><code>flowchart TD\n    C2[\"commit: e4f5a6b&lt;br/&gt;Add utils module\"] --&gt; C1[\"commit: a1b2c3d&lt;br/&gt;Initial commit\"]\n    C2 --&gt; T2[\"tree: 8fa3c9b&lt;br/&gt;(root)\"]\n    C1 --&gt; T1[\"tree: c3b8bb1&lt;br/&gt;(root)\"]\n\n    T1 --&gt; B1[\"blob: d670460&lt;br/&gt;README.md content\"]\n    T1 --&gt; B2[\"blob: f1e2d3c&lt;br/&gt;app.py content v1\"]\n\n    T2 --&gt; B1\n    T2 --&gt; B3[\"blob: a9b8c7d&lt;br/&gt;app.py content v2\"]\n    T2 --&gt; ST[\"tree: 7e8f9a0&lt;br/&gt;src/\"]\n    ST --&gt; B4[\"blob: 5d6e7f8&lt;br/&gt;utils.py content\"]</code></pre> <p>Notice that <code>B1</code> (README.md) is referenced by both trees - the file didn't change between commits, so Git reuses the same blob. This is content-addressable deduplication in action.</p>"},{"location":"Git/object-model/#further-reading","title":"Further Reading","text":"<ul> <li>Pro Git - Chapter 10.2: Git Objects - comprehensive walkthrough of blob, tree, commit, and tag objects</li> <li>Git Internals PDF (Scott Chacon) - deep dive into the object model</li> <li>Official git-cat-file documentation - inspecting objects</li> <li>Official git-hash-object documentation - creating objects</li> </ul> <p>Previous: Configuring Git | Next: Refs, the Reflog, and the DAG | Back to Index</p>"},{"location":"Git/platforms/","title":"GitHub, GitLab, and Bitbucket","text":"<p>Git is a distributed version control system. GitHub, GitLab, and Bitbucket are platforms built around Git that add collaboration features: pull/merge requests, CI/CD pipelines, issue tracking, code review, and project management. This guide covers the platform-specific features, CLIs, and CI/CD configuration for each.</p>"},{"location":"Git/platforms/#platform-comparison","title":"Platform Comparison","text":"Feature GitHub GitLab Bitbucket Code Review Pull Requests Merge Requests Pull Requests CI/CD GitHub Actions GitLab CI/CD Bitbucket Pipelines CI Config <code>.github/workflows/*.yml</code> <code>.gitlab-ci.yml</code> <code>bitbucket-pipelines.yml</code> CLI <code>gh</code> <code>glab</code> None (official) Container Registry GitHub Packages Built-in None (use Docker Hub) Issue Tracking Issues + Projects Issues + Boards + Epics Issues + Jira integration Self-Hosted GitHub Enterprise GitLab CE/EE (free self-host) Bitbucket Data Center Free Private Repos Unlimited Unlimited Unlimited (5 users) Unique Features Copilot, Codespaces, Discussions, Sponsors Built-in DevSecOps, DORA metrics, Value Stream Jira/Confluence integration <p>Are Pull Requests (GitHub/Bitbucket) and Merge Requests (GitLab) the same thing? (requires JavaScript)</p>"},{"location":"Git/platforms/#github","title":"GitHub","text":""},{"location":"Git/platforms/#the-gh-cli","title":"The <code>gh</code> CLI","text":"<p><code>gh</code> is GitHub's official CLI. It handles pull requests, issues, releases, actions, and more without leaving the terminal.</p> <pre><code># Install\nbrew install gh          # macOS\nsudo apt install gh      # Debian/Ubuntu\nwinget install GitHub.cli  # Windows\n\n# Authenticate\ngh auth login\n</code></pre>"},{"location":"Git/platforms/#pull-request-operations","title":"Pull Request Operations","text":"<pre><code># Create a PR\ngh pr create --title \"Add user search\" --body \"Implements search with elasticsearch\"\n\n# Create a draft PR\ngh pr create --draft --title \"WIP: Add user search\"\n\n# List open PRs\ngh pr list\n\n# View PR details\ngh pr view 42\n\n# Check out a PR locally\ngh pr checkout 42\n\n# Review a PR\ngh pr review 42 --approve\ngh pr review 42 --request-changes --body \"Need tests for edge cases\"\n\n# Merge a PR\ngh pr merge 42 --squash --delete-branch\n</code></pre>"},{"location":"Git/platforms/#issue-operations","title":"Issue Operations","text":"<pre><code># Create an issue\ngh issue create --title \"Search results missing pagination\" --label \"bug\"\n\n# List issues\ngh issue list --label \"bug\"\n\n# Close an issue\ngh issue close 15\n</code></pre>"},{"location":"Git/platforms/#other-operations","title":"Other Operations","text":"<pre><code># View repository\ngh repo view\n\n# Create a release\ngh release create v1.0.0 --title \"Version 1.0.0\" --notes \"First stable release\"\n\n# View Actions workflow runs\ngh run list\ngh run view 12345\n\n# Clone a repo\ngh repo clone user/repo\n</code></pre> <p>Using gh CLI for Pull Request Workflow (requires JavaScript)</p>"},{"location":"Git/platforms/#github-actions","title":"GitHub Actions","text":"<p>GitHub Actions is GitHub's CI/CD platform. Workflows are YAML files in <code>.github/workflows/</code>:</p> <p>GitHub Actions Workflow (requires JavaScript)</p>"},{"location":"Git/platforms/#code-owners","title":"Code Owners","text":"<p><code>.github/CODEOWNERS</code> assigns reviewers automatically based on file paths:</p> <pre><code># These owners are requested for review when someone opens a PR\n# that modifies files matching the pattern\n\n*.py        @backend-team\n*.js *.ts   @frontend-team\n/docs/      @tech-writers\n/infra/     @devops-team @security-team\n</code></pre>"},{"location":"Git/platforms/#gitlab","title":"GitLab","text":""},{"location":"Git/platforms/#the-glab-cli","title":"The <code>glab</code> CLI","text":"<p><code>glab</code> is GitLab's official CLI, modeled after <code>gh</code>:</p> <pre><code># Install\nbrew install glab       # macOS\nsudo apt install glab   # Debian/Ubuntu\n\n# Authenticate\nglab auth login\n</code></pre>"},{"location":"Git/platforms/#merge-request-operations","title":"Merge Request Operations","text":"<pre><code># Create an MR\nglab mr create --title \"Add user search\" --description \"Elasticsearch integration\"\n\n# List open MRs\nglab mr list\n\n# View an MR\nglab mr view 42\n\n# Approve an MR\nglab mr approve 42\n\n# Merge an MR\nglab mr merge 42 --squash\n</code></pre>"},{"location":"Git/platforms/#gitlab-cicd","title":"GitLab CI/CD","text":"<p>GitLab CI/CD is configured with <code>.gitlab-ci.yml</code> in the repository root:</p> <p>GitLab CI/CD Pipeline (requires JavaScript)</p>"},{"location":"Git/platforms/#gitlab-specific-features","title":"GitLab-Specific Features","text":"<ul> <li>Epics - group related issues across projects</li> <li>Merge trains - queue MRs for sequential merging with CI verification</li> <li>DORA metrics - built-in DevOps performance tracking</li> <li>Environments - track deployments with rollback support</li> <li>Protected environments - require approval before deployment</li> </ul>"},{"location":"Git/platforms/#bitbucket","title":"Bitbucket","text":"<p>Bitbucket focuses on Atlassian ecosystem integration - tight coupling with Jira and Confluence.</p>"},{"location":"Git/platforms/#bitbucket-pipelines","title":"Bitbucket Pipelines","text":"<p>CI/CD is configured with <code>bitbucket-pipelines.yml</code>:</p> <pre><code>image: python:3.12\n\npipelines:\n  default:\n    - step:\n        name: Test\n        caches:\n          - pip\n        script:\n          - pip install -r requirements.txt\n          - pytest --verbose\n\n  branches:\n    main:\n      - step:\n          name: Test\n          script:\n            - pip install -r requirements.txt\n            - pytest\n      - step:\n          name: Deploy\n          deployment: production\n          script:\n            - echo \"Deploying to production\"\n          trigger: manual\n\n  pull-requests:\n    '**':\n      - step:\n          name: Test PR\n          script:\n            - pip install -r requirements.txt\n            - pytest --verbose\n</code></pre>"},{"location":"Git/platforms/#bitbucket-specific-features","title":"Bitbucket-Specific Features","text":"<ul> <li>Jira integration - commits and PRs link to Jira issues automatically (include the issue key like <code>PROJ-123</code> in branch names or commit messages)</li> <li>Deployment permissions - require Jira approval before deploying</li> <li>Pipes - pre-built CI/CD integrations (similar to GitHub Actions marketplace)</li> <li>Merge checks - enforce minimum approvals, passing builds, and resolved tasks</li> </ul>"},{"location":"Git/platforms/#platform-feature-mapping","title":"Platform Feature Mapping","text":"<p>Which platform feature maps to which? (requires JavaScript)</p>"},{"location":"Git/platforms/#setting-up-ci-for-each-platform","title":"Setting Up CI for Each Platform","text":"<p>Write a Basic CI Pipeline Config (requires JavaScript)</p>"},{"location":"Git/platforms/#migrating-between-platforms","title":"Migrating Between Platforms","text":"<p>Since all three platforms use standard Git, migration is straightforward:</p> <pre><code># Clone from the old platform\ngit clone --mirror git@old-platform.com:user/repo.git\ncd repo.git\n\n# Push to the new platform (create the empty repo there first)\ngit push --mirror git@new-platform.com:user/repo.git\n</code></pre> <p><code>--mirror</code> copies all refs (branches, tags), all objects, and the complete history. You'll need to recreate platform-specific configuration (CI files, webhooks, branch protection) manually.</p> <p>Things that don't migrate automatically:</p> <ul> <li>Pull requests / merge requests and their comments</li> <li>Issues and project boards</li> <li>CI/CD configuration (different YAML formats)</li> <li>Webhooks and integrations</li> <li>Wiki content (if using platform wikis)</li> </ul> <p>Some tools help migrate issues and PRs (GitHub Importer, GitLab import), but review is needed to ensure nothing is lost.</p>"},{"location":"Git/platforms/#further-reading","title":"Further Reading","text":"<ul> <li>GitHub Docs - comprehensive documentation for all GitHub features</li> <li>GitHub Actions Reference - workflow syntax, events, and runner details</li> <li>GitHub CLI Manual - complete <code>gh</code> command reference</li> <li>GitLab Docs - CI/CD, MRs, administration, and DevOps features</li> <li>GitLab CI/CD Reference - <code>.gitlab-ci.yml</code> keyword reference</li> <li>glab CLI Documentation - GitLab CLI usage</li> <li>Bitbucket Docs - Pipelines, PRs, and Jira integration</li> <li>Bitbucket Pipelines Reference - pipeline YAML syntax</li> </ul> <p>Previous: Collaboration Workflows | Next: Git Hooks and Automation | Back to Index</p>"},{"location":"Git/refs-reflog-dag/","title":"Refs, the Reflog, and the DAG","text":"<p>The Object Model guide showed you that everything in Git is stored as objects identified by SHA-1 hashes. But nobody wants to type <code>e4f5a6b7c8d9e0f1a2b3c4d5e6f7a8b9c0d1e2f3</code> to reference a commit. References (refs) are human-readable names that point to objects. They're the layer that makes Git usable. This guide covers how refs work, how the reflog tracks every change, how commits form a directed acyclic graph, and how Git manages storage efficiency with garbage collection and packfiles.</p>"},{"location":"Git/refs-reflog-dag/#references","title":"References","text":"<p>A reference (ref) is a file that contains a SHA-1 hash pointing to a Git object - usually a commit. Branches, tags, and remote-tracking branches are all refs.</p>"},{"location":"Git/refs-reflog-dag/#the-gitrefs-directory","title":"The <code>.git/refs/</code> Directory","text":"<pre><code>.git/refs/\n\u251c\u2500\u2500 heads/          # Local branches\n\u2502   \u251c\u2500\u2500 main\n\u2502   \u2514\u2500\u2500 feature/auth\n\u251c\u2500\u2500 tags/           # Tags\n\u2502   \u251c\u2500\u2500 v1.0\n\u2502   \u2514\u2500\u2500 v2.0\n\u251c\u2500\u2500 remotes/        # Remote-tracking branches\n\u2502   \u2514\u2500\u2500 origin/\n\u2502       \u251c\u2500\u2500 main\n\u2502       \u2514\u2500\u2500 feature/auth\n\u2514\u2500\u2500 stash           # Stash ref\n</code></pre> <p>Each file contains a single line: the 40-character hash of the commit (or tag object) it points to.</p> <pre><code># Read a ref directly\ncat .git/refs/heads/main\n# e4f5a6b7c8d9e0f1a2b3c4d5e6f7a8b9c0d1e2f3\n\n# Or use the plumbing command\ngit rev-parse main\n# e4f5a6b7c8d9e0f1a2b3c4d5e6f7a8b9c0d1e2f3\n</code></pre>"},{"location":"Git/refs-reflog-dag/#packed-refs","title":"Packed Refs","text":"<p>When a repository has many refs, Git packs them into a single file for efficiency:</p> <pre><code>cat .git/packed-refs\n</code></pre> <pre><code># pack-refs with: peeled fully-peeled sorted\ne4f5a6b7c8d9e0f1a2b3c4d5e6f7a8b9c0d1e2f3 refs/heads/main\na1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0 refs/tags/v1.0\n^c9d0e1f2a3b4c5d6e7f8a9b0b1c2d3e4f5a6b7c8\n</code></pre> <p>Lines starting with <code>^</code> show the commit that an annotated tag points to (the \"peeled\" value). Loose refs in <code>.git/refs/</code> take precedence over packed refs.</p>"},{"location":"Git/refs-reflog-dag/#plumbing-commands-for-refs","title":"Plumbing Commands for Refs","text":"<pre><code># Update a ref to point to a commit\ngit update-ref refs/heads/new-branch a1b2c3d\n\n# Delete a ref\ngit update-ref -d refs/heads/old-branch\n\n# List all refs\ngit for-each-ref\n</code></pre>"},{"location":"Git/refs-reflog-dag/#symbolic-references","title":"Symbolic References","text":"<p>Most refs contain a commit hash. A symbolic reference contains the name of another ref instead. The most important symbolic ref is <code>HEAD</code>.</p>"},{"location":"Git/refs-reflog-dag/#head","title":"HEAD","text":"<p><code>HEAD</code> is a symbolic ref that points to the current branch:</p> <pre><code>cat .git/HEAD\n# ref: refs/heads/main\n</code></pre> <p>When you commit, Git: 1. Reads HEAD to find the current branch (<code>refs/heads/main</code>) 2. Creates the new commit with the current branch tip as parent 3. Updates the branch ref to point to the new commit</p> <p>When HEAD points directly to a commit hash (not a branch name), you're in detached HEAD state:</p> <pre><code>cat .git/HEAD\n# e4f5a6b7c8d9e0f1a2b3c4d5e6f7a8b9c0d1e2f3\n</code></pre>"},{"location":"Git/refs-reflog-dag/#other-symbolic-references","title":"Other Symbolic References","text":"Reference Created by Contains <code>HEAD</code> Always present Current branch or commit <code>ORIG_HEAD</code> <code>merge</code>, <code>rebase</code>, <code>reset</code> HEAD before the operation (for easy undo) <code>MERGE_HEAD</code> <code>merge</code> (during conflict) The commit being merged in <code>FETCH_HEAD</code> <code>fetch</code> The tips of fetched branches <code>CHERRY_PICK_HEAD</code> <code>cherry-pick</code> (during conflict) The commit being cherry-picked <code>REBASE_HEAD</code> <code>rebase</code> (during conflict) The current commit being rebased <pre><code># Read/write symbolic refs with plumbing\ngit symbolic-ref HEAD\n# refs/heads/main\n\ngit symbolic-ref HEAD refs/heads/feature/auth\n# Now on feature/auth (don't do this normally - use git switch)\n</code></pre> <p>Inspecting Refs and Packed Refs (requires JavaScript)</p>"},{"location":"Git/refs-reflog-dag/#the-directed-acyclic-graph-dag","title":"The Directed Acyclic Graph (DAG)","text":"<p>Commits in Git form a directed acyclic graph (DAG). Each commit points to its parent(s), creating directed edges. The graph is acyclic - you can never follow parent pointers and arrive back at the same commit.</p>"},{"location":"Git/refs-reflog-dag/#what-makes-it-a-dag","title":"What Makes It a DAG","text":"<ul> <li>Directed: Each edge goes one way - from child commit to parent commit</li> <li>Acyclic: No cycles - you can't follow parent links and loop back</li> <li>Graph (not a tree): Merge commits have multiple parents, creating diamond shapes</li> </ul> <pre><code>flowchart RL\n    E[\"E (HEAD -&gt; main)\"] --&gt; D\n    D --&gt; B\n    D --&gt; C\n    C --&gt; A\n    B --&gt; A\n    A[\"A (root)\"]</code></pre> <p>In this graph, <code>D</code> is a merge commit with two parents (<code>B</code> and <code>C</code>). Both <code>B</code> and <code>C</code> have <code>A</code> as their parent. The graph has a diamond shape - this can't happen in a simple tree.</p>"},{"location":"Git/refs-reflog-dag/#reachability","title":"Reachability","text":"<p>A commit is reachable from a ref if you can get to it by following parent pointers. In the graph above, all commits are reachable from <code>main</code> (which points to <code>E</code>). A commit that is unreachable from any ref is eligible for garbage collection.</p> <p>This is why deleting a branch can \"lose\" commits. If the branch was the only ref that could reach certain commits, those commits become unreachable. They still exist in the object database (and in the reflog for a time), but they're invisible to normal commands like <code>git log</code>.</p> <p>What makes Git's commit history a DAG rather than a simple tree? (requires JavaScript)</p>"},{"location":"Git/refs-reflog-dag/#the-reflog","title":"The Reflog","text":"<p>The reflog records every time a ref (branch tip or HEAD) changes. It's a per-repository, per-ref log of all movements. Reflog entries are local only - they're never pushed or shared.</p>"},{"location":"Git/refs-reflog-dag/#viewing-the-reflog","title":"Viewing the Reflog","text":"<pre><code># HEAD reflog (every checkout, commit, reset, rebase, etc.)\ngit reflog\n\n# Reflog for a specific branch\ngit reflog show main\n\n# With timestamps\ngit reflog --date=iso\n\n# With relative dates\ngit reflog --date=relative\n</code></pre>"},{"location":"Git/refs-reflog-dag/#reflog-entry-format","title":"Reflog Entry Format","text":"<pre><code>e5f6a7b HEAD@{0}: commit: Add error handling\nc3d4e5f HEAD@{1}: checkout: moving from main to feature/auth\nc3d4e5f HEAD@{2}: merge feature/search: Fast-forward\na1b2c3d HEAD@{3}: commit: Initial commit\n</code></pre> <p>Each entry records: the resulting hash, the ref position (<code>HEAD@{n}</code>), the operation type, and a description.</p>"},{"location":"Git/refs-reflog-dag/#using-reflog-references","title":"Using Reflog References","text":"<p>The <code>@{n}</code> syntax works in any Git command:</p> <pre><code># Show where HEAD was 3 moves ago\ngit show HEAD@{3}\n\n# Diff between current state and 5 moves ago\ngit diff HEAD@{5} HEAD\n\n# Create a branch at an old position\ngit branch recovery HEAD@{7}\n\n# Time-based references\ngit diff HEAD@{yesterday} HEAD\ngit log HEAD@{2.weeks.ago}..HEAD\n</code></pre>"},{"location":"Git/refs-reflog-dag/#reflog-expiration","title":"Reflog Expiration","text":"<p>Reflog entries don't live forever:</p> <ul> <li>Entries for reachable commits expire after 90 days (default)</li> <li>Entries for unreachable commits expire after 30 days (default)</li> </ul> <p>After expiration, <code>git gc</code> removes the entries. Configure with:</p> <pre><code>git config --global gc.reflogExpire 180.days\ngit config --global gc.reflogExpireUnreachable 90.days\n</code></pre> <p>Reading the Reflog (requires JavaScript)</p>"},{"location":"Git/refs-reflog-dag/#garbage-collection","title":"Garbage Collection","text":"<p>Git's object database grows over time. Unreachable objects - commits orphaned by rebase, old trees from amended commits, blobs from files no longer in any tree - accumulate. Garbage collection (<code>git gc</code>) cleans them up.</p>"},{"location":"Git/refs-reflog-dag/#what-git-gc-does","title":"What <code>git gc</code> Does","text":"<ol> <li>Packs loose objects into packfiles (delta-compressed, more efficient)</li> <li>Removes unreachable objects that are past the reflog expiration window</li> <li>Packs refs (consolidates loose ref files into <code>packed-refs</code>)</li> <li>Prunes the reflog (removes expired entries)</li> </ol> <pre><code># Run garbage collection\ngit gc\n\n# Aggressive GC (more thorough compression, slower)\ngit gc --aggressive\n\n# See what would be pruned\ngit prune --dry-run\n\n# Check for corruption and find unreachable objects\ngit fsck\n</code></pre>"},{"location":"Git/refs-reflog-dag/#when-gc-runs","title":"When GC Runs","text":"<p>Git runs <code>gc --auto</code> automatically after certain operations (like receiving a push). Auto GC only runs if there are more than ~6,700 loose objects or more than ~50 packfiles.</p> <p>When does garbage collection delete unreachable objects? (requires JavaScript)</p>"},{"location":"Git/refs-reflog-dag/#finding-unreachable-objects","title":"Finding Unreachable Objects","text":"<p><code>git fsck</code> (filesystem check) examines the object database for integrity and reports unreachable objects:</p> <pre><code># Check integrity and find unreachable objects\ngit fsck\n\n# Find dangling (unreachable) objects only\ngit fsck --unreachable\n\n# Find and recover lost commits\ngit fsck --lost-found\n</code></pre> <p><code>--lost-found</code> writes unreachable commits and blobs to <code>.git/lost-found/</code>. This is the last resort for recovering data that's been lost from the reflog.</p> <p>Using git fsck to Find Dangling Objects (requires JavaScript)</p>"},{"location":"Git/refs-reflog-dag/#packfiles","title":"Packfiles","text":"<p>When you first create objects, Git stores them as individual loose objects - one zlib-compressed file per object in <code>.git/objects/</code>. As the repository grows, this becomes inefficient. Git uses packfiles to compress many objects into a single file.</p>"},{"location":"Git/refs-reflog-dag/#how-packfiles-work","title":"How Packfiles Work","text":"<p>A packfile (<code>.pack</code>) stores objects sequentially, using delta compression. Instead of storing each version of a file as a full blob, Git stores one version in full (the base) and subsequent versions as deltas (differences) against the base.</p> <p>Git typically uses the most recent version as the base (since that's what you check out most often) and stores older versions as deltas. This is the opposite of what you might expect - older versions take slightly longer to reconstruct.</p> <p>Each packfile has an accompanying index file (<code>.idx</code>) that maps object hashes to their position in the packfile, enabling fast lookups.</p> <pre><code># List packfiles\nls .git/objects/pack/\n\n# Examine a packfile's contents\ngit verify-pack -v .git/objects/pack/pack-*.idx | head -20\n\n# Repack the repository\ngit repack -a -d\n\n# Count loose and packed objects\ngit count-objects -v\n</code></pre>"},{"location":"Git/refs-reflog-dag/#multi-pack-index","title":"Multi-Pack Index","text":"<p>Large repositories (especially those receiving frequent pushes) can accumulate many packfiles. The multi-pack index (MIDX, Git 2.34+) creates a single index across all packfiles, speeding up object lookups:</p> <pre><code># Generate multi-pack index\ngit multi-pack-index write\n\n# Verify multi-pack index\ngit multi-pack-index verify\n</code></pre> <p>Examining Packfiles (requires JavaScript)</p>"},{"location":"Git/refs-reflog-dag/#git-maintenance-automatic-optimization","title":"<code>git maintenance</code> - Automatic Optimization","text":"<p>Git 2.29+ includes <code>git maintenance</code> for scheduling background optimization tasks:</p> <pre><code># Register the current repo for maintenance\ngit maintenance register\n\n# Run all maintenance tasks now\ngit maintenance run\n\n# Start a background maintenance scheduler\ngit maintenance start\n\n# Stop the scheduler\ngit maintenance stop\n</code></pre> <p>Maintenance tasks include: <code>gc</code> (garbage collection), <code>commit-graph</code> (update commit graph file), <code>prefetch</code> (background fetch from remotes), <code>loose-objects</code> (pack loose objects), and <code>incremental-repack</code> (consolidate packfiles).</p>"},{"location":"Git/refs-reflog-dag/#exercise","title":"Exercise","text":"<p>Create and Recover Orphaned Commits (requires JavaScript)</p>"},{"location":"Git/refs-reflog-dag/#further-reading","title":"Further Reading","text":"<ul> <li>Pro Git - Chapter 10.3: Git References - refs, symbolic refs, and HEAD</li> <li>Pro Git - Chapter 10.4: Packfiles - delta compression and pack format</li> <li>Official git-gc documentation - garbage collection options and configuration</li> <li>Official git-fsck documentation - filesystem check and integrity verification</li> <li>Official git-reflog documentation - reflog usage and expiration</li> </ul> <p>Previous: The Object Model | Next: Transfer Protocols and Plumbing | Back to Index</p>"},{"location":"Git/remote-repositories/","title":"Remote Repositories","text":"<p>So far, everything has been local - one repository on your machine. In practice, you work with code that lives on a server, collaborate with other developers, and synchronize changes between multiple copies of the same repository. Remotes are Git's mechanism for connecting these repositories together.</p>"},{"location":"Git/remote-repositories/#what-is-a-remote","title":"What Is a Remote?","text":"<p>A remote is a bookmark to another Git repository. It's a short name (like <code>origin</code>) mapped to a URL where another copy of your repository lives. Remotes don't maintain a live connection - they're just URLs that Git uses when you explicitly fetch or push.</p> <p>When you clone a repository, Git automatically creates a remote called <code>origin</code> pointing to the URL you cloned from. You can add more remotes, rename them, or remove them.</p> <pre><code># List remotes\ngit remote\n\n# List remotes with URLs\ngit remote -v\n</code></pre> <pre><code>origin  git@github.com:yourname/project.git (fetch)\norigin  git@github.com:yourname/project.git (push)\n</code></pre> <p>The fetch and push URLs are usually the same, but they can differ (for example, fetching over HTTPS but pushing over SSH).</p>"},{"location":"Git/remote-repositories/#cloning-a-repository","title":"Cloning a Repository","text":"<p><code>git clone</code> creates a local copy of a remote repository. It does more than download files:</p> <ol> <li>Creates a new directory</li> <li>Initializes a <code>.git</code> directory</li> <li>Creates a remote called <code>origin</code> pointing to the source URL</li> <li>Fetches all branches and their history</li> <li>Creates remote-tracking branches (<code>origin/main</code>, <code>origin/feature/x</code>)</li> <li>Checks out the default branch (usually <code>main</code>) and creates a local tracking branch</li> </ol> <pre><code># Clone via SSH (preferred - uses your SSH key)\ngit clone git@github.com:user/repo.git\n\n# Clone via HTTPS (prompts for credentials)\ngit clone https://github.com/user/repo.git\n\n# Clone into a specific directory\ngit clone git@github.com:user/repo.git my-project\n\n# Clone only the most recent history (faster for large repos)\ngit clone --depth 1 git@github.com:user/repo.git\n</code></pre> <p>After cloning, <code>git remote -v</code> shows the origin:</p> <pre><code>git remote -v\n# origin  git@github.com:user/repo.git (fetch)\n# origin  git@github.com:user/repo.git (push)\n</code></pre>"},{"location":"Git/remote-repositories/#remote-tracking-branches","title":"Remote-Tracking Branches","text":"<p>When you clone or fetch, Git creates remote-tracking branches - read-only references that remember where branches are on the remote. They're named <code>&lt;remote&gt;/&lt;branch&gt;</code>:</p> <pre><code>origin/main\norigin/feature/auth\norigin/release/2.0\n</code></pre> <p>You can't commit to remote-tracking branches directly. They're updated only by <code>git fetch</code> or <code>git pull</code>. Think of them as bookmarks: \"this is where <code>main</code> was on <code>origin</code> the last time I checked.\"</p> <pre><code># List remote-tracking branches\ngit branch -r\n\n# List all branches (local + remote-tracking)\ngit branch -a\n</code></pre> <p>The relationship between local branches, remote-tracking branches, and remote branches:</p> <pre><code>flowchart LR\n    subgraph Local[\"Your Machine\"]\n        direction TB\n        LB[\"Local branch&lt;br/&gt;main\"]\n        RTB[\"Remote-tracking branch&lt;br/&gt;origin/main\"]\n    end\n\n    subgraph Remote[\"GitHub / GitLab\"]\n        direction TB\n        RB[\"Remote branch&lt;br/&gt;main\"]\n    end\n\n    RTB --&gt;|\"git push\"| RB\n    RB --&gt;|\"git fetch\"| RTB\n    RTB --&gt;|\"git merge origin/main\"| LB\n    LB --&gt;|\"git push\"| RB</code></pre>"},{"location":"Git/remote-repositories/#fetch-vs-pull","title":"Fetch vs Pull","text":"<p>This distinction trips up most beginners. They're related but different operations.</p>"},{"location":"Git/remote-repositories/#git-fetch","title":"<code>git fetch</code>","text":"<p><code>git fetch</code> downloads new commits, branches, and tags from a remote - but does not change your working directory or local branches. It only updates your remote-tracking branches.</p> <pre><code># Fetch from origin\ngit fetch origin\n\n# Fetch from all remotes\ngit fetch --all\n\n# Fetch and prune deleted remote branches\ngit fetch --prune\n</code></pre> <p>After fetching, you can inspect what changed before integrating:</p> <pre><code># See what's new on origin/main\ngit log main..origin/main --oneline\n\n# See the diff\ngit diff main origin/main\n</code></pre>"},{"location":"Git/remote-repositories/#git-pull","title":"<code>git pull</code>","text":"<p><code>git pull</code> is <code>git fetch</code> followed by <code>git merge</code> (by default). It downloads and immediately integrates remote changes into your current branch:</p> <pre><code># Equivalent operations:\ngit pull origin main\n# is the same as:\ngit fetch origin\ngit merge origin/main\n</code></pre>"},{"location":"Git/remote-repositories/#why-fetch-merge-is-often-safer","title":"Why <code>fetch</code> + <code>merge</code> Is Often Safer","text":"<p><code>git pull</code> merges automatically, which can create unexpected merge commits or conflicts when you're not ready. With <code>fetch</code> + <code>merge</code>, you can:</p> <ol> <li>Fetch to see what changed</li> <li>Inspect the incoming commits</li> <li>Decide how to integrate (merge, rebase, or wait)</li> </ol> <pre><code>git fetch origin\ngit log --oneline main..origin/main    # Review new commits\ngit merge origin/main                   # Integrate when ready\n</code></pre> <p>Configure pull behavior</p> <p>You can make <code>git pull</code> rebase instead of merge:</p> <pre><code>git config --global pull.rebase true\n</code></pre> <p>Or require explicit choice every time (no silent merges):</p> <pre><code>git config --global pull.ff only\n</code></pre> <p>With <code>pull.ff = only</code>, <code>git pull</code> refuses to create a merge commit. If a fast-forward isn't possible, it fails and tells you to choose between merge and rebase explicitly.</p> <p>What does git fetch do vs git pull? (requires JavaScript)</p>"},{"location":"Git/remote-repositories/#pushing-changes","title":"Pushing Changes","text":"<p><code>git push</code> uploads your local commits to a remote repository:</p> <pre><code># Push current branch to its upstream\ngit push\n\n# Push a specific branch to a specific remote\ngit push origin main\n\n# Push and set upstream tracking (first push of a new branch)\ngit push -u origin feature/auth\n\n# Push all branches\ngit push --all\n\n# Push tags\ngit push --tags\n</code></pre>"},{"location":"Git/remote-repositories/#push-rejection","title":"Push Rejection","text":"<p>If the remote has commits that you don't have locally, Git rejects the push to prevent overwriting others' work:</p> <pre><code>! [rejected]        main -&gt; main (fetch first)\nerror: failed to push some refs to 'git@github.com:user/repo.git'\nhint: Updates were rejected because the remote contains work that you do not\nhint: have locally. Integrate the remote changes (e.g., 'git pull ...') before\nhint: pushing again.\n</code></pre> <p>The fix: fetch, integrate (merge or rebase), then push again:</p> <pre><code>git fetch origin\ngit merge origin/main    # or: git rebase origin/main\ngit push origin main\n</code></pre> <p>Never force push shared branches</p> <p><code>git push --force</code> overwrites the remote branch with your local version, potentially destroying other people's commits. Only force push to branches that you alone work on (and even then, prefer <code>--force-with-lease</code> which fails if someone else has pushed since your last fetch).</p>"},{"location":"Git/remote-repositories/#tracking-branches-and-upstream-configuration","title":"Tracking Branches and Upstream Configuration","text":"<p>A tracking branch (or upstream branch) is a local branch configured to follow a remote branch. When you <code>git clone</code>, <code>main</code> automatically tracks <code>origin/main</code>. For new branches, you set tracking explicitly:</p> <pre><code># Set upstream when pushing a new branch\ngit push -u origin feature/auth\n\n# Set upstream for an existing branch\ngit branch --set-upstream-to=origin/feature/auth feature/auth\n\n# Check tracking configuration\ngit branch -vv\n</code></pre> <p><code>git branch -vv</code> shows each branch's upstream:</p> <pre><code>  feature/auth  a1b2c3d [origin/feature/auth: ahead 2] Add OAuth handler\n* main          e4f5a6b [origin/main] Latest commit message\n</code></pre> <p>\"ahead 2\" means your local branch has 2 commits not yet pushed. \"behind 3\" would mean the remote has 3 commits you haven't fetched.</p> <p>What is a tracking branch? (requires JavaScript)</p>"},{"location":"Git/remote-repositories/#managing-remotes","title":"Managing Remotes","text":""},{"location":"Git/remote-repositories/#adding-a-remote","title":"Adding a Remote","text":"<p>Common when working with forks - you have your fork as <code>origin</code> and the original repo as <code>upstream</code>:</p> <pre><code># Add a new remote\ngit remote add upstream git@github.com:original-author/repo.git\n\n# Verify\ngit remote -v\n# origin    git@github.com:yourname/repo.git (fetch)\n# origin    git@github.com:yourname/repo.git (push)\n# upstream  git@github.com:original-author/repo.git (fetch)\n# upstream  git@github.com:original-author/repo.git (push)\n</code></pre>"},{"location":"Git/remote-repositories/#renaming-and-removing","title":"Renaming and Removing","text":"<pre><code># Rename a remote\ngit remote rename origin github\n\n# Remove a remote\ngit remote remove upstream\n\n# Show details about a remote\ngit remote show origin\n</code></pre> <p><code>git remote show origin</code> displays useful information: the fetch/push URLs, tracked branches, and whether local branches are ahead or behind.</p> <p>git remote Command Builder (requires JavaScript)</p>"},{"location":"Git/remote-repositories/#ssh-vs-https-authentication","title":"SSH vs HTTPS Authentication","text":"<p>Git supports two main protocols for communicating with remote repositories.</p>"},{"location":"Git/remote-repositories/#https","title":"HTTPS","text":"<ul> <li>URL format: <code>https://github.com/user/repo.git</code></li> <li>Prompts for username/password (or token)</li> <li>Works through most firewalls and proxies</li> <li>Requires credential helper to avoid retyping passwords</li> </ul> <pre><code># Configure credential caching (in memory for 15 minutes)\ngit config --global credential.helper cache\n\n# macOS keychain\ngit config --global credential.helper osxkeychain\n\n# Windows credential manager\ngit config --global credential.helper manager\n</code></pre> <p>Use tokens, not passwords</p> <p>GitHub, GitLab, and Bitbucket no longer accept account passwords for HTTPS Git operations. Use a personal access token (PAT) instead of your password. Generate one in your platform's settings under Developer Settings or Access Tokens.</p>"},{"location":"Git/remote-repositories/#ssh","title":"SSH","text":"<ul> <li>URL format: <code>git@github.com:user/repo.git</code></li> <li>Uses SSH key pairs (no passwords after setup)</li> <li>More secure than HTTPS with passwords</li> <li>Requires SSH key generation and registration with the platform</li> </ul> <p>Setting Up SSH Authentication (requires JavaScript)</p>"},{"location":"Git/remote-repositories/#switching-a-remote-from-https-to-ssh","title":"Switching a Remote from HTTPS to SSH","text":"<pre><code># Check current URL\ngit remote -v\n# origin  https://github.com/user/repo.git (fetch)\n\n# Switch to SSH\ngit remote set-url origin git@github.com:user/repo.git\n\n# Verify\ngit remote -v\n# origin  git@github.com:user/repo.git (fetch)\n</code></pre>"},{"location":"Git/remote-repositories/#working-with-forks","title":"Working with Forks","text":"<p>In open-source projects, you typically don't have push access to the original repository. The workflow is:</p> <ol> <li>Fork the repository on the platform (creates your copy under your account)</li> <li>Clone your fork locally (this becomes <code>origin</code>)</li> <li>Add the original repository as a remote called <code>upstream</code></li> <li>Fetch from upstream to stay current</li> <li>Create branches for your work (based on <code>upstream/main</code>)</li> <li>Push to your fork (<code>origin</code>)</li> <li>Open a pull request from your fork to the original repository</li> </ol> <pre><code># Clone your fork\ngit clone git@github.com:yourname/project.git\ncd project\n\n# Add the original repo as upstream\ngit remote add upstream git@github.com:original-author/project.git\n\n# Stay current with upstream\ngit fetch upstream\ngit merge upstream/main    # or rebase\n\n# Work on a feature\ngit switch -c feature/my-contribution\n# ... make changes, commit ...\n\n# Push to your fork\ngit push -u origin feature/my-contribution\n\n# Open a pull request on the platform\n</code></pre> <p>Cloning, Inspecting Remotes, and Fetch/Merge Workflow (requires JavaScript)</p>"},{"location":"Git/remote-repositories/#exercise","title":"Exercise","text":"<p>Two-Remote Workflow (requires JavaScript)</p>"},{"location":"Git/remote-repositories/#further-reading","title":"Further Reading","text":"<ul> <li>Pro Git - Chapter 2.5: Working with Remotes - fetching, pulling, pushing, and remote management</li> <li>Pro Git - Chapter 5: Distributed Git - distributed workflows and contributing to projects</li> <li>GitHub SSH Documentation - setting up SSH keys for GitHub</li> <li>GitLab SSH Documentation - SSH setup for GitLab</li> <li>Official git-remote documentation - complete reference for remote management</li> <li>Official git-fetch documentation - fetch options and configuration</li> </ul> <p>Previous: Branches and Merging | Next: Rewriting History | Back to Index</p>"},{"location":"Git/rewriting-history/","title":"Rewriting History","text":"<p>Git's commit history is immutable in a cryptographic sense - every commit is identified by a hash of its contents, parents, and metadata. But Git gives you tools to create new commits that replace old ones, effectively rewriting the visible history. This is one of Git's most powerful capabilities and one of its most dangerous. Understanding when rewriting is safe, what tools are available, and how to recover from mistakes is essential.</p>"},{"location":"Git/rewriting-history/#the-golden-rule","title":"The Golden Rule","text":"<p>Never rewrite commits that have been pushed to a shared branch.</p> <p>When you rewrite history, you create new commit objects with new hashes. Anyone who based work on the old commits now has a diverged history. They'll get confusing merge conflicts when they try to pull, and the team will waste time untangling the mess.</p> <p>Rewrite freely on:</p> <ul> <li>Local branches that only you work on</li> <li>Feature branches before they're merged</li> <li>Commits you haven't pushed yet</li> </ul> <p>Leave shared history alone. Use <code>git revert</code> (which creates new commits) instead of rewriting when working on branches others depend on.</p>"},{"location":"Git/rewriting-history/#amending-the-last-commit","title":"Amending the Last Commit","text":"<p>The simplest form of rewriting. <code>git commit --amend</code> replaces the most recent commit:</p> <pre><code># Fix the commit message\ngit commit --amend -m \"Correct message\"\n\n# Add a forgotten file (keep the same message)\ngit add forgotten-file.py\ngit commit --amend --no-edit\n\n# Change both the content and message\ngit add extra-fix.py\ngit commit --amend -m \"Updated message with extra fix\"\n</code></pre> <p>Under the hood, <code>--amend</code> creates a brand-new commit with a new hash and points the branch at it. The old commit still exists in the object database (reachable via the reflog) until garbage collection removes it.</p>"},{"location":"Git/rewriting-history/#interactive-rebase","title":"Interactive Rebase","text":"<p><code>git rebase -i</code> (interactive rebase) is the Swiss army knife for history cleanup. It lets you reorder, squash, split, edit, and drop commits from a branch.</p>"},{"location":"Git/rewriting-history/#starting-an-interactive-rebase","title":"Starting an Interactive Rebase","text":"<pre><code># Rebase the last 4 commits\ngit rebase -i HEAD~4\n\n# Rebase all commits since branching from main\ngit rebase -i main\n</code></pre> <p>Git opens your editor with a list of commits (oldest first):</p> <pre><code>pick a1b2c3d Add user model\npick b2c3d4e Add user controller\npick c3d4e5f Fix typo in user model\npick d4e5f6a Add user tests\n</code></pre>"},{"location":"Git/rewriting-history/#rebase-commands","title":"Rebase Commands","text":"<p>Change the word <code>pick</code> to one of these to control what happens to each commit:</p> Command Short Effect <code>pick</code> <code>p</code> Keep the commit as-is <code>reword</code> <code>r</code> Keep the commit, edit the message <code>edit</code> <code>e</code> Pause at this commit so you can amend it <code>squash</code> <code>s</code> Combine with the previous commit, edit the combined message <code>fixup</code> <code>f</code> Combine with the previous commit, discard this commit's message <code>drop</code> <code>d</code> Delete the commit entirely <p>You can also reorder lines to reorder commits, or delete a line to drop a commit.</p>"},{"location":"Git/rewriting-history/#example-squashing-cleanup-commits","title":"Example: Squashing Cleanup Commits","text":"<p>You made three commits but the middle one is just a typo fix. Squash it into the first:</p> <pre><code>pick a1b2c3d Add user model\nfixup c3d4e5f Fix typo in user model\npick b2c3d4e Add user controller\npick d4e5f6a Add user tests\n</code></pre> <p>Notice the reordering: the typo fix is moved directly below the commit it fixes, then marked as <code>fixup</code> so its message is discarded. The result is three clean commits instead of four.</p>"},{"location":"Git/rewriting-history/#example-editing-a-commit","title":"Example: Editing a Commit","text":"<p>Mark a commit as <code>edit</code> to pause the rebase at that point:</p> <pre><code>edit a1b2c3d Add user model\npick b2c3d4e Add user controller\n</code></pre> <p>Git stops after replaying <code>a1b2c3d</code>. You can now:</p> <pre><code># Make changes to files\ngit add .\ngit commit --amend    # Modify the commit\n\n# Continue the rebase\ngit rebase --continue\n</code></pre>"},{"location":"Git/rewriting-history/#autosquash","title":"Autosquash","text":"<p>If you know while committing that a change should be squashed into an earlier commit, use <code>--fixup</code>:</p> <pre><code># Create a fixup commit targeting a specific commit\ngit commit --fixup=a1b2c3d\n\n# Later, rebase with autosquash to automatically reorder and squash\ngit rebase -i --autosquash main\n</code></pre> <p>Git creates a commit with a message like <code>fixup! Add user model</code>. When you run <code>rebase -i --autosquash</code>, it automatically reorders the fixup commit and marks it as <code>fixup</code>.</p> <p>To always autosquash during interactive rebase:</p> <pre><code>git config --global rebase.autoSquash true\n</code></pre> <p>Interactive Rebase - Squashing and Reordering (requires JavaScript)</p>"},{"location":"Git/rewriting-history/#cherry-pick","title":"Cherry-Pick","text":"<p><code>git cherry-pick</code> copies a specific commit from one branch to another. It creates a new commit with the same changes but a different hash and parent:</p> <pre><code># Apply a specific commit to the current branch\ngit cherry-pick a1b2c3d\n\n# Cherry-pick without committing (stage the changes instead)\ngit cherry-pick --no-commit a1b2c3d\n\n# Cherry-pick a range of commits\ngit cherry-pick a1b2c3d..d4e5f6a\n</code></pre> <p>Cherry-pick is useful when you need a specific fix from another branch but don't want to merge the entire branch. But use it sparingly - duplicated commits (same changes, different hashes) can cause confusion when the branches are eventually merged.</p>"},{"location":"Git/rewriting-history/#revert-vs-reset","title":"Revert vs Reset","text":"<p>Both undo changes, but in fundamentally different ways.</p>"},{"location":"Git/rewriting-history/#git-revert-safe-undo-creates-new-commits","title":"<code>git revert</code> - Safe Undo (Creates New Commits)","text":"<p><code>git revert</code> creates a new commit that undoes the changes from a specified commit. The original commit remains in history:</p> <pre><code># Revert a specific commit\ngit revert a1b2c3d\n\n# Revert without auto-committing (stage the undo changes)\ngit revert --no-commit a1b2c3d\n\n# Revert a merge commit (must specify which parent to keep)\ngit revert -m 1 a1b2c3d\n</code></pre> <p>Use revert on shared branches. It's safe because it adds to history rather than rewriting it. Everyone can pull the revert without conflicts.</p>"},{"location":"Git/rewriting-history/#git-reset-rewrite-history-moves-head","title":"<code>git reset</code> - Rewrite History (Moves HEAD)","text":"<p><code>git reset</code> moves the branch pointer backward, effectively removing commits from the branch's visible history. The three modes control what happens to the changes from those commits:</p> Mode HEAD moves? Index changes? Working dir changes? Effect <code>--soft</code> Yes No No Commits removed, changes stay staged <code>--mixed</code> (default) Yes Yes No Commits removed, changes unstaged but in working dir <code>--hard</code> Yes Yes Yes Commits removed, changes discarded entirely <pre><code># Soft reset: uncommit but keep changes staged\ngit reset --soft HEAD~1\n\n# Mixed reset: uncommit and unstage, keep changes in working dir\ngit reset HEAD~1\n\n# Hard reset: uncommit and discard all changes\ngit reset --hard HEAD~1\n\n# Reset to a specific commit\ngit reset --hard a1b2c3d\n</code></pre> <p>What is the difference between git reset --soft, --mixed, and --hard? (requires JavaScript)</p> <p>git reset --hard deletes uncommitted work</p> <p><code>--hard</code> discards changes from your working directory. If those changes were never committed or stashed, they're gone permanently. Always check <code>git status</code> and <code>git stash</code> before using <code>--hard</code>.</p> <pre><code>flowchart TD\n    A[\"Want to undo commits?\"] --&gt; B{\"Are they on a shared branch?\"}\n    B --&gt;|Yes| C[\"git revert&lt;br/&gt;(safe, creates undo commit)\"]\n    B --&gt;|No| D{\"What should happen to the changes?\"}\n    D --&gt;|Keep staged| E[\"git reset --soft\"]\n    D --&gt;|Keep unstaged| F[\"git reset --mixed\"]\n    D --&gt;|Discard completely| G[\"git reset --hard\"]</code></pre>"},{"location":"Git/rewriting-history/#the-reflog-your-safety-net","title":"The Reflog: Your Safety Net","text":"<p>The reflog (reference log) records every time a branch tip or HEAD changes. Every commit, reset, rebase, checkout, merge, and amend is recorded. Even after you rewrite history, the old commits are findable through the reflog.</p> <pre><code># View the reflog for HEAD\ngit reflog\n\n# View reflog for a specific branch\ngit reflog show main\n\n# View with dates\ngit reflog --date=iso\n</code></pre> <p>Sample output:</p> <pre><code>e5f6a7b (HEAD -&gt; main) HEAD@{0}: rebase (finish): returning to refs/heads/main\nd4e5f6a HEAD@{1}: rebase (start): checkout HEAD~3\nd4e5f6a HEAD@{2}: commit: Add tests\nc3d4e5f HEAD@{3}: commit: Fix typo in application\nb2c3d4e HEAD@{4}: commit: Update application logic\na1b2c3d HEAD@{5}: commit (initial): Add application\n</code></pre> <p>Each entry has a reference like <code>HEAD@{3}</code> that you can use in any Git command:</p> <pre><code># See what HEAD pointed to 3 moves ago\ngit show HEAD@{3}\n\n# Reset to where HEAD was before a bad rebase\ngit reset --hard HEAD@{1}\n\n# Create a branch at an old position\ngit branch recovery HEAD@{5}\n</code></pre> <p>Reflog entries expire</p> <p>By default, reflog entries for reachable commits expire after 90 days and unreachable commits after 30 days. If you need to recover something, don't wait months. You can adjust the expiration: <code>git config gc.reflogExpire 180.days</code>.</p> <p>Recovering from a Bad Reset Using the Reflog (requires JavaScript)</p>"},{"location":"Git/rewriting-history/#rebasing-onto-another-branch","title":"Rebasing onto Another Branch","text":"<p>Beyond interactive rebase for cleanup, <code>git rebase</code> is used to move a branch to start from a different base:</p> <pre><code># Rebase current branch onto main\ngit rebase main\n</code></pre> <p>Before rebase: <pre><code>main:    A \u2500\u2500 B \u2500\u2500 C\n               \\\nfeature:        D \u2500\u2500 E\n</code></pre></p> <p>After <code>git switch feature &amp;&amp; git rebase main</code>: <pre><code>main:    A \u2500\u2500 B \u2500\u2500 C\n                     \\\nfeature:              D' \u2500\u2500 E'\n</code></pre></p> <p><code>D'</code> and <code>E'</code> are new commits (new hashes) with the same changes as <code>D</code> and <code>E</code>, but they now build on <code>C</code> instead of <code>B</code>. The original <code>D</code> and <code>E</code> become unreachable.</p>"},{"location":"Git/rewriting-history/#rebase-vs-merge","title":"Rebase vs Merge","text":"Merge Rebase History Preserves branch topology Creates linear history Commits Adds a merge commit Rewrites existing commits Safety Non-destructive Rewrites history (dangerous for shared branches) Conflicts Resolve once May resolve per-commit Use when Integrating shared branches Cleaning up local/feature branches before merge <p>When is it safe to rebase? (requires JavaScript)</p>"},{"location":"Git/rewriting-history/#exercises","title":"Exercises","text":"<p>Clean Up a Messy Branch with Interactive Rebase (requires JavaScript)</p> <p>Revert vs Reset on Shared and Private Branches (requires JavaScript)</p>"},{"location":"Git/rewriting-history/#further-reading","title":"Further Reading","text":"<ul> <li>Pro Git - Chapter 7.6: Rewriting History - interactive rebase, amending, filter-branch</li> <li>Pro Git - Chapter 3.6: Rebasing - rebase fundamentals and the golden rule</li> <li>Official git-rebase documentation - complete reference for rebase modes and options</li> <li>Official git-reset documentation - the three reset modes explained</li> <li>Official git-reflog documentation - reflog usage and expiration</li> </ul> <p>Previous: Remote Repositories | Next: Stashing and the Worktree | Back to Index</p>"},{"location":"Git/security/","title":"Git Security","text":"<p>Git's cryptographic design provides integrity (every object is hashed), but it doesn't provide authentication or secrecy on its own. This guide covers signing commits to prove authorship, managing credentials safely, detecting and removing secrets from repositories, and securing your Git workflow end to end.</p>"},{"location":"Git/security/#why-sign-commits","title":"Why Sign Commits?","text":"<p>Anyone can set <code>user.name</code> and <code>user.email</code> to any value. Without signing, there's no proof that a commit was actually written by the person it claims. Signed commits use cryptographic signatures to prove that the committer holds a specific private key, and platforms like GitHub and GitLab display a \"Verified\" badge.</p> <p>Two signing methods are available:</p> Method Key type Git version Platform support GPG signing GPG key pair Any GitHub, GitLab, Bitbucket SSH signing SSH key pair 2.34+ GitHub, GitLab"},{"location":"Git/security/#gpg-signing","title":"GPG Signing","text":""},{"location":"Git/security/#setting-up-gpg","title":"Setting Up GPG","text":"<pre><code># Generate a GPG key (choose RSA 4096 or Ed25519)\ngpg --full-generate-key\n\n# List your keys\ngpg --list-secret-keys --keyid-format=long\n\n# Output:\n# sec   ed25519/ABC123DEF456 2024-01-15 [SC]\n#       ABCDEF1234567890ABCDEF1234567890ABC123DE\n# uid           [ultimate] Jane Developer &lt;jane@example.com&gt;\n</code></pre> <p>The key ID after the algorithm (<code>ABC123DEF456</code>) is what you configure Git with:</p> <pre><code># Tell Git which key to use\ngit config --global user.signingkey ABC123DEF456\n\n# Sign all commits by default\ngit config --global commit.gpgsign true\n\n# Sign all tags by default\ngit config --global tag.gpgsign true\n</code></pre>"},{"location":"Git/security/#adding-your-key-to-a-platform","title":"Adding Your Key to a Platform","text":"<p>Export the public key and paste it into your platform's settings:</p> <pre><code># Export public key\ngpg --armor --export ABC123DEF456\n</code></pre> <p>Copy the output (starting with <code>-----BEGIN PGP PUBLIC KEY BLOCK-----</code>) to: - GitHub: Settings &gt; SSH and GPG keys &gt; New GPG key - GitLab: User Settings &gt; GPG Keys</p>"},{"location":"Git/security/#making-signed-commits","title":"Making Signed Commits","text":"<pre><code># Sign a single commit\ngit commit -S -m \"Add signed authentication module\"\n\n# With commit.gpgsign = true, all commits are signed automatically\ngit commit -m \"This is signed automatically\"\n\n# Create a signed tag\ngit tag -s v1.0 -m \"Signed release 1.0\"\n</code></pre>"},{"location":"Git/security/#ssh-signing-git-234","title":"SSH Signing (Git 2.34+)","text":"<p>SSH signing uses your existing SSH key - no GPG required. This is simpler if you already have SSH keys for authentication.</p> <pre><code># Configure SSH signing\ngit config --global gpg.format ssh\ngit config --global user.signingkey ~/.ssh/id_ed25519.pub\n\n# Sign all commits\ngit config --global commit.gpgsign true\n</code></pre>"},{"location":"Git/security/#allowed-signers-file","title":"Allowed Signers File","text":"<p>For verifying SSH signatures locally, Git needs a list of trusted keys:</p> <pre><code># Create an allowed signers file\necho \"jane@example.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIG8r...\" &gt; ~/.config/git/allowed_signers\n\n# Tell Git about it\ngit config --global gpg.ssh.allowedSignersFile ~/.config/git/allowed_signers\n</code></pre> <p>Setting Up GPG Signing (requires JavaScript)</p> <p>Setting Up SSH Signing (Git 2.34+) (requires JavaScript)</p> <p>When should you use GPG signing vs SSH signing? (requires JavaScript)</p>"},{"location":"Git/security/#verifying-signatures","title":"Verifying Signatures","text":"<p>When you run <code>git verify-commit</code> or a platform checks a pushed commit, the following sequence plays out:</p> <pre><code>sequenceDiagram\n    participant V as Verifier\n    participant G as Git\n    participant K as Key Store\n\n    V-&gt;&gt;G: git verify-commit abc123\n    G-&gt;&gt;G: Extract gpgsig from commit object\n    G-&gt;&gt;K: Verify signature with public key\n    alt Key found and signature valid\n        K--&gt;&gt;G: Good signature\n        G--&gt;&gt;V: Verified (key ID, signer identity)\n    else Key not found\n        K--&gt;&gt;G: No public key\n        G--&gt;&gt;V: Cannot verify (unknown key)\n    else Signature invalid\n        K--&gt;&gt;G: Bad signature\n        G--&gt;&gt;V: Verification failed\n    end</code></pre> <p>The key store is GPG's keyring or the SSH <code>allowed_signers</code> file, depending on your signing format. Platforms perform the same check server-side using the public keys uploaded to your account.</p> <pre><code># Verify a commit\ngit verify-commit HEAD\n\n# Verify a tag\ngit verify-tag v1.0\n\n# Show signatures in log\ngit log --show-signature\n\n# Show signatures in one-line log\ngit log --oneline --format='%h %G? %s'\n# %G? shows: G (good), B (bad), U (untrusted), N (no signature), E (expired)\n</code></pre>"},{"location":"Git/security/#credential-management","title":"Credential Management","text":"<p>Storing credentials safely is essential. Typing passwords or tokens for every push is impractical, but hardcoding them is dangerous.</p>"},{"location":"Git/security/#credential-helpers","title":"Credential Helpers","text":"<p>Git's credential helpers cache or store authentication tokens:</p> <pre><code># Cache in memory (default 15 minutes)\ngit config --global credential.helper cache\ngit config --global credential.helper 'cache --timeout=3600'  # 1 hour\n\n# macOS Keychain\ngit config --global credential.helper osxkeychain\n\n# Windows Credential Manager\ngit config --global credential.helper manager\n\n# Linux libsecret (GNOME Keyring)\ngit config --global credential.helper /usr/lib/git-core/git-credential-libsecret\n</code></pre> <p>Credential Helper Configuration (requires JavaScript)</p>"},{"location":"Git/security/#personal-access-tokens","title":"Personal Access Tokens","text":"<p>All major platforms require tokens (not passwords) for HTTPS Git operations:</p> <ul> <li>GitHub: Settings &gt; Developer Settings &gt; Personal access tokens &gt; Tokens (classic) or Fine-grained tokens</li> <li>GitLab: User Settings &gt; Access Tokens</li> <li>Bitbucket: Personal Settings &gt; App passwords</li> </ul> <p>Use the token as your password when Git prompts. The credential helper caches it.</p> <p>Never commit tokens</p> <p>Tokens in your repository history are permanent (even after removal, they exist in old commits). Platforms scan for accidentally committed tokens and may revoke them, but the exposure window can be enough for damage.</p>"},{"location":"Git/security/#secret-scanning","title":"Secret Scanning","text":"<p>Accidentally committing secrets (API keys, passwords, tokens, private keys) is one of the most common security mistakes. Prevention is far easier than cleanup.</p>"},{"location":"Git/security/#pre-commit-detection","title":"Pre-Commit Detection","text":"<p>Install hooks that scan for secrets before they enter the repository:</p> <pre><code># Using the pre-commit framework\npip install pre-commit\n</code></pre> <pre><code># .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/gitleaks/gitleaks\n    rev: v8.18.0\n    hooks:\n      - id: gitleaks\n</code></pre>"},{"location":"Git/security/#dedicated-scanning-tools","title":"Dedicated Scanning Tools","text":"<p>Gitleaks scans repositories for hardcoded secrets:</p> <pre><code># Install\nbrew install gitleaks  # macOS\n\n# Scan the current repo\ngitleaks detect\n\n# Scan with verbose output\ngitleaks detect -v\n\n# Scan specific commits\ngitleaks detect --log-opts=\"HEAD~10..HEAD\"\n</code></pre> <p>TruffleHog performs deep scanning including entropy analysis:</p> <pre><code># Scan a repository\ntrufflehog git file://./\n\n# Scan a remote repository\ntrufflehog github --repo https://github.com/user/repo\n</code></pre>"},{"location":"Git/security/#platform-secret-scanning","title":"Platform Secret Scanning","text":"<ul> <li>GitHub: Automatic secret scanning for public repos (free) and private repos (Advanced Security license). Detects known token formats from 100+ providers.</li> <li>GitLab: Secret Detection CI component scans MR diffs automatically.</li> </ul> <p>A secret was committed 5 commits ago and pushed to GitHub. What should you do? (requires JavaScript)</p>"},{"location":"Git/security/#removing-secrets-from-history","title":"Removing Secrets from History","text":"<p>When a secret has been committed, you need to rewrite history to remove it from every commit.</p>"},{"location":"Git/security/#git-filter-repo-recommended","title":"<code>git filter-repo</code> (Recommended)","text":"<p>git filter-repo is the modern replacement for <code>git filter-branch</code>. It's faster, safer, and easier to use:</p> <pre><code># Install\npip install git-filter-repo\n\n# Remove a file from all history\ngit filter-repo --path secrets.env --invert-paths\n\n# Replace specific strings in all files across all history\ngit filter-repo --replace-text expressions.txt\n</code></pre> <p>The <code>expressions.txt</code> file maps secrets to replacements:</p> <pre><code>literal:sk_live_abc123def456==&gt;***REDACTED***\nregex:password\\s*=\\s*['\"].*?[']==&gt;password = \"***REDACTED***\"\n</code></pre> <p>Removing a Secret from History with git filter-repo (requires JavaScript)</p>"},{"location":"Git/security/#bfg-repo-cleaner","title":"BFG Repo-Cleaner","text":"<p>BFG is a simpler tool focused on common cleaning tasks:</p> <pre><code># Remove a file by name\nbfg --delete-files .env\n\n# Replace strings\nbfg --replace-text passwords.txt\n\n# Remove large files\nbfg --strip-blobs-bigger-than 100M\n</code></pre> <p>After either tool, force push the cleaned history:</p> <pre><code>git reflog expire --expire=now --all\ngit gc --prune=now --aggressive\ngit push --force --all\ngit push --force --tags\n</code></pre> <p>Force pushing after history rewrite</p> <p>History rewriting changes every commit hash from the rewritten point onward. All collaborators must re-clone or carefully rebase their work. Coordinate with your team before force pushing. Notify everyone that the history has changed.</p>"},{"location":"Git/security/#security-focused-gitignore","title":"Security-Focused <code>.gitignore</code>","text":"<p>Prevent secrets from being committed in the first place:</p> <pre><code># Secrets and credentials\n.env\n.env.*\n!.env.example\n*.key\n*.pem\n*.p12\n*.pfx\ncredentials.json\nservice-account.json\n**/secrets/\n\n# SSH keys (if stored in repo for some reason)\nid_rsa\nid_ed25519\n*.pub\n\n# Cloud provider configs\n.aws/credentials\n.gcp/\n</code></pre>"},{"location":"Git/security/#exercises","title":"Exercises","text":"<p>Set Up Commit Signing and Verify (requires JavaScript)</p> <p>Scan for Leaked Secrets (requires JavaScript)</p>"},{"location":"Git/security/#further-reading","title":"Further Reading","text":"<ul> <li>Pro Git - Chapter 7.4: Signing Your Work - GPG signing commits and tags</li> <li>GitHub: Signing Commits - setting up GPG and SSH signing on GitHub</li> <li>GitLab: Signing Commits with GPG - GitLab GPG verification</li> <li>git-filter-repo Documentation - history rewriting tool</li> <li>Gitleaks - secret scanning tool</li> <li>TruffleHog - deep secret scanning</li> <li>OWASP Secret Management Cheat Sheet - comprehensive secret management guidance</li> </ul> <p>Previous: Git Hooks and Automation | Next: Monorepos and Scaling Git | Back to Index</p>"},{"location":"Git/stashing-and-worktree/","title":"Stashing and the Worktree","text":"<p>You're halfway through a feature when an urgent bug comes in. Your working directory has uncommitted changes that aren't ready for a commit. You need to switch context, fix the bug, and come back to where you left off. Git provides two tools for this: stashing saves changes temporarily and restores them later, and worktrees let you work on multiple branches simultaneously in separate directories.</p>"},{"location":"Git/stashing-and-worktree/#git-stash","title":"Git Stash","text":"<p><code>git stash</code> takes your modified tracked files and staged changes, saves them on a stack, and reverts your working directory to match HEAD. You can then switch branches, do other work, and come back to apply the stashed changes.</p>"},{"location":"Git/stashing-and-worktree/#basic-stash-workflow","title":"Basic Stash Workflow","text":"<pre><code># Save current changes to the stash\ngit stash\n\n# Same as above, with a descriptive message\ngit stash push -m \"WIP: user profile validation\"\n\n# List all stashes\ngit stash list\n\n# Apply the most recent stash (keep it in the stack)\ngit stash apply\n\n# Apply and remove the most recent stash\ngit stash pop\n\n# Apply a specific stash\ngit stash apply stash@{2}\n</code></pre>"},{"location":"Git/stashing-and-worktree/#what-gets-stashed","title":"What Gets Stashed","text":"<p>By default, <code>git stash</code> saves:</p> <ul> <li>Modified tracked files</li> <li>Staged changes</li> </ul> <p>It does not stash:</p> <ul> <li>Untracked files (new files not yet added)</li> <li>Ignored files</li> </ul> <p>Use flags to include those:</p> <pre><code># Include untracked files\ngit stash push -u\n# or\ngit stash push --include-untracked\n\n# Include everything (untracked + ignored)\ngit stash push -a\n# or\ngit stash push --all\n</code></pre> <p>The following diagram shows how file states transition during stash operations:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; Modified: Edit files\n    Modified --&gt; Staged: git add\n    Staged --&gt; Stashed: git stash push\n    Modified --&gt; Stashed: git stash push\n    Stashed --&gt; Modified: git stash pop\n    Stashed --&gt; Modified: git stash apply\n    Stashed --&gt; [*]: git stash drop</code></pre>"},{"location":"Git/stashing-and-worktree/#inspecting-stashes","title":"Inspecting Stashes","text":"<pre><code># List all stashes with their messages\ngit stash list\n# stash@{0}: On feature/auth: WIP: user profile validation\n# stash@{1}: WIP on main: a1b2c3d Fix login bug\n# stash@{2}: On feature/search: search index prototype\n\n# Show what a stash contains (as a diff)\ngit stash show\n# src/auth.py | 12 ++++++------\n# src/user.py |  8 ++++++++\n\n# Show the full diff\ngit stash show -p\n\n# Show a specific stash\ngit stash show -p stash@{1}\n</code></pre>"},{"location":"Git/stashing-and-worktree/#dropping-stashes","title":"Dropping Stashes","text":"<pre><code># Drop the most recent stash\ngit stash drop\n\n# Drop a specific stash\ngit stash drop stash@{2}\n\n# Clear all stashes\ngit stash clear\n</code></pre> <p>Stashes can be lost</p> <p><code>git stash drop</code> and <code>git stash clear</code> permanently remove stashes. Unlike commits, dropped stashes are difficult to recover (possible only through <code>git fsck --lost-found</code> if you act quickly). Don't use the stash as long-term storage - commit your work or create a branch.</p>"},{"location":"Git/stashing-and-worktree/#creating-a-branch-from-a-stash","title":"Creating a Branch from a Stash","text":"<p>If your stashed changes conflict with work done since stashing, or if you realize the stashed work deserves its own branch:</p> <pre><code># Create a new branch from where you stashed, apply the stash, and drop it\ngit stash branch new-branch-name\n\n# From a specific stash\ngit stash branch new-branch-name stash@{2}\n</code></pre> <p>This creates the branch at the commit where the stash was originally created, applies the stash, and drops it if the apply succeeds.</p> <p>Invalid interactive component configuration (terminal)</p> <p>git stash Command Builder (requires JavaScript)</p>"},{"location":"Git/stashing-and-worktree/#git-worktree","title":"Git Worktree","text":"<p>Stashing works for quick context switches, but has limits. If you need to work on two branches simultaneously - reviewing a pull request while coding a feature, running tests on one branch while developing on another - you need separate working directories. That's what <code>git worktree</code> provides.</p> <p>A worktree is an additional working directory linked to the same repository. Each worktree has its own checked-out branch, its own staging area, and its own working files, but they all share the same <code>.git</code> object database. Changes committed in any worktree are immediately available to all others.</p>"},{"location":"Git/stashing-and-worktree/#creating-a-worktree","title":"Creating a Worktree","text":"<pre><code># Create a worktree for an existing branch\ngit worktree add ../project-hotfix hotfix/urgent-fix\n\n# Create a worktree with a new branch\ngit worktree add -b feature/new-search ../project-search\n\n# Create a worktree at a specific commit (detached HEAD)\ngit worktree add ../project-review a1b2c3d\n</code></pre> <p>The first argument is the directory path for the new worktree. The second is the branch or commit to check out.</p>"},{"location":"Git/stashing-and-worktree/#listing-and-removing-worktrees","title":"Listing and Removing Worktrees","text":"<pre><code># List all worktrees\ngit worktree list\n# /home/user/project         a1b2c3d [main]\n# /home/user/project-hotfix  b2c3d4e [hotfix/urgent-fix]\n# /home/user/project-search  c3d4e5f [feature/new-search]\n\n# Remove a worktree (after you're done with it)\ngit worktree remove ../project-hotfix\n\n# Clean up stale worktree references\ngit worktree prune\n</code></pre>"},{"location":"Git/stashing-and-worktree/#rules-and-constraints","title":"Rules and Constraints","text":"<ul> <li>Each branch can only be checked out in one worktree at a time</li> <li>The main worktree (the original clone) can't be removed with <code>git worktree remove</code></li> <li>All worktrees share the same object database, refs, and config</li> <li><code>git worktree lock</code> prevents a worktree from being pruned (useful for worktrees on removable drives)</li> </ul> <p>Using Worktrees for Parallel Development (requires JavaScript)</p>"},{"location":"Git/stashing-and-worktree/#stash-vs-worktree-when-to-use-which","title":"Stash vs Worktree: When to Use Which","text":"Situation Use Stash Use Worktree Quick context switch (minutes) Yes Overkill Working on two branches simultaneously No Yes Reviewing a PR while developing No Yes Running long tests on one branch while coding on another No Yes Saving work-in-progress before pulling Yes No Need separate build artifacts per branch No Yes One-off \"save and restore\" Yes Overkill <p>When should you use git worktree instead of git stash? (requires JavaScript)</p>"},{"location":"Git/stashing-and-worktree/#git-clean","title":"Git Clean","text":"<p><code>git clean</code> removes untracked files from your working directory. It's useful for resetting to a pristine state - removing build artifacts, generated files, or other clutter that isn't tracked by Git.</p> <pre><code># Dry run - show what would be removed (always do this first)\ngit clean -n\n\n# Remove untracked files\ngit clean -f\n\n# Remove untracked files and directories\ngit clean -fd\n\n# Remove untracked and ignored files (full reset)\ngit clean -fdx\n\n# Interactive mode - choose what to remove\ngit clean -i\n</code></pre> <p>git clean is irreversible</p> <p><code>git clean -f</code> permanently deletes files. There's no undo, no stash, no reflog for untracked files. Always run <code>git clean -n</code> (dry run) first to see what will be removed.</p> <p>The flags:</p> Flag Effect <code>-n</code> Dry run (show what would be removed) <code>-f</code> Force (required for actual deletion) <code>-d</code> Include untracked directories <code>-x</code> Also remove ignored files (build artifacts, etc.) <code>-X</code> Remove only ignored files (keep untracked) <code>-i</code> Interactive mode <p><code>git clean -fdx</code> is the nuclear option - it removes everything not tracked by Git, including files in <code>.gitignore</code>. Useful for getting a completely clean slate before a release build.</p>"},{"location":"Git/stashing-and-worktree/#exercises","title":"Exercises","text":"<p>Interrupt Work with a Stash (requires JavaScript)</p> <p>Parallel Work with Worktrees (requires JavaScript)</p>"},{"location":"Git/stashing-and-worktree/#further-reading","title":"Further Reading","text":"<ul> <li>Pro Git - Chapter 7.3: Stashing and Cleaning - comprehensive stash coverage</li> <li>Official git-stash documentation - complete reference for all stash subcommands</li> <li>Official git-worktree documentation - worktree creation, management, and pruning</li> <li>Official git-clean documentation - cleaning untracked files</li> </ul> <p>Previous: Rewriting History | Next: Configuring Git | Back to Index</p>"},{"location":"Git/three-trees/","title":"The Three Trees: Working Directory, Index, and Repository","text":"<p>Git organizes your work into three distinct areas, commonly called the \"three trees.\" Understanding how changes flow between them is the single most important concept in Git. Every command you run either moves data between these areas, inspects the differences between them, or manipulates what they contain.</p>"},{"location":"Git/three-trees/#the-three-areas","title":"The Three Areas","text":""},{"location":"Git/three-trees/#working-directory-working-tree","title":"Working Directory (Working Tree)","text":"<p>The working directory is the actual directory of files on your filesystem. When you open a file in your editor, you're editing the working copy. This is the only area you interact with directly - the other two are managed by Git.</p> <p>Your working directory contains every tracked file at its current state, plus any new files you've created that Git doesn't know about yet.</p>"},{"location":"Git/three-trees/#staging-area-index","title":"Staging Area (Index)","text":"<p>The staging area - also called the index - is a holding zone between your working directory and the repository. When you run <code>git add</code>, you're copying the current state of a file into the index. The index represents what will go into your next commit.</p> <p>The index is actually a binary file at <code>.git/index</code>. It doesn't store file contents directly - it stores references to blob objects and metadata (file paths, permissions, timestamps). You can think of it as a manifest of what the next commit will look like.</p> <p>Why have a staging area at all? Because it gives you precise control over commits. If you've changed five files but only two changes are related, you can stage just those two and commit them with a focused message. The other three changes stay in your working directory, ready for a separate commit.</p>"},{"location":"Git/three-trees/#repository-committed-history","title":"Repository (Committed History)","text":"<p>The repository is the <code>.git</code> directory at the root of your project. It contains the complete history of every committed snapshot, all branches and tags, configuration, and the object database. When you run <code>git commit</code>, Git takes everything in the staging area, creates a permanent snapshot, and adds it to the repository.</p> <p>Once something is committed, it's safe. Git's content-addressable storage means committed data is checksummed and extremely difficult to lose (even if you try).</p> <p>What are Git's 'three trees' (the three main areas where file changes live)? (requires JavaScript)</p>"},{"location":"Git/three-trees/#the-file-lifecycle","title":"The File Lifecycle","text":"<p>Every file in a Git repository is in one of these states:</p> State Where it lives What it means Untracked Working directory only Git sees the file but isn't tracking it Tracked, unmodified Working directory = repository File matches the last commit, no changes Modified Working directory differs from index You've changed the file but haven't staged the changes Staged Index differs from repository Changes are queued for the next commit Committed Repository Changes are permanently recorded <p>A file can be in multiple states simultaneously. If you modify a file, stage it, then modify it again, the file is both staged (the first set of changes) and modified (the second set). This is normal and useful - it means your next commit captures only the changes you explicitly staged.</p> <pre><code>flowchart LR\n    A[Untracked] --&gt;|git add| B[Staged]\n    B --&gt;|git commit| C[Unmodified]\n    C --&gt;|edit file| D[Modified]\n    D --&gt;|git add| B\n    C --&gt;|git rm| A\n    D --&gt;|git restore| C\n    B --&gt;|git restore --staged| D</code></pre>"},{"location":"Git/three-trees/#creating-a-repository-git-init","title":"Creating a Repository: <code>git init</code>","text":"<p>Every Git project starts with initialization. <code>git init</code> creates the <code>.git</code> directory with all the internal structures Git needs:</p> <pre><code>git init myproject\ncd myproject\n</code></pre> <p>Or initialize in an existing directory:</p> <pre><code>cd existing-project\ngit init\n</code></pre> <p>After <code>git init</code>, your directory looks like this:</p> <pre><code>myproject/\n\u2514\u2500\u2500 .git/\n    \u251c\u2500\u2500 HEAD            # Points to the current branch\n    \u251c\u2500\u2500 config          # Repository-specific configuration\n    \u251c\u2500\u2500 description     # Used by GitWeb (rarely relevant)\n    \u251c\u2500\u2500 hooks/          # Sample hook scripts\n    \u251c\u2500\u2500 info/           # Global exclude patterns\n    \u251c\u2500\u2500 objects/        # All content (blobs, trees, commits, tags)\n    \u2514\u2500\u2500 refs/           # Branch and tag pointers\n        \u251c\u2500\u2500 heads/      # Local branches\n        \u2514\u2500\u2500 tags/       # Tags\n</code></pre> <p>The important pieces: <code>HEAD</code> tells Git what branch you're on. <code>objects/</code> stores all your data. <code>refs/</code> stores branch and tag pointers. The Object Model and Refs, the Reflog, and the DAG guides cover these in depth.</p>"},{"location":"Git/three-trees/#checking-status-git-status","title":"Checking Status: <code>git status</code>","text":"<p><code>git status</code> is the command you'll run most often. It shows you exactly where things stand across all three areas:</p> <pre><code>git status\n</code></pre> <p>The output groups files by state:</p> <ul> <li>Changes to be committed - files in the staging area (ready for commit)</li> <li>Changes not staged for commit - tracked files that have been modified but not staged</li> <li>Untracked files - files Git doesn't know about</li> </ul> <p>For a compact view:</p> <pre><code>git status -s\n</code></pre> <p>This shows two-column status codes. The left column is the staging area status, the right column is the working tree status:</p> Code Meaning <code>??</code> Untracked <code>A</code> New file, staged <code>M</code> Modified, staged <code>M</code> Modified, not staged <code>MM</code> Modified, staged, then modified again <code>D</code> Deleted, staged <code>D</code> Deleted, not staged"},{"location":"Git/three-trees/#staging-changes-git-add","title":"Staging Changes: <code>git add</code>","text":"<p><code>git add</code> copies the current state of files from the working directory into the staging area:</p> <pre><code># Stage a specific file\ngit add README.md\n\n# Stage multiple files\ngit add file1.txt file2.txt\n\n# Stage all changes in a directory\ngit add src/\n\n# Stage all changes in the entire working tree\ngit add .\n\n# Stage parts of a file interactively\ngit add -p README.md\n</code></pre> <p><code>git add -p</code> (patch mode) is particularly powerful. It shows you each change in a file and lets you choose which ones to stage. This means you can make several unrelated edits to one file and commit them separately.</p> <p>Stage with intention</p> <p>Get in the habit of staging specific files rather than using <code>git add .</code> for everything. This forces you to review what you're about to commit and produces cleaner, more focused commits.</p>"},{"location":"Git/three-trees/#unstaging-and-restoring-git-restore","title":"Unstaging and Restoring: <code>git restore</code>","text":"<p>Made a mistake? <code>git restore</code> (introduced in Git 2.23) provides clear commands for undoing changes:</p> <pre><code># Discard changes in working directory (restore from index)\ngit restore README.md\n\n# Unstage a file (restore the index from HEAD, keep working changes)\ngit restore --staged README.md\n\n# Restore a file from a specific commit\ngit restore --source=HEAD~2 README.md\n</code></pre> <p>git restore discards changes</p> <p><code>git restore README.md</code> (without <code>--staged</code>) permanently discards your uncommitted working directory changes to that file. There is no undo. If you haven't committed or stashed those changes, they are gone. Use <code>git stash</code> first if you're unsure.</p> <p>Before Git 2.23, the same operations used <code>git checkout</code> and <code>git reset</code>:</p> Modern command Legacy equivalent <code>git restore file</code> <code>git checkout -- file</code> <code>git restore --staged file</code> <code>git reset HEAD file</code> <p>The modern <code>restore</code> commands are clearer about what they do, but you'll see the legacy forms in older documentation and tutorials.</p>"},{"location":"Git/three-trees/#removing-files-git-rm","title":"Removing Files: <code>git rm</code>","text":"<p>To remove a tracked file from both the working directory and the staging area:</p> <pre><code>git rm old-file.txt\n</code></pre> <p>This deletes the file from disk and stages the deletion. Your next commit will record the removal.</p> <p>To stop tracking a file but keep it on disk (useful for files that should have been in <code>.gitignore</code>):</p> <pre><code>git rm --cached config.local\n</code></pre> <p>The file stays in your working directory but Git stops tracking it. Add it to <code>.gitignore</code> to prevent accidentally re-adding it.</p>"},{"location":"Git/three-trees/#the-init-to-commit-workflow","title":"The Init-to-Commit Workflow","text":"<p>Here's the complete workflow from creating a repository through your first commit, with each step showing the state of the three trees:</p> <p>Git Init to Commit Workflow (requires JavaScript)</p>"},{"location":"Git/three-trees/#head-your-current-position","title":"HEAD: Your Current Position","text":"<p>HEAD is a special reference that points to whatever you currently have checked out - usually a branch, which in turn points to a commit. Think of HEAD as \"you are here\" on the commit graph.</p> <p>When HEAD points to a branch name (the normal case), it's a symbolic reference:</p> <pre><code>HEAD \u2192 main \u2192 e4f5g6h\n</code></pre> <p>When you make a new commit, the branch pointer moves forward and HEAD follows it.</p> <p>What does HEAD refer to in Git? (requires JavaScript)</p>"},{"location":"Git/three-trees/#detached-head","title":"Detached HEAD","text":"<p>If you check out a specific commit (by hash, tag, or remote branch) rather than a branch name, HEAD points directly to that commit. This is called detached HEAD state:</p> <pre><code>git checkout a1b2c3d    # Detached HEAD\ngit checkout v1.0       # Also detached HEAD (tag, not branch)\n</code></pre> <p>In detached HEAD, you can look around and even make commits, but those commits aren't on any branch. If you switch away without creating a branch, those commits become unreachable and will eventually be garbage collected.</p> <p>What happens when you're in a 'detached HEAD' state? (requires JavaScript)</p> <p>To get out of detached HEAD and keep any commits you made:</p> <pre><code>git branch my-new-branch    # Create a branch at the current commit\ngit switch my-new-branch     # Switch to it (HEAD is now attached)\n</code></pre>"},{"location":"Git/three-trees/#the-gitignore-file","title":"The <code>.gitignore</code> File","text":"<p>Not every file in your working directory should be tracked. Build artifacts, dependency directories, editor configs, OS files, and secrets should be excluded. The <code>.gitignore</code> file tells Git which files to ignore.</p> <p>Create <code>.gitignore</code> in your repository root:</p> <pre><code># Compiled output\n*.o\n*.pyc\n__pycache__/\n\n# Dependencies\nnode_modules/\nvendor/\n\n# Build directories\ndist/\nbuild/\n*.egg-info/\n\n# IDE and editor files\n.idea/\n.vscode/\n*.swp\n*~\n\n# OS files\n.DS_Store\nThumbs.db\n\n# Environment and secrets\n.env\n.env.local\n*.key\n*.pem\n</code></pre>"},{"location":"Git/three-trees/#pattern-syntax","title":"Pattern Syntax","text":"Pattern Matches Example <code>*.log</code> All <code>.log</code> files in any directory <code>error.log</code>, <code>src/debug.log</code> <code>/build</code> <code>build</code> directory in the repo root only <code>build/</code>, but not <code>src/build/</code> <code>build/</code> Any directory named <code>build</code> <code>build/</code>, <code>src/build/</code> <code>doc/*.txt</code> <code>.txt</code> files in <code>doc/</code> (not subdirs) <code>doc/notes.txt</code>, but not <code>doc/sub/notes.txt</code> <code>doc/**/*.txt</code> <code>.txt</code> files anywhere under <code>doc/</code> <code>doc/notes.txt</code>, <code>doc/sub/notes.txt</code> <code>!important.log</code> Exception - track this file even if <code>*.log</code> matches Negation must come after the pattern it overrides <code>#</code> Comment line Ignored by Git <p>Global gitignore</p> <p>For OS-specific files (<code>.DS_Store</code>, <code>Thumbs.db</code>) and editor files (<code>.idea/</code>, <code>*.swp</code>) that apply to all your projects, use a global gitignore instead of adding them to every repository:</p> <pre><code>git config --global core.excludesFile ~/.gitignore_global\n</code></pre> <p>Then put your personal patterns in <code>~/.gitignore_global</code>. This keeps each repository's <code>.gitignore</code> focused on project-specific patterns.</p>"},{"location":"Git/three-trees/#checking-whats-ignored","title":"Checking What's Ignored","text":"<p>To verify your <code>.gitignore</code> patterns work:</p> <pre><code># Check if a specific file would be ignored\ngit check-ignore -v debug.log\n\n# List all ignored files\ngit status --ignored\n</code></pre> <p>Building and Testing a .gitignore (requires JavaScript)</p> <p>.gitignore Pattern Matching (requires JavaScript)</p>"},{"location":"Git/three-trees/#putting-it-all-together","title":"Putting It All Together","text":"<p>File Lifecycle Practice (requires JavaScript)</p>"},{"location":"Git/three-trees/#further-reading","title":"Further Reading","text":"<ul> <li>Pro Git - Chapter 2: Git Basics - covers init, clone, status, add, commit, rm, and gitignore</li> <li>Official git-status documentation - complete reference for status output formats</li> <li>Official gitignore documentation - full pattern syntax specification</li> <li>github/gitignore - a collection of <code>.gitignore</code> templates for many languages and frameworks</li> </ul> <p>Previous: Introduction: Why Git, and Why Version Control | Next: Commits and History | Back to Index</p>"},{"location":"Git/transfer-protocols/","title":"Transfer Protocols and Plumbing","text":"<p>When you run <code>git fetch</code> or <code>git push</code>, Git negotiates with a remote server to figure out which objects need to be transferred, packages them efficiently, and sends them over the wire. This guide covers how that transfer works at the protocol level, the different transport mechanisms, and clone strategies for large repositories.</p>"},{"location":"Git/transfer-protocols/#how-transfers-work","title":"How Transfers Work","text":"<p>Every transfer between Git repositories follows the same basic pattern:</p> <ol> <li>Discovery - the client and server exchange lists of refs (branches, tags) and their commit hashes</li> <li>Negotiation - they compare what each side has to determine the minimal set of objects to transfer</li> <li>Transfer - the sender packages the needed objects into a packfile and transmits it</li> <li>Update - the receiver integrates the objects and updates its refs</li> </ol> <p>The two processes involved are <code>git-upload-pack</code> (runs on the server during fetch) and <code>git-receive-pack</code> (runs on the server during push).</p>"},{"location":"Git/transfer-protocols/#transport-protocols","title":"Transport Protocols","text":""},{"location":"Git/transfer-protocols/#ssh-transport","title":"SSH Transport","text":"<p>The most common protocol for authenticated access. Git connects via SSH and runs <code>git-upload-pack</code> or <code>git-receive-pack</code> on the server:</p> <pre><code>git@github.com:user/repo.git\nssh://git@github.com/user/repo.git\n</code></pre> <p>SSH handles authentication (key-based or password) and encryption. Git just pipes data through the SSH channel.</p>"},{"location":"Git/transfer-protocols/#smart-http","title":"Smart HTTP","text":"<p>The standard for HTTPS access. The server runs a CGI program (or equivalent) that speaks Git's pack protocol over HTTP POST/GET:</p> <pre><code>https://github.com/user/repo.git\n</code></pre> <p>Smart HTTP supports both read and write, authentication via HTTP headers (tokens, Basic auth), and works through proxies and firewalls. Most hosting platforms default to this.</p>"},{"location":"Git/transfer-protocols/#dumb-http","title":"Dumb HTTP","text":"<p>An older, read-only protocol where Git downloads objects individually over plain HTTP. It doesn't require any special server-side software - just a web server serving static files. Rarely used today because it's much slower (no pack negotiation).</p>"},{"location":"Git/transfer-protocols/#native-git-protocol","title":"Native Git Protocol","text":"<pre><code>git://github.com/user/repo.git\n</code></pre> <p>The <code>git://</code> protocol is unauthenticated and unencrypted. It was used for fast, read-only access to public repositories. Most platforms no longer support it due to security concerns.</p>"},{"location":"Git/transfer-protocols/#pack-negotiation","title":"Pack Negotiation","text":"<p>The most interesting part of the transfer is pack negotiation - how the client and server figure out which objects to send. This is what makes <code>git fetch</code> fast even for large repositories.</p>"},{"location":"Git/transfer-protocols/#the-wanthave-exchange","title":"The Want/Have Exchange","text":"<p>During a fetch:</p> <ol> <li>The server sends a list of all its refs and their hashes</li> <li>The client identifies which commits it wants (remote refs it doesn't have)</li> <li>The client sends have lines - commits it already has</li> <li>The server uses the have list to find the common ancestor - the newest commit both sides share</li> <li>The server sends a packfile containing all objects reachable from the wanted commits but not from the common ancestors</li> </ol> <pre><code>CLIENT                          SERVER\n                                refs: main=abc123, feature=def456\nwant abc123\nwant def456\nhave 789fed\nhave 456abc\n                                ACK 456abc (common ancestor found)\n                                &lt;sends packfile&gt;\ndone\n</code></pre> <p>The negotiation is optimized with multi-ack: the server can acknowledge multiple common ancestors, allowing it to send a more precisely targeted packfile.</p>"},{"location":"Git/transfer-protocols/#protocol-v2","title":"Protocol v2","text":"<p>Git protocol v2 (default since Git 2.26) improves on v1 with:</p> <ul> <li>Ref filtering - the server only sends refs the client asks about, not the entire ref list (huge improvement for repos with thousands of branches)</li> <li>Server capabilities - structured capability negotiation</li> <li>Stateless mode - better for HTTP-based transports</li> </ul> <pre><code># Force protocol v2\ngit config --global protocol.version 2\n\n# See protocol exchange\nGIT_TRACE=1 GIT_TRACE_PACKET=1 git fetch origin 2&gt;&amp;1 | head -40\n</code></pre> <p>What happens during pack negotiation? (requires JavaScript)</p>"},{"location":"Git/transfer-protocols/#watching-the-protocol","title":"Watching the Protocol","text":"<p>You can observe Git's transfer protocol in action using trace environment variables:</p> <pre><code># General trace output\nGIT_TRACE=1 git fetch origin\n\n# Packet-level protocol exchange\nGIT_TRACE_PACKET=1 git fetch origin\n\n# HTTP request/response details\nGIT_CURL_VERBOSE=1 git fetch origin\n\n# Performance timing for each operation\nGIT_TRACE_PERFORMANCE=1 git fetch origin\n\n# Combine multiple traces\nGIT_TRACE=1 GIT_TRACE_PACKET=1 git fetch origin 2&gt;&amp;1 | less\n</code></pre> <p>Watching Protocol Exchange with GIT_TRACE (requires JavaScript)</p>"},{"location":"Git/transfer-protocols/#clone-strategies-for-large-repositories","title":"Clone Strategies for Large Repositories","text":"<p>Not every clone needs the entire history. Git provides several strategies for faster clones of large repositories.</p>"},{"location":"Git/transfer-protocols/#shallow-clone","title":"Shallow Clone","text":"<p>Downloads only recent history, not the full commit chain:</p> <pre><code># Only the most recent commit\ngit clone --depth 1 https://github.com/user/large-repo.git\n\n# Last 10 commits\ngit clone --depth 10 https://github.com/user/large-repo.git\n\n# Deepen a shallow clone later\ngit fetch --deepen=50\n\n# Convert shallow to full clone\ngit fetch --unshallow\n</code></pre> <p>Shallow clones are fast but limited. Some operations (like <code>git log</code> and <code>git blame</code>) only show the shallow history. You can't push from a shallow clone in some configurations.</p>"},{"location":"Git/transfer-protocols/#partial-clone","title":"Partial Clone","text":"<p>Downloads commits and trees but skips large blobs until you actually need them (Git 2.22+):</p> <pre><code># Skip all blobs (download on demand when you checkout)\ngit clone --filter=blob:none https://github.com/user/large-repo.git\n\n# Skip blobs larger than 1MB\ngit clone --filter=blob:limit=1m https://github.com/user/large-repo.git\n\n# Skip all trees (extremely minimal, mainly for CI)\ngit clone --filter=tree:0 https://github.com/user/large-repo.git\n</code></pre> <p>Partial clones maintain full history (all commits) but defer downloading file content until checkout or diff. Git fetches missing blobs transparently when needed. This is ideal for large repositories where you don't need every file's content upfront.</p>"},{"location":"Git/transfer-protocols/#single-branch-clone","title":"Single-Branch Clone","text":"<p>Only downloads one branch:</p> <pre><code>git clone --single-branch https://github.com/user/repo.git\ngit clone --single-branch --branch develop https://github.com/user/repo.git\n</code></pre>"},{"location":"Git/transfer-protocols/#sparse-checkout","title":"Sparse Checkout","text":"<p>After cloning, check out only a subset of files:</p> <pre><code>git clone --filter=blob:none --sparse https://github.com/user/monorepo.git\ncd monorepo\ngit sparse-checkout set src/my-service tests/my-service\n</code></pre> <p>This combination (partial clone + sparse checkout) is the fastest way to work with a large monorepo when you only need a few directories.</p> <p>What is the difference between a shallow clone and a partial clone? (requires JavaScript)</p> <p>Clone Strategies for Large Repositories (requires JavaScript)</p>"},{"location":"Git/transfer-protocols/#git-bundle","title":"Git Bundle","text":"<p><code>git bundle</code> creates a file containing Git objects and refs - a portable repository snapshot that can be transferred offline (USB drive, email, sneakernet). Useful when network access to a remote isn't available.</p>"},{"location":"Git/transfer-protocols/#creating-a-bundle","title":"Creating a Bundle","text":"<pre><code># Bundle the entire repository\ngit bundle create repo.bundle --all\n\n# Bundle a specific branch\ngit bundle create feature.bundle main\n\n# Bundle only new commits since a known point\ngit bundle create update.bundle main ^v1.0\n</code></pre>"},{"location":"Git/transfer-protocols/#using-a-bundle","title":"Using a Bundle","text":"<pre><code># Verify a bundle file\ngit bundle verify repo.bundle\n\n# Clone from a bundle\ngit clone repo.bundle my-repo\n\n# Fetch from a bundle into an existing repo\ngit fetch repo.bundle main:refs/remotes/bundle/main\n</code></pre> <p>Creating and Restoring a Git Bundle (requires JavaScript)</p>"},{"location":"Git/transfer-protocols/#git-archive-export-without-git","title":"<code>git archive</code> - Export Without <code>.git</code>","text":"<p><code>git archive</code> creates a tar or zip archive of a tree without the <code>.git</code> directory - useful for creating release tarballs:</p> <pre><code># Create a tar.gz of the current HEAD\ngit archive --format=tar.gz --prefix=project-v1.0/ HEAD &gt; project-v1.0.tar.gz\n\n# Create a zip of a specific tag\ngit archive --format=zip v1.0 &gt; project-v1.0.zip\n\n# Archive a specific directory\ngit archive HEAD src/ &gt; src-only.tar\n</code></pre> <p>The <code>--prefix</code> option adds a directory prefix so the archive extracts into a named directory rather than the current directory.</p>"},{"location":"Git/transfer-protocols/#exercise","title":"Exercise","text":"<p>Bundle and Unbundle a Repository (requires JavaScript)</p>"},{"location":"Git/transfer-protocols/#further-reading","title":"Further Reading","text":"<ul> <li>Pro Git - Chapter 10.6: Transfer Protocols - how fetch and push work at the protocol level</li> <li>Git Protocol v2 Documentation - improvements over protocol v1</li> <li>Official git-bundle documentation - offline transfers</li> <li>Official git-archive documentation - creating release tarballs</li> <li>Partial Clone Documentation - filtering objects during clone and fetch</li> </ul> <p>Previous: Refs, the Reflog, and the DAG | Next: Collaboration Workflows | Back to Index</p>"},{"location":"Git/troubleshooting-and-recovery/","title":"Troubleshooting and Recovery","text":"<p>Things go wrong with Git. Commits end up on the wrong branch, force pushes destroy history, secrets get committed, merge conflicts spiral out of control, and sometimes the repository itself gets corrupted. This guide is your recovery playbook - a reference for undoing every common mistake and diagnosing problems when Git behaves unexpectedly.</p>"},{"location":"Git/troubleshooting-and-recovery/#the-recovery-decision-tree","title":"The Recovery Decision Tree","text":"<p>Before reaching for a specific command, identify your situation:</p> What happened Recovery Bad commit message <code>git commit --amend</code> (if unpushed) Committed to wrong branch <code>git cherry-pick</code> + <code>git reset</code> Want to undo last commit (keep changes) <code>git reset --soft HEAD~1</code> Want to undo last commit (discard changes) <code>git reset --hard HEAD~1</code> Accidentally deleted a branch <code>git reflog</code> + <code>git branch</code> Bad merge <code>git revert -m 1 &lt;merge-commit&gt;</code> (shared) or <code>git reset</code> (local) Committed a secret Revoke secret, then <code>git filter-repo</code> Pushed something bad to shared branch <code>git revert</code> (never force push shared branches) Lost commits after rebase <code>git reflog</code> + <code>git reset</code> Repository corruption <code>git fsck</code> + <code>git gc</code> <p>Which recovery method should you use for each mistake? (requires JavaScript)</p>"},{"location":"Git/troubleshooting-and-recovery/#recovering-lost-commits","title":"Recovering Lost Commits","text":""},{"location":"Git/troubleshooting-and-recovery/#from-the-reflog","title":"From the Reflog","text":"<p>The reflog records every HEAD movement. Even after <code>git reset --hard</code>, the old commits exist in the object database and appear in the reflog:</p> <pre><code># See recent HEAD history\ngit reflog\n\n# Find the commit you want\ngit reflog | grep \"commit: Add user auth\"\n\n# Recover by resetting to that point\ngit reset --hard HEAD@{3}\n\n# Or create a branch at that point (safer)\ngit branch recovery HEAD@{3}\n</code></pre>"},{"location":"Git/troubleshooting-and-recovery/#from-git-fsck","title":"From <code>git fsck</code>","text":"<p>If reflog entries have expired (after 30+ days), use <code>git fsck</code>:</p> <pre><code># Find unreachable (dangling) commits\ngit fsck --unreachable | grep commit\n\n# Examine a dangling commit\ngit show &lt;commit-hash&gt;\n\n# Recover it\ngit branch recovery &lt;commit-hash&gt;\n\n# Or use --lost-found to dump all recoverable objects\ngit fsck --lost-found\nls .git/lost-found/commit/\n</code></pre> <p>Recovering a Deleted Branch from the Reflog (requires JavaScript)</p>"},{"location":"Git/troubleshooting-and-recovery/#undoing-common-mistakes","title":"Undoing Common Mistakes","text":""},{"location":"Git/troubleshooting-and-recovery/#wrong-commit-message","title":"Wrong Commit Message","text":"<pre><code># Fix the last commit message (unpushed only)\ngit commit --amend -m \"Corrected message\"\n</code></pre>"},{"location":"Git/troubleshooting-and-recovery/#committed-to-the-wrong-branch","title":"Committed to the Wrong Branch","text":"<pre><code># You committed to main instead of feature/auth\n# Step 1: Note the commit hash\ngit log --oneline -1\n# a1b2c3d Accidental commit on main\n\n# Step 2: Move the commit to the right branch\ngit switch feature/auth\ngit cherry-pick a1b2c3d\n\n# Step 3: Remove from main\ngit switch main\ngit reset --hard HEAD~1\n</code></pre>"},{"location":"Git/troubleshooting-and-recovery/#undo-the-last-commit-keep-changes","title":"Undo the Last Commit (Keep Changes)","text":"<pre><code># Soft reset: uncommit but keep changes staged\ngit reset --soft HEAD~1\n\n# Mixed reset: uncommit and unstage, changes in working dir\ngit reset HEAD~1\n</code></pre>"},{"location":"Git/troubleshooting-and-recovery/#accidental-git-reset-hard","title":"Accidental <code>git reset --hard</code>","text":"<pre><code># Find where HEAD was before the reset\ngit reflog\n# a1b2c3d HEAD@{1}: commit: Important work\n\n# Recover\ngit reset --hard HEAD@{1}\n</code></pre>"},{"location":"Git/troubleshooting-and-recovery/#bad-merge","title":"Bad Merge","text":"<pre><code># On a shared branch: create a revert commit\ngit revert -m 1 &lt;merge-commit-hash&gt;\n# -m 1 keeps the mainline parent, reverting the merged branch's changes\n\n# On a local branch: reset before the merge\ngit reset --hard HEAD~1\n</code></pre>"},{"location":"Git/troubleshooting-and-recovery/#accidental-git-add-staged-something-wrong","title":"Accidental <code>git add</code> (Staged Something Wrong)","text":"<pre><code># Unstage a specific file\ngit restore --staged secret.env\n\n# Unstage everything\ngit restore --staged .\n</code></pre>"},{"location":"Git/troubleshooting-and-recovery/#git-lfs-large-file-storage","title":"Git LFS (Large File Storage)","text":"<p>Git isn't designed for large binary files. Each version of a large file is stored as a full blob, and the repository size grows linearly with each change. Git LFS solves this by replacing large files with lightweight pointers in the repository and storing the actual file content on a separate server.</p>"},{"location":"Git/troubleshooting-and-recovery/#setup","title":"Setup","text":"<pre><code># Install Git LFS\nbrew install git-lfs    # macOS\nsudo apt install git-lfs  # Debian/Ubuntu\n\n# Initialize LFS in a repository\ngit lfs install\n\n# Track file patterns\ngit lfs track \"*.psd\"\ngit lfs track \"*.zip\"\ngit lfs track \"assets/videos/**\"\n\n# Check what's tracked\ngit lfs track\ncat .gitattributes\n</code></pre>"},{"location":"Git/troubleshooting-and-recovery/#how-it-works","title":"How It Works","text":"<p>When you <code>git add</code> a tracked file, LFS replaces it with a pointer file:</p> <pre><code>version https://git-lfs.github.com/spec/v1\noid sha256:4d7a214614ab2935c943f9e0ff69d22eadbb8f32b1258daaa5e2ca24d17e2393\nsize 12345678\n</code></pre> <p>The actual file content is uploaded to the LFS server during <code>git push</code> and downloaded during <code>git pull</code> or <code>git checkout</code>.</p> <p>What does Git LFS store in the Git repository vs on the LFS server? (requires JavaScript)</p>"},{"location":"Git/troubleshooting-and-recovery/#migrating-existing-files-to-lfs","title":"Migrating Existing Files to LFS","text":"<pre><code># Migrate existing large files in history\ngit lfs migrate import --include=\"*.psd,*.zip\" --everything\n\n# Check which files are taking the most space\ngit lfs migrate info\n</code></pre> <p>LFS migration rewrites history</p> <p><code>git lfs migrate import</code> rewrites commit history to replace files with LFS pointers. This requires force pushing and coordination with your team. All collaborators must re-clone.</p> <p>Setting Up Git LFS (requires JavaScript)</p>"},{"location":"Git/troubleshooting-and-recovery/#history-rewriting-at-scale","title":"History Rewriting at Scale","text":""},{"location":"Git/troubleshooting-and-recovery/#git-filter-repo","title":"<code>git filter-repo</code>","text":"<p>git filter-repo is the recommended tool for large-scale history rewriting. It's faster and safer than the older <code>git filter-branch</code>:</p> <pre><code># Remove a directory from all history\ngit filter-repo --path old-vendor/ --invert-paths\n\n# Remove files by pattern\ngit filter-repo --path-glob '*.log' --invert-paths\n\n# Change author email across all history\ngit filter-repo --email-callback 'return email.replace(b\"old@email.com\", b\"new@email.com\")'\n\n# Reduce to only a subdirectory (extract a project from a monorepo)\ngit filter-repo --subdirectory-filter services/auth\n</code></pre>"},{"location":"Git/troubleshooting-and-recovery/#bfg-repo-cleaner","title":"BFG Repo-Cleaner","text":"<p>BFG is simpler for common tasks:</p> <pre><code># Remove all files larger than 100MB from history\nbfg --strip-blobs-bigger-than 100M\n\n# Remove specific files\nbfg --delete-files passwords.txt\n\n# Replace sensitive strings\nbfg --replace-text replacements.txt\n</code></pre> <p>After any history rewriting:</p> <pre><code>git reflog expire --expire=now --all\ngit gc --prune=now --aggressive\ngit push --force --all\ngit push --force --tags\n</code></pre>"},{"location":"Git/troubleshooting-and-recovery/#performance-diagnosis","title":"Performance Diagnosis","text":"<p>When Git feels slow, these tools help identify the bottleneck:</p>"},{"location":"Git/troubleshooting-and-recovery/#trace-environment-variables","title":"Trace Environment Variables","text":"<pre><code># General trace (shows internal commands)\nGIT_TRACE=1 git status\n\n# Performance timing per operation\nGIT_TRACE_PERFORMANCE=1 git status\n\n# Pack protocol traces (for fetch/push)\nGIT_TRACE_PACKET=1 git fetch\n\n# HTTP request details\nGIT_CURL_VERBOSE=1 git fetch\n</code></pre> <p>Diagnosing Performance with GIT_TRACE_PERFORMANCE (requires JavaScript)</p>"},{"location":"Git/troubleshooting-and-recovery/#common-performance-fixes","title":"Common Performance Fixes","text":"Symptom Likely cause Fix <code>git status</code> slow Large working directory Enable <code>core.fsmonitor</code> <code>git log</code> slow Large history without commit graph <code>git commit-graph write</code> <code>git clone</code> slow Large repository Partial clone + sparse checkout <code>git push/fetch</code> slow Many packfiles <code>git repack -a -d</code> <code>git gc</code> slow Too many loose objects Already running GC, just wait"},{"location":"Git/troubleshooting-and-recovery/#common-error-messages-decoded","title":"Common Error Messages Decoded","text":"Error Meaning Fix <code>fatal: not a git repository</code> No <code>.git</code> directory in this or parent directories <code>cd</code> to a repo or <code>git init</code> <code>error: failed to push some refs</code> Remote has commits you don't have <code>git pull</code> then <code>git push</code> <code>CONFLICT (content): Merge conflict</code> Both branches changed the same lines Edit files, remove markers, <code>git add</code>, <code>git commit</code> <code>detached HEAD</code> HEAD points to a commit, not a branch <code>git switch main</code> or <code>git switch -c new-branch</code> <code>fatal: refusing to merge unrelated histories</code> Two repos with no common ancestor <code>git pull --allow-unrelated-histories</code> <code>error: Your local changes would be overwritten</code> Uncommitted changes conflict with the operation <code>git stash</code> then retry <code>warning: LF will be replaced by CRLF</code> Line ending conversion happening Check <code>core.autocrlf</code> and <code>.gitattributes</code> <code>fatal: bad object HEAD</code> Corrupted HEAD reference Check <code>.git/HEAD</code>, may need to rewrite it"},{"location":"Git/troubleshooting-and-recovery/#repository-corruption-and-repair","title":"Repository Corruption and Repair","text":"<p>Corruption is rare but can happen from disk failures, interrupted operations, or filesystem bugs.</p>"},{"location":"Git/troubleshooting-and-recovery/#detecting-corruption","title":"Detecting Corruption","text":"<pre><code># Full integrity check\ngit fsck --full\n\n# Common output:\n# broken link from commit abc123 to tree def456\n# dangling commit 789abc\n# missing blob fed321\n</code></pre>"},{"location":"Git/troubleshooting-and-recovery/#repair-strategies","title":"Repair Strategies","text":"<ol> <li> <p>Missing objects: If you have a remote, fetch the missing objects:    <pre><code>git fetch origin\n</code></pre></p> </li> <li> <p>Corrupted index: Delete and rebuild:    <pre><code>rm .git/index\ngit reset\n</code></pre></p> </li> <li> <p>Corrupted HEAD: Manually fix the reference:    <pre><code>echo \"ref: refs/heads/main\" &gt; .git/HEAD\n</code></pre></p> </li> <li> <p>Corrupted packfile: Remove and re-fetch:    <pre><code>mv .git/objects/pack/pack-*.pack /tmp/backup/\nmv .git/objects/pack/pack-*.idx /tmp/backup/\ngit fetch origin\n</code></pre></p> </li> <li> <p>Last resort: Clone fresh and copy local branches:    <pre><code>git clone origin-url fresh-clone\ncd fresh-clone\ngit remote add broken ../broken-repo\ngit fetch broken    # Salvage what you can\n</code></pre></p> </li> </ol>"},{"location":"Git/troubleshooting-and-recovery/#exercises","title":"Exercises","text":"<p>Create Disasters and Recover (requires JavaScript)</p> <p>Migrate a Repository to Git LFS (requires JavaScript)</p>"},{"location":"Git/troubleshooting-and-recovery/#further-reading","title":"Further Reading","text":"<ul> <li>Pro Git - Chapter 7: Git Tools - stashing, searching, rewriting, debugging</li> <li>Git LFS Documentation - large file storage setup and usage</li> <li>git-filter-repo Documentation - history rewriting tool</li> <li>BFG Repo-Cleaner Documentation - simple history cleaning</li> <li>Official git-fsck documentation - repository integrity verification</li> </ul> <p>Previous: Monorepos and Scaling Git | Back to Index</p>"},{"location":"Linux%20Essentials/","title":"Linux CLI Essentials","text":"<p>A comprehensive guide to working effectively on the Linux command line. These guides take you from \"I can type commands\" to understanding how the shell actually works and using it productively.</p> <p>Each topic is covered in its own guide. Start anywhere - they're self-contained, but the order below follows a natural learning path.</p>"},{"location":"Linux%20Essentials/#guides","title":"Guides","text":""},{"location":"Linux%20Essentials/#shell-basics","title":"Shell Basics","text":"<p>What the shell is, how it starts up, and how it processes your input. Covers shell types, configuration files, the <code>PATH</code> variable, variables, quoting rules, and the full set of shell expansions (brace, tilde, parameter, arithmetic, command substitution, and globbing).</p>"},{"location":"Linux%20Essentials/#streams-and-redirection","title":"Streams and Redirection","text":"<p>How programs communicate through STDIN, STDOUT, and STDERR. Covers basic redirection, appending, <code>/dev/null</code>, here documents, here strings, file descriptor manipulation, pipelines (exit status, <code>PIPESTATUS</code>, named pipes), <code>tee</code>, subshells, and process substitution.</p>"},{"location":"Linux%20Essentials/#text-processing","title":"Text Processing","text":"<p>The core toolkit for searching, transforming, and analyzing text. Covers <code>grep</code> (with regular expressions - basic, extended, and POSIX classes), <code>sed</code> (substitution, addresses, in-place editing), <code>awk</code> (fields, patterns, BEGIN/END, variables), <code>cut</code>, <code>sort</code>, <code>uniq</code>, <code>tr</code>, <code>wc</code>, <code>head</code>, <code>tail</code>, and <code>tee</code>.</p>"},{"location":"Linux%20Essentials/#finding-files","title":"Finding Files","text":"<p>Searching directory trees and operating on the results. Covers <code>find</code> (name, type, size, time, permission, and owner tests, depth control, logical operators, and actions) and <code>xargs</code> (null-delimited input, placeholder substitution, parallel execution).</p>"},{"location":"Linux%20Essentials/#file-permissions","title":"File Permissions","text":"<p>The Linux permission model explained. Covers reading <code>ls -l</code> output, <code>chmod</code> (symbolic and octal modes), <code>chown</code>, <code>chgrp</code>, <code>umask</code>, and special permission bits (setuid, setgid, sticky bit) with real-world examples.</p>"},{"location":"Linux%20Essentials/#job-control","title":"Job Control","text":"<p>Managing processes from the terminal. Covers foreground/background processes, <code>Ctrl-Z</code>/<code>bg</code>/<code>fg</code>/<code>jobs</code>, signals (<code>SIGTERM</code>, <code>SIGKILL</code>, etc.), <code>kill</code>/<code>killall</code>/<code>pkill</code>, <code>nohup</code>, <code>disown</code>, <code>ps</code>, <code>top</code>/<code>htop</code>, and terminal multiplexers (<code>screen</code> and <code>tmux</code>).</p>"},{"location":"Linux%20Essentials/#scripting-fundamentals","title":"Scripting Fundamentals","text":"<p>Writing reliable bash scripts. Covers exit codes, conditionals (<code>test</code>/<code>[ ]</code>/<code>[[ ]]</code> and their differences), <code>if</code>/<code>elif</code>/<code>else</code>, <code>case</code>, <code>for</code>/<code>while</code>/<code>until</code> loops, functions (arguments, return values, local variables), and error handling (<code>set -euo pipefail</code>, <code>trap</code>).</p>"},{"location":"Linux%20Essentials/#disk-and-filesystem","title":"Disk and Filesystem","text":"<p>Managing storage. Covers <code>df</code> (filesystem usage), <code>du</code> (directory sizes), <code>mount</code>/<code>umount</code>, <code>/etc/fstab</code>, <code>lsblk</code>, partition management (<code>fdisk</code>/<code>parted</code>), <code>mkfs</code>, and <code>fsck</code>.</p>"},{"location":"Linux%20Essentials/#networking","title":"Networking","text":"<p>Essential networking from the command line. Covers <code>ping</code>/<code>traceroute</code>/<code>mtr</code>, <code>curl</code>/<code>wget</code>, <code>ssh</code> (keys, config, port forwarding, jump hosts), <code>scp</code>/<code>rsync</code>, <code>ss</code>/<code>ip</code>, <code>dig</code>/<code>nslookup</code>, and <code>nc</code> (netcat).</p>"},{"location":"Linux%20Essentials/#system-information","title":"System Information","text":"<p>Understanding what's running on a system. Covers <code>uname</code>, <code>uptime</code> (and how to interpret load averages), <code>free</code> (and what buff/cache means), <code>lscpu</code>, <code>lsof</code>, <code>vmstat</code>, the <code>/proc</code> and <code>/sys</code> virtual filesystems, and <code>dmesg</code>.</p>"},{"location":"Linux%20Essentials/#archiving-and-compression","title":"Archiving and Compression","text":"<p>Bundling and compressing files. Covers <code>tar</code> (with gzip, bzip2, and xz), standalone <code>gzip</code>/<code>bzip2</code>/<code>xz</code>, <code>zip</code>/<code>unzip</code>, and guidance on when to use each format.</p>"},{"location":"Linux%20Essentials/#best-practices","title":"Best Practices","text":"<p>Conventions that prevent real bugs. Covers <code>set -euo pipefail</code>, quoting variables, <code>[[ ]]</code> vs <code>[ ]</code>, <code>$()</code> vs backticks, <code>mktemp</code>, <code>shellcheck</code>, avoiding <code>ls</code> parsing, using arrays, portability considerations, and a script template.</p>"},{"location":"Linux%20Essentials/archiving-and-compression/","title":"Archiving and Compression","text":"<p>Archiving and compression are two different operations that are often combined. Archiving bundles multiple files and directories into a single file, preserving directory structure, permissions, timestamps, and ownership - but without reducing size. Compression reduces file size by encoding redundant data more efficiently, but operates on a single file. On Linux, these are usually separate steps (unlike zip, which does both at once): <code>tar</code> creates the archive, then a compression tool like <code>gzip</code> or <code>xz</code> shrinks it. The <code>tar</code> command can invoke compression tools in a single command for convenience.</p>"},{"location":"Linux%20Essentials/archiving-and-compression/#tar","title":"tar","text":"<p><code>tar</code> (tape archive) creates, extracts, and lists archives.</p>"},{"location":"Linux%20Essentials/archiving-and-compression/#creating-archives","title":"Creating Archives","text":"<pre><code>tar -cf archive.tar file1.txt file2.txt     # create archive\ntar -cf archive.tar directory/              # archive a directory\ntar -cf archive.tar *.log                   # archive matching files\n</code></pre>"},{"location":"Linux%20Essentials/archiving-and-compression/#creating-compressed-archives","title":"Creating Compressed Archives","text":"<p>Add a compression flag:</p> <pre><code>tar -czf archive.tar.gz directory/          # gzip compression\ntar -cjf archive.tar.bz2 directory/         # bzip2 compression\ntar -cJf archive.tar.xz directory/          # xz compression\n</code></pre> Flag Compression Extension Speed Ratio <code>-z</code> gzip <code>.tar.gz</code> or <code>.tgz</code> Fast Good <code>-j</code> bzip2 <code>.tar.bz2</code> Slow Better <code>-J</code> xz <code>.tar.xz</code> Slowest Best <p>What is the difference between tar -czf and tar -cJf? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/archiving-and-compression/#extracting","title":"Extracting","text":"<pre><code>tar -xf archive.tar                         # extract (auto-detects compression)\ntar -xf archive.tar.gz                      # works the same\ntar -xf archive.tar.xz                      # works the same\ntar -xf archive.tar -C /target/directory/   # extract to specific directory\n</code></pre> <p>Modern <code>tar</code> auto-detects the compression format, so you don't need <code>-z</code>, <code>-j</code>, or <code>-J</code> when extracting.</p> <p>Modern tar auto-detects compression when extracting</p> <p>You don't need to remember whether an archive uses gzip, bzip2, or xz. Just use <code>tar xf archive.tar.*</code> and tar reads the magic bytes to determine the compression format automatically. The <code>-z</code>, <code>-j</code>, <code>-J</code> flags are only needed when creating archives - tar needs to know which compressor to invoke.</p> <p>When extracting with modern tar, do you need to specify -z, -j, or -J? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/archiving-and-compression/#listing-contents","title":"Listing Contents","text":"<pre><code>tar -tf archive.tar.gz                      # list files without extracting\ntar -tvf archive.tar.gz                     # verbose listing (like ls -l)\n</code></pre>"},{"location":"Linux%20Essentials/archiving-and-compression/#common-options","title":"Common Options","text":"Option Meaning <code>-c</code> Create archive <code>-x</code> Extract archive <code>-t</code> List contents <code>-f</code> Specify archive filename (put <code>-f</code> last when combining flags, e.g., <code>-czf</code> not <code>-cfz</code>) <code>-v</code> Verbose output <code>-C</code> Change to directory before extracting <code>--exclude</code> Exclude files matching pattern <code>-p</code> Preserve permissions"},{"location":"Linux%20Essentials/archiving-and-compression/#practical-examples","title":"Practical Examples","text":"<pre><code># Archive excluding certain patterns\ntar -czf backup.tar.gz --exclude='*.log' --exclude='.git' project/\n\n# Extract a single file from an archive\ntar -xf archive.tar.gz path/to/specific/file.txt\n\n# Create an archive with a date in the filename\ntar -czf \"backup_$(date +%Y%m%d).tar.gz\" /var/www\n\n# Append files to an existing (uncompressed) archive\ntar -rf archive.tar newfile.txt\n</code></pre> <p>tar -rf only works on uncompressed archives</p> <p>The <code>-r</code> (append) flag only works on plain <code>.tar</code> files, not on compressed archives (<code>.tar.gz</code>, <code>.tar.xz</code>). Attempting <code>tar -rf archive.tar.gz newfile</code> silently fails or corrupts the archive. To add files to a compressed archive, you must decompress it first, append, then recompress.</p> <pre><code># Compare archive against filesystem\ntar -df archive.tar\n</code></pre> <p>Create a Tar Archive with Exclusions (requires JavaScript)</p> <p>Command Builder (requires JavaScript)</p>"},{"location":"Linux%20Essentials/archiving-and-compression/#gzip-gunzip","title":"gzip / gunzip","text":"<p><code>gzip</code> compresses individual files. It replaces the original file with a <code>.gz</code> version.</p> <p>gzip replaces the original file by default</p> <p>Running <code>gzip file.txt</code> deletes the original and creates <code>file.txt.gz</code>. This catches many people off guard. Use <code>gzip -k</code> (keep) to preserve the original file, or <code>gzip -c file.txt &gt; file.txt.gz</code> to write to stdout without modifying the original.</p> <pre><code>gzip file.txt                   # creates file.txt.gz, removes file.txt\ngzip -k file.txt                # keep the original file\ngzip -9 file.txt                # maximum compression (slower)\ngzip -1 file.txt                # fastest compression (less compression)\ngzip -d file.txt.gz             # decompress (same as gunzip)\n</code></pre> <p>What happens to the original file when you run gzip file.txt? (requires JavaScript)</p> <p>Default gzip level (-6) is usually optimal</p> <p>Compression levels <code>-1</code> through <code>-9</code> trade speed for size, but the returns diminish sharply above <code>-6</code>. The jump from <code>-6</code> to <code>-9</code> typically saves only 5-15% more space while taking 2-3x longer. Use <code>-1</code> when speed matters (pipelines, real-time compression) and <code>-9</code> only when compressing once for many downloads (software releases).</p> <p>Compression levels from <code>-1</code> to <code>-9</code> control the tradeoff between speed and compression ratio. Lower numbers use less CPU time and memory but produce larger files. Higher numbers spend more CPU and memory searching for better ways to encode the data. The difference in file size between <code>-1</code> and <code>-9</code> is often modest (5-15% on typical files), so the default level (<code>-6</code> for gzip) is usually the right choice. Use <code>-1</code> when speed matters (compressing data in a pipeline or on a slow machine) and <code>-9</code> only when you're compressing once and distributing many times (like software releases).</p> <p><code>gunzip</code> decompresses:</p> <pre><code>gunzip file.txt.gz              # creates file.txt, removes file.txt.gz\n</code></pre> <p><code>zcat</code> reads compressed files without decompressing:</p> <pre><code>zcat file.txt.gz                # print contents to STDOUT\nzcat file.txt.gz | grep \"error\" # search compressed file\n</code></pre> <p>Also available: <code>zless</code>, <code>zgrep</code> for working with gzip files directly.</p>"},{"location":"Linux%20Essentials/archiving-and-compression/#bzip2-bunzip2","title":"bzip2 / bunzip2","text":"<p><code>bzip2</code> compresses with a better ratio than gzip but is slower.</p> <pre><code>bzip2 file.txt                  # creates file.txt.bz2\nbzip2 -k file.txt               # keep original\nbzip2 -d file.txt.bz2           # decompress (same as bunzip2)\n</code></pre> <p><code>bunzip2</code> decompresses:</p> <pre><code>bunzip2 file.txt.bz2\n</code></pre> <p><code>bzcat</code> reads compressed files:</p> <pre><code>bzcat file.txt.bz2\n</code></pre>"},{"location":"Linux%20Essentials/archiving-and-compression/#xz-unxz","title":"xz / unxz","text":"<p><code>xz</code> provides the best compression ratio of the three but is the slowest.</p> <pre><code>xz file.txt                    # creates file.txt.xz\nxz -k file.txt                 # keep original\nxz -9 file.txt                 # maximum compression\nxz -d file.txt.xz              # decompress (same as unxz)\nxz -T 0 file.txt               # use all CPU cores (much faster)\n</code></pre> <p>Use xz -T 0 to use all CPU cores</p> <p>By default, <code>xz</code> uses a single CPU core, making it painfully slow on large files. The <code>-T 0</code> flag enables multithreaded compression using all available cores, dramatically reducing compression time. For a 1GB file on an 8-core machine, this can cut compression time by 5-7x.</p> <p><code>unxz</code> decompresses:</p> <pre><code>unxz file.txt.xz\n</code></pre> <p><code>xzcat</code> reads compressed files:</p> <pre><code>xzcat file.txt.xz\n</code></pre>"},{"location":"Linux%20Essentials/archiving-and-compression/#zip-unzip","title":"zip / unzip","text":"<p><code>zip</code> creates archives compatible with Windows and macOS. It handles both archiving and compression in one step.</p> <pre><code>zip archive.zip file1.txt file2.txt        # create zip with files\nzip -r archive.zip directory/              # recursive (include directories)\nzip -e archive.zip sensitive.txt           # encrypt with password\nzip -u archive.zip newfile.txt             # add/update files in existing zip\n</code></pre> <p><code>unzip</code> extracts:</p> <pre><code>unzip archive.zip                          # extract to current directory\nunzip archive.zip -d /target/directory/    # extract to specific directory\nunzip -l archive.zip                       # list contents\nunzip -o archive.zip                       # overwrite without prompting\n</code></pre>"},{"location":"Linux%20Essentials/archiving-and-compression/#zip-limitations","title":"zip Limitations","text":"<p>zip doesn't preserve Unix file permissions</p> <p>Extracted files get default permissions based on your umask, not the original permissions. Scripts that were executable before zipping won't be executable after unzipping. For Unix-to-Unix transfers where permissions, ownership, and symlinks matter, use <code>tar</code> instead.</p> <p>Classic zip has a few limitations to be aware of. It doesn't preserve Unix file permissions by default - extracted files get default permissions based on your umask, which can break scripts that need to be executable. The original zip format has a 4GB limit for individual files and a 4GB limit for the total archive size. Modern implementations support zip64 extensions to overcome this, but not all unzip tools handle zip64 correctly. For Unix-to-Unix transfers where you need to preserve permissions, ownership, and symlinks, <code>tar</code> archives are the better choice.</p>"},{"location":"Linux%20Essentials/archiving-and-compression/#when-to-use-which","title":"When to Use Which","text":"Format Use When <code>.tar.gz</code> General purpose. Fast, good compression, universally supported on Linux. Default choice for most things. <code>.tar.bz2</code> You need better compression and can wait longer. Less common now that xz exists. <code>.tar.xz</code> Maximum compression matters (distributing software, long-term storage). Standard for Linux distro packages. <code>.zip</code> Sharing with Windows/macOS users, or when recipients might not have tar. <code>.gz</code> (no tar) Compressing a single file (like log rotation). <p>Concrete scenarios:</p> <ul> <li>Distributing software - <code>.tar.xz</code> is the standard. Users download once, the slow compression time is paid by the developer, and the small size saves bandwidth.</li> <li>Log rotation - <code>.gz</code> (single file, no tar needed). logrotate uses gzip by default because the fast compression/decompression cycle matters when rotating logs on a busy server.</li> <li>Backups - <code>.tar.gz</code> balances compression with speed. For large backup jobs, the time difference between gzip and xz can be hours.</li> <li>Sharing with non-Linux users - <code>.zip</code> is universally supported on Windows and macOS without extra software.</li> <li>Archiving for long-term storage - <code>.tar.xz</code> gives the best size reduction. If the data won't be accessed frequently, the slow compression is worth it.</li> </ul>"},{"location":"Linux%20Essentials/archiving-and-compression/#compression-comparison","title":"Compression Comparison","text":"<p>Approximate results for a typical 100MB text file (actual results vary with content):</p> Format Compressed Size Compression Time Decompression Time gzip ~30MB (~70% reduction) ~2 seconds ~1 second bzip2 ~25MB (~75% reduction) ~8 seconds ~4 seconds xz ~20MB (~80% reduction) ~30 seconds ~2 seconds zip ~30MB (~70% reduction) ~2 seconds ~1 second <p>Notable: xz decompresses much faster than it compresses, making it a good choice when you compress once and decompress many times (like software distribution). Binary files and already-compressed data (images, video) will see much smaller reductions.</p> <p>Compression Algorithm Comparison (requires JavaScript)</p>"},{"location":"Linux%20Essentials/archiving-and-compression/#further-reading","title":"Further Reading","text":"<ul> <li>GNU Tar Manual - archiving utility documentation</li> <li>GNU Gzip Manual - compression utility reference</li> <li>bzip2 - block-sorting file compressor</li> <li>XZ Utils - LZMA/LZMA2 compression tools</li> <li>Info-ZIP - zip and unzip utilities</li> </ul> <p>Previous: System Information | Next: Best Practices | Back to Index</p>"},{"location":"Linux%20Essentials/best-practices/","title":"Shell Scripting Best Practices","text":"<p>A collection of conventions and patterns that make shell scripts more reliable, portable, and maintainable. These aren't opinions - they prevent real bugs.</p>"},{"location":"Linux%20Essentials/best-practices/#start-every-script-right","title":"Start Every Script Right","text":"<p>Every <code>bash</code> script should begin with:</p> <pre><code>#!/bin/bash\nset -euo pipefail\n</code></pre>"},{"location":"Linux%20Essentials/best-practices/#what-each-option-does","title":"What Each Option Does","text":"<p><code>set -e</code> (errexit) - exit immediately if any command returns non-zero:</p> <pre><code>set -e\ncp important.txt /backup/     # if this fails, the script stops\nrm important.txt               # this won't run if cp failed\n</code></pre> <p>Without <code>-e</code>, the script would happily continue after the failed <code>cp</code> and delete the file.</p> <p><code>set -u</code> (nounset) - treat references to unset variables as errors:</p> <pre><code>set -u\nrm -rf \"$DEPLOY_DIR/app\"      # if DEPLOY_DIR is unset, script exits with an error\n</code></pre> <p>Without <code>-u</code>, an unset <code>DEPLOY_DIR</code> expands to empty, and you'd run <code>rm -rf /app</code>.</p> <p>Without set -u, unset variable in rm -rf expands to empty</p> <p>This is one of the most dangerous scripting bugs. <code>rm -rf \"$UNSET_VAR/\"</code> becomes <code>rm -rf /</code> when the variable is empty. The <code>-u</code> option makes bash abort immediately on any reference to an unset variable, turning a catastrophic silent failure into a clear error message.</p> <p><code>set -o pipefail</code> - a pipeline fails if any command in it fails:</p> <pre><code>set -o pipefail\ncat /nonexistent | sort        # pipeline returns non-zero (cat failed)\n</code></pre> <p>Without <code>pipefail</code>, only the exit code of the last command (<code>sort</code>) matters, hiding the <code>cat</code> failure.</p> <p>Which of these commands would NOT trigger set -e to exit the script? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/best-practices/#always-quote-your-variables","title":"Always Quote Your Variables","text":"<p>Unquoted variables undergo word splitting and glob expansion. This causes bugs with filenames containing spaces, wildcards, or empty values.</p> <pre><code># BAD\nfile=$1\nrm $file              # if file=\"my documents\", this runs: rm my documents\n\n# GOOD\nfile=\"$1\"\nrm \"$file\"            # runs: rm \"my documents\"\n</code></pre> <pre><code># BAD\nif [ -n $var ]; then   # if var is empty, this becomes: [ -n ] (always true)\n\n# GOOD\nif [ -n \"$var\" ]; then # correctly tests for non-empty\n</code></pre> <p>The rule is simple: always double-quote variable expansions (<code>\"$var\"</code>) unless you specifically want word splitting.</p> <p>Exceptions where quoting is unnecessary: - Inside <code>[[ ]]</code> (no word splitting, but quoting doesn't hurt) - Inside <code>$(( ))</code> arithmetic</p> <p>What's wrong with: [ -f $file ] (without quotes around $file)? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/best-practices/#use-over-in-bash","title":"Use [[ ]] Over [ ] in Bash","text":"<p><code>[[ ]]</code> is a bash keyword that's safer and more powerful than <code>[ ]</code>:</p> <pre><code># [ ] requires careful quoting\n[ -n \"$var\" ]           # works\n[ -n $var ]             # BUG if var is empty\n\n# [[ ]] handles it\n[[ -n $var ]]           # works even without quotes\n\n# [[ ]] supports pattern matching\n[[ $file == *.txt ]]    # glob pattern\n\n# [[ ]] supports regex\n[[ $email =~ ^[a-z]+@[a-z]+\\.[a-z]+$ ]]\n\n# [[ ]] uses familiar logical operators\n[[ $a -gt 0 &amp;&amp; $b -gt 0 ]]     # clean\n[ \"$a\" -gt 0 ] &amp;&amp; [ \"$b\" -gt 0 ]  # clunky equivalent with [ ]\n</code></pre> <p>If you're writing a bash script (not a POSIX sh script), always use <code>[[ ]]</code>.</p>"},{"location":"Linux%20Essentials/best-practices/#prefer-over-backticks","title":"Prefer $() Over Backticks","text":"<p>Both perform command substitution, but <code>$()</code> is clearer and nests properly:</p> <pre><code># BAD - backticks are hard to read and don't nest\nresult=`echo \\`date\\``\n\n# GOOD - $() nests cleanly\nresult=$(echo $(date))\n</code></pre> <p>Backticks also have surprising escaping rules. <code>$()</code> behaves predictably.</p>"},{"location":"Linux%20Essentials/best-practices/#use-mktemp-for-temporary-files","title":"Use mktemp for Temporary Files","text":"<p>Never hardcode temporary file paths. Multiple script instances would collide, and predictable paths are a security risk.</p> <pre><code># BAD\ntmpfile=\"/tmp/mydata.tmp\"\n\n# GOOD\ntmpfile=$(mktemp)\ntmpdir=$(mktemp -d)\n\n# Always clean up\ntrap 'rm -f \"$tmpfile\"' EXIT\n</code></pre> <p><code>mktemp</code> creates a unique filename that doesn't already exist. Combine with <code>trap</code> to ensure cleanup.</p>"},{"location":"Linux%20Essentials/best-practices/#use-shellcheck","title":"Use shellcheck","text":"<p><code>shellcheck</code> is a static analysis tool that catches common mistakes in shell scripts:</p> <pre><code>shellcheck myscript.sh\n</code></pre> <p>It finds issues like: - Unquoted variables - Useless <code>cat</code> usage - Incorrect test syntax - Word splitting bugs - Unreachable code</p> <p>Install it:</p> <pre><code>sudo apt install shellcheck       # Debian/Ubuntu\nsudo dnf install ShellCheck       # Fedora\nbrew install shellcheck           # macOS\n</code></pre> <p>Run it in CI/CD to catch shell script bugs before they reach production.</p> <p>Use shellcheck in CI/CD pipelines</p> <p>Add <code>shellcheck *.sh</code> to your CI pipeline to catch bugs automatically on every commit. ShellCheck's exit code is non-zero when it finds issues, making it easy to integrate. Use <code># shellcheck disable=SC2086</code> comments to suppress specific warnings when you know what you're doing.</p> <p>Here's what shellcheck output looks like on a buggy script:</p> <pre><code>$ cat buggy.sh\n#!/bin/bash\nfiles=$(ls *.txt)\nfor f in $files; do\n    if [ $f == \"important\" ]; then\n        rm $f\n    fi\ndone\n\n$ shellcheck buggy.sh\nIn buggy.sh line 2:\nfiles=$(ls *.txt)\n        ^------^ SC2012: Use find instead of ls to better handle non-alphanumeric filenames.\n\nIn buggy.sh line 3:\nfor f in $files; do\n         ^----^ SC2086: Double quote to prevent globbing and word splitting.\n\nIn buggy.sh line 4:\n    if [ $f == \"important\" ]; then\n         ^-- SC2086: Double quote to prevent globbing and word splitting.\n         ^-- SC2039: In POSIX sh, == in place of = is undefined.\n</code></pre> <p>Each finding includes a code (like SC2086) that you can look up at the shellcheck wiki for a detailed explanation and fix. Editor integration makes this even more useful - the VS Code ShellCheck extension and vim plugins like ALE or Syntastic show warnings inline as you type, catching bugs before you even save the file.</p> <p>What is ShellCheck? (requires JavaScript)</p> <p>Fix a Buggy Script Using ShellCheck Principles (requires JavaScript)</p>"},{"location":"Linux%20Essentials/best-practices/#dont-parse-ls-output","title":"Don't Parse ls Output","text":"<p>The output of <code>ls</code> is meant for humans, not scripts. Filenames with spaces, newlines, or special characters will break parsing.</p> <pre><code># BAD - breaks on filenames with spaces\nfor file in $(ls); do\n    echo \"$file\"\ndone\n\n# GOOD - globbing handles filenames correctly\nfor file in *; do\n    echo \"$file\"\ndone\n\n# GOOD - find with -print0 for truly safe handling\nfind . -type f -print0 | while IFS= read -r -d '' file; do\n    echo \"$file\"\ndone\n</code></pre> <p>Globbing handles spaces correctly where ls parsing does not</p> <p>Shell globbing (<code>for f in *.txt</code>) expands filenames as properly quoted tokens, so <code>my file.txt</code> stays as one argument. Parsing <code>ls</code> output (<code>for f in $(ls)</code>) splits on whitespace, turning <code>my file.txt</code> into two separate arguments: <code>my</code> and <code>file.txt</code>. Globbing is both safer and simpler.</p> <p>Why is it dangerous to parse the output of ls in scripts? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/best-practices/#use-arrays-for-lists","title":"Use Arrays for Lists","text":"<p>Don't store lists of items in a string variable. Use arrays.</p> <pre><code># BAD - breaks on filenames with spaces\nfiles=\"file one.txt file two.txt\"\nfor f in $files; do\n    echo \"$f\"        # prints: file, one.txt, file, two.txt\ndone\n\n# GOOD - arrays preserve elements\nfiles=(\"file one.txt\" \"file two.txt\")\nfor f in \"${files[@]}\"; do\n    echo \"$f\"        # prints: file one.txt, file two.txt\ndone\n</code></pre> <p>Array operations:</p> <pre><code>arr=(\"one\" \"two\" \"three\")\necho \"${arr[0]}\"              # first element\necho \"${arr[@]}\"              # all elements\necho \"${#arr[@]}\"             # number of elements\narr+=(\"four\")                 # append\n\n# Build arrays from command output\nmapfile -t lines &lt; file.txt   # read file into array (one line per element)\n</code></pre> <p>Use mapfile to read files into arrays</p> <p><code>mapfile -t lines &lt; file.txt</code> (also called <code>readarray</code>) reads an entire file into an array with one element per line. The <code>-t</code> flag strips trailing newlines. This is safer than <code>lines=($(cat file))</code> which breaks on spaces and globbing characters.</p> <p>Rewrite a Script with Proper Quoting and Arrays (requires JavaScript)</p>"},{"location":"Linux%20Essentials/best-practices/#portable-vs-bash-specific","title":"Portable vs Bash-Specific","text":"<p>If your script needs to run on minimal systems (Docker containers, embedded systems, Debian's <code>dash</code>), avoid bash-specific features:</p> Bash-Specific POSIX Alternative <code>[[ ]]</code> <code>[ ]</code> <code>$(( ))</code> <code>$(( ))</code> (this one is POSIX) <code>{1..10}</code> <code>seq 1 10</code> <code>function name {</code> <code>name() {</code> <code>source file</code> <code>. file</code> <code>$RANDOM</code> No POSIX equivalent Arrays No POSIX equivalent <code>&lt;&lt;&lt;</code> here strings <code>echo \"str\" \\| cmd</code> <p>If you need bash features, make sure your shebang is <code>#!/bin/bash</code>, not <code>#!/bin/sh</code>. On some systems, <code>/bin/sh</code> is <code>dash</code>, which doesn't support bash extensions.</p> <p>#!/bin/sh may not be bash on all systems</p> <p>On Debian, Ubuntu, and Alpine, <code>/bin/sh</code> is <code>dash</code> (or <code>ash</code>), not bash. If your script uses <code>[[ ]]</code>, arrays, <code>$RANDOM</code>, process substitution, or any bash-specific feature, it will silently break or produce wrong results under <code>dash</code>. Always use <code>#!/bin/bash</code> for bash scripts.</p> <p>When to target POSIX sh: Docker scratch images and minimal containers often only have <code>/bin/sh</code> (usually dash or busybox ash). Cron jobs on minimal systems may run under <code>sh</code> by default. CI/CD pipeline scripts that need to run across different environments (Alpine Linux, Ubuntu, macOS) are safer with POSIX sh. When bash is fine: application scripts, developer tools, interactive helpers, and anything where you control the execution environment. If you're writing a deployment script that only runs on your Ubuntu servers, use bash and enjoy its features - forcing POSIX compatibility on a known-bash environment just makes the code harder to read.</p>"},{"location":"Linux%20Essentials/best-practices/#avoid-common-pitfalls","title":"Avoid Common Pitfalls","text":""},{"location":"Linux%20Essentials/best-practices/#dont-use-eval","title":"Don't Use eval","text":"<p>eval is almost always a security risk</p> <p><code>eval</code> interprets its arguments as shell code, meaning any user-controlled input becomes executable commands. An attacker passing <code>; rm -rf /</code> as input to <code>eval \"process $input\"</code> gets arbitrary command execution. Use indirect expansion (<code>${!var}</code>), associative arrays, or <code>declare -n</code> nameref variables instead.</p> <p><code>eval</code> executes a string as a command. It's almost always a security risk and there's usually a better way.</p> <pre><code># BAD - eval is dangerous\neval \"rm $user_input\"\n\n# GOOD - pass arguments directly\nrm \"$filename\"\n</code></pre> <p>If you find yourself reaching for <code>eval</code> to dynamically construct variable names, bash has safer alternatives. Indirect expansion with <code>${!var}</code> lets you dereference a variable name stored in another variable:</p> <pre><code>key='HOME'\necho \"${!key}\"    # prints the value of $HOME\n</code></pre> <p>Associative arrays (bash 4+) are even better for key-value lookups:</p> <pre><code>declare -A config\nconfig[host]='localhost'\nconfig[port]='5432'\necho \"${config[host]}\"\n</code></pre> <p>Both approaches avoid the security risk of <code>eval</code> interpreting arbitrary strings as code.</p>"},{"location":"Linux%20Essentials/best-practices/#handle-missing-commands","title":"Handle Missing Commands","text":"<p>Check that required tools exist before using them:</p> <pre><code>for cmd in jq curl git; do\n    if ! command -v \"$cmd\" &amp;&gt;/dev/null; then\n        echo \"Required command not found: $cmd\" &gt;&amp;2\n        exit 1\n    fi\ndone\n</code></pre>"},{"location":"Linux%20Essentials/best-practices/#use-readonly-for-constants","title":"Use readonly for Constants","text":"<pre><code>readonly CONFIG_DIR=\"/etc/myapp\"\nreadonly LOG_FILE=\"/var/log/myapp.log\"\nreadonly MAX_RETRIES=3\n</code></pre> <p><code>readonly</code> prevents accidental reassignment. Use it for values that should never change.</p> <p>Use readonly for configuration constants</p> <p>Declare values that should never change with <code>readonly</code>: paths, URLs, retry counts, and other configuration. If any code accidentally tries to reassign a <code>readonly</code> variable, bash raises an error immediately rather than silently using the wrong value.</p>"},{"location":"Linux%20Essentials/best-practices/#use-epoch-timestamps","title":"Use Epoch Timestamps","text":"<p>For log files and backups, use epoch timestamps or ISO dates to avoid collisions and ensure sorting:</p> <pre><code># Epoch seconds\nlogfile=\"deploy_$(date +%s).log\"\n\n# ISO format (human-readable and sortable)\nlogfile=\"deploy_$(date +%Y%m%d_%H%M%S).log\"\n</code></pre> <p>Always use --dry-run when available before destructive operations</p> <p>Many commands offer a <code>--dry-run</code> (or <code>-n</code>) flag that shows what would happen without actually doing it. Use it before <code>rsync --delete</code>, <code>rm -rf</code>, <code>apt autoremove</code>, <code>git clean</code>, and similar destructive operations. The few seconds spent previewing can prevent hours of recovery work.</p>"},{"location":"Linux%20Essentials/best-practices/#script-template","title":"Script Template","text":"<p>A starting point for new scripts:</p> <pre><code>#!/bin/bash\nset -euo pipefail\n\n# BASH_SOURCE[0] is the path to this script, even when sourced from another script.\n# $0 would give the caller's name instead. cd + pwd resolves symlinks and relative paths.\nreadonly SCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" &amp;&amp; pwd)\"\nreadonly SCRIPT_NAME=\"$(basename \"$0\")\"\n\nusage() {\n    cat &lt;&lt;EOF\nUsage: $SCRIPT_NAME [options] &lt;argument&gt;\n\nDescription of what this script does.\n\nOptions:\n    -h, --help    Show this help message\n    -v, --verbose Enable verbose output\nEOF\n    exit 1\n}\n\nlog() {\n    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $*\" &gt;&amp;2\n}\n\n# main() is defined here but called at the bottom of the file. This pattern lets you\n# define helper functions anywhere in the file without worrying about order - bash reads\n# function definitions before executing main(). It also makes the entry point obvious.\nmain() {\n    local verbose=false\n\n    while [[ $# -gt 0 ]]; do\n        case \"$1\" in\n            -h|--help) usage ;;\n            -v|--verbose) verbose=true; shift ;;\n            -*) echo \"Unknown option: $1\" &gt;&amp;2; usage ;;\n            *) break ;;\n        esac\n    done\n\n    [[ $# -lt 1 ]] &amp;&amp; usage\n\n    log \"Starting $SCRIPT_NAME\"\n    # ... your logic here ...\n    log \"Done\"\n}\n\n# \"$@\" passes all command-line arguments to main, preserving quoting.\nmain \"$@\"\n</code></pre> <p>Production-Ready Script Template (requires JavaScript)</p>"},{"location":"Linux%20Essentials/best-practices/#further-reading","title":"Further Reading","text":"<ul> <li>ShellCheck Wiki - explanations for every ShellCheck warning and suggestion</li> <li>ShellCheck - online shell script analysis tool</li> <li>Bash Reference Manual - official bash documentation</li> <li>POSIX Shell Command Language - portable shell scripting specification</li> <li>Google Shell Style Guide - widely-referenced style conventions for shell scripts</li> </ul> <p>Previous: Archiving and Compression | Back to Index</p>"},{"location":"Linux%20Essentials/disk-and-filesystem/","title":"Disk and Filesystem","text":"<p>Understanding disk usage, partitions, and filesystem management is a core sysadmin skill. These tools help you monitor space, mount storage, and maintain filesystem health.</p>"},{"location":"Linux%20Essentials/disk-and-filesystem/#df-filesystem-usage","title":"df - Filesystem Usage","text":"<p><code>df</code> (disk free) shows how much space is used and available on mounted filesystems.</p> <pre><code>df -h                    # human-readable sizes (K, M, G)\ndf -hT                   # include filesystem type\ndf -h /                  # specific filesystem\ndf -i                    # inode usage instead of block usage\n</code></pre> <p>Example output:</p> <pre><code>Filesystem      Size  Used Avail Use% Mounted on\n/dev/sda1        50G   32G   16G  67% /\n/dev/sda2       200G  150G   40G  79% /home\ntmpfs           3.9G     0  3.9G   0% /dev/shm\n</code></pre> <p>Key columns: - Size - total capacity - Used - space consumed - Avail - space remaining (may not equal Size minus Used due to reserved space for root) - Use% - percentage used (calculated against non-reserved space) - Mounted on - where the filesystem is accessible</p> <p>A filesystem at 100% causes unpredictable application failures</p> <p>When a filesystem fills up, applications don't get a clean \"disk full\" error - they crash, corrupt data, fail to write logs, or hang. Databases refuse writes, log rotation breaks, and even <code>ssh</code> may stop working if <code>/var</code> is full. Monitor <code>Use%</code> with alerts at 80% and 90% thresholds to avoid emergencies.</p> <p>A filesystem at 100% causes applications to fail in unexpected ways. Keep an eye on <code>Use%</code>.</p> <p>Running out of inodes (<code>df -i</code>) is a less common but equally disabling problem. It happens when you have a huge number of tiny files.</p>"},{"location":"Linux%20Essentials/disk-and-filesystem/#du-directory-sizes","title":"du - Directory Sizes","text":"<p><code>du</code> (disk usage) shows how much space files and directories consume.</p> <pre><code>du -sh /var/log              # total size of a directory\ndu -sh *                     # size of each item in current directory\ndu -sh /home/*               # size of each user's home directory\ndu -h --max-depth=1 /var     # one level deep\ndu -ah /var/log              # all files, not just directories\n</code></pre>"},{"location":"Linux%20Essentials/disk-and-filesystem/#finding-whats-using-space","title":"Finding What's Using Space","text":"<pre><code># Top 10 largest directories under /var\ndu -h --max-depth=1 /var | sort -rh | head -10\n\n# Largest files on the system\nfind / -type f -exec du -h {} + 2&gt;/dev/null | sort -rh | head -20\n\n# Quick check of current directory\ndu -sh * | sort -rh\n</code></pre>"},{"location":"Linux%20Essentials/disk-and-filesystem/#du-vs-df-discrepancy","title":"du vs df Discrepancy","text":"<p>df and du can show different numbers</p> <p><code>df</code> reports from the filesystem's perspective (including space held by deleted-but-open files). <code>du</code> only counts visible files. When <code>df</code> shows more usage than <code>du</code> can account for, a process is holding a deleted file open. Find it with <code>lsof +L1 | grep deleted</code>.</p> <p><code>du</code> and <code>df</code> can show different numbers. <code>df</code> reports space from the filesystem's perspective (including space held by deleted-but-open files). <code>du</code> counts only visible files. If <code>df</code> shows full but <code>du</code> doesn't account for all the space, a process may be holding a deleted file open. Find it with:</p> <pre><code>lsof +L1    # files with zero link count (deleted but still held open)\n</code></pre> <p>Why might df show a disk as 90% full while du -sh / shows only 70% used? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/disk-and-filesystem/#mount","title":"mount","text":"<p><code>mount</code> attaches a filesystem to the directory tree.</p>"},{"location":"Linux%20Essentials/disk-and-filesystem/#viewing-mounts","title":"Viewing Mounts","text":"<pre><code>mount                        # list all mounted filesystems\nmount | grep ext4            # filter by type\nfindmnt                      # tree view of mounts (cleaner output)\nfindmnt -t ext4              # filter by filesystem type\n</code></pre> <p>Use findmnt for cleaner mount output</p> <p>The plain <code>mount</code> command outputs a wall of text that's hard to parse. <code>findmnt</code> shows mounts as a tree with clean columns. Use <code>findmnt -t ext4,xfs</code> to filter by filesystem type, or <code>findmnt /data</code> to check a specific mount point. It's the modern replacement for <code>mount | grep</code>.</p>"},{"location":"Linux%20Essentials/disk-and-filesystem/#mounting-a-filesystem","title":"Mounting a Filesystem","text":"<pre><code>mount /dev/sdb1 /mnt/data              # mount a partition\nmount -t ext4 /dev/sdb1 /mnt/data      # specify filesystem type\nmount -o ro /dev/sdb1 /mnt/data        # mount read-only\nmount -o remount,rw /                   # remount root as read-write\n</code></pre> <p>Command Builder (requires JavaScript)</p>"},{"location":"Linux%20Essentials/disk-and-filesystem/#unmounting","title":"Unmounting","text":"<pre><code>umount /mnt/data                        # unmount by mount point\numount /dev/sdb1                        # unmount by device\n</code></pre> <p>If a filesystem is busy (files are open), <code>umount</code> will fail. Find what's using it:</p> <pre><code>lsof +D /mnt/data\nfuser -vm /mnt/data\n</code></pre> <p>Then either close those files or force unmount (risky):</p> <pre><code>umount -l /mnt/data    # lazy unmount - detaches immediately, cleans up later\n</code></pre>"},{"location":"Linux%20Essentials/disk-and-filesystem/#etcfstab","title":"/etc/fstab","text":"<p>The <code>/etc/fstab</code> file defines filesystems to mount at boot:</p> <pre><code># &lt;device&gt;       &lt;mount point&gt;  &lt;type&gt;  &lt;options&gt;        &lt;dump&gt; &lt;pass&gt;\n/dev/sda1        /              ext4    defaults         0      1\n/dev/sda2        /home          ext4    defaults         0      2\nUUID=abc123...   /data          xfs     defaults,noatime 0      2\n/dev/sdb1        /mnt/backup    ext4    noauto,user      0      0\ntmpfs            /tmp           tmpfs   defaults,size=2G 0      0\n</code></pre> <p>Fields: - device - partition, UUID, or label - mount point - where to mount - type - filesystem type (ext4, xfs, tmpfs, etc.) - options - mount options (comma-separated) - dump - backup flag (usually 0) - pass - fsck order (1 for root, 2 for others, 0 to skip)</p> <p>Understanding an fstab Entry (requires JavaScript)</p> <p>In an fstab entry, what does the sixth field (fs_passno) control? (requires JavaScript)</p> <p>Common options: | Option | Meaning | |--------|---------| | <code>defaults</code> | rw, suid, dev, exec, auto, nouser, async | | <code>noatime</code> | Don't update access time (improves performance) | | <code>ro</code> | Read-only | | <code>noexec</code> | Don't allow execution of binaries | | <code>nosuid</code> | Ignore setuid/setgid bits | | <code>noauto</code> | Don't mount at boot (mount manually) | | <code>user</code> | Allow non-root users to mount |</p> <p>Some options worth understanding:</p> <p>noatime improves performance on SSDs</p> <p>The <code>noatime</code> mount option stops the kernel from writing a new \"last accessed\" timestamp on every file read. On SSDs, this reduces unnecessary writes (extending drive life). On HDDs, it reduces I/O overhead. There's rarely a reason not to use it unless you have software that depends on access times.</p> <ul> <li><code>noatime</code> stops the kernel from updating the 'last accessed' timestamp every time a file is read. On SSDs, this reduces unnecessary writes and improves performance. On HDDs with busy filesystems, it reduces I/O overhead. There's rarely a reason not to use <code>noatime</code> unless you have software that depends on access times.</li> <li><code>noexec</code> prevents executing any binary on the filesystem. It's a security hardening measure commonly applied to <code>/tmp</code> - if an attacker writes a malicious binary to <code>/tmp</code>, they can't execute it directly. Note that it doesn't prevent <code>bash /tmp/script.sh</code> (which runs bash, not the script), so it's a layer of defense, not a complete solution.</li> <li><code>nosuid</code> tells the kernel to ignore setuid and setgid bits on the filesystem. This is important for removable media and network mounts - you don't want someone plugging in a USB drive containing a setuid-root binary that could escalate privileges.</li> </ul> <p>Use UUIDs in fstab instead of device names</p> <p>Device names like <code>/dev/sdb1</code> can change between boots if disks are added, removed, or detected in a different order. UUIDs are stored on the filesystem itself and never change. Always use <code>UUID=...</code> in <code>/etc/fstab</code> instead of <code>/dev/sdX</code> paths. Run <code>blkid</code> to find UUIDs.</p> <p>Using UUIDs instead of device names (like <code>/dev/sdb1</code>) is safer because device names can change between boots:</p> <pre><code>blkid                  # show UUIDs of all block devices\n</code></pre> <p>Why does fstab use UUIDs instead of device names like /dev/sdb1? (requires JavaScript)</p> <p>After editing <code>/etc/fstab</code>, test it before rebooting:</p> <pre><code>mount -a    # mount everything in fstab that isn't already mounted\n</code></pre>"},{"location":"Linux%20Essentials/disk-and-filesystem/#lsblk-block-devices","title":"lsblk - Block Devices","text":"<p><code>lsblk</code> lists block devices (disks, partitions, loop devices) in a tree format.</p> <pre><code>lsblk                    # basic tree view\nlsblk -f                 # include filesystem type, label, and UUID\nlsblk -o NAME,SIZE,TYPE,MOUNTPOINT,FSTYPE    # custom columns\n</code></pre> <p>Example output:</p> <pre><code>NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nsda      8:0    0   500G  0 disk\n\u251c\u2500sda1   8:1    0    50G  0 part /\n\u251c\u2500sda2   8:2    0   200G  0 part /home\n\u2514\u2500sda3   8:3    0   250G  0 part /data\nsdb      8:16   0     1T  0 disk\n\u2514\u2500sdb1   8:17   0     1T  0 part /mnt/backup\nsr0     11:0    1  1024M  0 rom\n</code></pre> <p>Viewing Disk Layout with lsblk (requires JavaScript)</p>"},{"location":"Linux%20Essentials/disk-and-filesystem/#partition-management","title":"Partition Management","text":""},{"location":"Linux%20Essentials/disk-and-filesystem/#fdisk","title":"fdisk","text":"<p><code>fdisk</code> manages MBR partition tables (disks up to 2TB, max 4 primary partitions):</p> <pre><code>fdisk -l                   # list all partitions on all disks\nfdisk -l /dev/sdb          # list partitions on a specific disk\nfdisk /dev/sdb             # interactive partition editor\n</code></pre> <p>Inside <code>fdisk</code>:</p> Command Action <code>p</code> Print partition table <code>n</code> Create new partition <code>d</code> Delete partition <code>t</code> Change partition type <code>w</code> Write changes and exit <code>q</code> Quit without saving"},{"location":"Linux%20Essentials/disk-and-filesystem/#parted","title":"parted","text":"<p><code>parted</code> manages GPT partition tables (supports disks larger than 2TB):</p> <pre><code>parted -l                            # list all partitions\nparted /dev/sdb print                # print partition table\nparted /dev/sdb mklabel gpt         # create GPT partition table\nparted /dev/sdb mkpart primary ext4 0% 100%   # create partition using full disk\n</code></pre> <p>When to choose fdisk vs parted: <code>fdisk</code> works with MBR (Master Boot Record) partition tables. MBR has a 2TB per-disk limit and supports a maximum of 4 primary partitions (or 3 primary + 1 extended partition containing logical partitions). <code>parted</code> works with GPT (GUID Partition Table), which supports disks larger than 2TB, up to 128 partitions by default, and is required for UEFI boot. For any new system with disks over 2TB or that uses UEFI, use <code>parted</code> with GPT. For older systems with MBR, or when modifying existing MBR layouts, use <code>fdisk</code>. Modern versions of <code>fdisk</code> can also handle GPT, but <code>parted</code> is the standard tool for it.</p>"},{"location":"Linux%20Essentials/disk-and-filesystem/#creating-filesystems","title":"Creating Filesystems","text":"<p><code>mkfs</code> creates (formats) a filesystem on a partition:</p> <pre><code>mkfs.ext4 /dev/sdb1          # create ext4 filesystem\nmkfs.xfs /dev/sdb1           # create XFS filesystem\nmkfs.vfat /dev/sdb1          # create FAT32 filesystem (USB drives)\n</code></pre> <p>Options:</p> <pre><code>mkfs.ext4 -L \"backups\" /dev/sdb1         # set a label\nmkfs.ext4 -m 1 /dev/sdb1                 # reserve only 1% for root (default is 5%)\n</code></pre> <p>XFS filesystems can be grown but not shrunk</p> <p>Once you create an XFS filesystem, you can expand it with <code>xfs_growfs</code> but you can never shrink it. If you might need to resize partitions smaller in the future, choose ext4 instead, which supports both growing (<code>resize2fs</code>) and shrinking. Plan your XFS partition sizes carefully.</p> <p>ext4 vs XFS: ext4 is the default choice on most Linux distributions. It's well-tested, has excellent tooling (<code>e2fsck</code>, <code>tune2fs</code>, <code>resize2fs</code>), and handles general workloads well. XFS excels at handling large files and parallel I/O - it's the default on RHEL/CentOS and a strong choice for database servers, media storage, or any workload with many concurrent large writes. XFS can't be shrunk after creation (only grown), while ext4 can be both grown and shrunk. For most use cases, go with your distribution's default.</p>"},{"location":"Linux%20Essentials/disk-and-filesystem/#filesystem-checks","title":"Filesystem Checks","text":"<p><code>fsck</code> checks and repairs filesystems. Only run this on unmounted or read-only filesystems.</p> <p>Never run fsck on a mounted filesystem</p> <p>Running <code>fsck</code> on a mounted filesystem can corrupt data irreversibly. The kernel's in-memory metadata cache and <code>fsck</code>'s direct disk writes will conflict, causing lost files, broken directories, or an unmountable filesystem. Always unmount first, or boot from a live USB to check the root partition.</p> <pre><code>fsck /dev/sdb1               # check and prompt for repairs\nfsck -y /dev/sdb1            # automatically fix errors\nfsck -n /dev/sdb1            # check only, don't fix anything\n</code></pre> <p>For ext4 specifically:</p> <pre><code>e2fsck -f /dev/sdb1          # force check even if clean\ntune2fs -l /dev/sdb1         # show filesystem metadata (last check, mount count, etc.)\n</code></pre> <p>Running <code>fsck</code> on a mounted filesystem can cause data corruption. The reason is that the mounted kernel has its own in-memory view of the filesystem's metadata (which blocks belong to which files, free space maps, directory entries). When <code>fsck</code> reads and writes to the raw disk device, it modifies on-disk structures that the kernel doesn't know about. The kernel's cached metadata now disagrees with what's on disk, leading to corrupted files, lost data, or an unmountable filesystem. Always unmount first, or boot from a live USB to check the root filesystem.</p>"},{"location":"Linux%20Essentials/disk-and-filesystem/#practical-scenarios","title":"Practical Scenarios","text":""},{"location":"Linux%20Essentials/disk-and-filesystem/#disk-full-finding-the-cause","title":"Disk Full - Finding the Cause","text":"<pre><code># 1. Check which filesystem is full\ndf -h\n\n# 2. Find the largest directories\ndu -h --max-depth=1 /var | sort -rh | head\n\n# 3. Find large files\nfind /var -type f -size +100M -exec ls -lh {} +\n\n# 4. Check for deleted files still held open\nlsof +L1\n</code></pre> <p>Diagnose a Full Disk (requires JavaScript)</p>"},{"location":"Linux%20Essentials/disk-and-filesystem/#adding-a-new-disk","title":"Adding a New Disk","text":"<pre><code># 1. Identify the new disk\nlsblk\n\n# 2. Create a partition\nparted /dev/sdb mklabel gpt\nparted /dev/sdb mkpart primary ext4 0% 100%\n\n# 3. Create a filesystem\nmkfs.ext4 /dev/sdb1\n\n# 4. Create mount point and mount\nmkdir -p /mnt/data\nmount /dev/sdb1 /mnt/data\n\n# 5. Add to fstab for persistence\nblkid /dev/sdb1    # get UUID\necho 'UUID=your-uuid-here /mnt/data ext4 defaults 0 2' &gt;&gt; /etc/fstab\n\n# 6. Verify\nmount -a\ndf -h /mnt/data\n</code></pre>"},{"location":"Linux%20Essentials/disk-and-filesystem/#further-reading","title":"Further Reading","text":"<ul> <li>e2fsprogs - ext2/ext3/ext4 filesystem utilities including mkfs.ext4, e2fsck, and tune2fs</li> <li>XFS Wiki - XFS filesystem documentation and administration guides</li> <li>util-linux - collection of system utilities including fdisk, lsblk, mount, and more</li> <li>Linux Kernel Filesystem Documentation - kernel-level filesystem and block device documentation</li> </ul> <p>Previous: Scripting Fundamentals | Next: Networking | Back to Index</p>"},{"location":"Linux%20Essentials/file-permissions/","title":"File Permissions","text":"<p>Every file and directory on a Linux system has an owner, a group, and a set of permission bits that control who can read, write, and execute it. Understanding this model is essential for system administration and security.</p>"},{"location":"Linux%20Essentials/file-permissions/#the-permission-model","title":"The Permission Model","text":"<p>Linux uses three levels of access:</p> Level Meaning User (u) The file's owner Group (g) Members of the file's group Other (o) Everyone else <p>Each level has three permissions:</p> Permission Files Directories Read (r) View file contents List directory contents Write (w) Modify file contents Create, delete, rename files in directory Execute (x) Run as a program Enter (cd into) the directory"},{"location":"Linux%20Essentials/file-permissions/#reading-ls-l-output","title":"Reading ls -l Output","text":"<pre><code>$ ls -l\n-rwxr-xr-- 1 ryan developers 4096 Jan 15 10:30 deploy.sh\ndrwxrwxr-x 3 ryan developers 4096 Jan 15 10:30 scripts/\nlrwxrwxrwx 1 ryan developers   12 Jan 15 10:30 link -&gt; target\n</code></pre> <p>Breaking down <code>-rwxr-xr--</code>:</p> Position Meaning Value 1 File type <code>-</code> regular, <code>d</code> directory, <code>l</code> symlink 2-4 User permissions <code>rwx</code> (read, write, execute) 5-7 Group permissions <code>r-x</code> (read, execute) 8-10 Other permissions <code>r--</code> (read only) <p>The remaining columns show: link count, owner, group, size, modification time, and name.</p>"},{"location":"Linux%20Essentials/file-permissions/#chmod-changing-permissions","title":"chmod - Changing Permissions","text":""},{"location":"Linux%20Essentials/file-permissions/#symbolic-mode","title":"Symbolic Mode","text":"<p>Symbolic mode uses letters to add, remove, or set permissions:</p> <pre><code>chmod u+x script.sh        # add execute for user\nchmod g+rw file.txt         # add read and write for group\nchmod o-w file.txt          # remove write for others\nchmod a+r file.txt          # add read for all (user, group, other)\nchmod go-rwx secret.txt     # remove all permissions for group and others\nchmod u+x,g+r file.txt     # multiple changes at once\nchmod u=rwx,g=rx,o=r file   # set exact permissions\n</code></pre> <p>The operators: - <code>+</code> adds permissions - <code>-</code> removes permissions - <code>=</code> sets exact permissions (removes anything not specified)</p>"},{"location":"Linux%20Essentials/file-permissions/#octal-mode","title":"Octal Mode","text":"<p>Each permission has a numeric value:</p> Permission Value Read (r) 4 Write (w) 2 Execute (x) 1 <p>Add the values together for each level. Three digits represent user, group, and other:</p> Octal Symbolic Meaning <code>755</code> <code>rwxr-xr-x</code> Owner full, others read/execute <code>644</code> <code>rw-r--r--</code> Owner read/write, others read only <code>700</code> <code>rwx------</code> Owner full, others nothing <code>600</code> <code>rw-------</code> Owner read/write, others nothing <code>775</code> <code>rwxrwxr-x</code> Owner/group full, others read/execute <code>666</code> <code>rw-rw-rw-</code> Everyone read/write (usually a bad idea) <p>To calculate octal permissions from a symbolic string like <code>rwxr-xr-x</code>, work through each group of three:</p> <ul> <li>User <code>rwx</code>: r(4) + w(2) + x(1) = 7</li> <li>Group <code>r-x</code>: r(4) + 0 + x(1) = 5</li> <li>Other <code>r-x</code>: r(4) + 0 + x(1) = 5</li> </ul> <p>Result: 755. Going the other direction, if someone tells you to <code>chmod 640</code>, break it down: 6 = r+w, 4 = r, 0 = nothing. So <code>640</code> means <code>rw-r-----</code> - the owner can read and write, the group can read, and others have no access.</p> <p>Use octal for exact permissions, symbolic for changes</p> <p>Octal mode (<code>chmod 644</code>) sets all permission bits at once - ideal for scripting where you need a known state. Symbolic mode (<code>chmod u+x</code>) adds or removes specific bits without affecting others - ideal for interactive use. When you \"know the number,\" use octal. When you \"just need to add execute,\" use symbolic.</p> <pre><code>chmod 755 script.sh\nchmod 644 config.txt\nchmod 600 id_rsa\n</code></pre> <p>What is the octal representation of rwxr-xr--? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/file-permissions/#recursive","title":"Recursive","text":"<pre><code>chmod -R 755 directory/     # apply to directory and all contents\n</code></pre> <p>Recursive chmod applies same permissions to files and directories</p> <p>Running <code>chmod -R 755</code> makes every file executable, which is almost never what you want. Use <code>find</code> to set directories to <code>755</code> and files to <code>644</code> separately: <code>find path -type d -exec chmod 755 {} +</code> followed by <code>find path -type f -exec chmod 644 {} +</code>.</p> <p>Be careful with recursive chmod. You usually don't want the same permissions on files and directories (files shouldn't be executable unless they're scripts). A common pattern:</p> <pre><code># Set directories to 755, files to 644\nfind /var/www -type d -exec chmod 755 {} +\nfind /var/www -type f -exec chmod 644 {} +\n</code></pre> <p>Command Builder (requires JavaScript)</p>"},{"location":"Linux%20Essentials/file-permissions/#chown-and-chgrp","title":"chown and chgrp","text":""},{"location":"Linux%20Essentials/file-permissions/#chown-change-owner","title":"chown - Change Owner","text":"<pre><code>chown ryan file.txt              # change owner\nchown ryan:developers file.txt   # change owner and group\nchown :developers file.txt       # change group only\nchown -R ryan:developers dir/    # recursive\n</code></pre>"},{"location":"Linux%20Essentials/file-permissions/#chgrp-change-group","title":"chgrp - Change Group","text":"<pre><code>chgrp developers file.txt        # change group\nchgrp -R developers dir/         # recursive\n</code></pre> <p>Only root can chown files</p> <p>Regular users cannot change file ownership - only root can run <code>chown</code>. This prevents users from evading disk quotas by giving files away and from creating setuid binaries owned by other users. Regular users can use <code>chgrp</code> to change a file's group, but only to a group they belong to.</p> <p>Only root can change a file's owner. Regular users can change the group to any group they belong to.</p> <p>The reason only root can change file ownership is to prevent two abuses. First, quota bypass: if users could give their files to other users, they could evade disk quotas by assigning large files to someone else's account. Second, setuid abuse: if a user could create a program, set the setuid bit, then change ownership to root, they'd have a root-owned setuid binary - an instant privilege escalation. Restricting <code>chown</code> to root prevents both scenarios.</p> <p>Why can't regular users use chown to change a file's owner? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/file-permissions/#umask","title":"umask","text":"<p>The <code>umask</code> sets the default permissions for newly created files and directories. It works by masking (removing) permissions from the maximum defaults.</p> <p>The maximum defaults are: - Files: <code>666</code> (no execute by default) - Directories: <code>777</code></p> <p>The umask is bitwise masked from these maximums. You can think of it as subtraction for common values, but it's technically a bitwise AND NOT operation:</p> umask File permissions Directory permissions <code>022</code> <code>644</code> (rw-r--r--) <code>755</code> (rwxr-xr-x) <code>027</code> <code>640</code> (rw-r-----) <code>750</code> (rwxr-x---) <code>077</code> <code>600</code> (rw-------) <code>700</code> (rwx------) <code>002</code> <code>664</code> (rw-rw-r--) <code>775</code> (rwxrwxr-x) <p>Here's how the mask works concretely. With a umask of <code>027</code>:</p> <ul> <li>Files: start with <code>666</code>, mask off <code>027</code>. The result is <code>640</code> (<code>rw-r-----</code>). The owner keeps read/write, the group keeps read, and others get nothing.</li> <li>Directories: start with <code>777</code>, mask off <code>027</code>. The result is <code>750</code> (<code>rwxr-x---</code>). The owner gets full access, the group can read and enter, others are locked out.</li> </ul> <p>Technically, the umask is a bitwise AND NOT operation (<code>default AND (NOT umask)</code>), but thinking of it as 'subtract these permissions' gives the right answer for all common values. To make the umask permanent, add it to your <code>~/.bashrc</code> or <code>~/.profile</code>:</p> <pre><code>umask 027\n</code></pre> <p>In scripts, setting a restrictive umask at the top ensures any files the script creates are protected by default.</p> <pre><code>umask            # display current umask\numask 022        # set umask\numask -S         # display in symbolic form (u=rwx,g=rx,o=rx)\n</code></pre> <p>umask 027 is good for multi-user servers</p> <p>The default umask <code>022</code> lets everyone read your files. On shared servers, <code>027</code> is more appropriate: the owner gets full access, the group gets read, and others get nothing. Add <code>umask 027</code> to your <code>.bashrc</code> to make it permanent.</p> <p>The typical default is <code>022</code>, which gives the owner full access and everyone else read access.</p> <p>If the umask is 027, what permissions will a newly created regular file have? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/file-permissions/#special-permission-bits","title":"Special Permission Bits","text":"<p>Three additional permission bits exist beyond the standard read/write/execute.</p>"},{"location":"Linux%20Essentials/file-permissions/#setuid-4","title":"Setuid (4)","text":"<p>When set on an executable, the program runs with the permissions of the file's owner instead of the user who launched it.</p> <pre><code>ls -l /usr/bin/passwd\n-rwsr-xr-x 1 root root 68208 Mar 14 11:31 /usr/bin/passwd\n</code></pre> <p>The <code>s</code> in the user execute position means setuid is set. This is why regular users can run <code>passwd</code> to change their password - it needs to write to <code>/etc/shadow</code>, which is owned by root.</p> <pre><code>chmod u+s program          # set setuid\nchmod 4755 program         # set setuid with octal (note the leading 4)\n</code></pre> <p>setuid on scripts is ignored by modern Linux kernels</p> <p>The setuid bit only works on compiled binaries. Modern Linux kernels ignore setuid on interpreted scripts (<code>#!/bin/bash</code>, <code>#!/usr/bin/python</code>) because of a race condition between the kernel opening the script and the interpreter reading it. If you need elevated privileges from a script, use <code>sudo</code> with a properly configured sudoers entry.</p>"},{"location":"Linux%20Essentials/file-permissions/#setgid-2","title":"Setgid (2)","text":"<p>On executables, the program runs with the file's group permissions.</p> <p>On directories, new files created inside inherit the directory's group instead of the creating user's primary group. This is useful for shared directories.</p> <pre><code>ls -l\ndrwxrwsr-x 2 ryan developers 4096 Jan 15 10:30 shared/\n</code></pre> <p>The <code>s</code> in the group execute position indicates setgid.</p> <pre><code>chmod g+s directory/       # set setgid on directory\nchmod 2775 directory/      # set setgid with octal (note the leading 2)\n</code></pre>"},{"location":"Linux%20Essentials/file-permissions/#sticky-bit-1","title":"Sticky Bit (1)","text":"<p>When set on a directory, only the file's owner (or root) can delete or rename files within it. Other users can create files but can't delete each other's files.</p> <p>The classic example is <code>/tmp</code>:</p> <pre><code>ls -ld /tmp\ndrwxrwxrwt 15 root root 4096 Jan 15 10:30 /tmp/\n</code></pre> <p>The <code>t</code> in the other execute position indicates the sticky bit.</p> <pre><code>chmod +t directory/        # set sticky bit\nchmod 1777 directory/      # set sticky bit with octal (note the leading 1)\n</code></pre>"},{"location":"Linux%20Essentials/file-permissions/#special-bits-summary","title":"Special Bits Summary","text":"Bit Octal On Files On Directories Setuid 4 Run as file owner No effect Setgid 2 Run as file group New files inherit directory group Sticky 1 No effect Only owner can delete files <p>What does the sticky bit do on a directory? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/file-permissions/#combined","title":"Combined","text":"<p>The leading digit in octal mode sets all three:</p> <pre><code>chmod 4755 program      # setuid + rwxr-xr-x\nchmod 2775 shared/      # setgid + rwxrwxr-x\nchmod 1777 /tmp         # sticky + rwxrwxrwx\n</code></pre>"},{"location":"Linux%20Essentials/file-permissions/#practical-examples","title":"Practical Examples","text":"<p>SSH refuses keys with incorrect permissions silently</p> <p>If your private key file is readable by group or others, SSH will silently ignore it and fall back to password authentication (or fail entirely). You won't get an error message unless you use <code>ssh -v</code>. Always set private keys to <code>600</code> and <code>~/.ssh</code> to <code>700</code>.</p>"},{"location":"Linux%20Essentials/file-permissions/#securing-ssh-keys","title":"Securing SSH Keys","text":"<pre><code>chmod 700 ~/.ssh\nchmod 600 ~/.ssh/id_rsa          # private key - owner read/write only\nchmod 644 ~/.ssh/id_rsa.pub      # public key - readable by all\nchmod 600 ~/.ssh/authorized_keys\n</code></pre> <p>SSH Key Directory Security Setup (requires JavaScript)</p> <p>Set Up SSH Keys with Correct Permissions (requires JavaScript)</p>"},{"location":"Linux%20Essentials/file-permissions/#web-server-directory","title":"Web Server Directory","text":"<pre><code>chown -R www-data:www-data /var/www/html\nfind /var/www/html -type d -exec chmod 755 {} +\nfind /var/www/html -type f -exec chmod 644 {} +\n</code></pre>"},{"location":"Linux%20Essentials/file-permissions/#shared-project-directory","title":"Shared Project Directory","text":"<pre><code>mkdir /opt/project\nchown root:developers /opt/project\nchmod 2775 /opt/project    # setgid so new files belong to 'developers' group\n</code></pre> <p>Create a Shared Directory with setgid (requires JavaScript)</p>"},{"location":"Linux%20Essentials/file-permissions/#finding-permission-issues","title":"Finding Permission Issues","text":"<p>Find world-writable files as a security audit</p> <p>Run <code>find / -type f -perm -002 2&gt;/dev/null</code> periodically to discover files writable by any user. World-writable files outside <code>/tmp</code> are potential security risks - an attacker who gains limited access can modify them to escalate privileges.</p> <pre><code># Find world-writable files\nfind / -type f -perm -002 2&gt;/dev/null\n\n# Find setuid programs\nfind / -type f -perm -4000 2&gt;/dev/null\n\n# Find files not owned by any user\nfind / -nouser 2&gt;/dev/null\n</code></pre>"},{"location":"Linux%20Essentials/file-permissions/#further-reading","title":"Further Reading","text":"<ul> <li>GNU Coreutils Manual - official documentation for chmod, chown, and other file utilities</li> <li>POSIX File Permissions - portable specification for file permission semantics</li> <li>Linux man-pages Project - manual pages for chmod(1), chown(1), and related system calls</li> </ul> <p>Previous: Finding Files | Next: Job Control | Back to Index</p>"},{"location":"Linux%20Essentials/finding-files/","title":"Finding Files","text":"<p>The <code>find</code> command searches directory trees for files matching specified criteria. Combined with <code>xargs</code>, it forms a powerful pattern for batch operations on files.</p>"},{"location":"Linux%20Essentials/finding-files/#find","title":"find","text":""},{"location":"Linux%20Essentials/finding-files/#basic-usage","title":"Basic Usage","text":"<pre><code>find /path/to/search -name \"*.txt\"     # find by filename pattern\nfind . -type f                          # all regular files\nfind . -type d                          # all directories\nfind /var/log -name \"*.log\"             # absolute path search\n</code></pre> <p>The general form is <code>find [path] [expression]</code>. If you omit the path, <code>find</code> searches the current directory.</p>"},{"location":"Linux%20Essentials/finding-files/#tests","title":"Tests","text":"<p>By name:</p> <pre><code>find . -name \"*.conf\"          # case-sensitive name match\nfind . -iname \"readme*\"        # case-insensitive name match\nfind . -path \"*/src/*.js\"      # match against the full path\n</code></pre> <p>Use -iname for case-insensitive matching</p> <p>File naming conventions vary across projects and platforms. Use <code>-iname</code> instead of <code>-name</code> when you're unsure about capitalization: <code>find . -iname 'readme*'</code> matches <code>README.md</code>, <code>Readme.txt</code>, and <code>readme.rst</code> all at once.</p> <p>By type:</p> Flag Type <code>-type f</code> Regular file <code>-type d</code> Directory <code>-type l</code> Symbolic link <code>-type b</code> Block device <code>-type c</code> Character device <code>-type p</code> Named pipe (FIFO) <code>-type s</code> Socket <p>In practice, you'll use <code>-type f</code> (regular files) and <code>-type d</code> (directories) constantly, and <code>-type l</code> (symlinks) occasionally. The others are rare: <code>-type b</code> (block devices) for finding disk devices in <code>/dev</code>, <code>-type c</code> (character devices) for things like terminal devices and <code>/dev/null</code>, <code>-type p</code> (named pipes) when debugging inter-process communication, and <code>-type s</code> (sockets) when tracking down Unix domain sockets used by services like MySQL or Docker.</p> <p>By size:</p> <pre><code>find . -size +10M              # larger than 10 megabytes\nfind . -size -1k               # smaller than 1 kilobyte\nfind . -size 100c              # exactly 100 bytes\n</code></pre> <p>Size suffixes: <code>c</code> (bytes), <code>k</code> (kilobytes), <code>M</code> (megabytes), <code>G</code> (gigabytes). Without a suffix, the unit is 512-byte blocks.</p> <p>By time:</p> <p>-mtime counts in 24-hour periods, not calendar days</p> <p><code>find -mtime +7</code> means \"more than 7 full 24-hour periods ago,\" not \"more than 7 calendar days.\" A file modified 7.5 days ago has an mtime of 7 (truncated), so <code>+7</code> won't match it - you'd need <code>+6</code>. For minute-level precision, use <code>-mmin</code> instead.</p> <p>Timestamps are measured in 24-hour periods. <code>+7</code> means \"more than 7 days ago\", <code>-1</code> means \"within the last day\", and <code>7</code> (no sign) means \"between exactly 7 and 8 days ago.\"</p> <pre><code>find . -mtime -7               # modified within the last 7 days\nfind . -mtime +30              # modified more than 30 days ago\nfind . -atime -1               # accessed within the last day\nfind . -ctime +90              # metadata changed more than 90 days ago\nfind . -newer reference.txt    # modified more recently than reference.txt\n</code></pre> <p>For minute-level precision, use <code>-mmin</code>, <code>-amin</code>, <code>-cmin</code>:</p> <pre><code>find . -mmin -60               # modified within the last 60 minutes\n</code></pre> <p>By permissions:</p> <pre><code>find . -perm 644               # exactly 644\nfind . -perm -644              # at least these permissions (all specified bits set)\nfind . -perm /111              # any execute bit set (user, group, or other)\nfind . -perm -u+x              # user execute bit set\n</code></pre> <p>The three <code>-perm</code> modes correspond to different questions. <code>-perm 644</code> (exact) asks 'are the permissions exactly 644?' - nothing more, nothing less. <code>-perm -644</code> (dash prefix, all-bits) asks 'are at least these bits set?' - the file could have more permissions than specified. <code>-perm /111</code> (slash prefix, any-bit) asks 'is any of these bits set?' - useful for finding anything executable. Think of exact as '=', dash as 'includes all of', and slash as 'includes any of'.</p> <p>What is the difference between find -perm 644 and find -perm -644? (requires JavaScript)</p> <p>By owner:</p> <pre><code>find . -user root              # owned by root\nfind . -group www-data         # group is www-data\nfind . -nouser                 # files with no matching user in /etc/passwd\nfind . -nogroup                # files with no matching group\n</code></pre>"},{"location":"Linux%20Essentials/finding-files/#depth-control","title":"Depth Control","text":"<pre><code>find . -maxdepth 1             # current directory only (no recursion)\nfind . -maxdepth 2             # at most 2 levels deep\nfind . -mindepth 1             # skip the starting directory itself\nfind . -mindepth 2 -maxdepth 3 # between 2 and 3 levels deep\n</code></pre> <p>Use -maxdepth 1 for current directory only</p> <p><code>find . -maxdepth 1 -type f</code> lists files in the current directory without recursing into subdirectories. This is often more reliable than <code>ls</code> parsing for scripts, and you can combine it with other find tests like <code>-name</code> or <code>-mtime</code>.</p> <p>What does find /home -maxdepth 1 -type d do? (requires JavaScript)</p> <p>find Depth and Type Filtering (requires JavaScript)</p>"},{"location":"Linux%20Essentials/finding-files/#logical-operators","title":"Logical Operators","text":"<pre><code>find . -name \"*.txt\" -and -size +1M    # both conditions (-and is implicit)\nfind . -name \"*.txt\" -or -name \"*.md\"  # either condition\nfind . ! -name \"*.tmp\"                 # negation\nfind . \\( -name \"*.txt\" -or -name \"*.md\" \\) -and -mtime -7  # grouping\n</code></pre> <p>Note that <code>-and</code> is the default operator between tests. When you write <code>find . -name '*.txt' -size +1M</code>, the <code>-and</code> is implicit - both conditions must be true. You only need to write <code>-and</code> explicitly for readability, or when combining it with <code>-or</code> and grouping.</p>"},{"location":"Linux%20Essentials/finding-files/#actions","title":"Actions","text":"<p><code>-exec</code> (per file):</p> <pre><code>find . -name \"*.tmp\" -exec rm {} \\;\n</code></pre> <p>The <code>{}</code> is replaced with each filename. The <code>\\;</code> marks the end of the command. One <code>rm</code> process runs per file.</p> <p><code>-exec</code> (batching):</p> <pre><code>find . -name \"*.tmp\" -exec rm {} +\n</code></pre> <p>The <code>+</code> passes as many filenames as possible to a single command invocation. This is much faster when operating on many files.</p> <p>-exec {} + doesn't work with commands needing a single filename</p> <p>The <code>+</code> terminator batches multiple filenames into one command invocation. This means the command sees all files as arguments at once. Commands like <code>mv</code> that expect a specific filename argument structure need <code>\\;</code> (one invocation per file) or <code>xargs -I {}</code> for placeholder substitution.</p> <p>The performance difference between <code>\\;</code> and <code>+</code> is significant. With <code>\\;</code>, find spawns a new process for every single file. If you're operating on 1000 files, that's 1000 separate <code>rm</code> processes. With <code>+</code>, find passes as many filenames as will fit on one command line, so 1000 files might be handled in a single <code>rm</code> invocation. The limit on how many arguments <code>+</code> can batch is determined by <code>ARG_MAX</code> (the kernel's maximum argument length, typically 2MB on modern Linux). You can check it with <code>getconf ARG_MAX</code>. For very large file sets, <code>+</code> will make multiple invocations as needed to stay within this limit.</p> <p>What is the difference between find -exec cmd {} \\; and find -exec cmd {} +? (requires JavaScript)</p> <p><code>-delete</code>:</p> <pre><code>find . -name \"*.tmp\" -delete\n</code></pre> <p>Built-in deletion - faster than <code>-exec rm</code>. Note that <code>-delete</code> implies <code>-depth</code> (processes files before their parent directories).</p> <p>-delete implies -depth processing order</p> <p>When you use <code>-delete</code>, find automatically enables <code>-depth</code> mode, processing directory contents before the directory itself. This changes the order that other expressions see files. If your command combines <code>-delete</code> with <code>-prune</code>, they will conflict - <code>-prune</code> needs breadth-first traversal, but <code>-delete</code> forces depth-first.</p> <p><code>-print0</code>:</p> <pre><code>find . -name \"*.txt\" -print0\n</code></pre> <p>Separates results with null characters instead of newlines. This handles filenames containing spaces, newlines, or other special characters. Pair with <code>xargs -0</code>.</p> <p>Always pair -print0 with xargs -0</p> <p>The null byte is the only character that cannot appear in a Unix filename. Using <code>-print0 | xargs -0</code> is the only fully safe way to pass filenames between commands. Regular newline-delimited output breaks on the (legal) filename <code>my\\nfile.txt</code>.</p>"},{"location":"Linux%20Essentials/finding-files/#practical-examples","title":"Practical Examples","text":"<pre><code># Delete files older than 30 days\nfind /tmp -type f -mtime +30 -delete\n\n# Find large files\nfind / -type f -size +100M 2&gt;/dev/null\n\n# Find and chmod directories\nfind . -type d -exec chmod 755 {} +\n\n# Find empty files and directories\nfind . -empty\n\n# Find zero-byte files only\nfind . -type f -empty\n\n# Find broken symlinks\nfind . -xtype l\n\n# Find files modified today\nfind . -daystart -mtime -1\n\n# Find setuid programs\nfind / -perm -4000 -type f 2&gt;/dev/null\n</code></pre> <p>Find Large Log Files (requires JavaScript)</p> <p>Fix File Permissions Recursively (requires JavaScript)</p> <p>Command Builder (requires JavaScript)</p>"},{"location":"Linux%20Essentials/finding-files/#xargs","title":"xargs","text":"<p><code>xargs</code> reads items from STDIN and passes them as arguments to a command. It bridges the gap between commands that produce output (like <code>find</code>) and commands that expect arguments.</p>"},{"location":"Linux%20Essentials/finding-files/#basic-usage_1","title":"Basic Usage","text":"<pre><code>echo \"file1 file2 file3\" | xargs rm\n# equivalent to: rm file1 file2 file3\n</code></pre>"},{"location":"Linux%20Essentials/finding-files/#safe-filename-handling","title":"Safe Filename Handling","text":"<p>The default <code>xargs</code> splits on whitespace, which breaks on filenames with spaces. Use null-delimited input:</p> <pre><code>find . -name \"*.txt\" -print0 | xargs -0 rm\nfind . -name \"*.log\" -print0 | xargs -0 grep \"error\"\n</code></pre> <p>The <code>-print0</code> / <code>-0</code> pair is the standard pattern for safely processing arbitrary filenames.</p> <p>Why would you use find -print0 | xargs -0 instead of find | xargs? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/finding-files/#placeholder-substitution","title":"Placeholder Substitution","text":"<p>Use <code>-I {}</code> to control where arguments are placed:</p> <pre><code>find . -name \"*.bak\" | xargs -I {} mv {} /tmp/backups/\n</code></pre> <p>With <code>-I</code>, xargs runs the command once per input line (not batched).</p>"},{"location":"Linux%20Essentials/finding-files/#argument-batching","title":"Argument Batching","text":"<p>Control how many arguments are passed at once:</p> <pre><code>echo \"1 2 3 4 5 6\" | xargs -n 2 echo\n# echo 1 2\n# echo 3 4\n# echo 5 6\n</code></pre>"},{"location":"Linux%20Essentials/finding-files/#parallel-execution","title":"Parallel Execution","text":"<p>Use <code>-P</code> to run multiple processes in parallel:</p> <pre><code>find . -name \"*.png\" -print0 | xargs -0 -P 4 -I {} convert {} -resize 50% {}\n</code></pre> <p>This runs up to 4 <code>convert</code> processes at a time.</p> <p>xargs -P with output-producing commands causes interleaved output</p> <p>When using <code>xargs -P</code> for parallel execution, output from concurrent processes can mix together unpredictably. This is safe for silent operations like <code>gzip</code> or <code>chmod</code>, but produces garbled results for commands like <code>grep</code> or <code>wc</code>. For parallel output, consider GNU <code>parallel</code> which buffers output per job.</p> <p>A few things to be aware of with parallel <code>xargs</code>. First, output interleaving: when multiple processes write to the terminal simultaneously, their output lines can mix together. This is fine for operations that don't produce output (like <code>gzip</code> or <code>chmod</code>), but problematic for commands that do. Second, choosing a <code>-P</code> value: a good starting point is the number of CPU cores (<code>nproc</code>), but for I/O-bound tasks you can often go higher. Third, safety: parallel execution is safe when each invocation operates on independent files. It's risky when operations have side effects that interact - for example, parallel appends to the same log file will produce garbled output.</p>"},{"location":"Linux%20Essentials/finding-files/#confirmation","title":"Confirmation","text":"<p>Use <code>-p</code> to prompt before each execution:</p> <pre><code>find . -name \"*.tmp\" | xargs -p rm\n# rm file1.tmp file2.tmp?...y\n</code></pre>"},{"location":"Linux%20Essentials/finding-files/#practical-examples_1","title":"Practical Examples","text":"<pre><code># Delete files found by grep\ngrep -rl \"deprecated\" src/ | xargs rm\n\n# Count lines in all Python files\nfind . -name \"*.py\" -print0 | xargs -0 wc -l\n\n# Compress files in parallel\nfind . -name \"*.log\" -print0 | xargs -0 -P 4 gzip\n\n# Search for a pattern in files found by find\nfind . -name \"*.conf\" -print0 | xargs -0 grep -l \"listen\"\n\n# Rename files\nls *.jpeg | xargs -I {} bash -c 'mv \"$1\" \"${1%.jpeg}.jpg\"' _ {}\n</code></pre>"},{"location":"Linux%20Essentials/finding-files/#find-exec-vs-xargs","title":"find -exec vs xargs","text":"<p>Both can run commands on found files. The differences:</p> Feature <code>find -exec {} \\;</code> <code>find -exec {} +</code> <code>find | xargs</code> Process per file Yes No (batched) No (batched) Handles special filenames Yes Yes Only with <code>-print0 \\| xargs -0</code> Parallel execution No No Yes (<code>-P</code>) Speed Slowest Fast Fast <p>For most tasks, <code>find -exec {} +</code> is the simplest safe option. Use <code>xargs</code> when you need parallel execution or more control over argument handling.</p> <p>Command Builder (requires JavaScript)</p>"},{"location":"Linux%20Essentials/finding-files/#further-reading","title":"Further Reading","text":"<ul> <li>GNU Findutils Manual - official documentation for find, xargs, and locate</li> <li>Linux man-pages Project - comprehensive manual pages including find(1) and xargs(1)</li> </ul> <p>Previous: Text Processing | Next: File Permissions | Back to Index</p>"},{"location":"Linux%20Essentials/job-control/","title":"Job Control","text":"<p>Job control lets you manage multiple processes from a single terminal - running tasks in the background, pausing them, and switching between them. Combined with knowledge of signals and terminal multiplexers, you can keep long-running processes alive and organized.</p>"},{"location":"Linux%20Essentials/job-control/#foreground-and-background-processes","title":"Foreground and Background Processes","text":"<p>By default, when you run a command, it runs in the foreground. Your terminal waits for it to finish before giving you a new prompt.</p> <p>A background process runs without blocking your terminal.</p>"},{"location":"Linux%20Essentials/job-control/#starting-a-background-process","title":"Starting a Background Process","text":"<p>Append <code>&amp;</code> to run a command in the background:</p> <pre><code>sleep 300 &amp;\n# [1] 12345    (job number and PID)\n</code></pre> <p>The shell prints the job number in brackets and the process ID. You get your prompt back immediately.</p> <p>Background processes still output to the terminal</p> <p>Running a command with <code>&amp;</code> only detaches it from stdin. The process can still write to stdout and stderr, which will appear in your terminal unexpectedly. Redirect output when backgrounding noisy commands: <code>command &gt; output.log 2&gt;&amp;1 &amp;</code>.</p>"},{"location":"Linux%20Essentials/job-control/#viewing-jobs","title":"Viewing Jobs","text":"<pre><code>jobs           # list all jobs in current shell\njobs -l        # include PIDs\n</code></pre> <pre><code>[1]+  Running                 sleep 300 &amp;\n[2]-  Stopped                 vim notes.txt\n</code></pre> <p>The <code>+</code> marks the current job (default target for <code>fg</code>/<code>bg</code>). The <code>-</code> marks the previous job.</p>"},{"location":"Linux%20Essentials/job-control/#switching-between-foreground-and-background","title":"Switching Between Foreground and Background","text":"<p><code>Ctrl-Z</code> suspends (pauses) the current foreground process:</p> <pre><code>vim notes.txt      # editing a file\n# press Ctrl-Z\n# [1]+  Stopped                 vim notes.txt\n</code></pre> <p><code>bg</code> resumes a stopped job in the background:</p> <pre><code>bg              # resume most recently stopped job\nbg %2           # resume job number 2\n</code></pre> <p><code>fg</code> brings a job to the foreground:</p> <pre><code>fg              # bring most recent job to foreground\nfg %1           # bring job 1 to foreground\n</code></pre> <p>Job Suspend/Background/Foreground Workflow (requires JavaScript)</p>"},{"location":"Linux%20Essentials/job-control/#job-references","title":"Job References","text":"Reference Meaning <code>%1</code> Job number 1 <code>%+</code> or <code>%%</code> Current job <code>%-</code> Previous job <code>%string</code> Job whose command starts with \"string\" <code>%?string</code> Job whose command contains \"string\" <p>Use %?string to reference jobs by partial command</p> <p>Instead of remembering job numbers, use <code>%?</code> to match by substring: <code>fg %?deploy</code> brings back the job whose command contains \"deploy.\" This is faster than running <code>jobs</code> to look up the number, especially when you have several background jobs.</p>"},{"location":"Linux%20Essentials/job-control/#signals","title":"Signals","text":"<p>Signals are software interrupts sent to processes. They tell a process to do something - usually to stop, pause, or terminate.</p>"},{"location":"Linux%20Essentials/job-control/#common-signals","title":"Common Signals","text":"Signal Number Default Action Meaning <code>SIGHUP</code> 1 Terminate Hangup - terminal closed <code>SIGINT</code> 2 Terminate Interrupt - <code>Ctrl-C</code> <code>SIGQUIT</code> 3 Core dump Quit - <code>Ctrl-\\</code> <code>SIGKILL</code> 9 Terminate Kill - cannot be caught or ignored <code>SIGTERM</code> 15 Terminate Terminate - polite request to exit <code>SIGTSTP</code> 20 Stop Terminal stop - <code>Ctrl-Z</code> <code>SIGCONT</code> 18 Continue Resume a stopped process <code>SIGUSR1</code> 10 Terminate User-defined signal 1 <code>SIGUSR2</code> 12 Terminate User-defined signal 2 <p>SIGKILL is unique because the kernel handles it directly - the signal is never delivered to the process. The kernel simply removes the process from its scheduling table. This is why SIGKILL can't be caught, ignored, or handled: the process never gets a chance to run any code in response. It also means the process can't clean up - temporary files stay behind, network connections are left half-open, and shared resources may be left in an inconsistent state.</p> <p>SIGQUIT produces a core dump in addition to terminating the process. This is useful for debugging a hung process: if a program is stuck and you want to analyze what it was doing, press <code>Ctrl-\\</code> to send SIGQUIT. The resulting core file can be loaded into <code>gdb</code> for post-mortem analysis. In normal usage, you'll send SIGTERM (polite request) first, SIGQUIT (terminate with dump) if you need diagnostic information, and SIGKILL (force kill) only as a last resort.</p> <p>What is the difference between Ctrl-C and Ctrl-\\? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/job-control/#sending-signals","title":"Sending Signals","text":"<p><code>kill</code> sends a signal to a process by PID:</p> <pre><code>kill 12345            # sends SIGTERM (default)\nkill -15 12345        # same thing, explicit\nkill -TERM 12345      # same thing, by name\nkill -9 12345         # sends SIGKILL (cannot be caught)\nkill -KILL 12345      # same thing, by name\n</code></pre> <p>SIGKILL gives no chance for cleanup</p> <p><code>kill -9</code> terminates the process immediately at the kernel level. Temporary files are left behind, database transactions remain uncommitted, lock files are not removed, and child processes may be orphaned. Always try <code>kill</code> (SIGTERM) first and wait a few seconds before resorting to <code>kill -9</code>.</p> <p>Always try <code>SIGTERM</code> first. It gives the process a chance to clean up (close files, remove temp files, etc.). Only use <code>SIGKILL</code> as a last resort - the process gets no chance to clean up.</p> <p><code>killall</code> kills processes by name:</p> <pre><code>killall nginx          # SIGTERM to all nginx processes\nkillall -9 python3     # SIGKILL to all python3 processes\n</code></pre> <p><code>pkill</code> kills processes matching a pattern:</p> <pre><code>pkill -f \"python script.py\"    # match against full command line\npkill -u ryan                   # kill all processes owned by ryan\npkill -t pts/2                  # kill all processes on terminal pts/2\n</code></pre>"},{"location":"Linux%20Essentials/job-control/#listing-signals","title":"Listing Signals","text":"<pre><code>kill -l             # list all signal names and numbers\n</code></pre> <p>Why should you try kill (SIGTERM) before kill -9 (SIGKILL)? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/job-control/#keeping-processes-running","title":"Keeping Processes Running","text":"<p>When you close a terminal, the shell sends <code>SIGHUP</code> to all its child processes, which usually terminates them. There are several ways to prevent this.</p>"},{"location":"Linux%20Essentials/job-control/#nohup","title":"nohup","text":"<p><code>nohup</code> runs a command immune to hangup signals:</p> <pre><code>nohup long_running_script.sh &amp;\n</code></pre> <p>Output goes to <code>nohup.out</code> by default if STDOUT isn't redirected. Better to redirect explicitly:</p> <pre><code>nohup ./script.sh &gt; output.log 2&gt;&amp;1 &amp;\n</code></pre>"},{"location":"Linux%20Essentials/job-control/#disown","title":"disown","text":"<p>If you forgot to start a process with <code>nohup</code>, you can use <code>disown</code> to remove it from the shell's job table:</p> <pre><code>./long_process.sh &amp;\ndisown %1          # remove job 1 from shell's job table\ndisown -h %1       # keep in job table but don't send SIGHUP on exit\n</code></pre> <p>Use disown when you forgot nohup</p> <p>Started a long process and now need to log out? Press <code>Ctrl-Z</code>, then <code>bg</code> to resume it in the background, then <code>disown %1</code> to detach it from your shell. The process will continue running after you close the terminal.</p> <p>The key difference: <code>nohup</code> is preventive - you use it before starting a process. <code>disown</code> is reactive - you use it after a process is already running and you realize you need it to survive terminal closure. A common workflow: you start a long build, realize you need to log out, press <code>Ctrl-Z</code> to suspend it, <code>bg</code> to resume in the background, and <code>disown</code> to detach it from the shell. If you'd planned ahead, you would have used <code>nohup</code> from the start.</p> <p>What is the difference between nohup and disown? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/job-control/#process-information","title":"Process Information","text":""},{"location":"Linux%20Essentials/job-control/#ps","title":"ps","text":"<p><code>ps</code> shows a snapshot of current processes:</p> <pre><code>ps                     # processes in current terminal\nps aux                 # all processes, all users, detailed\nps -ef                 # same thing, different format (POSIX)\nps aux --sort=-%mem    # sort by memory usage (descending)\nps -u ryan             # processes owned by ryan\nps -p 12345            # info on specific PID\n</code></pre> <p>Key columns in <code>ps aux</code>:</p> Column Meaning <code>USER</code> Process owner <code>PID</code> Process ID <code>%CPU</code> CPU usage percentage <code>%MEM</code> Memory usage percentage <code>VSZ</code> Virtual memory size (KB) <code>RSS</code> Resident set size - physical memory (KB) <code>TTY</code> Controlling terminal <code>STAT</code> Process state <code>START</code> Start time <code>TIME</code> Cumulative CPU time <code>COMMAND</code> Command that started the process <p>Check VSZ vs RSS for memory usage</p> <p>RSS (Resident Set Size) shows actual physical RAM usage - this is what matters for system capacity. VSZ (Virtual Memory Size) includes mapped but unused memory and is almost always much larger. When checking if a process is consuming too much memory, look at RSS.</p> <p>VSZ (Virtual Memory Size) is the total amount of memory the process has mapped, including shared libraries, memory-mapped files, and memory that's been allocated but never used. RSS (Resident Set Size) is how much physical RAM the process is actually using right now. VSZ is almost always larger than RSS because virtual memory includes pages that haven't been loaded from disk yet and shared libraries counted in full even though they're shared with other processes. When checking if a process is using too much memory, look at RSS. When checking if a process might run into address space limits, look at VSZ.</p> <p>Process states in the <code>STAT</code> column:</p> State Meaning <code>R</code> Running <code>S</code> Sleeping (waiting for event) <code>D</code> Uninterruptible sleep (usually I/O) <code>T</code> Stopped <code>Z</code> Zombie (finished but parent hasn't collected status) <p>D-state processes cannot be killed, even with SIGKILL</p> <p>A process in D (uninterruptible sleep) state is waiting for kernel-level I/O that cannot be interrupted. No signal, including <code>SIGKILL</code>, can terminate it until the I/O completes. If you see stuck D-state processes, the root cause is usually failing hardware or an unresponsive NFS mount.</p> <p>The D state (uninterruptible sleep) deserves special attention. A process in D state is waiting on I/O that the kernel considers non-interruptible - typically disk or network I/O at the kernel level. You cannot kill a D-state process, not even with SIGKILL, because the kernel won't deliver signals to it until the I/O completes. Processes stuck in D state are commonly seen with NFS mounts that have become unreachable or failing disk drives. If you see many processes in D state, investigate your storage and network mounts.</p> <p>Zombie processes can't be killed directly</p> <p>A zombie (Z state) has already exited - there's no process to kill. The zombie entry exists only because the parent hasn't called <code>wait()</code> to collect the exit status. To clear zombies, kill or restart the parent process. The <code>init</code>/<code>systemd</code> process will then adopt and reap the orphaned zombies.</p> <p>The Z state (zombie) means the process has finished executing but its parent hasn't called <code>wait()</code> to collect its exit status. The zombie takes up no resources other than a process table entry. A few zombies are harmless, but a large accumulation suggests the parent process has a bug. You can't kill a zombie directly - killing the parent (or restarting it) clears them, because <code>init</code>/<code>systemd</code> adopts orphaned processes and reaps their exit status.</p> <p>What does a process state of D (uninterruptible sleep) typically mean? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/job-control/#top-and-htop","title":"top and htop","text":"<p><code>top</code> shows a live, updating view of processes:</p> <pre><code>top\n</code></pre> <p>Useful keystrokes inside <code>top</code>:</p> Key Action <code>q</code> Quit <code>M</code> Sort by memory <code>P</code> Sort by CPU <code>k</code> Kill a process (prompts for PID) <code>1</code> Toggle per-CPU display <code>c</code> Show full command line <p><code>htop</code> is an improved version of <code>top</code> with color, mouse support, and easier process management. It's not installed by default but is available in most package managers:</p> <pre><code>sudo apt install htop    # Debian/Ubuntu\nsudo dnf install htop    # Fedora/RHEL\nhtop\n</code></pre> <p>Find and Manage a Runaway Process (requires JavaScript)</p>"},{"location":"Linux%20Essentials/job-control/#terminal-multiplexers","title":"Terminal Multiplexers","text":"<p>Terminal multiplexers let you create multiple virtual terminals inside a single terminal session. Crucially, sessions persist even if your connection drops.</p>"},{"location":"Linux%20Essentials/job-control/#screen","title":"screen","text":"<p><code>screen</code> is the older, widely available multiplexer.</p> <pre><code>screen                     # start a new session\nscreen -S mywork           # start a named session\nscreen -ls                 # list sessions\nscreen -r mywork           # reattach to a session\nscreen -d -r mywork        # detach elsewhere and reattach here\n</code></pre> <p>Key bindings inside screen (prefixed with <code>Ctrl-a</code>):</p> Keys Action <code>Ctrl-a c</code> Create new window <code>Ctrl-a n</code> Next window <code>Ctrl-a p</code> Previous window <code>Ctrl-a d</code> Detach from session <code>Ctrl-a \"</code> List windows <code>Ctrl-a k</code> Kill current window <code>Ctrl-a [</code> Enter copy/scroll mode"},{"location":"Linux%20Essentials/job-control/#tmux","title":"tmux","text":"<p><code>tmux</code> is the more modern alternative with better scripting support and split panes.</p> <pre><code>tmux                       # start a new session\ntmux new -s mywork         # start a named session\ntmux ls                    # list sessions\ntmux attach -t mywork      # attach to a session\ntmux kill-session -t mywork # kill a session\n</code></pre> <p>Key bindings (prefixed with <code>Ctrl-b</code>):</p> Keys Action <code>Ctrl-b c</code> Create new window <code>Ctrl-b n</code> Next window <code>Ctrl-b p</code> Previous window <code>Ctrl-b d</code> Detach from session <code>Ctrl-b %</code> Split pane vertically <code>Ctrl-b \"</code> Split pane horizontally <code>Ctrl-b arrow</code> Switch between panes <code>Ctrl-b z</code> Toggle pane zoom (fullscreen) <code>Ctrl-b [</code> Enter copy mode <code>Ctrl-b w</code> List windows <code>Ctrl-b x</code> Kill current pane <p>tmux Session Management (requires JavaScript)</p>"},{"location":"Linux%20Essentials/job-control/#common-workflow","title":"Common Workflow","text":"<pre><code># Start a named session for a project\ntmux new -s deploy\n\n# Run a long task\n./deploy.sh\n\n# Detach (Ctrl-b d) and close the terminal\n\n# Later, reattach\ntmux attach -t deploy\n# Your process is still running\n</code></pre> <p>This is especially useful over SSH connections. If your connection drops, the tmux session keeps running. Just SSH back in and reattach.</p> <p>tmux over screen for new setups</p> <p>If you're choosing a terminal multiplexer for a new system, pick tmux. It has better split-pane support, a scriptable command interface, and more active development. Use screen only when it's already installed on a server and you can't install tmux.</p> <p>Which to choose? If you're setting up a new system, use tmux - it has better split-pane support, more intuitive configuration, and a scriptable command interface. Use screen if it's already installed on a server you're working with and you don't want to (or can't) install additional software. screen is nearly universal on older systems, while tmux may not be pre-installed. The core workflow (start a session, detach, reattach later) is the same in both.</p>"},{"location":"Linux%20Essentials/job-control/#further-reading","title":"Further Reading","text":"<ul> <li>tmux Wiki - terminal multiplexer documentation and FAQ</li> <li>GNU Screen Manual - official Screen user manual</li> <li>procps-ng - source and docs for ps, top, vmstat, free, and related utilities</li> <li>htop - interactive process viewer</li> <li>GNU Coreutils - nohup - run commands immune to hangups</li> <li>Linux man-pages - signal(7) - comprehensive signal reference</li> </ul> <p>Previous: File Permissions | Next: Scripting Fundamentals | Back to Index</p>"},{"location":"Linux%20Essentials/networking/","title":"Networking","text":"<p>This guide covers the essential networking tools for diagnostics, data transfer, remote access, and network inspection from the command line.</p>"},{"location":"Linux%20Essentials/networking/#connectivity-and-diagnostics","title":"Connectivity and Diagnostics","text":""},{"location":"Linux%20Essentials/networking/#ping","title":"ping","text":"<p><code>ping</code> sends ICMP echo requests to test whether a host is reachable and measure round-trip time.</p> <pre><code>ping google.com               # ping until you Ctrl-C\nping -c 5 google.com          # send 5 packets and stop\nping -i 0.5 google.com        # ping every 0.5 seconds (default is 1)\nping -W 2 192.168.1.1         # 2-second timeout per packet\nsudo ping -f google.com       # flood ping (requires root, sends as fast as possible)\n</code></pre> <p>Reading the output:</p> <pre><code>64 bytes from 142.250.80.46: icmp_seq=1 ttl=118 time=12.3 ms\n</code></pre> <ul> <li>bytes - packet size</li> <li>icmp_seq - sequence number (watch for gaps indicating packet loss)</li> <li>ttl - time to live (decremented by each router hop)</li> <li>time - round-trip latency</li> </ul> <p>The summary at the end shows packet loss percentage and min/avg/max/stddev latency.</p> <p>TTL starting values hint at the remote OS</p> <p>A TTL of 64 usually means Linux or macOS, 128 means Windows, and 255 means a network device (router/switch). Each hop decrements TTL by 1, so a received TTL of 118 likely means a Windows host 10 hops away (128 - 10 = 118). This is a quick heuristic, not a guarantee.</p> <p>Common TTL starting values can hint at the remote OS: 64 usually means Linux or macOS, 128 means Windows, and 255 means a network device (router, switch). Each router hop decrements TTL by 1, so a TTL of 118 likely means a Windows host 10 hops away (128 - 118 = 10). The stddev (standard deviation) in the summary indicates latency consistency - a low stddev means a stable connection, while a high stddev suggests network congestion or routing instability.</p>"},{"location":"Linux%20Essentials/networking/#traceroute-and-tracepath","title":"traceroute and tracepath","text":"<p><code>traceroute</code> shows the path packets take to reach a host, listing each router hop:</p> <pre><code>traceroute google.com\ntraceroute -n google.com      # don't resolve hostnames (faster)\n</code></pre> <p>Each line shows a hop number, the router address, and three round-trip times. Stars (<code>* * *</code>) mean the router didn't respond (common with routers that block ICMP).</p> <p><code>tracepath</code> is similar but doesn't require root and can discover MTU:</p> <pre><code>tracepath google.com\n</code></pre> <p>How traceroute works: it sends packets with incrementally increasing TTL (Time To Live) values. The first packet has TTL=1. The first router decrements it to 0, discards the packet, and sends back an ICMP \"Time Exceeded\" message - revealing its address. The next packet has TTL=2, making it one hop further before the same thing happens. This continues until the packet reaches the destination. Each hop shows three round-trip times because traceroute sends three probes per TTL value.</p> <p>Stars in traceroute don't always mean a problem</p> <p>Many routers are configured to not respond to ICMP or UDP probes, so <code>* * *</code> at a hop just means the router is silent - not that packets are being dropped. Only worry if all subsequent hops also show stars (indicating a true block) or if latency jumps sharply and stays high past a specific hop.</p> <p>Reading the output: stars (<code>* * *</code>) at a hop don't necessarily mean a problem - many routers are configured to not respond to these probes. A sudden jump in latency at a specific hop suggests congestion at that point. If latency increases at one hop but stays high for all subsequent hops, the bottleneck is at that hop. If latency spikes at one hop but returns to normal at the next, the router is just slow at responding to ICMP (not a real bottleneck).</p> <p>In traceroute output, what does the TTL (Time to Live) field actually count? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/networking/#mtr","title":"mtr","text":"<p><code>mtr</code> combines <code>ping</code> and <code>traceroute</code> into a continuous display:</p> <pre><code>mtr google.com                # live updating view\nmtr -n google.com             # don't resolve hostnames\nmtr -r -c 100 google.com     # report mode: send 100 packets and print summary\n</code></pre> <p><code>mtr</code> is excellent for diagnosing intermittent packet loss at specific hops.</p>"},{"location":"Linux%20Essentials/networking/#data-transfer","title":"Data Transfer","text":""},{"location":"Linux%20Essentials/networking/#curl","title":"curl","text":"<p><code>curl</code> transfers data to or from a server. It supports HTTP, HTTPS, FTP, and many other protocols.</p> <p>Basic requests:</p> <pre><code>curl https://example.com                    # GET request, output to terminal\ncurl -o file.html https://example.com       # save to file\ncurl -O https://example.com/file.tar.gz     # save with original filename\ncurl -s https://api.example.com/data        # silent mode (no progress bar)\n</code></pre> <p>What is the difference between curl -O and curl -o filename? (requires JavaScript)</p> <p>HTTP methods and headers:</p> <pre><code>curl -X POST https://api.example.com/users \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"name\": \"Ryan\", \"email\": \"ryan@example.com\"}'\n\ncurl -X PUT https://api.example.com/users/1 \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"name\": \"Updated\"}'\n\ncurl -X DELETE https://api.example.com/users/1\n</code></pre> <p>Authentication:</p> <pre><code>curl -u username:password https://api.example.com     # basic auth\ncurl -H \"Authorization: Bearer TOKEN\" https://api.example.com   # bearer token\n</code></pre> <p>Following redirects:</p> <pre><code>curl -L https://example.com      # follow HTTP redirects\n</code></pre> <p>Verbose and debugging:</p> <pre><code>curl -v https://example.com      # verbose output (headers, TLS details)\ncurl -I https://example.com      # HEAD request (headers only)\n</code></pre> <p>curl Verbose Request/Response (requires JavaScript)</p> <p>curl -w for custom output formats</p> <p>The <code>-w</code> (write-out) flag lets you extract specific response data: <code>curl -s -o /dev/null -w '%{http_code}'</code> gets just the status code, <code>%{time_total}</code> gives request duration, and <code>%{size_download}</code> gives response size. Combine with <code>-s -o /dev/null</code> to suppress all other output.</p> <p>HTTP method semantics in brief: GET reads a resource without changing anything, POST creates a new resource or triggers an action, PUT replaces an entire resource with the provided data, PATCH updates specific fields of a resource, and DELETE removes a resource. When debugging failed requests, start with <code>curl -v</code> to see the full request and response headers - the status code and response body usually tell you what went wrong. Common issues: <code>401</code> means your authentication is wrong, <code>403</code> means the server understood your credentials but you lack permission, <code>404</code> means the URL is wrong, and <code>422</code> means the request body doesn't match what the API expects.</p> <p>Downloading files:</p> <pre><code>curl -O -L https://example.com/archive.tar.gz        # follow redirects and save\ncurl -C - -O https://example.com/large.iso            # resume interrupted download\n</code></pre> <p>Command Builder (requires JavaScript)</p>"},{"location":"Linux%20Essentials/networking/#wget","title":"wget","text":"<p><code>wget</code> is designed for downloading files, including recursive downloads.</p> <pre><code>wget https://example.com/file.tar.gz           # download a file\nwget -q https://example.com/file.tar.gz        # quiet mode\nwget -c https://example.com/large.iso          # resume interrupted download\nwget -O output.txt https://example.com/data    # save with specific name\n</code></pre> <p>Recursive downloads:</p> <pre><code>wget -r -l 2 https://example.com               # recursive, 2 levels deep\nwget -m https://example.com                     # mirror a site\nwget -r --no-parent https://example.com/docs/   # don't go above starting directory\n</code></pre> <p>Use <code>curl</code> or <code>wget</code>? <code>curl</code> is better for API interaction and scripting with HTTP methods. <code>wget</code> is better for downloading files, especially recursive downloads and mirroring.</p>"},{"location":"Linux%20Essentials/networking/#remote-access","title":"Remote Access","text":""},{"location":"Linux%20Essentials/networking/#ssh","title":"ssh","text":"<p><code>ssh</code> (Secure Shell) provides encrypted remote access to other machines.</p> <p>Basic connection:</p> <pre><code>ssh user@hostname\nssh user@192.168.1.100\nssh -p 2222 user@hostname      # non-standard port\n</code></pre> <p>Key-based authentication:</p> <p>Password authentication is fine for personal use, but key-based auth is more secure and allows passwordless login.</p> <pre><code># Generate a key pair (if you don't have one)\nssh-keygen -t ed25519 -C \"ryan@workstation\"\n\n# Copy your public key to the server\nssh-copy-id user@hostname\n\n# Now connect without a password\nssh user@hostname\n</code></pre> <p>SSH config file (<code>~/.ssh/config</code>):</p> <pre><code>Host web\n    HostName 192.168.1.100\n    User deploy\n    Port 2222\n    IdentityFile ~/.ssh/deploy_key\n\nHost db\n    HostName 10.0.0.50\n    User admin\n    ProxyJump web\n\nHost *.internal\n    User ryan\n    IdentityFile ~/.ssh/internal_key\n</code></pre> <p>With this config, <code>ssh web</code> connects to 192.168.1.100 as user \"deploy\" on port 2222.</p> <p>Create an SSH Config for Multiple Hosts (requires JavaScript)</p> <p>Port forwarding:</p> <pre><code># Local: forward localhost:3307 to port 3306 on remote_host's end\nssh -L 3307:localhost:3306 user@remote_host\n\n# Remote: make your localhost:8080 accessible on remote_host:9090\nssh -R 9090:localhost:8080 user@remote_host\n\n# Dynamic: create a SOCKS proxy on localhost:1080\nssh -D 1080 user@remote_host\n</code></pre> <p>Jump hosts:</p> <pre><code>ssh -J jumphost user@internal_server\n</code></pre> <p>This tunnels through <code>jumphost</code> to reach <code>internal_server</code>. Useful for accessing machines behind a firewall.</p> <p>Agent forwarding:</p> <pre><code>ssh -A user@hostname     # forward your SSH agent (use your local keys on the remote)\n</code></pre> <p>Only use agent forwarding with trusted hosts - a compromised server could use your forwarded agent.</p> <p>Agent forwarding on untrusted hosts is a security risk</p> <p>When you use <code>ssh -A</code>, the remote server can use your local SSH keys to authenticate to other servers as long as your session is active. A compromised host's root user could hijack your forwarded agent socket to access any server your keys unlock. Only forward your agent to hosts you fully trust, and prefer <code>ProxyJump</code> (<code>ssh -J</code>) for reaching internal hosts through bastion servers instead.</p>"},{"location":"Linux%20Essentials/networking/#scp","title":"scp","text":"<p><code>scp</code> copies files over SSH:</p> <pre><code>scp file.txt user@host:/remote/path/          # upload\nscp user@host:/remote/file.txt ./local/       # download\nscp -r directory/ user@host:/remote/path/     # recursive (directories)\nscp -P 2222 file.txt user@host:/path/         # non-standard port\n</code></pre> <p>scp is deprecated in favor of rsync and sftp</p> <p>The OpenSSH project has deprecated <code>scp</code> because its protocol can't handle filenames with special characters safely and has limited features. Use <code>rsync</code> for file synchronization (it's faster for repeated transfers) or <code>sftp</code> for interactive file management. Both use SSH transport and are drop-in replacements for most <code>scp</code> use cases.</p>"},{"location":"Linux%20Essentials/networking/#rsync","title":"rsync","text":"<p><code>rsync</code> is the preferred tool for copying files - it only transfers what's changed.</p> <pre><code>rsync -av source/ dest/                                # local sync\nrsync -av source/ user@host:/remote/dest/              # sync to remote\nrsync -av user@host:/remote/source/ ./local/           # sync from remote\nrsync -av --delete source/ dest/                       # delete files in dest not in source\nrsync -av --dry-run source/ dest/                      # preview what would happen\nrsync -av --exclude='*.log' --exclude='.git' source/ dest/   # exclude patterns\nrsync -avz source/ user@host:/dest/                    # compress during transfer\n</code></pre> <p>Common flags: | Flag | Meaning | |------|---------| | <code>-a</code> | Archive mode (preserves permissions, timestamps, symlinks, etc.) | | <code>-v</code> | Verbose | | <code>-z</code> | Compress during transfer | | <code>-n</code> / <code>--dry-run</code> | Show what would be done without doing it | | <code>--delete</code> | Remove files in destination that aren't in source | | <code>--exclude</code> | Skip matching patterns | | <code>-P</code> | Show progress and allow resuming |</p> <p>Note the trailing slash on source paths. <code>source/</code> means \"the contents of source.\" <code>source</code> (no slash) means \"the directory itself.\"</p> <p>rsync trailing slash on source changes behavior</p> <p><code>rsync source/ dest/</code> copies the contents of <code>source</code> into <code>dest</code>. <code>rsync source dest/</code> copies the directory itself, creating <code>dest/source/</code>. This single character is the most common rsync mistake and can create unexpected nested directories or put files in the wrong location.</p> <p>Use rsync --dry-run before large transfers</p> <p>Always preview with <code>rsync -avn</code> (or <code>--dry-run</code>) before running destructive operations, especially with <code>--delete</code>. The dry run shows exactly which files would be transferred, deleted, or skipped - catching mistakes before they cause data loss.</p> <p>The trailing slash difference is the most common rsync mistake:</p> <pre><code># WITH trailing slash: copies CONTENTS of source into dest\nrsync -av photos/ /backup/photos/\n# Result: /backup/photos/img1.jpg, /backup/photos/img2.jpg\n\n# WITHOUT trailing slash: copies the DIRECTORY ITSELF into dest\nrsync -av photos /backup/photos/\n# Result: /backup/photos/photos/img1.jpg, /backup/photos/photos/img2.jpg\n</code></pre> <p>When in doubt, use a trailing slash on the source and make sure the destination path ends where you want the files to land.</p> <p>What is the difference between rsync -a source and rsync -a source/? (requires JavaScript)</p> <p>rsync Dry Run Preview (requires JavaScript)</p> <p>Command Builder (requires JavaScript)</p>"},{"location":"Linux%20Essentials/networking/#network-inspection","title":"Network Inspection","text":""},{"location":"Linux%20Essentials/networking/#ss","title":"ss","text":"<p>ss -tlnp shows what's listening on which port</p> <p>The combination <code>ss -tlnp</code> is the go-to command for answering \"what's running on this port?\" - <code>-t</code> filters to TCP, <code>-l</code> shows only listening sockets, <code>-n</code> skips slow DNS resolution, and <code>-p</code> reveals the process name and PID. Memorize this one.</p> <p><code>ss</code> (socket statistics) shows network connections. It replaces the older <code>netstat</code> command.</p> <pre><code>ss -tlnp               # TCP listening sockets with process info\nss -ulnp               # UDP listening sockets with process info\nss -a                  # all sockets\nss -s                  # summary statistics\n</code></pre> <p>Flags: | Flag | Meaning | |------|---------| | <code>-t</code> | TCP | | <code>-u</code> | UDP | | <code>-l</code> | Listening only | | <code>-n</code> | Don't resolve names (show numbers) | | <code>-p</code> | Show process using the socket | | <code>-a</code> | All (listening and non-listening) |</p> <p>What does ss -tlnp show? (requires JavaScript)</p> <p>Filtering:</p> <pre><code>ss -tn state established          # only established connections\nss -tn sport = :443               # connections from local port 443\nss -tn dport = :22                # connections to remote port 22\nss -tn dst 10.0.0.0/24            # connections to a subnet\n</code></pre> <p>Listening vs established: a listening socket is waiting for incoming connections (a server). An established socket is an active connection between two endpoints (a client connected to a server). To answer \"what's using port 8080?\":</p> <pre><code>ss -tlnp | grep 8080     # find the listening process on port 8080\n</code></pre> <p>The <code>-p</code> flag shows the process name and PID, so you can immediately identify which program is binding the port. If you need to find all connections to a remote service:</p> <pre><code>ss -tn dst :443           # all established connections to remote port 443\n</code></pre>"},{"location":"Linux%20Essentials/networking/#ip","title":"ip","text":"<p>The <code>ip</code> command manages network interfaces, addresses, and routes. It replaces the older <code>ifconfig</code> and <code>route</code> commands.</p> <p>Addresses:</p> <pre><code>ip addr                     # show all interfaces and addresses\nip addr show eth0           # specific interface\nip -4 addr                  # IPv4 only\nip -6 addr                  # IPv6 only\n</code></pre> <p>Link (interface) state:</p> <pre><code>ip link                     # show interface status\nip link set eth0 up         # bring interface up\nip link set eth0 down       # bring interface down\n</code></pre> <p>Routing:</p> <pre><code>ip route                    # show routing table\nip route get 8.8.8.8        # show which route a packet would take\n</code></pre> <p>The default route (also called the default gateway) is where packets go when no more specific route matches. It's the \"catch-all\" path to the rest of the internet.</p> <pre><code>ip route\n# default via 192.168.1.1 dev eth0 proto dhcp metric 100\n# 192.168.1.0/24 dev eth0 proto kernel scope link src 192.168.1.50\n</code></pre> <p>Reading this: the first line says \"for anything not in the local network, send packets to 192.168.1.1 via eth0.\" The second line says \"the 192.168.1.0/24 network is directly connected via eth0, and this machine's address on that network is 192.168.1.50.\"</p>"},{"location":"Linux%20Essentials/networking/#netstat-legacy","title":"netstat (Legacy)","text":"<p><code>netstat</code> is largely replaced by <code>ss</code> and <code>ip</code>, but you'll still see it in older documentation:</p> <pre><code>netstat -tlnp              # equivalent to ss -tlnp\nnetstat -rn                # equivalent to ip route\n</code></pre>"},{"location":"Linux%20Essentials/networking/#dns-tools","title":"DNS Tools","text":""},{"location":"Linux%20Essentials/networking/#dig","title":"dig","text":"<p><code>dig</code> queries DNS servers for records. It's the most informative DNS lookup tool.</p> <pre><code>dig example.com                     # default A record query\ndig example.com MX                  # query MX records\ndig example.com ANY                 # query all record types\ndig +short example.com              # concise output (just the answer)\ndig @8.8.8.8 example.com            # query a specific nameserver\ndig -x 142.250.80.46                # reverse DNS lookup\ndig +trace example.com              # show the full resolution path\n</code></pre> <p>Use dig +short for concise DNS answers</p> <p><code>dig +short</code> strips away all the verbose output and returns just the answer records. It's perfect for scripting: <code>ip=$(dig +short example.com)</code> gives you the IP address directly. For debugging, use the full output - the ANSWER, AUTHORITY, and ADDITIONAL sections tell you exactly what the resolver did.</p> <p>Trace DNS Resolution with dig (requires JavaScript)</p> <p>For deeper DNS coverage, see the DNS Administration guides in this repo.</p>"},{"location":"Linux%20Essentials/networking/#nslookup","title":"nslookup","text":"<p><code>nslookup</code> is a simpler DNS lookup tool:</p> <pre><code>nslookup example.com                # basic lookup\nnslookup example.com 8.8.8.8        # use specific DNS server\nnslookup -type=MX example.com       # query specific record type\n</code></pre> <p><code>dig</code> provides more detailed output and is generally preferred for troubleshooting.</p>"},{"location":"Linux%20Essentials/networking/#miscellaneous","title":"Miscellaneous","text":""},{"location":"Linux%20Essentials/networking/#hostname","title":"hostname","text":"<pre><code>hostname                     # show hostname\nhostname -f                  # fully qualified domain name\nhostname -I                  # all IP addresses\nhostnamectl                  # detailed host info (systemd)\nhostnamectl set-hostname server01    # set hostname (persistent)\n</code></pre>"},{"location":"Linux%20Essentials/networking/#nc-netcat","title":"nc (netcat)","text":"<p><code>nc</code> (netcat) is a versatile networking utility - a \"Swiss army knife\" for TCP/UDP connections.</p> <p>Port testing:</p> <pre><code>nc -zv hostname 80           # check if port 80 is open\nnc -zv hostname 20-25        # scan a port range\n</code></pre> <p>Simple client/server:</p> <pre><code># On server (listening)\nnc -l 9999\n\n# On client (connecting)\nnc hostname 9999\n# Type messages - they appear on the other side\n</code></pre> <p>File transfer:</p> <pre><code># On receiving end\nnc -l 9999 &gt; received_file.txt\n\n# On sending end\nnc hostname 9999 &lt; file_to_send.txt\n</code></pre>"},{"location":"Linux%20Essentials/networking/#further-reading","title":"Further Reading","text":"<ul> <li>curl Documentation - comprehensive guide to curl usage and options</li> <li>OpenSSH Manual Pages - official SSH client and server documentation</li> <li>rsync Documentation - file synchronization reference</li> <li>GNU Wget Manual - non-interactive network downloader</li> <li>iproute2 - modern Linux networking utilities (ss, ip)</li> <li>ISC BIND / dig - DNS lookup utility documentation</li> <li>mtr - network diagnostic tool combining ping and traceroute</li> </ul> <p>Previous: Disk and Filesystem | Next: System Information | Back to Index</p>"},{"location":"Linux%20Essentials/scripting-fundamentals/","title":"Scripting Fundamentals","text":"<p>Bash scripts automate sequences of commands. This guide covers the control structures, functions, and error handling patterns that form the backbone of reliable shell scripts.</p>"},{"location":"Linux%20Essentials/scripting-fundamentals/#exit-codes","title":"Exit Codes","text":"<p>Every command returns an exit code when it finishes. By convention:</p> <ul> <li>0 means success</li> <li>Non-zero means failure (the specific number can indicate different error types)</li> </ul> <pre><code>ls /tmp\necho $?    # 0 (success)\n\nls /nonexistent\necho $?    # 2 (no such file)\n</code></pre> <p>The special variable <code>$?</code> holds the exit code of the most recently executed command.</p> <p>You can set an exit code in your own scripts with <code>exit</code>:</p> <pre><code>#!/bin/bash\nif [ ! -f \"$1\" ]; then\n    echo \"File not found: $1\" &gt;&amp;2\n    exit 1\nfi\n</code></pre> <p>Exit Codes and Conditional Execution (requires JavaScript)</p>"},{"location":"Linux%20Essentials/scripting-fundamentals/#conditionals","title":"Conditionals","text":""},{"location":"Linux%20Essentials/scripting-fundamentals/#test-and","title":"test, [ ], and [[ ]]","text":"<p>There are three ways to evaluate conditions in bash:</p> <p><code>test</code> is the original command:</p> <pre><code>test -f /etc/passwd &amp;&amp; echo \"exists\"\n</code></pre> <p><code>[ ]</code> is equivalent to <code>test</code> (it's the same command under a different name):</p> <pre><code>[ -f /etc/passwd ] &amp;&amp; echo \"exists\"\n</code></pre> <p><code>[[ ]]</code> is a bash keyword with extra features:</p> <pre><code>[[ -f /etc/passwd ]] &amp;&amp; echo \"exists\"\n</code></pre> <p>The spaces inside <code>[ ]</code> and <code>[[ ]]</code> are required. They're not just syntax - <code>[</code> is actually a command, and <code>]</code> is its final argument.</p>"},{"location":"Linux%20Essentials/scripting-fundamentals/#differences-between-and","title":"Differences Between [ ] and [[ ]]","text":"Feature <code>[ ]</code> <code>[[ ]]</code> POSIX compatible Yes No (bash/zsh only) Pattern matching No <code>[[ $str == glob* ]]</code> Regex matching No <code>[[ $str =~ regex ]]</code> Logical operators <code>-a</code>, <code>-o</code> <code>&amp;&amp;</code>, <code>\\|\\|</code> Word splitting on variables Yes (must quote) No <p>In bash scripts, prefer <code>[[ ]]</code> - it's safer and more powerful.</p> <p>Use [[ ]] over [ ] in bash scripts</p> <p><code>[[ ]]</code> doesn't word-split variables, so <code>[[ -n $var ]]</code> works even if <code>$var</code> is empty or contains spaces. It also supports pattern matching (<code>==</code>), regex (<code>=~</code>), and logical operators (<code>&amp;&amp;</code>, <code>||</code>) directly. The only reason to use <code>[ ]</code> is when writing portable POSIX <code>sh</code> scripts.</p> <p>What is the main advantage of [[ ]] over [ ] in bash? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/scripting-fundamentals/#test-operators","title":"Test Operators","text":"<p>File tests:</p> Operator True if... <code>-f file</code> Regular file exists <code>-d file</code> Directory exists <code>-e file</code> Any file exists <code>-r file</code> File is readable <code>-w file</code> File is writable <code>-x file</code> File is executable <code>-s file</code> File exists and is non-empty <code>-L file</code> File is a symbolic link <code>file1 -nt file2</code> file1 is newer than file2 <code>file1 -ot file2</code> file1 is older than file2 <p>The ones you'll use constantly: <code>-f</code> to check that a config file exists before trying to read it, <code>-d</code> to verify a directory is there before writing into it, <code>-x</code> to check that a command or script is executable before running it, <code>-s</code> to make sure a file isn't empty before processing it, and <code>-nt</code> to compare timestamps (useful in build systems to decide whether a target needs rebuilding).</p> <p>String tests:</p> Operator True if... <code>-z \"$str\"</code> String is empty (zero length) <code>-n \"$str\"</code> String is non-empty <code>\"$a\" = \"$b\"</code> Strings are equal <code>\"$a\" != \"$b\"</code> Strings are not equal <p><code>-z</code> is the go-to for checking whether a required variable has been set: <code>[[ -z \"$DB_HOST\" ]] &amp;&amp; echo 'DB_HOST is required' &gt;&amp;2 &amp;&amp; exit 1</code>. <code>-n</code> is its opposite - use it when you want to run something only if a variable has a value.</p> <p>What does the test [ -z \"$var\" ] check? (requires JavaScript)</p> <p>Numeric comparison:</p> Operator Meaning <code>-eq</code> Equal <code>-ne</code> Not equal <code>-lt</code> Less than <code>-le</code> Less than or equal <code>-gt</code> Greater than <code>-ge</code> Greater than or equal <pre><code>[ \"$count\" -gt 10 ]          # numeric comparison with [ ]\n[[ $count -gt 10 ]]          # same with [[ ]] (quoting optional)\n(( count &gt; 10 ))             # arithmetic context (cleanest for numbers)\n</code></pre> <p>Use (( )) for arithmetic comparisons</p> <p><code>(( ))</code> lets you write math comparisons using familiar operators like <code>&gt;</code>, <code>&lt;</code>, <code>==</code>, <code>&gt;=</code> instead of the cryptic <code>-gt</code>, <code>-lt</code>, <code>-eq</code>, <code>-ge</code> flags. Variables inside <code>(( ))</code> don't need the <code>$</code> prefix. Use <code>(( ))</code> for numeric logic, <code>[[ ]]</code> for string and file tests.</p>"},{"location":"Linux%20Essentials/scripting-fundamentals/#if-elif-else","title":"if / elif / else","text":"<pre><code>if [[ -f \"$file\" ]]; then\n    echo \"File exists\"\nelif [[ -d \"$file\" ]]; then\n    echo \"It's a directory\"\nelse\n    echo \"Not found\"\nfi\n</code></pre> <p>You can use any command as a condition - <code>if</code> checks the exit code:</p> <pre><code>if grep -q \"error\" log.txt; then\n    echo \"Errors found\"\nfi\n\nif ping -c 1 -W 2 google.com &amp;&gt;/dev/null; then\n    echo \"Network is up\"\nfi\n</code></pre>"},{"location":"Linux%20Essentials/scripting-fundamentals/#case-esac","title":"case / esac","text":"<p><code>case</code> matches a value against patterns. It's cleaner than a chain of <code>elif</code> for multiple string comparisons.</p> <pre><code>case \"$1\" in\n    start)\n        echo \"Starting...\"\n        ;;\n    stop)\n        echo \"Stopping...\"\n        ;;\n    restart|reload)\n        echo \"Restarting...\"\n        ;;\n    *)\n        echo \"Usage: $0 {start|stop|restart}\"\n        exit 1\n        ;;\nesac\n</code></pre> <p>Patterns support globbing: <code>*</code> matches anything, <code>?</code> matches one character, <code>[...]</code> matches character classes.</p> <pre><code>case \"$filename\" in\n    *.tar.gz)  tar xzf \"$filename\" ;;\n    *.tar.bz2) tar xjf \"$filename\" ;;\n    *.zip)     unzip \"$filename\" ;;\n    *)         echo \"Unknown format\" ;;\nesac\n</code></pre> <p>Build a CLI Argument Parser with case (requires JavaScript)</p>"},{"location":"Linux%20Essentials/scripting-fundamentals/#short-circuit-operators","title":"Short-Circuit Operators","text":"<p><code>&amp;&amp;</code> runs the second command only if the first succeeds:</p> <pre><code>mkdir -p /tmp/work &amp;&amp; cd /tmp/work\n</code></pre> <p><code>||</code> runs the second command only if the first fails:</p> <pre><code>cd /tmp/work || exit 1\n</code></pre> <p>Combined for a simple if/else:</p> <pre><code>[[ -f config.yaml ]] &amp;&amp; echo \"Config found\" || echo \"No config\"\n</code></pre> <p>Be careful with this pattern. If the <code>&amp;&amp;</code> command fails, the <code>||</code> command also runs. For real conditional logic, use <code>if</code>.</p> <p>Avoid the &amp;&amp; || pseudo-if for complex logic</p> <p>The pattern <code>condition &amp;&amp; do_this || do_that</code> looks like an if/else but isn't. If <code>do_this</code> fails, <code>do_that</code> also runs - you get both branches. This is fine for simple cases like <code>[[ -f file ]] &amp;&amp; echo \"yes\" || echo \"no\"</code> where <code>echo</code> won't fail, but for anything more complex, use a proper <code>if</code> statement.</p>"},{"location":"Linux%20Essentials/scripting-fundamentals/#loops","title":"Loops","text":""},{"location":"Linux%20Essentials/scripting-fundamentals/#for-loop-list","title":"for Loop (List)","text":"<pre><code>for item in apple banana cherry; do\n    echo \"$item\"\ndone\n\n# Over files\nfor file in *.txt; do\n    echo \"Processing $file\"\ndone\n\n# Over command output\nfor user in $(cut -d: -f1 /etc/passwd); do\n    echo \"$user\"\ndone\n\n# Over a range\nfor i in {1..10}; do\n    echo \"$i\"\ndone\n</code></pre>"},{"location":"Linux%20Essentials/scripting-fundamentals/#for-loop-c-style","title":"for Loop (C-Style)","text":"<pre><code>for (( i=0; i&lt;10; i++ )); do\n    echo \"$i\"\ndone\n</code></pre>"},{"location":"Linux%20Essentials/scripting-fundamentals/#while-loop","title":"while Loop","text":"<pre><code>count=0\nwhile [[ $count -lt 5 ]]; do\n    echo \"$count\"\n    (( count++ ))\ndone\n</code></pre> <p>Reading a file line by line:</p> <pre><code>while IFS= read -r line; do\n    echo \"$line\"\ndone &lt; input.txt\n</code></pre> <p>The <code>IFS=</code> prevents stripping leading/trailing whitespace. The <code>-r</code> prevents backslash interpretation.</p> <p>Reading from a command:</p> <pre><code>while IFS= read -r line; do\n    echo \"$line\"\ndone &lt; &lt;(find . -name \"*.txt\")\n</code></pre> <p>The while-read-pipe subshell problem</p> <p>Piping into a <code>while</code> loop runs it in a subshell, so variable changes inside the loop are lost when it finishes: <code>cat file | while read line; do count=$((count+1)); done; echo $count</code> prints <code>0</code>. Use process substitution instead: <code>while read line; do ...; done &lt; &lt;(cat file)</code> or redirect from a file: <code>while read line; do ...; done &lt; file</code>.</p>"},{"location":"Linux%20Essentials/scripting-fundamentals/#until-loop","title":"until Loop","text":"<p>Runs while the condition is false (the inverse of <code>while</code>):</p> <pre><code>until ping -c 1 -W 2 server.example.com &amp;&gt;/dev/null; do\n    echo \"Waiting for server...\"\n    sleep 5\ndone\necho \"Server is up\"\n</code></pre>"},{"location":"Linux%20Essentials/scripting-fundamentals/#break-and-continue","title":"break and continue","text":"<p><code>break</code> exits the loop entirely:</p> <pre><code>for file in *.log; do\n    if [[ $(wc -l &lt; \"$file\") -gt 1000 ]]; then\n        echo \"Found large log: $file\"\n        break\n    fi\ndone\n</code></pre> <p><code>continue</code> skips to the next iteration:</p> <pre><code>for file in *.txt; do\n    [[ -d \"$file\" ]] &amp;&amp; continue    # skip directories\n    echo \"Processing $file\"\ndone\n</code></pre> <p>Both <code>break</code> and <code>continue</code> accept a numeric argument for nested loops. <code>break 2</code> exits two levels of nesting, <code>continue 2</code> skips to the next iteration of the outer loop. This avoids the need for flag variables when you want an inner loop's result to control the outer loop.</p>"},{"location":"Linux%20Essentials/scripting-fundamentals/#functions","title":"Functions","text":""},{"location":"Linux%20Essentials/scripting-fundamentals/#definition","title":"Definition","text":"<p>Two equivalent syntaxes:</p> <pre><code>greet() {\n    echo \"Hello, $1\"\n}\n\nfunction greet {\n    echo \"Hello, $1\"\n}\n</code></pre> <p>The first form is POSIX-compatible. The second is bash-specific.</p>"},{"location":"Linux%20Essentials/scripting-fundamentals/#arguments","title":"Arguments","text":"<p>Functions receive arguments the same way scripts do:</p> Variable Meaning <code>$1</code>, <code>$2</code>, ... Positional arguments <code>$@</code> All arguments (as separate words) <code>$*</code> All arguments (joined as a single string when quoted as <code>\"$*\"</code>) <code>$#</code> Number of arguments <code>$0</code> Still the script name (not the function name) <p>The critical difference between <code>\"$@\"</code> and <code>\"$*\"</code> appears when they're double-quoted. <code>\"$@\"</code> expands to each argument as a separate word, preserving the original argument boundaries. <code>\"$*\"</code> joins all arguments into a single string separated by the first character of <code>IFS</code> (normally a space). This matters when passing filenames with spaces:</p> <pre><code># If called with: ./script.sh \"my file.txt\" \"other file.txt\"\n\n# CORRECT: passes two separate arguments to rm\nfor f in \"$@\"; do rm \"$f\"; done    # rm \"my file.txt\"; rm \"other file.txt\"\n\n# BUG: joins into one string, then word-splits on spaces\nfor f in \"$*\"; do rm \"$f\"; done    # rm \"my file.txt other file.txt\" (one argument with spaces)\n</code></pre> <p>In almost all cases, you want <code>\"$@\"</code>. The only time <code>\"$*\"</code> is useful is when you intentionally want to join arguments into a single string, like building a log message: <code>log \"Arguments: $*\"</code>.</p> <p>What is the difference between \"$@\" and \"$*\" in a shell script? (requires JavaScript)</p> <pre><code>backup() {\n    local src=\"$1\"\n    local dest=\"$2\"\n\n    if [[ $# -lt 2 ]]; then\n        echo \"Usage: backup &lt;source&gt; &lt;destination&gt;\" &gt;&amp;2\n        return 1\n    fi\n\n    cp -r \"$src\" \"$dest\"\n}\n\nbackup /var/www /tmp/backup\n</code></pre>"},{"location":"Linux%20Essentials/scripting-fundamentals/#return-values","title":"Return Values","text":"<p>Functions use <code>return</code> to set an exit code (0-255). They don't \"return\" data the way functions in other languages do.</p> <pre><code>is_valid_ip() {\n    local ip=\"$1\"\n    [[ $ip =~ ^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$ ]]\n    return $?    # returns exit code of the test\n}\n\nif is_valid_ip \"192.168.1.1\"; then\n    echo \"Valid\"\nfi\n</code></pre> <p>return sets exit status, echo outputs data</p> <p><code>return</code> only sets a numeric exit status (0-255) - it does not send data back to the caller. To pass data out of a function, use <code>echo</code> (or <code>printf</code>) and capture it with <code>$(function_name)</code>. Confusing the two is a common bug: <code>result=$(my_func)</code> captures stdout, while <code>$?</code> captures the return code.</p> <p>To get data out of a function, print it and capture with command substitution:</p> <pre><code>get_extension() {\n    echo \"${1##*.}\"\n}\n\next=$(get_extension \"archive.tar.gz\")\necho \"$ext\"    # gz\n</code></pre>"},{"location":"Linux%20Essentials/scripting-fundamentals/#local-variables","title":"Local Variables","text":"<p>Variables inside functions are global by default. Use <code>local</code> to scope them to the function:</p> <pre><code>my_func() {\n    local temp=\"this stays inside\"\n    global_var=\"this leaks out\"\n}\n\nmy_func\necho \"$temp\"        # empty\necho \"$global_var\"  # this leaks out\n</code></pre> <p>Always use <code>local</code> for function variables unless you intentionally want them to be global.</p> <p>Use local for all function variables</p> <p>Without <code>local</code>, every variable in a function is global - it persists after the function returns and can collide with variables in other functions or the main script. Always declare function variables with <code>local</code> unless you deliberately want them visible outside the function. This is especially important in scripts with multiple functions that might reuse common names like <code>i</code>, <code>result</code>, or <code>file</code>.</p> <p>Functions with Local Variables and Return Values (requires JavaScript)</p>"},{"location":"Linux%20Essentials/scripting-fundamentals/#error-handling","title":"Error Handling","text":""},{"location":"Linux%20Essentials/scripting-fundamentals/#set-options","title":"set Options","text":"<p>Three options that make scripts much safer:</p> <p><code>set -e</code> (errexit) - exit immediately if a command fails:</p> <pre><code>set -e\nrm /tmp/workfile      # if this fails, script exits\necho \"This won't run if rm failed\"\n</code></pre> <p><code>set -u</code> (nounset) - treat unset variables as errors:</p> <pre><code>set -u\necho \"$undefined_var\"    # error: unbound variable\n</code></pre> <p><code>set -o pipefail</code> - pipeline fails if any command in it fails:</p> <pre><code>set -o pipefail\ncat /nonexistent | sort    # pipeline returns non-zero (cat failed)\n</code></pre>"},{"location":"Linux%20Essentials/scripting-fundamentals/#the-combination","title":"The Combination","text":"<p>Start every script with:</p> <pre><code>#!/bin/bash\nset -euo pipefail\n</code></pre> <p>This catches the vast majority of common scripting errors: unhandled failures, typos in variable names, and hidden pipeline failures.</p> <p>set -e exceptions: if, &amp;&amp;, ||, while</p> <p><code>set -e</code> doesn't exit on every failure. Commands used as conditions in <code>if</code>, <code>while</code>, or <code>until</code> statements are exempt, as are commands in <code>&amp;&amp;</code> and <code>||</code> chains. This is by design - the shell needs to evaluate the exit code to make a decision. Be aware that <code>cmd &amp;&amp; other</code> silently swallows <code>cmd</code>'s failure under <code>set -e</code>.</p> <p>set -u catches rm -rf $UNSET_VAR expanding to rm -rf /</p> <p>Without <code>set -u</code>, referencing an unset variable silently expands to an empty string. This turns <code>rm -rf \"$DEPLOY_DIR/app\"</code> into <code>rm -rf /app</code> when <code>DEPLOY_DIR</code> is unset. With <code>-u</code>, bash immediately raises an error instead of expanding the empty variable. This single option prevents an entire class of catastrophic scripting bugs.</p> <p>What does set -e do in a bash script? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/scripting-fundamentals/#trap","title":"trap","text":"<p><code>trap</code> runs a command when the script receives a signal or exits. It's essential for cleanup.</p> <pre><code>cleanup() {\n    rm -f \"$tmpfile\"\n    echo \"Cleaned up\"\n}\n\ntrap cleanup EXIT          # runs cleanup when script exits (any reason)\ntrap cleanup ERR           # runs cleanup on error\ntrap cleanup INT TERM      # runs cleanup on Ctrl-C or kill\n\ntmpfile=$(mktemp)\n# ... use tmpfile ...\n# cleanup runs automatically when the script exits\n</code></pre> <p>trap EXIT for guaranteed cleanup</p> <p><code>trap cleanup EXIT</code> fires when the script exits for any reason: normal completion, <code>set -e</code> abort, <code>Ctrl-C</code>, or <code>kill</code>. This makes it the single most reliable cleanup mechanism. Always use <code>EXIT</code> rather than trapping individual signals, unless you need different behavior for different signals.</p> <p>Common signals to trap:</p> Signal When <code>EXIT</code> Script exits (any reason) <code>ERR</code> A command fails (with <code>set -e</code>) <code>INT</code> <code>Ctrl-C</code> <code>TERM</code> <code>kill</code> (default signal) <p>Write a Robust Script Header (requires JavaScript)</p>"},{"location":"Linux%20Essentials/scripting-fundamentals/#complete-example","title":"Complete Example","text":"<p>Here's a script that demonstrates proper error handling:</p> <pre><code>#!/bin/bash\nset -euo pipefail\n\nreadonly SCRIPT_NAME=\"$(basename \"$0\")\"\nreadonly WORK_DIR=\"$(mktemp -d)\"\n\nlog() {\n    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $*\" &gt;&amp;2\n}\n\ncleanup() {\n    local exit_code=$?\n    rm -rf \"$WORK_DIR\"\n    if [[ $exit_code -ne 0 ]]; then\n        log \"ERROR: $SCRIPT_NAME failed with exit code $exit_code\"\n    fi\n    exit $exit_code\n}\n\ntrap cleanup EXIT\n\nusage() {\n    echo \"Usage: $SCRIPT_NAME &lt;input-file&gt; &lt;output-file&gt;\" &gt;&amp;2\n    exit 1\n}\n\nmain() {\n    [[ $# -ne 2 ]] &amp;&amp; usage\n\n    local input=\"$1\"\n    local output=\"$2\"\n\n    [[ -f \"$input\" ]] || { log \"Input file not found: $input\"; exit 1; }\n\n    log \"Processing $input...\"\n\n    local tmpfile=\"$WORK_DIR/processed.tmp\"\n    sort -u \"$input\" &gt; \"$tmpfile\"\n    mv \"$tmpfile\" \"$output\"\n\n    log \"Done. Output written to $output\"\n}\n\nmain \"$@\"\n</code></pre> <p>This script: - Fails immediately on errors (<code>set -euo pipefail</code>) - Creates a temporary working directory - Cleans up automatically on exit (success or failure) - Logs to STDERR - Validates arguments - Uses <code>local</code> for all function variables - Wraps logic in a <code>main</code> function</p> <p>Bash Error Handling Template (requires JavaScript)</p>"},{"location":"Linux%20Essentials/scripting-fundamentals/#further-reading","title":"Further Reading","text":"<ul> <li>Bash Reference Manual - official bash documentation covering scripting syntax, conditionals, loops, and builtins</li> <li>POSIX Shell Command Language - the portable shell scripting specification</li> <li>ShellCheck - static analysis tool for shell scripts</li> </ul> <p>Previous: Job Control | Next: Disk and Filesystem | Back to Index</p>"},{"location":"Linux%20Essentials/shell-basics/","title":"Shell Basics","text":"<p>This guide covers how the shell works under the hood - what it is, how it starts up, how it finds commands, and how it processes what you type before anything runs.</p>"},{"location":"Linux%20Essentials/shell-basics/#what-is-a-shell","title":"What Is a Shell?","text":"<p>A shell is a program that interprets your commands and passes them to the operating system. When you open a terminal, the shell is the program that shows you a prompt and waits for input.</p>  Ken Thompson (sitting) and Dennis Ritchie at a PDP-11 running Unix, 1973. Thompson wrote the first Unix shell; Ritchie created the C language that made Unix portable. Photo: Wikimedia Commons, Public Domain <p>There are several shells in common use:</p> Shell Path Notes <code>bash</code> <code>/bin/bash</code> Default on most Linux distributions <code>zsh</code> <code>/bin/zsh</code> Default on macOS since Catalina <code>sh</code> <code>/bin/sh</code> POSIX-compliant, minimal <code>dash</code> <code>/bin/dash</code> Lightweight, used as <code>/bin/sh</code> on Debian/Ubuntu <p>bash is the default on most Linux servers and the shell you'll encounter in nearly every tutorial and sysadmin guide. If you're SSH'd into a Linux box, you're probably running bash.</p> <p>zsh has better interactive features like smarter tab completion and spelling correction. macOS switched to it as the default because of bash's licensing - newer bash versions are GPLv3, which Apple avoids.</p> <p>dash exists for speed. Debian and Ubuntu use it as <code>/bin/sh</code> for system boot scripts because it starts and runs faster than bash. You won't use it interactively, but your system runs hundreds of dash scripts during boot.</p> <p>sh is the POSIX compatibility baseline. Scripts written for <code>sh</code> are portable across Unix-like systems. On modern Linux, <code>/bin/sh</code> is usually a symlink to dash or bash.</p> <p>Check which shell you're currently running:</p> <pre><code>echo $SHELL      # Your default login shell\necho $0          # The shell running right now\n</code></pre> <p>These can differ. <code>$SHELL</code> is set at login based on <code>/etc/passwd</code>. <code>$0</code> shows what's actually executing.</p> <p>To see all available shells on your system:</p> <pre><code>cat /etc/shells\n</code></pre> <p>Identifying Your Shell (requires JavaScript)</p> <p>What does the $SHELL variable tell you? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/shell-basics/#login-vs-non-login-shells","title":"Login vs Non-Login Shells","text":"<p>The distinction matters because it determines which configuration files get loaded.</p> <p>A login shell is the first shell that starts when you log in to the system - via SSH, a virtual console, or <code>su -</code>. It reads a specific set of startup files.</p> <p>A non-login shell is what you get when you open a new terminal window in a desktop environment, or run <code>bash</code> from an existing shell. It reads a different set of files.</p> <p>You can test whether your current shell is a login shell:</p> <pre><code>shopt -q login_shell &amp;&amp; echo \"login\" || echo \"non-login\"\n</code></pre> <p>An interactive shell is one where you type commands at a prompt. A non-interactive shell runs a script file without user input.</p> <p>Quick mental model</p> <p>Think of it as a 2x2 grid: shells are login or non-login, and independently interactive or non-interactive. SSH gives you a login interactive shell. Running <code>./script.sh</code> gives you a non-login non-interactive shell. The startup files loaded depend on both dimensions.</p> <p>Which of the following gives you a login shell? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/shell-basics/#configuration-file-load-order","title":"Configuration File Load Order","text":"<p>Bash reads different files depending on the shell type. Here's the load order:</p>"},{"location":"Linux%20Essentials/shell-basics/#login-shell","title":"Login Shell","text":"<ol> <li><code>/etc/profile</code> - system-wide, runs for all users</li> <li>Then the first of these that exists (in order):</li> <li><code>~/.bash_profile</code></li> <li><code>~/.bash_login</code></li> <li><code>~/.profile</code></li> </ol> <p>On logout, bash reads <code>~/.bash_logout</code> if it exists.</p>"},{"location":"Linux%20Essentials/shell-basics/#non-login-interactive-shell","title":"Non-Login Interactive Shell","text":"<ol> <li><code>~/.bashrc</code></li> </ol> <p>This is why most people put their aliases and prompt customizations in <code>~/.bashrc</code>, and then source it from <code>~/.bash_profile</code>:</p> <pre><code># ~/.bash_profile\nif [ -f ~/.bashrc ]; then\n    source ~/.bashrc\nfi\n</code></pre>"},{"location":"Linux%20Essentials/shell-basics/#zsh-for-comparison","title":"Zsh (for comparison)","text":"<p>Zsh has its own load order:</p> <ol> <li><code>/etc/zshenv</code> then <code>~/.zshenv</code> (always)</li> <li><code>/etc/zprofile</code> then <code>~/.zprofile</code> (login shells)</li> <li><code>/etc/zshrc</code> then <code>~/.zshrc</code> (interactive shells)</li> <li><code>/etc/zlogin</code> then <code>~/.zlogin</code> (login shells)</li> </ol> <p>Common gotcha</p> <p>If you put aliases in <code>~/.bash_profile</code> but not <code>~/.bashrc</code>, they'll be available in SSH sessions but missing when you open a terminal tab on a desktop. Always put interactive settings (aliases, prompt, functions) in <code>~/.bashrc</code> and source it from <code>~/.bash_profile</code>.</p> <p>Understanding .bash_profile (requires JavaScript)</p> <p>Find Your Shell Configuration (requires JavaScript)</p>"},{"location":"Linux%20Essentials/shell-basics/#sourcing-files","title":"Sourcing Files","text":"<p>Sourcing a file runs it in the current shell rather than in a subshell. This means any variables, functions, or settings defined in the file affect your current session.</p> <pre><code>source ~/.bashrc\n# or equivalently:\n. ~/.bashrc\n</code></pre> <p>The dot (<code>.</code>) is the POSIX-compatible way to source a file. <code>source</code> is a bash built-in that does the same thing.</p> <p>The dot (.) shorthand</p> <p><code>.</code> is the POSIX-standard way to source a file and works in every shell. <code>source</code> is a bash/zsh convenience alias. In scripts targeting <code>/bin/sh</code> or running on minimal systems, always use <code>.</code> instead of <code>source</code>.</p> <p>If you run a script normally (<code>bash script.sh</code> or <code>./script.sh</code>), it executes in a new subshell. Any variables it sets disappear when it finishes.</p> <pre><code># This sets VAR in a subshell - your current shell won't see it\nbash -c 'VAR=hello'\necho $VAR    # empty\n\n# This sets VAR in the current shell\nsource &lt;(echo 'VAR=hello')\necho $VAR    # hello\n</code></pre> <p>You added a new alias to ~/.bashrc. What's the fastest way to use it without logging out? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/shell-basics/#the-path-variable","title":"The PATH Variable","text":"<p>When you type a command like <code>ls</code>, the shell needs to find the actual program to run. It does this by searching through directories listed in the <code>PATH</code> environment variable.</p> <pre><code>echo $PATH\n</code></pre> <p>This prints a colon-separated list of directories:</p> <pre><code>/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin\n</code></pre> <p>The shell searches these directories left to right and runs the first match it finds. If the command isn't found in any <code>PATH</code> directory, you get <code>command not found</code>.</p> <p>To add a directory to your <code>PATH</code>:</p> <pre><code># Prepend (searched first)\nexport PATH=\"/opt/mytools/bin:$PATH\"\n\n# Append (searched last)\nexport PATH=\"$PATH:$HOME/bin\"\n</code></pre> <p>Put <code>PATH</code> modifications in <code>~/.bashrc</code> or <code>~/.bash_profile</code> to make them permanent.</p> <p>Prepend vs Append</p> <p>Prepending puts your directory first, so your version of a command wins over the system version. Appending puts it last, so system commands take priority. Prepend when you want to override system tools (e.g., a newer version of <code>git</code>). Append when you're adding new commands that don't conflict.</p>"},{"location":"Linux%20Essentials/shell-basics/#finding-commands","title":"Finding Commands","text":"<p>The shell provides several ways to find out what a command actually is and where it lives.</p> <p><code>type</code> - shows how the shell interprets a command:</p> <pre><code>type ls        # ls is aliased to 'ls --color=auto'\ntype cd        # cd is a shell builtin\ntype bash      # bash is /usr/bin/bash\ntype if        # if is a shell keyword\n</code></pre> <p><code>which</code> - searches <code>PATH</code> for the command's location:</p> <pre><code>which python3  # /usr/bin/python3\n</code></pre> <p><code>which</code> only finds external commands. It won't find builtins or aliases.</p> <p><code>command -v</code> - the POSIX-portable way to check if a command exists:</p> <pre><code>command -v git   # /usr/bin/git\ncommand -v cd    # cd (builtin)\n</code></pre> <p>This is the preferred method for scripts because it works across shells and handles builtins:</p> <pre><code>if command -v docker &amp;&gt;/dev/null; then\n    echo \"Docker is installed\"\nfi\n</code></pre> <p>Which command should you use in a portable shell script to check if a program is installed? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/shell-basics/#variables","title":"Variables","text":""},{"location":"Linux%20Essentials/shell-basics/#shell-variables-vs-environment-variables","title":"Shell Variables vs Environment Variables","text":"<p>A shell variable exists only in the current shell session:</p> <pre><code>greeting=\"hello\"\necho $greeting    # hello\n</code></pre> <p>Shell vs Environment Variables</p> <p>A shell variable only exists in the current session. An environment variable (created with <code>export</code>) is inherited by child processes. Use <code>export</code> when programs you launch need to see the value - for example, <code>EDITOR</code>, <code>PATH</code>, or application config. Keep internal script variables unexported to avoid polluting child environments.</p> <p>An environment variable is exported to child processes. Any program you launch from the shell can read it:</p> <pre><code>export API_KEY=\"abc123\"\n# or:\nAPI_KEY=\"abc123\"\nexport API_KEY\n</code></pre> <p>To see all environment variables:</p> <pre><code>env       # or: printenv\n</code></pre> <p>To see all shell variables (including non-exported ones):</p> <pre><code>set\n</code></pre> <p>To remove a variable:</p> <pre><code>unset greeting\n</code></pre>"},{"location":"Linux%20Essentials/shell-basics/#variable-naming","title":"Variable Naming","text":"<p>Variable names can contain letters, numbers, and underscores. They cannot start with a number. By convention, environment variables use <code>UPPER_CASE</code> and local shell variables use <code>lower_case</code>.</p> <pre><code>MY_CONFIG=\"/etc/app.conf\"    # conventional naming for an env var (export separately)\ncounter=0                     # local shell variable\n</code></pre>"},{"location":"Linux%20Essentials/shell-basics/#referencing-variables","title":"Referencing Variables","text":"<p>Use <code>$VARIABLE</code> or <code>${VARIABLE}</code> to reference a variable's value:</p> <pre><code>name=\"world\"\necho \"Hello $name\"       # Hello world\necho \"Hello ${name}\"     # Hello world\n</code></pre> <p>Braces are required when the variable name could be ambiguous:</p> <pre><code>fruit=\"apple\"\necho \"$fruits\"       # empty - shell looks for variable 'fruits'\necho \"${fruit}s\"     # apples\n</code></pre> <p>Variables in Action (requires JavaScript)</p>"},{"location":"Linux%20Essentials/shell-basics/#quoting","title":"Quoting","text":"<p>Quoting controls how the shell interprets special characters.</p>"},{"location":"Linux%20Essentials/shell-basics/#double-quotes","title":"Double Quotes","text":"<p>Double quotes preserve whitespace and prevent word splitting and pathname expansion, but allow variable expansion and command substitution:</p> <pre><code>name=\"Ryan Robson\"\necho $name       # Ryan Robson (two arguments to echo - works by coincidence)\necho \"$name\"     # Ryan Robson (one argument to echo - correct)\n\nfiles=\"*.txt\"\necho $files      # expands to matching filenames\necho \"$files\"    # literally: *.txt\n</code></pre>"},{"location":"Linux%20Essentials/shell-basics/#single-quotes","title":"Single Quotes","text":"<p>Single quotes preserve everything literally. No expansion of any kind happens inside single quotes:</p> <pre><code>echo '$HOME'           # literally: $HOME\necho '$(whoami)'       # literally: $(whoami)\necho 'it'\\''s here'    # it's here (break out and back in to include a single quote)\n</code></pre>"},{"location":"Linux%20Essentials/shell-basics/#no-quotes","title":"No Quotes","text":"<p>Without quotes, the shell performs word splitting, pathname expansion, and variable expansion. This is usually not what you want for strings that might contain spaces:</p> <pre><code>file=\"my document.txt\"\ncat $file        # tries to open 'my' and 'document.txt' separately\ncat \"$file\"      # opens 'my document.txt' correctly\n</code></pre> <p>Always quote your variables</p> <p>Unquoted variables are the #1 source of subtle shell bugs. When <code>$file</code> contains spaces, <code>cat $file</code> silently does the wrong thing. Always use <code>\"$file\"</code> unless you specifically need word splitting.</p>"},{"location":"Linux%20Essentials/shell-basics/#escaping-with-backslash","title":"Escaping with Backslash","text":"<p>A backslash (<code>\\</code>) escapes a single character, removing its special meaning:</p> <pre><code>echo \"The price is \\$5\"    # The price is $5\necho \"She said \\\"hi\\\"\"     # She said \"hi\"\n</code></pre>"},{"location":"Linux%20Essentials/shell-basics/#word-splitting-and-ifs","title":"Word Splitting and IFS","text":"<p>When the shell expands an unquoted variable, it splits the result into separate words based on the <code>IFS</code> (Internal Field Separator) variable. By default, <code>IFS</code> contains space, tab, and newline.</p> <pre><code>data=\"one:two:three\"\n\n# Default IFS - no splitting on colons\nfor item in $data; do echo \"$item\"; done\n# one:two:three\n\n# Custom IFS\nIFS=\":\"\nfor item in $data; do echo \"$item\"; done\n# one\n# two\n# three\n</code></pre> <p>IFS changes are global</p> <p>Modifying <code>IFS</code> affects every subsequent unquoted expansion in the current shell. A forgotten <code>IFS</code> change can silently break commands that rely on default whitespace splitting. Always reset it or confine the change to a subshell.</p> <p>Always reset <code>IFS</code> after changing it, or set it only in a subshell:</p> <pre><code>(IFS=\":\"; for item in $data; do echo \"$item\"; done)\n</code></pre> <p>What does echo '$HOME' print? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/shell-basics/#shell-expansions","title":"Shell Expansions","text":"<p>The shell processes your command line through several expansion stages before executing anything. Understanding the order helps you predict what the shell will do with your input.</p>"},{"location":"Linux%20Essentials/shell-basics/#order-of-expansion","title":"Order of Expansion","text":"<ol> <li>Brace expansion</li> <li>Tilde expansion</li> <li>Parameter and variable expansion</li> <li>Arithmetic expansion</li> <li>Command substitution (left to right)</li> <li>Word splitting</li> <li>Pathname expansion (globbing)</li> <li>Quote removal</li> </ol> <p>Otherwise known as: Big Tasty Pies Always Come With Perfect Quiche.</p>"},{"location":"Linux%20Essentials/shell-basics/#brace-expansion","title":"Brace Expansion","text":"<p>Brace expansion generates strings. It happens before any other expansion, so it works even with non-existent files.</p> <p>Lists:</p> <pre><code>echo {a,b,c}           # a b c\necho file.{txt,md,sh}  # file.txt file.md file.sh\nmkdir -p project/{src,tests,docs}\n</code></pre> <p>Brace expansion happens before variable expansion</p> <p>Because brace expansion is step 1, expressions like <code>{$a,$b}</code> don't work as expected - the braces are processed before <code>$a</code> and <code>$b</code> are resolved. Use an array or a loop if you need variable-driven lists.</p> <p>Sequences:</p> <pre><code>echo {1..5}            # 1 2 3 4 5\necho {a..f}            # a b c d e f\necho {01..10}          # 01 02 03 04 05 06 07 08 09 10\necho {0..20..5}        # 0 5 10 15 20 (step of 5)\n</code></pre> <p>Combinations:</p> <pre><code>echo {A,B}{1,2}        # A1 A2 B1 B2\n</code></pre>"},{"location":"Linux%20Essentials/shell-basics/#tilde-expansion","title":"Tilde Expansion","text":"<p>Tilde expansion converts <code>~</code> to directory paths:</p> <pre><code>echo ~           # /home/ryan (your home directory)\necho ~root       # /root (root's home directory)\necho ~+         # current working directory ($PWD)\necho ~-         # previous working directory ($OLDPWD)\n</code></pre>"},{"location":"Linux%20Essentials/shell-basics/#parameter-expansion","title":"Parameter Expansion","text":"<p>Parameter expansion is how the shell substitutes variable values. Beyond basic <code>${var}</code>, bash supports several transformations:</p> <p>Default values:</p> <pre><code>echo ${name:-\"Anonymous\"}     # Use \"Anonymous\" if name is unset or empty\necho ${name:=\"Anonymous\"}     # Same, but also assign the default to name\necho ${name:+\"has a name\"}    # If name is set and non-empty, use \"has a name\"\necho ${name:?\"name is required\"}  # If name is unset or empty, print error and exit\n</code></pre> <p>String length:</p> <pre><code>path=\"/usr/local/bin\"\necho ${#path}                 # 14\n</code></pre> <p>Substring removal:</p> <pre><code>file=\"archive.tar.gz\"\necho ${file%.*}       # archive.tar  (remove shortest match from end)\necho ${file%%.*}      # archive      (remove longest match from end)\necho ${file#*.}       # tar.gz       (remove shortest match from start)\necho ${file##*.}      # gz           (remove longest match from start)\n</code></pre> <p>A handy mnemonic: <code>#</code> is on the left side of <code>$</code> on the keyboard (removes from the left), <code>%</code> is on the right (removes from the right).</p> <p>String replacement:</p> <pre><code>text=\"hello world hello\"\necho ${text/hello/goodbye}    # goodbye world hello  (first match)\necho ${text//hello/goodbye}   # goodbye world goodbye (all matches)\n</code></pre> <p>Substring extraction:</p> <pre><code>str=\"Hello World\"\necho ${str:6}       # World       (from position 6 to end)\necho ${str:0:5}     # Hello       (from position 0, length 5)\n</code></pre> <p>Case modification is bash 4+</p> <p>The <code>${var^}</code> and <code>${var,,}</code> syntax requires bash 4.0 or later. macOS ships with bash 3.2 (due to GPLv3 licensing), so these won't work there unless you install a newer bash via Homebrew.</p> <p>Case modification (bash 4+):</p> <pre><code>name=\"hello world\"\necho ${name^}       # Hello world  (capitalize first character)\necho ${name^^}      # HELLO WORLD  (capitalize all)\n\nupper=\"HELLO\"\necho ${upper,}      # hELLO        (lowercase first character)\necho ${upper,,}     # hello         (lowercase all)\n</code></pre>"},{"location":"Linux%20Essentials/shell-basics/#arithmetic-expansion","title":"Arithmetic Expansion","text":"<p>Arithmetic expansion evaluates mathematical expressions:</p> <pre><code>echo $(( 5 + 3 ))       # 8\necho $(( 10 / 3 ))      # 3 (integer division)\necho $(( 10 % 3 ))      # 1 (modulo)\necho $(( 2 ** 10 ))     # 1024 (exponentiation)\n\ncount=5\necho $(( count + 1 ))   # 6 (no $ needed inside $(( )))\n(( count++ ))            # increment count\n</code></pre>"},{"location":"Linux%20Essentials/shell-basics/#command-substitution","title":"Command Substitution","text":"<p>Command substitution captures the output of a command and inserts it into the command line:</p> <pre><code>today=$(date +%Y-%m-%d)\necho \"Today is $today\"\n\n# Backtick syntax (older, harder to nest - prefer $())\ntoday=`date +%Y-%m-%d`\n</code></pre> <p>Command substitutions can be nested:</p> <pre><code>echo \"Files in $(basename $(pwd)): $(ls | wc -l)\"\n</code></pre>"},{"location":"Linux%20Essentials/shell-basics/#pathname-expansion-globbing","title":"Pathname Expansion (Globbing)","text":"<p>Pathname expansion matches filenames using wildcard patterns:</p> Pattern Matches <code>*</code> Any string of characters (including empty) <code>?</code> Any single character <code>[abc]</code> Any one of <code>a</code>, <code>b</code>, or <code>c</code> <code>[a-z]</code> Any character in the range <code>a</code> through <code>z</code> <code>[^abc]</code> or <code>[!abc]</code> Any character NOT <code>a</code>, <code>b</code>, or <code>c</code> <pre><code>ls *.txt          # all .txt files\nls file?.log      # file1.log, fileA.log, etc.\nls [Mm]akefile    # Makefile or makefile\nls log[0-9].txt   # log0.txt through log9.txt\n</code></pre> <p>Globbing only matches filenames that exist. If no files match, the pattern is passed through literally (unless <code>failglob</code> or <code>nullglob</code> is set).</p> <p>Globbing treats dotfiles differently</p> <p>By default, <code>*</code> does not match files starting with <code>.</code> (hidden files). This protects dotfiles like <code>.bashrc</code> from accidental bulk operations. Enable <code>shopt -s dotglob</code> if you intentionally need to match them.</p> <p>Hidden files (starting with <code>.</code>) are not matched by <code>*</code> unless you enable <code>dotglob</code>:</p> <pre><code>shopt -s dotglob    # now * matches hidden files too\n</code></pre> <p>Extended globbing (enabled with <code>shopt -s extglob</code>):</p> <pre><code>shopt -s extglob\nls !(*.log)          # everything except .log files\nls *(pattern)        # zero or more matches\nls +(pattern)        # one or more matches\nls ?(pattern)        # zero or one match\nls @(pat1|pat2)      # exactly one of the patterns\n</code></pre> <p>Extended globs are most useful for selecting everything except certain files. For example, to delete everything in a directory except <code>.conf</code> files:</p> <pre><code>shopt -s extglob\nrm !(*.conf)\n</code></pre> <p>Or match multiple extensions at once:</p> <pre><code>ls *.@(jpg|png|gif)    # all image files\ncp !(*.log|*.tmp) /backup/    # copy everything except logs and temp files\n</code></pre> <p>Without extglob, you'd need <code>find</code> with <code>-not</code> flags to achieve the same thing.</p> <p>Expansion Prediction Challenge (requires JavaScript)</p>"},{"location":"Linux%20Essentials/shell-basics/#further-reading","title":"Further Reading","text":"<ul> <li>Bash Reference Manual - comprehensive guide to bash syntax, builtins, and behavior</li> <li>Zsh Documentation - official zsh project and manual</li> <li>POSIX Shell Command Language - the portable shell specification</li> <li>dash - Debian Almquist Shell project page</li> </ul> <p>Next: Streams and Redirection | Back to Index</p>"},{"location":"Linux%20Essentials/streams-and-redirection/","title":"Streams and Redirection","text":"<p>Every program in a Unix-like system communicates through streams. Understanding how to redirect, combine, and manipulate these streams is fundamental to working on the command line.</p>"},{"location":"Linux%20Essentials/streams-and-redirection/#standard-streams","title":"Standard Streams","text":"The DEC VT100 (1978) - one of the most influential video terminals. Its ANSI escape codes became the standard that modern terminal emulators still follow. Photo: Jason Scott, CC BY 2.0 <p>Every process starts with three open file descriptors:</p> Stream File Descriptor Default Purpose STDIN 0 Keyboard Input to the program STDOUT 1 Terminal Normal output STDERR 2 Terminal Error messages and diagnostics <p>These are just numbers the kernel uses to track open files. The fact that 0, 1, and 2 are pre-assigned is a convention that every Unix program follows.</p> <pre><code># A program reads from STDIN (fd 0), writes output to STDOUT (fd 1),\n# and writes errors to STDERR (fd 2)\ngrep \"pattern\" &lt; input.txt &gt; output.txt 2&gt; errors.txt\n</code></pre> <p>What happens to stderr when you redirect stdout with &gt; file.txt? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/streams-and-redirection/#basic-redirection","title":"Basic Redirection","text":""},{"location":"Linux%20Essentials/streams-and-redirection/#redirecting-stdout","title":"Redirecting STDOUT","text":"<p>Use <code>&gt;</code> to send standard output to a file instead of the terminal:</p> <pre><code>echo \"Hello, World!\" &gt; output.txt\n</code></pre> <p>This creates <code>output.txt</code> if it doesn't exist, or overwrites it if it does.</p> <p>Overwriting files with &gt;</p> <p>The <code>&gt;</code> operator silently destroys the previous contents of the target file. There is no confirmation prompt and no undo. If you meant to append, use <code>&gt;&gt;</code> instead. Enable <code>noclobber</code> (<code>set -o noclobber</code>) as a safety net against accidental overwrites.</p>"},{"location":"Linux%20Essentials/streams-and-redirection/#redirecting-stdin","title":"Redirecting STDIN","text":"<p>Use <code>&lt;</code> to feed a file into a command's standard input:</p> <pre><code>grep \"pattern\" &lt; input.txt\n</code></pre> <p>Many commands accept filenames as arguments (<code>grep \"pattern\" input.txt</code>), but STDIN redirection is useful when a command only reads from STDIN, or when you want to make the data flow explicit.</p>"},{"location":"Linux%20Essentials/streams-and-redirection/#redirecting-stderr","title":"Redirecting STDERR","text":"<p>Use <code>2&gt;</code> to redirect error output:</p> <pre><code>find / -name \"*.conf\" 2&gt; /dev/null\n</code></pre> <p>This runs <code>find</code> and discards all error messages (like \"Permission denied\") by sending them to <code>/dev/null</code>.</p>"},{"location":"Linux%20Essentials/streams-and-redirection/#appending-output","title":"Appending Output","text":"<p>Use <code>&gt;&gt;</code> to append instead of overwrite:</p> <pre><code>echo \"New entry\" &gt;&gt; log.txt\necho \"Another error\" 2&gt;&gt; errors.txt\n</code></pre> <p>Redirection Data Flow (requires JavaScript)</p>"},{"location":"Linux%20Essentials/streams-and-redirection/#redirecting-both-stdout-and-stderr","title":"Redirecting Both STDOUT and STDERR","text":"<p>There are several ways to combine or separate these streams.</p>"},{"location":"Linux%20Essentials/streams-and-redirection/#to-the-same-file","title":"To the Same File","text":"<pre><code># Method 1: redirect STDOUT to file, then STDERR to where STDOUT goes\ncommand &gt; output.txt 2&gt;&amp;1\n\n# Method 2: shorthand (bash)\ncommand &amp;&gt; output.txt\n\n# Method 3: appending both\ncommand &gt;&gt; output.txt 2&gt;&amp;1\ncommand &amp;&gt;&gt; output.txt\n</code></pre>"},{"location":"Linux%20Essentials/streams-and-redirection/#to-different-files","title":"To Different Files","text":"<pre><code>command &gt; stdout.txt 2&gt; stderr.txt\n</code></pre>"},{"location":"Linux%20Essentials/streams-and-redirection/#order-matters","title":"Order Matters","text":"<p>The order of redirections is processed left to right, and it matters:</p> <pre><code># WRONG: redirects STDERR to where STDOUT currently points (terminal),\n# then redirects STDOUT to file\ncommand 2&gt;&amp;1 &gt; file.txt    # STDERR still goes to terminal\n\n# RIGHT: redirects STDOUT to file, then STDERR to where STDOUT now points (file)\ncommand &gt; file.txt 2&gt;&amp;1    # both go to file\n</code></pre> <p>Think of <code>2&gt;&amp;1</code> as \"make fd 2 point to wherever fd 1 is pointing right now.\" If fd 1 hasn't been redirected yet, it still points to the terminal.</p> <p>Redirection order matters</p> <p><code>cmd &gt; file 2&gt;&amp;1</code> captures both streams. <code>cmd 2&gt;&amp;1 &gt; file</code> only captures stdout - stderr still goes to the terminal. Redirections are processed left to right, and <code>2&gt;&amp;1</code> copies fd 2 to wherever fd 1 is pointing at that moment, not where it will point later.</p> <p>What is the difference between cmd &gt; file 2&gt;&amp;1 and cmd 2&gt;&amp;1 &gt; file? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/streams-and-redirection/#redirecting-to-devnull","title":"Redirecting to /dev/null","text":"<p><code>/dev/null</code> is a special file that discards everything written to it. It's the system's trash can.</p> <pre><code># Discard all output\ncommand &gt; /dev/null 2&gt;&amp;1\n\n# Discard only errors\ncommand 2&gt; /dev/null\n\n# Discard only normal output, see only errors\ncommand &gt; /dev/null\n</code></pre> <p>/dev/null reads as empty</p> <p>Reading from <code>/dev/null</code> returns EOF immediately. This makes it useful as empty input too: <code>command &lt; /dev/null</code> ensures a command gets no stdin, preventing it from blocking while waiting for input.</p> <p>A common pattern for checking if a command succeeds without seeing its output:</p> <pre><code>if grep -q \"pattern\" file.txt 2&gt;/dev/null; then\n    echo \"Found it\"\nfi\n</code></pre> <p>Command Builder (requires JavaScript)</p>"},{"location":"Linux%20Essentials/streams-and-redirection/#the-noclobber-option","title":"The noclobber Option","text":"<p>If you're worried about accidentally overwriting files with <code>&gt;</code>, you can enable <code>noclobber</code>:</p> <pre><code>set -o noclobber\necho \"data\" &gt; existing_file.txt   # bash: existing_file.txt: cannot overwrite existing file\n</code></pre> <p>noclobber as a safety net</p> <p>Enable <code>noclobber</code> in your <code>.bashrc</code> (<code>set -o noclobber</code>) to prevent <code>&gt;</code> from overwriting existing files. When you actually need to overwrite, use <code>&gt;|</code> to bypass the protection. This one setting prevents the most common data-loss mistake on the command line.</p> <p>To force overwrite even with <code>noclobber</code> set, use <code>&gt;|</code>:</p> <pre><code>echo \"data\" &gt;| existing_file.txt   # works even with noclobber\n</code></pre> <p><code>noclobber</code> is worth enabling in multi-user environments where multiple people share a server, or in scripts that write to shared log paths. It's also a good safety net during long terminal sessions where you might accidentally redirect to a file you've been building up - one stray <code>&gt;</code> instead of <code>&gt;&gt;</code> and hours of collected output is gone. Some sysadmins enable it in their <code>.bashrc</code> by default and use <code>&gt;|</code> for the rare cases they actually want to overwrite.</p> <p>Disable <code>noclobber</code> with:</p> <pre><code>set +o noclobber\n</code></pre> <p>Redirect Both Streams to a File (requires JavaScript)</p>"},{"location":"Linux%20Essentials/streams-and-redirection/#here-documents","title":"Here Documents","text":"<p>A here document feeds a block of text into a command's STDIN. The text continues until the shell sees the delimiter on a line by itself.</p> <pre><code>cat &lt;&lt; EOF\nHello, $USER.\nToday is $(date).\nEOF\n</code></pre> <p>Variables and command substitutions are expanded inside here documents. To prevent expansion, quote the delimiter:</p> <pre><code>cat &lt;&lt; 'EOF'\nThis is literal: $USER $(date)\nNo expansion happens here.\nEOF\n</code></pre> <p>Here document delimiter must start at column 1</p> <p>The closing delimiter (e.g., <code>EOF</code>) must appear on a line by itself with no leading whitespace. If you indent it to match your code, the shell won't recognize the end of the here document. Use <code>&lt;&lt;-</code> with tabs (not spaces) if you need indented here documents.</p>"},{"location":"Linux%20Essentials/streams-and-redirection/#indented-here-documents","title":"Indented Here Documents","text":"<p>Use <code>&lt;&lt;-</code> to strip leading tabs (not spaces) from the content. This is useful for keeping here documents indented inside functions or loops:</p> <pre><code>if true; then\n    cat &lt;&lt;-EOF\n    This text can be indented with tabs.\n    The tabs are stripped from the output.\n    EOF\nfi\n</code></pre>"},{"location":"Linux%20Essentials/streams-and-redirection/#here-strings","title":"Here Strings","text":"<p>A here string (<code>&lt;&lt;&lt;</code>) passes a single string as STDIN:</p> <pre><code>grep \"pattern\" &lt;&lt;&lt; \"search this string for a pattern\"\n\n# Useful with variables\ndata=\"one two three\"\nread -r first rest &lt;&lt;&lt; \"$data\"\necho \"$first\"    # one\necho \"$rest\"     # two three\n</code></pre> <p>What is the key difference between a here document (&lt; (requires JavaScript)"},{"location":"Linux%20Essentials/streams-and-redirection/#file-descriptor-manipulation","title":"File Descriptor Manipulation","text":"<p>Beyond the three standard streams, you can open and manage additional file descriptors using <code>exec</code>.</p>"},{"location":"Linux%20Essentials/streams-and-redirection/#opening-file-descriptors","title":"Opening File Descriptors","text":"<pre><code># Open fd 3 for writing to a file\nexec 3&gt; output.txt\necho \"Written to fd 3\" &gt;&amp;3\necho \"More data\" &gt;&amp;3\nexec 3&gt;&amp;-    # Close fd 3\n\n# Open fd 4 for reading from a file\nexec 4&lt; input.txt\nread -r line &lt;&amp;4\necho \"Read: $line\"\nexec 4&lt;&amp;-    # Close fd 4\n\n# Open fd 5 for both reading and writing\nexec 5&lt;&gt; file.txt\n</code></pre>"},{"location":"Linux%20Essentials/streams-and-redirection/#practical-example-logging","title":"Practical Example: Logging","text":"<pre><code># Open a log file on fd 3\nexec 3&gt;&gt; /var/log/myscript.log\n\nlog() {\n    echo \"$(date '+%Y-%m-%d %H:%M:%S') $*\" &gt;&amp;3\n}\n\nlog \"Script started\"\n# ... do work ...\nlog \"Script finished\"\n\nexec 3&gt;&amp;-    # Close the log file\n</code></pre>"},{"location":"Linux%20Essentials/streams-and-redirection/#swapping-stdout-and-stderr","title":"Swapping STDOUT and STDERR","text":"<pre><code># Swap so STDOUT goes to stderr and STDERR goes to stdout\ncommand 3&gt;&amp;1 1&gt;&amp;2 2&gt;&amp;3 3&gt;&amp;-\n</code></pre> <p>This uses fd 3 as a temporary holding place, similar to swapping two variables with a temp variable.</p> <p>The exec File Descriptor Logging Pattern (requires JavaScript)</p>"},{"location":"Linux%20Essentials/streams-and-redirection/#pipelines","title":"Pipelines","text":"<p>A pipeline connects the STDOUT of one command to the STDIN of the next using the <code>|</code> operator:</p> <pre><code>ls -la | grep \"\\.txt\" | sort -k5 -n\n</code></pre> <p>Each command in a pipeline runs in its own subshell, and they all run concurrently. Data flows between them through kernel buffers.</p>"},{"location":"Linux%20Essentials/streams-and-redirection/#pipeline-exit-status","title":"Pipeline Exit Status","text":"<p>By default, the exit status of a pipeline is the exit status of the last command:</p> <pre><code>false | true\necho $?    # 0 (true succeeded, even though false failed)\n</code></pre> <p>Use <code>set -o pipefail</code> to make the pipeline return the exit status of the last command that failed:</p> <pre><code>set -o pipefail\nfalse | true\necho $?    # 1 (false failed)\n</code></pre> <p>The <code>PIPESTATUS</code> array (bash-specific) captures the exit status of every command in the last pipeline:</p> <pre><code>cat /nonexistent | grep \"pattern\" | wc -l\necho \"${PIPESTATUS[@]}\"    # 1 1 0\necho \"${PIPESTATUS[0]}\"    # 1 (cat failed)\necho \"${PIPESTATUS[2]}\"    # 0 (wc succeeded)\n</code></pre>"},{"location":"Linux%20Essentials/streams-and-redirection/#piping-stderr","title":"Piping STDERR","text":"<p>By default, only STDOUT is piped. To include STDERR in the pipeline:</p> <pre><code># Method 1: redirect STDERR to STDOUT first\ncommand 2&gt;&amp;1 | grep \"error\"\n\n# Method 2: bash shorthand\ncommand |&amp; grep \"error\"\n</code></pre> <p>This is useful when a program writes important data to STDOUT but buries diagnostics in STDERR. For example, filtering compiler warnings without losing the build output:</p> <pre><code># Count errors from a build, ignoring normal output\nmake 2&gt;&amp;1 &gt;/dev/null | grep -c 'error:'\n\n# Separate error output from data in a script\n./process_data.sh 2&gt; &gt;(grep -v 'warning' &gt;&gt; errors.log) &gt; results.txt\n</code></pre> <p>The <code>|&amp;</code> shorthand is convenient for quick debugging, but in scripts, the explicit <code>2&gt;&amp;1 |</code> form is clearer about what's happening.</p>"},{"location":"Linux%20Essentials/streams-and-redirection/#tee","title":"tee","text":"<p>The <code>tee</code> command reads from STDIN and writes to both STDOUT and one or more files simultaneously. It splits the stream.</p> <pre><code># Write to file and still see output on terminal\ncommand | tee output.txt\n\n# Append instead of overwrite\ncommand | tee -a output.txt\n\n# Write to multiple files\ncommand | tee file1.txt file2.txt\n\n# Use in a pipeline\nls -la | tee listing.txt | grep \"\\.txt\"\n</code></pre> <p>Use tee /dev/stderr for pipeline debugging</p> <p>Insert <code>tee /dev/stderr</code> between pipeline stages to see intermediate data on your terminal without disrupting the pipeline's data flow. Since stderr bypasses the pipe, the debug output appears on screen while stdout continues to the next command.</p> <p>A common use is capturing intermediate output in a pipeline for debugging:</p> <pre><code>cat data.csv | tee /dev/stderr | sort -t, -k2 | tee /dev/stderr | uniq -c\n</code></pre> <p>This prints the data at each pipeline stage to STDERR (your terminal) while the actual data flows through the pipeline.</p> <p>What does the tee command do? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/streams-and-redirection/#named-pipes-fifos","title":"Named Pipes (FIFOs)","text":"<p>A named pipe is a special file that acts as a pipeline between processes. Unlike anonymous pipes (<code>|</code>), named pipes have a name in the filesystem and can be used by unrelated processes. The data itself passes through kernel buffers, not through disk.</p> <p>Create one with <code>mkfifo</code>:</p> <pre><code>mkfifo /tmp/mypipe\n</code></pre> <p>In one terminal, write to the pipe:</p> <pre><code>echo \"Hello from process A\" &gt; /tmp/mypipe\n</code></pre> <p>In another terminal, read from the pipe:</p> <pre><code>cat /tmp/mypipe    # Hello from process A\n</code></pre> <p>The writer blocks until a reader opens the pipe, and vice versa. This makes named pipes useful for inter-process communication.</p> <p>Named pipes block until both ends connect</p> <p>A process writing to a named pipe will hang indefinitely until another process opens it for reading, and vice versa. If you forget to open the other end, your terminal appears frozen with no error message. Always test named pipes with two terminals, and remember to <code>rm</code> the FIFO when you're done.</p> <p>Clean up when you're done:</p> <pre><code>rm /tmp/mypipe\n</code></pre>"},{"location":"Linux%20Essentials/streams-and-redirection/#subshells","title":"Subshells","text":"<p>A subshell is a child copy of the current shell. It inherits the parent's environment but runs in a separate process. Changes made in a subshell don't affect the parent.</p>"},{"location":"Linux%20Essentials/streams-and-redirection/#explicit-subshells","title":"Explicit Subshells","text":"<p>Parentheses create a subshell:</p> <pre><code>(cd /tmp &amp;&amp; ls)\npwd    # still in original directory\n</code></pre> <pre><code>x=10\n(x=20; echo \"inside: $x\")    # inside: 20\necho \"outside: $x\"           # outside: 10\n</code></pre>"},{"location":"Linux%20Essentials/streams-and-redirection/#where-subshells-appear","title":"Where Subshells Appear","text":"<p>Subshells are created in several situations:</p> <ul> <li>Parentheses: <code>(commands)</code></li> <li>Pipelines: each command in <code>cmd1 | cmd2 | cmd3</code> runs in its own subshell</li> <li>Command substitution: <code>$(command)</code></li> <li>Background processes: <code>command &amp;</code></li> <li>Process substitution: <code>&lt;(command)</code> and <code>&gt;(command)</code></li> </ul> <p>Pipeline subshell gotcha</p> <p>Each command in a pipeline runs in its own subshell. Variables set inside a <code>while read</code> loop on the right side of a pipe are lost when the subshell exits. This is the most common cause of \"my variable is still empty after the loop\" bugs.</p> <p>The pipeline subshell issue is a common gotcha:</p> <pre><code>count=0\necho -e \"one\\ntwo\\nthree\" | while read -r line; do\n    (( count++ ))\ndone\necho $count    # 0! The while loop ran in a subshell\n</code></pre> <p>To avoid this in bash 4.2+, use <code>lastpipe</code>. Note that <code>lastpipe</code> only takes effect when job control is off, so it works in scripts but not in an interactive shell (unless you also run <code>set +m</code>):</p> <pre><code>#!/bin/bash\nshopt -s lastpipe\ncount=0\necho -e \"one\\ntwo\\nthree\" | while read -r line; do\n    (( count++ ))\ndone\necho $count    # 3\n</code></pre> <p>Use process substitution to avoid subshell variable loss</p> <p>Rewrite <code>cmd | while read line</code> as <code>while read line; done &lt; &lt;(cmd)</code>. Process substitution feeds the command's output through a file descriptor, so the <code>while</code> loop runs in the current shell and variable changes persist.</p> <p>Or use process substitution instead (works everywhere, including interactive shells):</p> <pre><code>count=0\nwhile read -r line; do\n    (( count++ ))\ndone &lt; &lt;(echo -e \"one\\ntwo\\nthree\")\necho $count    # 3\n</code></pre>"},{"location":"Linux%20Essentials/streams-and-redirection/#process-substitution","title":"Process Substitution","text":"<p>Process substitution lets you use the output of a command where a filename is expected. It creates a temporary file descriptor that acts like a file.</p>"},{"location":"Linux%20Essentials/streams-and-redirection/#output-process-substitution","title":"Output Process Substitution","text":"<p><code>&lt;(command)</code> provides command output as a readable file:</p> <pre><code>diff &lt;(ls dir1) &lt;(ls dir2)\n</code></pre> <p>This compares the file listings of two directories without creating temporary files. <code>diff</code> thinks it's reading two files, but they're actually the output of two <code>ls</code> commands.</p> <pre><code># Compare sorted output of two commands\ndiff &lt;(sort file1.txt) &lt;(sort file2.txt)\n\n# Feed command output to a program that only accepts files\npaste &lt;(cut -f1 data.tsv) &lt;(cut -f3 data.tsv)\n</code></pre>"},{"location":"Linux%20Essentials/streams-and-redirection/#input-process-substitution","title":"Input Process Substitution","text":"<p><code>&gt;(command)</code> provides a writable file descriptor:</p> <pre><code># Write to two files simultaneously with different processing\ntee &gt;(gzip &gt; compressed.gz) &gt;(wc -l &gt; linecount.txt) &gt; /dev/null &lt; input.txt\n</code></pre>"},{"location":"Linux%20Essentials/streams-and-redirection/#whats-actually-happening","title":"What's Actually Happening","text":"<p>Process substitution creates a path like <code>/dev/fd/63</code> that the command can open and read from (or write to). You can see this:</p> <pre><code>echo &lt;(true)    # /dev/fd/63 (or similar)\n</code></pre> <p>This is why it works with commands that expect filenames but not with commands that expect STDIN.</p> <p>Process Substitution in Action (requires JavaScript)</p> <p>Compare Command Outputs with Process Substitution (requires JavaScript)</p>"},{"location":"Linux%20Essentials/streams-and-redirection/#further-reading","title":"Further Reading","text":"<ul> <li>Bash Reference Manual - Redirections - official documentation on redirection operators and file descriptors</li> <li>GNU Coreutils - tee - tee invocation and options</li> <li>POSIX Shell Command Language - portable redirection and pipeline semantics</li> </ul> <p>Previous: Shell Basics | Next: Text Processing | Back to Index</p>"},{"location":"Linux%20Essentials/system-information/","title":"System Information","text":"<p>These tools help you understand what's running on a system - hardware details, resource usage, and what processes are doing. Essential for troubleshooting and capacity planning.</p>"},{"location":"Linux%20Essentials/system-information/#uname-kernel-and-os-info","title":"uname - Kernel and OS Info","text":"<p><code>uname</code> prints system information about the kernel and OS.</p> <pre><code>uname              # kernel name (e.g., Linux)\nuname -r           # kernel release (e.g., 5.15.0-91-generic)\nuname -m           # machine architecture (e.g., x86_64, aarch64)\nuname -n           # hostname\nuname -a           # all of the above combined\n</code></pre> <p>For distribution-specific info:</p> <pre><code>cat /etc/os-release       # distro name, version, ID\nlsb_release -a            # if lsb_release is installed\n</code></pre>"},{"location":"Linux%20Essentials/system-information/#uptime-and-load-averages","title":"uptime and Load Averages","text":"<p><code>uptime</code> shows how long the system has been running and current load averages.</p> <pre><code>uptime\n# 14:32:07 up 45 days, 3:12, 2 users, load average: 0.52, 0.78, 0.91\n</code></pre> <p>The three load average numbers represent the average number of processes waiting to run over the last 1, 5, and 15 minutes.</p> <p>How to interpret them: - On a single-CPU system, a load of 1.0 means the CPU is fully utilized. Above 1.0, processes are waiting. - On a 4-core system, a load of 4.0 means full utilization. Above 4.0, processes are queuing.</p> <p>Divide load average by nproc to assess CPU pressure</p> <p>A load average of 8.0 on a 2-core machine means severe overload, but on a 16-core machine it means the system is barely working. Always divide by the number of CPU cores (<code>nproc</code>) to get a meaningful ratio. Consistently above 1.0 per core indicates the system needs attention.</p> <p>General rule: divide the load by the number of CPU cores. If the result is consistently above 1.0, the system is overloaded.</p> <pre><code># Check number of cores\nnproc\n</code></pre> <p>A rising 1-minute average with stable 15-minute average indicates a recent spike. A high 15-minute average means sustained load.</p> <p>A server with 4 CPU cores shows a load average of 4.00. What does this mean? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/system-information/#free-memory-usage","title":"free - Memory Usage","text":"<p><code>free</code> shows RAM and swap usage.</p> <pre><code>free -h            # human-readable\nfree -m            # megabytes\n</code></pre> <p>Example output:</p> <pre><code>              total        used        free      shared  buff/cache   available\nMem:          15Gi        6.2Gi       1.8Gi       312Mi        7.5Gi        8.7Gi\nSwap:          4Gi        0.1Gi       3.9Gi\n</code></pre> <p>Key columns: - total - physical RAM installed - used - memory actively used by processes - free - completely unused memory - buff/cache - memory used for filesystem buffers and page cache - available - memory that can be used for new processes (free + reclaimable cache)</p> <p>Low 'free' memory is normal - check 'available' instead</p> <p>Linux intentionally uses free RAM for disk caching, so the <code>free</code> column in <code>free -h</code> is often near zero on a healthy system. The <code>available</code> column is what matters - it shows how much memory can actually be used by new applications, including reclaimable cache. Only worry if <code>available</code> is low.</p> <p>The available column is what matters. Linux aggressively uses free memory for caching disk data. This cached memory is immediately available when a process needs it. A system with low \"free\" but high \"available\" is healthy.</p> <p>If swap is heavily used, the system is running low on RAM and performance will suffer.</p> <p>In the output of the free command, what does the 'available' column represent? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/system-information/#lscpu-cpu-information","title":"lscpu - CPU Information","text":"<p><code>lscpu</code> displays detailed CPU architecture information.</p> <pre><code>lscpu\n</code></pre> <p>Key fields: - Architecture - x86_64, aarch64, etc. - CPU(s) - total logical CPUs (cores x threads) - Core(s) per socket - physical cores per CPU - Socket(s) - number of physical CPUs - Model name - CPU model - CPU MHz - current clock speed</p> <p>Sockets x Cores x Threads = Total logical CPUs</p> <p>The formula <code>Sockets x Cores per socket x Threads per core</code> gives the total logical CPU count shown by <code>nproc</code>. A 2-socket server with 8 cores per socket and 2 threads per core has 32 logical CPUs. This number is what you compare load averages against.</p> <p>The formula for total logical CPUs is: Sockets x Cores per socket x Threads per core. For example, a machine with 2 sockets, 8 cores per socket, and 2 threads per core has 2 x 8 x 2 = 32 logical CPUs. The \"threads per core\" value is usually 1 (no hyperthreading) or 2 (hyperthreading/SMT enabled). Hyperthreading lets each physical core present itself as two logical CPUs by sharing execution resources. It helps with workloads that have a mix of CPU-bound and I/O-waiting threads, but doesn't double performance - expect 15-30% improvement at best for most workloads.</p> <pre><code># Quick core count\nnproc                      # number of processing units\nnproc --all                # all installed (may differ if some are offline)\n</code></pre>"},{"location":"Linux%20Essentials/system-information/#lsof-open-files-and-connections","title":"lsof - Open Files and Connections","text":"<p><code>lsof</code> (list open files) shows which files, sockets, and pipes are in use by which processes. In Unix, everything is a file - including network connections.</p> <pre><code>lsof                             # all open files (very long output)\nlsof -u ryan                     # files opened by a user\nlsof -p 12345                    # files opened by a specific PID\nlsof /var/log/syslog             # processes using a specific file\nlsof +D /var/log                 # processes using files in a directory\n</code></pre> <p>Network connections:</p> <pre><code>lsof -i                          # all network connections\nlsof -i :80                      # processes using port 80\nlsof -i TCP                      # TCP connections only\nlsof -i TCP:443                  # TCP connections on port 443\nlsof -i @192.168.1.100           # connections to a specific host\n</code></pre> <p>Finding deleted files still held open:</p> <pre><code>lsof +L1                         # files with zero link count (deleted but open)\n</code></pre> <p>lsof +L1 finds deleted files still consuming disk space</p> <p>When <code>df</code> shows a full disk but <code>du</code> can't account for all the space, deleted files still held open by running processes are often the cause. Run <code>lsof +L1</code> to find them. The space is only freed when the process closes the file or is restarted.</p> <p>This is useful when <code>df</code> shows a disk is full but <code>du</code> can't account for all the space.</p>"},{"location":"Linux%20Essentials/system-information/#vmstat-system-performance-snapshot","title":"vmstat - System Performance Snapshot","text":"<p><code>vmstat</code> reports virtual memory, CPU, and I/O statistics.</p> <p>vmstat's first line is an average since boot - ignore it</p> <p>The first line of <code>vmstat</code> output shows averages since the system booted, not current values. Always use <code>vmstat N</code> (e.g., <code>vmstat 1 5</code>) to get real-time samples, and read from the second line onward for meaningful data.</p> <pre><code>vmstat                   # single snapshot\nvmstat 5                 # update every 5 seconds\nvmstat 5 10              # update every 5 seconds, 10 times\nvmstat -S M              # show memory in megabytes\n</code></pre> <p>Example output:</p> <pre><code>procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n 1  0  10240 183264  45872 768432    0    0     5    12  125  230  8  2 89  1  0\n</code></pre> <p>Key columns: - r - processes waiting to run (high = CPU bottleneck) - b - processes in uninterruptible sleep (high = I/O bottleneck) - si/so - swap in/out (should be near zero) - bi/bo - blocks read/written to disk - us - user CPU time - sy - system CPU time - id - idle CPU time - wa - I/O wait time (high = disk bottleneck)</p> <p>What values indicate problems:</p> <ul> <li>wa &gt; 10% - the CPU is spending significant time waiting for disk I/O. Investigate with <code>iostat -x</code> to find which disk is the bottleneck.</li> <li>r consistently &gt; number of CPUs - more processes want to run than you have CPUs. The system needs more CPU power or the workload needs optimization.</li> <li>si/so &gt; 0 - the system is actively swapping memory to/from disk. Even small amounts of swap activity cause noticeable slowdowns because disk is orders of magnitude slower than RAM. If you see sustained swapping, the system needs more RAM or a process is using too much.</li> <li>b &gt; 0 for extended periods - processes are blocked on I/O. Combined with high wa%, this points to a storage bottleneck.</li> </ul> <p>In vmstat output, what do the wa (I/O wait) and id (idle) columns under CPU represent? (requires JavaScript)</p> <p>Reading vmstat Output (requires JavaScript)</p> <p>Identify the Bottleneck from vmstat Output (requires JavaScript)</p>"},{"location":"Linux%20Essentials/system-information/#proc-and-sys","title":"/proc and /sys","text":"<p>The <code>/proc</code> filesystem is a virtual filesystem that exposes kernel and process information as files.</p> <p>/proc and /sys are virtual filesystems - they don't exist on disk. The kernel generates their contents on the fly when you read them. /proc exposes process information and kernel internals: every running process gets a directory at <code>/proc/&lt;PID&gt;/</code>, and files like <code>/proc/cpuinfo</code> and <code>/proc/meminfo</code> provide system-wide stats. /sys is organized around the kernel's internal object model - devices, drivers, buses, and kernel subsystems. The key difference: <code>/proc</code> is older and somewhat disorganized (it mixes process info with hardware info), while <code>/sys</code> follows a clean hierarchy. New kernel features expose their interfaces through <code>/sys</code>.</p> <pre><code>cat /proc/cpuinfo          # CPU details\ncat /proc/meminfo          # detailed memory info\ncat /proc/version          # kernel version\ncat /proc/uptime           # uptime in seconds\ncat /proc/loadavg          # load averages\ncat /proc/mounts           # mounted filesystems\n</code></pre> <p>Process-specific info lives in <code>/proc/&lt;PID&gt;/</code>:</p> <pre><code>cat /proc/1234/cmdline     # command that started the process\ncat /proc/1234/status      # process status (memory, state, etc.)\nls -l /proc/1234/fd        # see what files a process has open\ncat /proc/1234/environ     # environment variables (null-separated)\n</code></pre> <p>The <code>/sys</code> filesystem exposes kernel objects - devices, drivers, and configuration:</p> <pre><code>cat /sys/class/net/eth0/speed          # network interface speed\ncat /sys/block/sda/size                # disk size in sectors\ncat /sys/class/thermal/thermal_zone0/temp    # CPU temperature\n</code></pre> <p>Both <code>/proc</code> and <code>/sys</code> are virtual - they don't take up disk space. They're generated on the fly by the kernel.</p>"},{"location":"Linux%20Essentials/system-information/#dmesg-kernel-messages","title":"dmesg - Kernel Messages","text":"<p><code>dmesg</code> displays kernel ring buffer messages - hardware detection, driver loading, errors, and warnings.</p> <p>dmesg -T timestamps may not be accurate across suspend/resume</p> <p>The <code>-T</code> flag converts kernel timestamps to wall-clock time using the system's boot time as a reference. If the system has been suspended and resumed, these timestamps drift because the kernel timer doesn't advance during suspend. For accurate timestamps on systems that sleep, cross-reference with <code>journalctl</code>.</p> <pre><code>dmesg                      # all kernel messages\ndmesg -T                   # human-readable timestamps\ndmesg -l err,warn          # only errors and warnings\ndmesg | tail -20           # recent messages\ndmesg -w                   # follow new messages in real time\n</code></pre> <p>Useful for: - Diagnosing hardware issues (disk errors, USB detection) - Checking boot messages - Investigating OOM (out of memory) kills - Spotting driver errors</p> <pre><code># Check for disk errors\ndmesg -T | grep -i \"error\\|fail\\|i/o\"\n\n# See USB device detection\ndmesg -T | grep -i usb\n\n# Find OOM killer activity\ndmesg -T | grep -i \"oom\\|killed process\"\n</code></pre> <p>Reading dmesg output for real problems. Here's an example of spotting a disk error:</p> <pre><code>dmesg -T | grep -i 'error\\|fail\\|i/o'\n# [Mon Jan 15 14:23:01 2024] ata1.00: failed command: READ FPDMA QUEUED\n# [Mon Jan 15 14:23:01 2024] ata1.00: status: { DRDY ERR }\n# [Mon Jan 15 14:23:01 2024] ata1.00: error: { UNC }\n</code></pre> <p>The <code>UNC</code> (uncorrectable) error means the disk has a bad sector it can't recover from. Repeated disk errors like this mean the drive is failing and should be replaced. Another common scenario:</p> <pre><code>dmesg -T | grep -i 'oom\\|killed process'\n# [Mon Jan 15 15:45:22 2024] Out of memory: Killed process 3421 (java) total-vm:4096000kB\n</code></pre> <p>OOM killer events in dmesg indicate critical memory pressure</p> <p>If <code>dmesg</code> shows \"Out of memory: Killed process,\" the system exhausted both RAM and swap and the kernel's OOM killer chose a process to terminate. This is a last-resort measure that indicates the system needs more RAM, a larger swap file, or investigation into which process is consuming excessive memory.</p> <p>The OOM (Out of Memory) killer activates when the system runs out of RAM and swap. It picks the process using the most memory and kills it. If you see this, you either need more RAM, a swap file, or to investigate why that process consumed so much memory.</p>"},{"location":"Linux%20Essentials/system-information/#putting-it-together","title":"Putting It Together","text":"<p>When troubleshooting a slow or unresponsive system, check in this order:</p> <pre><code># 1. What's the load? Is the system overloaded?\nuptime\n\n# 2. Is it CPU, memory, or I/O?\nvmstat 1 5\n\n# 3. Memory pressure?\nfree -h\n\n# 4. What processes are consuming resources?\ntop    # or htop\n\n# 5. Disk I/O issues?\niostat -x 1 5    # if sysstat is installed\n\n# 6. Disk space?\ndf -h\n\n# 7. Any kernel errors?\ndmesg -T | tail -30\n\n# 8. What's a specific process doing?\nlsof -p &lt;PID&gt;\nstrace -p &lt;PID&gt;   # system calls (careful - high overhead)\n</code></pre> <p>System Troubleshooting Quick-Check Script (requires JavaScript)</p>"},{"location":"Linux%20Essentials/system-information/#further-reading","title":"Further Reading","text":"<ul> <li>procps-ng - source and docs for free, vmstat, ps, top, and related utilities</li> <li>util-linux - lscpu, dmesg, lsblk, and other system utilities</li> <li>strace - system call tracer for Linux</li> <li>lsof - list open files utility</li> <li>Linux Kernel /proc Documentation - documentation for /proc and /sys virtual filesystems</li> <li>Linux man-pages Project - comprehensive manual pages for system information commands</li> </ul> <p>Previous: Networking | Next: Archiving and Compression | Back to Index</p>"},{"location":"Linux%20Essentials/text-processing/","title":"Text Processing","text":"<p>Linux provides a rich set of tools for searching, transforming, and analyzing text. These commands are designed to work with streams and pipelines, making them composable building blocks for data processing.</p>"},{"location":"Linux%20Essentials/text-processing/#grep","title":"grep","text":"<p><code>grep</code> searches for lines matching a pattern. It's one of the most frequently used commands on any Linux system.</p>"},{"location":"Linux%20Essentials/text-processing/#basic-usage","title":"Basic Usage","text":"<pre><code>grep \"error\" /var/log/syslog           # lines containing \"error\"\ngrep \"error\" file1.txt file2.txt       # search multiple files\ngrep \"error\" *.log                     # search files matching a glob\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#common-options","title":"Common Options","text":"Option Effect <code>-i</code> Case-insensitive search <code>-v</code> Invert match (lines that don't contain the pattern) <code>-c</code> Print count of matching lines instead of the lines <code>-n</code> Show line numbers <code>-l</code> Print only filenames containing a match <code>-L</code> Print only filenames NOT containing a match <code>-o</code> Print only the matched portion of each line <code>-w</code> Match whole words only <code>-x</code> Match whole lines only <code>-r</code> Recursive search through directories <code>-F</code> Treat pattern as a fixed string (no regex) <code>-E</code> Use extended regular expressions <code>-P</code> Use Perl-compatible regular expressions <p>Use -F for literal string searches</p> <p>When searching for strings that contain regex metacharacters like <code>.</code>, <code>*</code>, <code>[</code>, or <code>?</code>, use <code>grep -F</code> to treat the pattern as a fixed string. This avoids unexpected matches and is also slightly faster since grep skips the regex engine entirely.</p> <p>The most used flags in practice: <code>-r</code> to search an entire project tree, <code>-l</code> when you just need to know which files contain something (not the matching lines themselves), <code>-o</code> to extract just the matched text (useful for pulling values out of structured data), <code>-c</code> to count matches per file without seeing the actual lines, <code>-F</code> when your search string contains regex metacharacters like <code>.</code> or <code>*</code> and you want to search for them literally, and <code>-P</code> when you need advanced features like lookahead or non-greedy matching that basic and extended regex can't do.</p>"},{"location":"Linux%20Essentials/text-processing/#context-lines","title":"Context Lines","text":"<p>Show lines surrounding each match:</p> <pre><code>grep -A 3 \"error\" log.txt     # 3 lines After each match\ngrep -B 2 \"error\" log.txt     # 2 lines Before each match\ngrep -C 2 \"error\" log.txt     # 2 lines of Context (before and after)\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#recursive-search","title":"Recursive Search","text":"<pre><code>grep -r \"TODO\" src/                    # search all files under src/\ngrep -rn \"TODO\" src/                   # same, with line numbers\ngrep -r --include=\"*.py\" \"import\" .    # only search .py files\ngrep -r --exclude-dir=\".git\" \"pattern\" .  # skip .git directory\n</code></pre> <p>grep -r --include is more efficient than find | grep</p> <p>Use <code>grep -r --include='*.py' pattern .</code> instead of <code>find . -name '*.py' | xargs grep pattern</code>. The <code>--include</code> flag lets grep handle file filtering internally, avoiding the overhead of spawning separate processes. Add <code>--exclude-dir=.git</code> to skip version control directories.</p>"},{"location":"Linux%20Essentials/text-processing/#practical-examples","title":"Practical Examples","text":"<pre><code># Count occurrences in each file\ngrep -c \"function\" *.js\n\n# Find files that contain a pattern\ngrep -rl \"deprecated\" src/\n\n# Extract email addresses\ngrep -oE '[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}' contacts.txt\n\n# Show processes matching a name (excluding the grep itself)\nps aux | grep \"[n]ginx\"\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#regular-expressions","title":"Regular Expressions","text":"<p>Regular expressions (regex) describe search patterns. grep supports three flavors.</p>"},{"location":"Linux%20Essentials/text-processing/#basic-regular-expressions-bre","title":"Basic Regular Expressions (BRE)","text":"<p>This is grep's default mode. Some metacharacters require backslash escaping.</p> Pattern Matches <code>.</code> Any single character <code>^</code> Start of line <code>$</code> End of line <code>*</code> Zero or more of the preceding element <code>\\+</code> One or more of the preceding element <code>\\?</code> Zero or one of the preceding element <code>\\{n\\}</code> Exactly n of the preceding element <code>\\{n,m\\}</code> Between n and m of the preceding element <code>[abc]</code> Any one character in the set <code>[^abc]</code> Any one character NOT in the set <code>[a-z]</code> Any character in the range <code>\\(group\\)</code> Capture group <code>\\1</code> Backreference to first group"},{"location":"Linux%20Essentials/text-processing/#extended-regular-expressions-ere","title":"Extended Regular Expressions (ERE)","text":"<p>Use <code>grep -E</code>. Metacharacters work without backslashes. (<code>egrep</code> is deprecated - use <code>grep -E</code> instead.)</p> Pattern Matches <code>+</code> One or more <code>?</code> Zero or one <code>{n,m}</code> Repetition <code>(group)</code> Capture group <code>\\|</code> Alternation (no backslash needed in ERE) <pre><code># BRE\ngrep 'error\\|warning' log.txt\n\n# ERE (cleaner)\ngrep -E 'error|warning' log.txt\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#posix-character-classes","title":"POSIX Character Classes","text":"<p>These work inside bracket expressions and are locale-aware:</p> Class Equivalent Matches <code>[:digit:]</code> <code>[0-9]</code> Digits <code>[:alpha:]</code> <code>[a-zA-Z]</code> Letters <code>[:alnum:]</code> <code>[a-zA-Z0-9]</code> Letters and digits <code>[:upper:]</code> <code>[A-Z]</code> Uppercase letters <code>[:lower:]</code> <code>[a-z]</code> Lowercase letters <code>[:space:]</code> <code>[ \\t\\n\\r\\f\\v]</code> Whitespace <code>[:blank:]</code> <code>[ \\t]</code> Space and tab <code>[:punct:]</code> Punctuation characters <p>Note the double brackets - the outer <code>[]</code> is the bracket expression, the inner <code>[:class:]</code> is the class name:</p> <pre><code>grep '[[:digit:]]\\{3\\}-[[:digit:]]\\{4\\}' phones.txt    # matches 555-1234\n</code></pre> <p>POSIX character classes exist because of locale awareness. The range <code>[a-z]</code> doesn't always mean what you'd expect - in some locales (like <code>en_US.UTF-8</code>), the sort order interleaves upper and lowercase, so <code>[a-z]</code> can match uppercase letters too. <code>[:lower:]</code> always means 'lowercase letters in the current locale,' regardless of sorting rules. If you're writing scripts that will run on systems with different locale settings, POSIX classes are the safe choice. For quick interactive use where you know your locale, <code>[a-z]</code> is fine.</p>"},{"location":"Linux%20Essentials/text-processing/#backreferences","title":"Backreferences","text":"<p>Capture a group and match it again later:</p> <pre><code># Find repeated words (GNU grep - \\b, \\w, and \\1 in ERE are GNU extensions)\ngrep -E '\\b(\\w+)\\s+\\1\\b' document.txt\n\n# Find lines where first and last word match (GNU grep - \\w is a GNU extension)\ngrep '^\\([[:alpha:]]\\+\\).*\\1$' file.txt\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#perl-compatible-regular-expressions","title":"Perl-Compatible Regular Expressions","text":"<p>Use <code>grep -P</code> for Perl-compatible regex (PCRE), which adds shorthand classes and advanced features not available in BRE or ERE:</p> <pre><code># Shorthand character classes\ngrep -P '\\d{3}-\\d{4}' phones.txt          # \\d = digit (like [0-9])\ngrep -P '\\bword\\b' file.txt               # \\b = word boundary\ngrep -P '\\s+' file.txt                    # \\s = whitespace\n\n# Lookahead and lookbehind\ngrep -P '(?&lt;=price: )\\d+' catalog.txt     # match digits preceded by \"price: \"\ngrep -P '\\d+(?= dollars)' catalog.txt     # match digits followed by \" dollars\"\n\n# Non-greedy matching\ngrep -oP '\".*?\"' data.json                # match shortest quoted strings\n</code></pre> <p>grep -P is not available everywhere</p> <p>The <code>-P</code> flag (Perl-compatible regex) is a GNU grep extension. macOS ships BSD grep, which lacks <code>-P</code> entirely. Install GNU grep via Homebrew (<code>brew install grep</code>, then use <code>ggrep -P</code>) or rewrite the pattern using <code>-E</code> (extended regex) when portability matters.</p> <p><code>-P</code> is a GNU grep extension and may not be available on all systems (notably macOS, where you can install GNU grep via Homebrew as <code>ggrep</code>).</p> <p>What is the difference between grep and grep -E (or egrep)? (requires JavaScript)</p> <p>Command Builder (requires JavaScript)</p>"},{"location":"Linux%20Essentials/text-processing/#sed","title":"sed","text":"<p><code>sed</code> (stream editor) transforms text line by line. It reads input, applies editing commands, and writes the result.</p>"},{"location":"Linux%20Essentials/text-processing/#substitute","title":"Substitute","text":"<p>The most common sed operation is substitution:</p> <pre><code>sed 's/old/new/' file.txt        # replace first occurrence on each line\nsed 's/old/new/g' file.txt       # replace all occurrences on each line\nsed 's/old/new/gI' file.txt      # case-insensitive, all occurrences (GNU sed only)\nsed 's/old/new/3' file.txt       # replace only the 3rd occurrence on each line\n</code></pre> <p>The delimiter doesn't have to be <code>/</code>. Use any character to avoid escaping:</p> <pre><code>sed 's|/usr/local|/opt|g' config.txt\nsed 's#http://#https://#g' urls.txt\n</code></pre> <p>sed Line-by-Line Processing (requires JavaScript)</p>"},{"location":"Linux%20Essentials/text-processing/#addresses","title":"Addresses","text":"<p>Sed commands can target specific lines:</p> <pre><code>sed '5s/old/new/' file.txt           # only line 5\nsed '1,10s/old/new/' file.txt        # lines 1 through 10\nsed '/pattern/s/old/new/' file.txt   # lines matching pattern\nsed '1,/END/s/old/new/' file.txt     # line 1 through first line matching END\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#delete","title":"Delete","text":"<pre><code>sed '5d' file.txt                # delete line 5\nsed '/^#/d' file.txt             # delete comment lines\nsed '/^$/d' file.txt             # delete empty lines\nsed '1,5d' file.txt              # delete lines 1-5\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#insert-and-append","title":"Insert and Append","text":"<pre><code>sed '3i\\New line before line 3' file.txt    # insert before line 3\nsed '3a\\New line after line 3' file.txt     # append after line 3\nsed '/pattern/a\\Added after matching line' file.txt\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#print","title":"Print","text":"<pre><code>sed -n '5p' file.txt             # print only line 5\nsed -n '10,20p' file.txt         # print lines 10-20\nsed -n '/start/,/end/p' file.txt # print between patterns (inclusive)\n</code></pre> <p>The <code>-n</code> flag suppresses default output, so only explicitly printed lines appear.</p> <p>Use sed -n with p for selective printing</p> <p>Combining <code>-n</code> (suppress default output) with the <code>p</code> command turns sed into a powerful line extractor. <code>sed -n '10,20p'</code> prints only lines 10-20, and <code>sed -n '/start/,/end/p'</code> prints blocks between patterns. Without <code>-n</code>, you'd see every line twice - once from default output and once from <code>p</code>.</p>"},{"location":"Linux%20Essentials/text-processing/#in-place-editing","title":"In-Place Editing","text":"<p>In-place sed without backup can destroy data</p> <p>Running <code>sed -i</code> without a backup suffix modifies the file directly with no undo. If your substitution has a bug, the original content is gone. Always use <code>sed -i.bak</code> to create a backup, or test with <code>sed 's/old/new/g' file.txt</code> (no <code>-i</code>) first to preview the output.</p> <p>macOS sed requires -i ''</p> <p>GNU sed and BSD sed (macOS) handle <code>-i</code> differently. GNU sed allows <code>sed -i 's/...'</code> with no argument, but macOS sed requires an explicit backup extension: <code>sed -i '' 's/...'</code> (empty string for no backup) or <code>sed -i .bak 's/...'</code>. Scripts that must run on both need conditional handling or should use GNU sed from Homebrew.</p> <pre><code>sed -i 's/old/new/g' file.txt           # edit file directly (GNU sed)\nsed -i '' 's/old/new/g' file.txt        # macOS sed (requires empty string for backup)\nsed -i.bak 's/old/new/g' file.txt       # create backup before editing\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#extended-regex","title":"Extended Regex","text":"<p>Use <code>-E</code> for extended regex (same as <code>grep -E</code>):</p> <pre><code>sed -E 's/[0-9]{3}-[0-9]{4}/XXX-XXXX/g' data.txt\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#multiple-commands","title":"Multiple Commands","text":"<pre><code>sed -e 's/foo/bar/g' -e 's/baz/qux/g' file.txt\n\n# Or use a semicolon\nsed 's/foo/bar/g; s/baz/qux/g' file.txt\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#practical-examples_1","title":"Practical Examples","text":"<pre><code># Remove trailing whitespace\nsed 's/[[:space:]]*$//' file.txt\n\n# Add a prefix to every line\nsed 's/^/PREFIX: /' file.txt\n\n# Extract text between markers\nsed -n '/BEGIN/,/END/p' file.txt\n\n# Replace the 2nd line\nsed '2c\\This replaces the second line' file.txt\n\n# Number all non-empty lines\nsed '/./=' file.txt | sed 'N; s/\\n/ /'\n</code></pre> <p>What does the g flag do at the end of a sed substitution (s/old/new/g)? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/text-processing/#awk","title":"awk","text":"Brian Kernighan at a Bell Labs tribute to Dennis Ritchie, 2012. The \"K\" in awk, Kernighan co-authored \"The C Programming Language\" and coined the name \"Unix.\" Photo: Ben Lowe, CC BY 2.0 <p><code>awk</code> is a pattern-scanning and text-processing language. It excels at working with structured, column-based data.</p>"},{"location":"Linux%20Essentials/text-processing/#basic-structure","title":"Basic Structure","text":"<p>An awk program is a series of pattern-action pairs:</p> <pre><code>awk 'pattern { action }' file\n</code></pre> <p>If no pattern is given, the action runs on every line. If no action is given, the matching line is printed.</p>"},{"location":"Linux%20Essentials/text-processing/#fields","title":"Fields","text":"<p>Awk automatically splits each line into fields. By default, the separator is whitespace.</p> Variable Meaning <code>$0</code> The entire line <code>$1</code>, <code>$2</code>, ... Individual fields <code>$NF</code> The last field <code>NF</code> Number of fields on the current line <code>NR</code> Current line number (across all files) <code>FNR</code> Line number in the current file <pre><code># Print the second column\nawk '{ print $2 }' data.txt\n\n# Print the last column\nawk '{ print $NF }' data.txt\n\n# Print line number and first field\nawk '{ print NR, $1 }' data.txt\n</code></pre> <p>awk Field Splitting (requires JavaScript)</p>"},{"location":"Linux%20Essentials/text-processing/#field-separator","title":"Field Separator","text":"<p>Use <code>-F</code> to set the field separator:</p> <pre><code>awk -F: '{ print $1, $7 }' /etc/passwd          # username and shell\nawk -F, '{ print $2 }' data.csv                  # second column of CSV\nawk -F'\\t' '{ print $3 }' data.tsv               # tab-separated\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#patterns","title":"Patterns","text":"<pre><code># Lines matching a regex\nawk '/error/' log.txt\n\n# Lines where a field matches\nawk '$3 &gt; 100' data.txt\n\n# Lines where a specific field matches a pattern\nawk '$1 ~ /^server/' config.txt\n\n# Lines where a field does NOT match\nawk '$1 !~ /^#/' config.txt\n\n# Range patterns\nawk '/START/,/END/' file.txt\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#begin-and-end","title":"BEGIN and END","text":"<p><code>BEGIN</code> runs before processing any input. <code>END</code> runs after all input is processed.</p> <pre><code>awk 'BEGIN { print \"Name\\tScore\" } { print $1, $2 } END { print \"---done---\" }' data.txt\n</code></pre> <p>awk BEGIN/END Block Pattern (requires JavaScript)</p>"},{"location":"Linux%20Essentials/text-processing/#built-in-functions-and-printf","title":"Built-in Functions and printf","text":"<pre><code># String length\nawk '{ print length($0) }' file.txt\n\n# Substring\nawk '{ print substr($1, 1, 3) }' file.txt\n\n# Formatted output\nawk '{ printf \"%-20s %10.2f\\n\", $1, $3 }' data.txt\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#variables-and-arithmetic","title":"Variables and Arithmetic","text":"<p>awk initializes unset variables to 0 or empty string</p> <p>Unlike most languages, awk doesn't require variable declarations or initialization. Unset numeric variables default to <code>0</code> and unset string variables default to <code>\"\"</code>. This is why <code>awk '{ sum += $3 }'</code> works without initializing <code>sum</code> - it starts as <code>0</code> automatically.</p> <pre><code># Sum a column\nawk '{ sum += $3 } END { print sum }' data.txt\n\n# Average\nawk '{ sum += $3; count++ } END { print sum/count }' data.txt\n\n# Track maximum\nawk '$3 &gt; max { max = $3; line = $0 } END { print line }' data.txt\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#practical-examples_2","title":"Practical Examples","text":"<pre><code># Print lines longer than 80 characters\nawk 'length &gt; 80' file.txt\n\n# Print unique values in column 1\nawk '!seen[$1]++' data.txt\n\n# Swap first two columns\nawk '{ temp = $1; $1 = $2; $2 = temp; print }' data.txt\n\n# Sum file sizes from ls -l\nls -l | awk 'NR &gt; 1 { sum += $5 } END { print sum }'\n\n# Process /etc/passwd: list users with bash shell\nawk -F: '$7 == \"/bin/bash\" { print $1 }' /etc/passwd\n\n# Print every other line\nawk 'NR % 2 == 1' file.txt\n</code></pre> <p>In awk, what does $NF refer to? (requires JavaScript)</p> <p>Command Builder (requires JavaScript)</p>"},{"location":"Linux%20Essentials/text-processing/#cut","title":"cut","text":"<p><code>cut</code> extracts specific columns or character positions from each line.</p>"},{"location":"Linux%20Essentials/text-processing/#by-field","title":"By Field","text":"<pre><code>cut -f1 data.tsv                # first field (tab-delimited by default)\ncut -f1,3 data.tsv              # first and third fields\ncut -f2- data.tsv               # second field through end\ncut -d',' -f1,3 data.csv        # comma-delimited, fields 1 and 3\ncut -d: -f1,7 /etc/passwd       # colon-delimited, fields 1 and 7\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#by-character-position","title":"By Character Position","text":"<pre><code>cut -c1-10 file.txt             # first 10 characters\ncut -c5- file.txt               # from character 5 to end\n</code></pre> <p>cut cannot handle quoted CSV fields</p> <p><code>cut</code> splits on every occurrence of the delimiter character, even inside quoted fields. A CSV line like <code>\"Smith, John\",25,Engineer</code> will be split at the comma inside the quotes. For quoted CSV data, use <code>awk</code> with <code>FPAT</code> (GNU awk) or a dedicated CSV parser like <code>csvtool</code> or Python's <code>csv</code> module.</p> <p><code>cut</code> is fast but limited. For complex field extraction, use <code>awk</code>.</p> <p>When to use <code>cut</code> vs <code>awk</code>: <code>cut</code> is simpler and faster for extracting fixed-position fields from cleanly delimited data. Use <code>awk</code> when you need conditional logic, multiple delimiters, or field reordering. One gotcha: <code>cut</code> defaults to tab as its delimiter, not spaces. If your data is space-separated, you need <code>-d' '</code>, but <code>cut</code> can't handle multiple consecutive spaces as a single delimiter the way <code>awk</code> does. Also, <code>cut</code> has no way to handle quoted CSV fields - a field containing a comma inside quotes will be split incorrectly. For anything beyond simple TSV or single-character-delimited data, use <code>awk</code>.</p>"},{"location":"Linux%20Essentials/text-processing/#sort","title":"sort","text":"<p><code>sort</code> orders lines of text.</p> <pre><code>sort file.txt                # alphabetical sort\nsort -n file.txt             # numeric sort\nsort -r file.txt             # reverse order\nsort -u file.txt             # remove duplicate lines\nsort -k2 file.txt            # sort by second field\nsort -k2,2n file.txt         # sort by second field, numerically\nsort -k3,3 -k1,1 file.txt   # sort by field 3, then field 1 as tiebreaker\nsort -t, -k2 data.csv        # comma-delimited, sort by field 2\nsort -s -k2 file.txt         # stable sort (preserves original order for equal elements)\nsort -h file.txt             # human-numeric sort (handles 1K, 2M, 3G)\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#practical-examples_3","title":"Practical Examples","text":"<pre><code># Sort du output by size\ndu -sh * | sort -h\n\n# Sort /etc/passwd by UID\nsort -t: -k3 -n /etc/passwd\n\n# Find the 10 largest files\ndu -a /var/log | sort -rn | head -10\n</code></pre> <p>The <code>-s</code> (stable sort) flag preserves the original order of lines that compare as equal. This matters when sorting by multiple keys in separate passes - without stable sort, the second sort might scramble the ordering you established in the first. For example, to sort a list of employees by department and then by name within each department, a stable sort on name followed by a stable sort on department gives you the right result.</p> <p>sort -h for human-readable sizes</p> <p>Use <code>sort -h</code> to correctly sort output from <code>du -h</code> or <code>ls -lh</code>. It understands unit suffixes like K, M, and G, so <code>2M</code> sorts above <code>100K</code>. Without <code>-h</code>, numeric sort sees only the leading digit and gets the order wrong.</p> <p>Human-numeric sort (<code>-h</code>) understands unit suffixes, which makes it essential for sorting <code>du</code> output:</p> <pre><code>du -sh /var/* | sort -h\n</code></pre> <p>Without <code>-h</code>, <code>sort -n</code> would rank '2M' below '100K' because it only looks at the leading digit. With <code>-h</code>, it correctly understands that 2M is larger than 100K.</p>"},{"location":"Linux%20Essentials/text-processing/#uniq","title":"uniq","text":"<p><code>uniq</code> filters adjacent duplicate lines. Input must be sorted first (or use <code>sort -u</code> instead).</p> <pre><code>sort file.txt | uniq           # remove duplicates\nsort file.txt | uniq -c        # count occurrences\nsort file.txt | uniq -d        # show only duplicated lines\nsort file.txt | uniq -u        # show only unique lines (no duplicates)\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#practical-examples_4","title":"Practical Examples","text":"<pre><code># Top 10 most common lines\nsort access.log | uniq -c | sort -rn | head -10\n\n# Count unique IP addresses\nawk '{ print $1 }' access.log | sort | uniq -c | sort -rn\n</code></pre> <p>uniq only removes adjacent duplicates</p> <p><code>uniq</code> compares each line to the one immediately before it. If identical lines are separated by other content, they won't be detected as duplicates. Always <code>sort</code> first, or use <code>sort -u</code> to deduplicate in one step. To deduplicate while preserving original order, use <code>awk '!seen[$0]++'</code>.</p> <p>Why must input be sorted? <code>uniq</code> only compares each line against the one directly before it. If duplicate lines appear in different parts of the file with other lines between them, <code>uniq</code> won't detect them. Sorting brings identical lines together so <code>uniq</code> can find them. If sorting would destroy meaningful ordering, use <code>sort -u</code> (which deduplicates as it sorts) or <code>awk '!seen[$0]++'</code> (which deduplicates while preserving original order).</p> <p>Why does uniq only remove consecutive duplicate lines? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/text-processing/#tr","title":"tr","text":"<p><code>tr</code> translates, squeezes, or deletes characters. It reads from STDIN only (no file arguments).</p>"},{"location":"Linux%20Essentials/text-processing/#translate","title":"Translate","text":"<pre><code>echo \"hello\" | tr 'a-z' 'A-Z'          # HELLO\necho \"hello\" | tr 'aeiou' '*'          # h*ll*\ncat file.txt | tr '\\t' ' '             # tabs to spaces\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#squeeze","title":"Squeeze","text":"<p>Replace repeated characters with a single instance:</p> <pre><code>echo \"too    many    spaces\" | tr -s ' '     # too many spaces\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#delete_1","title":"Delete","text":"<pre><code>echo \"Hello, World! 123\" | tr -d '[:digit:]'    # Hello, World!\necho \"Hello, World! 123\" | tr -d '[:punct:]'    # Hello World 123\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#character-classes","title":"Character Classes","text":"<pre><code>tr '[:lower:]' '[:upper:]' &lt; file.txt    # uppercase everything\ntr -d '[:space:]' &lt; file.txt             # remove all whitespace\ntr -dc '[:print:]' &lt; file.txt            # keep only printable characters\n</code></pre> <p>tr operates on characters, not strings</p> <p><code>tr 'hello' 'world'</code> does not replace the word \"hello\" with \"world.\" It maps individual characters: h to w, e to o, l to r, l to l, o to d. For string replacement, use <code>sed 's/hello/world/g'</code>.</p> <p><code>tr</code> works on a one-to-one character mapping model. Each character in SET1 maps to the character at the same position in SET2. If SET1 is longer than SET2, the last character of SET2 is repeated to fill the gap. That's why <code>tr 'aeiou' '*'</code> replaces all five vowels with <code>*</code> - the single <code>*</code> gets repeated for each remaining position.</p>"},{"location":"Linux%20Essentials/text-processing/#practical-examples_5","title":"Practical Examples","text":"<pre><code># Convert Windows line endings to Unix\ntr -d '\\r' &lt; windows.txt &gt; unix.txt\n\n# Generate a random password\ntr -dc 'A-Za-z0-9' &lt; /dev/urandom | head -c 32; echo\n\n# Remove all non-alphanumeric characters\ntr -dc '[:alnum:]\\n' &lt; file.txt\n</code></pre> <p>Which of these is a valid use of tr? (requires JavaScript)</p>"},{"location":"Linux%20Essentials/text-processing/#wc","title":"wc","text":"<p><code>wc</code> (word count) counts lines, words, and characters.</p> <pre><code>wc file.txt           # lines, words, characters (all three)\nwc -l file.txt        # line count only\nwc -w file.txt        # word count only\nwc -c file.txt        # byte count\nwc -m file.txt        # character count (handles multibyte)\n</code></pre> <pre><code># Count files in a directory\nls | wc -l\n\n# Count matching lines\ngrep -c \"error\" log.txt    # more efficient than grep | wc -l\n</code></pre> <p>wc -m vs wc -c for multibyte encodings</p> <p>Use <code>wc -c</code> for byte count (file size on disk) and <code>wc -m</code> for character count (human-readable characters). They differ for UTF-8 files: a single emoji or CJK character can be 3-4 bytes but counts as 1 character.</p> <p>The difference between <code>-c</code> (bytes) and <code>-m</code> (characters) matters with multibyte encodings like UTF-8. An ASCII file will show the same count for both, but a file containing non-ASCII characters (accented letters, emoji, CJK characters) will have more bytes than characters. Use <code>-c</code> when you care about file size on disk, and <code>-m</code> when you care about the number of human-readable characters.</p>"},{"location":"Linux%20Essentials/text-processing/#head-and-tail","title":"head and tail","text":""},{"location":"Linux%20Essentials/text-processing/#head","title":"head","text":"<p><code>head</code> prints the first N lines of a file (default 10):</p> <pre><code>head file.txt            # first 10 lines\nhead -n 5 file.txt       # first 5 lines\nhead -n -5 file.txt      # all lines EXCEPT the last 5 (GNU only)\nhead -c 100 file.txt     # first 100 bytes\n</code></pre> <p>head -n -N syntax is GNU-specific</p> <p>The negative form <code>head -n -5</code> (all lines except the last 5) is a GNU coreutils extension. BSD <code>head</code> (macOS) doesn't support it. For portable scripts, use <code>awk</code> or <code>sed</code> alternatives instead.</p>"},{"location":"Linux%20Essentials/text-processing/#tail","title":"tail","text":"<p><code>tail</code> prints the last N lines (default 10):</p> <pre><code>tail file.txt            # last 10 lines\ntail -n 5 file.txt       # last 5 lines\ntail -n +5 file.txt      # from line 5 to end\n</code></pre> <p>The <code>+</code> prefix reverses <code>tail</code>'s logic. Instead of 'show the last N lines,' <code>tail -n +N</code> means 'start from line N.' This is useful for skipping headers: <code>tail -n +2 data.csv</code> gives you everything except the first line.</p> <pre><code>tail -c 100 file.txt     # last 100 bytes\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#following-files","title":"Following Files","text":"<p><code>tail -f</code> watches a file for new content in real time:</p> <pre><code>tail -f /var/log/syslog                     # follow log updates\ntail -f /var/log/syslog | grep \"error\"      # follow with filtering\ntail -F /var/log/syslog                     # follow even if file is rotated\n</code></pre> <p><code>-F</code> is like <code>-f</code> but handles log rotation (file is deleted and recreated).</p> <p>tail -F survives log rotation</p> <p>Use <code>tail -F</code> (uppercase) instead of <code>tail -f</code> (lowercase) when following log files. Lowercase <code>-f</code> tracks the file descriptor, so when logrotate replaces the file, <code>-f</code> keeps reading the old (now renamed) file. Uppercase <code>-F</code> tracks the filename and automatically reopens it after rotation.</p>"},{"location":"Linux%20Essentials/text-processing/#tee","title":"tee","text":"<p><code>tee</code> is covered in Streams and Redirection. In brief, it reads from STDIN and writes to both STDOUT and one or more files, making it useful for saving intermediate pipeline results while still passing data to the next stage.</p>"},{"location":"Linux%20Essentials/text-processing/#combining-tools","title":"Combining Tools","text":"<p>The real power of text processing tools comes from combining them in pipelines.</p> <pre><code># Top 10 most common words in a file\ntr -s '[:space:]' '\\n' &lt; file.txt | tr '[:upper:]' '[:lower:]' | sort | uniq -c | sort -rn | head -10\n\n# Extract and count HTTP status codes from access logs\nawk '{ print $9 }' access.log | sort | uniq -c | sort -rn\n\n# Find all unique file extensions in a directory tree\nfind . -type f | sed 's/.*\\.//' | sort -u\n\n# Compare two sorted CSV columns\ndiff &lt;(cut -d, -f1 old.csv | sort) &lt;(cut -d, -f1 new.csv | sort)\n\n# Sum numbers from a file, one per line\npaste -sd+ numbers.txt | bc\n\n# Convert CSV to a formatted table\ncolumn -t -s, data.csv\n</code></pre>"},{"location":"Linux%20Essentials/text-processing/#breaking-down-pipelines","title":"Breaking Down Pipelines","text":"<p>Here's how to read these pipelines stage by stage.</p> <p>Top 10 most common words in a file:</p> <pre><code>tr -s '[:space:]' '\\n' &lt; file.txt | tr '[:upper:]' '[:lower:]' | sort | uniq -c | sort -rn | head -10\n</code></pre> <ol> <li><code>tr -s '[:space:]' '\\n'</code> - split text into one word per line by converting all whitespace (spaces, tabs, newlines) to newlines, squeezing consecutive whitespace into one</li> <li><code>tr '[:upper:]' '[:lower:]'</code> - lowercase everything so 'The' and 'the' count as the same word</li> <li><code>sort</code> - sort alphabetically so identical words are adjacent (required for <code>uniq</code>)</li> <li><code>uniq -c</code> - collapse adjacent identical lines and prefix each with its count</li> <li><code>sort -rn</code> - sort numerically in reverse so the highest counts come first</li> <li><code>head -10</code> - show only the first 10 results</li> </ol> <p>Extract and count HTTP status codes:</p> <pre><code>awk '{ print $9 }' access.log | sort | uniq -c | sort -rn\n</code></pre> <ol> <li><code>awk '{ print $9 }'</code> - pull the 9th whitespace-delimited field from each line (the HTTP status code in standard access log format)</li> <li><code>sort</code> - sort the status codes so identical ones are adjacent</li> <li><code>uniq -c</code> - count consecutive identical lines</li> <li><code>sort -rn</code> - show most frequent codes first</li> </ol> <p>Extract and Count HTTP Status Codes (requires JavaScript)</p> <p>Build a Word Frequency Pipeline (requires JavaScript)</p>"},{"location":"Linux%20Essentials/text-processing/#further-reading","title":"Further Reading","text":"<ul> <li>GNU Grep Manual - pattern matching with regular expressions</li> <li>GNU Sed Manual - stream editing reference</li> <li>GAWK Manual - the GNU awk programming language</li> <li>GNU Coreutils Manual - documentation for sort, cut, uniq, tr, wc, head, tail, and other core utilities</li> <li>Linux man-pages Project - comprehensive manual pages for Linux commands and system calls</li> <li>regular-expressions.info - regex tutorial and reference across flavors</li> </ul> <p>Previous: Streams and Redirection | Next: Finding Files | Back to Index</p>"},{"location":"assets/images/perl/context-flowchart-philosophy/","title":"Systematic Clarity","text":"<p>A design philosophy for technical diagrams that communicate through geometric precision and deliberate spatial hierarchy.</p>"},{"location":"assets/images/perl/context-flowchart-philosophy/#philosophy","title":"Philosophy","text":"<p>Information flows like water through a well-designed system - from a single origin point, branching into distinct channels, each arriving at its destination with purpose. The visual language borrows from circuit schematics and architectural blueprints: clean paths, precise nodes, and generous whitespace that lets each element breathe. Every connection is intentional. Every alignment is earned through meticulous calibration.</p> <p>Color operates as a functional signal, not decoration. A single accent hue - teal drawn from the surrounding environment - marks the critical decision points and category headings. The remaining palette lives in careful gradations of neutral tones: soft charcoal for primary text, muted gray for secondary information, and near-black for the canvas ground. This restraint creates a visual hierarchy that the eye navigates without instruction, the product of painstaking chromatic decisions refined until nothing competes.</p> <p>Typography serves two masters: the sans-serif voice of structure (headings, labels, categories) and the monospace voice of code (the actual Perl expressions that live inside this system). These two typefaces create a natural separation between \"what the diagram explains\" and \"what the programmer writes.\" Text is sparse - only what's essential to decode the system. Each letterform is placed with the precision of a master typesetter who understands that a single pixel of misalignment destroys the illusion of effortless order.</p> <p>The composition divides space into three equal columns beneath a central decision point, creating visual symmetry that mirrors the conceptual symmetry of Perl's three contexts. Rounded rectangles with subtle borders float on the dark ground, connected by thin paths that carry small inline labels. Result boxes sit below their parent categories with additional visual differentiation - slightly different background treatment, smaller type, concrete code examples. The entire piece should feel like a single organism: interconnected, balanced, and meticulously crafted by someone at the absolute top of their field who labored over every junction, every radius, every gram of visual weight until the diagram achieved the quiet authority of something that has always existed.</p>"}]}